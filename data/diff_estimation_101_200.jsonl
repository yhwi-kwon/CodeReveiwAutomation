{"patch": "@@ -269,7 +269,7 @@ func runExecutions(\n \t\tif nonce, ok = nonces[executor]; !ok {\n \t\t\tstate, err := accountutil.AccountState(sf, executor)\n \t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n+\t\t\t\treturn nil, err, nil\n \t\t\t}\n \t\t\tnonce = state.Nonce\n \t\t}", "y": 0, "oldf": "// Copyright (c) 2019 IoTeX Foundation\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage execution\n\nimport (\n\t\"bytes\"\n\t\"context\"\n\t\"encoding/hex\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/big\"\n\t\"os\"\n\t\"testing\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/require\"\n\t\"go.uber.org/zap\"\n\n\t\"github.com/iotexproject/go-pkgs/crypto\"\n\t\"github.com/iotexproject/go-pkgs/hash\"\n\t\"github.com/iotexproject/iotex-address/address\"\n\t\"github.com/iotexproject/iotex-proto/golang/iotextypes\"\n\n\t\"github.com/iotexproject/iotex-core/action\"\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/account\"\n\taccountutil \"github.com/iotexproject/iotex-core/action/protocol/account/util\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/execution/evm\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rewarding\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rolldpos\"\n\t\"github.com/iotexproject/iotex-core/actpool\"\n\t\"github.com/iotexproject/iotex-core/blockchain\"\n\t\"github.com/iotexproject/iotex-core/blockchain/block\"\n\t\"github.com/iotexproject/iotex-core/blockchain/blockdao\"\n\t\"github.com/iotexproject/iotex-core/blockindex\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/db\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/pkg/unit\"\n\t\"github.com/iotexproject/iotex-core/state/factory\"\n\t\"github.com/iotexproject/iotex-core/test/identityset\"\n\t\"github.com/iotexproject/iotex-core/testutil\"\n)\n\n// ExpectedBalance defines an account-balance pair\ntype ExpectedBalance struct {\n\tAccount    string `json:\"account\"`\n\tRawBalance string `json:\"rawBalance\"`\n}\n\n// GenesisBlockHeight defines an genesis blockHeight\ntype GenesisBlockHeight struct {\n\tIsBering  bool `json:\"isBering\"`\n\tIsIceland bool `json:\"isIceland\"`\n}\n\nfunc (eb *ExpectedBalance) Balance() *big.Int {\n\tbalance, ok := new(big.Int).SetString(eb.RawBalance, 10)\n\tif !ok {\n\t\tlog.L().Panic(\"invalid balance\", zap.String(\"balance\", eb.RawBalance))\n\t}\n\treturn balance\n}\n\nfunc readCode(sr protocol.StateReader, addr []byte) ([]byte, error) {\n\tvar c evm.SerializableBytes\n\taccount, err := accountutil.LoadAccount(sr, hash.BytesToHash160(addr))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t_, err = sr.State(&c, protocol.NamespaceOption(evm.CodeKVNameSpace), protocol.KeyOption(account.CodeHash[:]))\n\n\treturn c[:], err\n}\n\ntype Log struct {\n\tTopics []string `json:\"topics\"`\n\tData   string   `json:\"data\"`\n}\n\ntype ExecutionConfig struct {\n\tComment                 string            `json:\"comment\"`\n\tContractIndex           int               `json:\"contractIndex\"`\n\tAppendContractAddress   bool              `json:\"appendContractAddress\"`\n\tContractIndexToAppend   int               `json:\"contractIndexToAppend\"`\n\tContractAddressToAppend string            `json:\"contractAddressToAppend\"`\n\tReadOnly                bool              `json:\"readOnly\"`\n\tRawPrivateKey           string            `json:\"rawPrivateKey\"`\n\tRawByteCode             string            `json:\"rawByteCode\"`\n\tRawAmount               string            `json:\"rawAmount\"`\n\tRawGasLimit             uint              `json:\"rawGasLimit\"`\n\tRawGasPrice             string            `json:\"rawGasPrice\"`\n\tFailed                  bool              `json:\"failed\"`\n\tRawReturnValue          string            `json:\"rawReturnValue\"`\n\tRawExpectedGasConsumed  uint              `json:\"rawExpectedGasConsumed\"`\n\tExpectedStatus          uint64            `json:\"expectedStatus\"`\n\tExpectedBalances        []ExpectedBalance `json:\"expectedBalances\"`\n\tExpectedLogs            []Log             `json:\"expectedLogs\"`\n\tExpectedErrorMsg        string            `json:\"expectedErrorMsg\"`\n}\n\nfunc (cfg *ExecutionConfig) PrivateKey() crypto.PrivateKey {\n\tpriKey, err := crypto.HexStringToPrivateKey(cfg.RawPrivateKey)\n\tif err != nil {\n\t\tlog.L().Panic(\n\t\t\t\"invalid private key\",\n\t\t\tzap.String(\"privateKey\", cfg.RawPrivateKey),\n\t\t\tzap.Error(err),\n\t\t)\n\t}\n\n\treturn priKey\n}\n\nfunc (cfg *ExecutionConfig) Executor() address.Address {\n\tpriKey := cfg.PrivateKey()\n\taddr := priKey.PublicKey().Address()\n\tif addr == nil {\n\t\tlog.L().Panic(\n\t\t\t\"invalid private key\",\n\t\t\tzap.String(\"privateKey\", cfg.RawPrivateKey),\n\t\t\tzap.Error(errors.New(\"failed to get address\")),\n\t\t)\n\t}\n\n\treturn addr\n}\n\nfunc (cfg *ExecutionConfig) ByteCode() []byte {\n\tbyteCode, err := hex.DecodeString(cfg.RawByteCode)\n\tif err != nil {\n\t\tlog.L().Panic(\n\t\t\t\"invalid byte code\",\n\t\t\tzap.String(\"byteCode\", cfg.RawByteCode),\n\t\t\tzap.Error(err),\n\t\t)\n\t}\n\tif cfg.AppendContractAddress {\n\t\taddr, err := address.FromString(cfg.ContractAddressToAppend)\n\t\tif err != nil {\n\t\t\tlog.L().Panic(\n\t\t\t\t\"invalid contract address to append\",\n\t\t\t\tzap.String(\"contractAddressToAppend\", cfg.ContractAddressToAppend),\n\t\t\t\tzap.Error(err),\n\t\t\t)\n\t\t}\n\t\tba := addr.Bytes()\n\t\tba = append(make([]byte, 12), ba...)\n\t\tbyteCode = append(byteCode, ba...)\n\t}\n\n\treturn byteCode\n}\n\nfunc (cfg *ExecutionConfig) Amount() *big.Int {\n\tamount, ok := new(big.Int).SetString(cfg.RawAmount, 10)\n\tif !ok {\n\t\tlog.L().Panic(\"invalid amount\", zap.String(\"amount\", cfg.RawAmount))\n\t}\n\n\treturn amount\n}\n\nfunc (cfg *ExecutionConfig) GasPrice() *big.Int {\n\tprice, ok := new(big.Int).SetString(cfg.RawGasPrice, 10)\n\tif !ok {\n\t\tlog.L().Panic(\"invalid gas price\", zap.String(\"gasPrice\", cfg.RawGasPrice))\n\t}\n\n\treturn price\n}\n\nfunc (cfg *ExecutionConfig) GasLimit() uint64 {\n\treturn uint64(cfg.RawGasLimit)\n}\n\nfunc (cfg *ExecutionConfig) ExpectedGasConsumed() uint64 {\n\treturn uint64(cfg.RawExpectedGasConsumed)\n}\n\nfunc (cfg *ExecutionConfig) ExpectedReturnValue() []byte {\n\tretval, err := hex.DecodeString(cfg.RawReturnValue)\n\tif err != nil {\n\t\tlog.L().Panic(\n\t\t\t\"invalid return value\",\n\t\t\tzap.String(\"returnValue\", cfg.RawReturnValue),\n\t\t\tzap.Error(err),\n\t\t)\n\t}\n\n\treturn retval\n}\n\ntype SmartContractTest struct {\n\t// the order matters\n\tInitGenesis  GenesisBlockHeight `json:\"initGenesis\"`\n\tInitBalances []ExpectedBalance  `json:\"initBalances\"`\n\tDeployments  []ExecutionConfig  `json:\"deployments\"`\n\tExecutions   []ExecutionConfig  `json:\"executions\"`\n}\n\nfunc NewSmartContractTest(t *testing.T, file string) {\n\trequire := require.New(t)\n\tjsonFile, err := os.Open(file)\n\trequire.NoError(err)\n\tsctBytes, err := ioutil.ReadAll(jsonFile)\n\trequire.NoError(err)\n\tsct := &SmartContractTest{}\n\trequire.NoError(json.Unmarshal(sctBytes, sct))\n\tsct.run(require)\n}\n\nfunc readExecution(\n\tbc blockchain.Blockchain,\n\tsf factory.Factory,\n\tdao blockdao.BlockDAO,\n\tap actpool.ActPool,\n\tecfg *ExecutionConfig,\n\tcontractAddr string,\n) ([]byte, *action.Receipt, error) {\n\tlog.S().Info(ecfg.Comment)\n\tstate, err := accountutil.AccountState(sf, ecfg.Executor().String())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\texec, err := action.NewExecution(\n\t\tcontractAddr,\n\t\tstate.Nonce+1,\n\t\tecfg.Amount(),\n\t\tecfg.GasLimit(),\n\t\tecfg.GasPrice(),\n\t\tecfg.ByteCode(),\n\t)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\taddr := ecfg.PrivateKey().PublicKey().Address()\n\tif addr == nil {\n\t\treturn nil, nil, errors.New(\"failed to get address\")\n\t}\n\tctx, err := bc.Context(context.Background())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn sf.SimulateExecution(ctx, addr, exec, dao.GetBlockHash)\n}\n\nfunc runExecutions(\n\tbc blockchain.Blockchain,\n\tsf factory.Factory,\n\tdao blockdao.BlockDAO,\n\tap actpool.ActPool,\n\tecfgs []*ExecutionConfig,\n\tcontractAddrs []string,\n) ([]*action.Receipt, error) {\n\tnonces := map[string]uint64{}\n\thashes := []hash.Hash256{}\n\tfor i, ecfg := range ecfgs {\n\t\tlog.S().Info(ecfg.Comment)\n\t\tvar nonce uint64\n\t\tvar ok bool\n\t\texecutor := ecfg.Executor().String()\n\t\tif nonce, ok = nonces[executor]; !ok {\n\t\t\tstate, err := accountutil.AccountState(sf, executor)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tnonce = state.Nonce\n\t\t}\n\t\tnonce = nonce + 1\n\t\tnonces[executor] = nonce\n\t\texec, err := action.NewExecution(\n\t\t\tcontractAddrs[i],\n\t\t\tnonce,\n\t\t\tecfg.Amount(),\n\t\t\tecfg.GasLimit(),\n\t\t\tecfg.GasPrice(),\n\t\t\tecfg.ByteCode(),\n\t\t)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tbuilder := &action.EnvelopeBuilder{}\n\t\telp := builder.SetAction(exec).\n\t\t\tSetNonce(exec.Nonce()).\n\t\t\tSetGasLimit(ecfg.GasLimit()).\n\t\t\tSetGasPrice(ecfg.GasPrice()).\n\t\t\tBuild()\n\t\tselp, err := action.Sign(elp, ecfg.PrivateKey())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err := ap.Add(context.Background(), selp); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tselpHash, err := selp.Hash()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\thashes = append(hashes, selpHash)\n\t}\n\tblk, err := bc.MintNewBlock(testutil.TimestampNow())\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif err := bc.CommitBlock(blk); err != nil {\n\t\treturn nil, err\n\t}\n\treceipts := []*action.Receipt{}\n\tfor _, hash := range hashes {\n\t\treceipt, err := dao.GetReceiptByActionHash(hash, blk.Height())\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treceipts = append(receipts, receipt)\n\t}\n\n\treturn receipts, nil\n}\n\nfunc (sct *SmartContractTest) prepareBlockchain(\n\tctx context.Context,\n\tcfg config.Config,\n\tr *require.Assertions,\n) (blockchain.Blockchain, factory.Factory, blockdao.BlockDAO, actpool.ActPool) {\n\tdefer func() {\n\t\tdelete(cfg.Plugins, config.GatewayPlugin)\n\t}()\n\tcfg.Plugins[config.GatewayPlugin] = true\n\tcfg.Chain.EnableAsyncIndexWrite = false\n\tcfg.Genesis.EnableGravityChainVoting = false\n\ttestTriePath, err := testutil.PathOfTempFile(\"trie\")\n\tr.NoError(err)\n\n\tcfg.Chain.TrieDBPath = testTriePath\n\tcfg.ActPool.MinGasPriceStr = \"0\"\n\tif sct.InitGenesis.IsBering {\n\t\tcfg.Genesis.Blockchain.AleutianBlockHeight = 0\n\t\tcfg.Genesis.Blockchain.BeringBlockHeight = 0\n\t}\n\tcfg.Genesis.HawaiiBlockHeight = 0\n\tif sct.InitGenesis.IsIceland {\n\t\tcfg.Genesis.CookBlockHeight = 0\n\t\tcfg.Genesis.DardanellesBlockHeight = 0\n\t\tcfg.Genesis.DaytonaBlockHeight = 0\n\t\tcfg.Genesis.EasterBlockHeight = 0\n\t\tcfg.Genesis.FbkMigrationBlockHeight = 0\n\t\tcfg.Genesis.FairbankBlockHeight = 0\n\t\tcfg.Genesis.GreenlandBlockHeight = 0\n\t\tcfg.Genesis.IcelandBlockHeight = 0\n\t}\n\tfor _, expectedBalance := range sct.InitBalances {\n\t\tcfg.Genesis.InitBalanceMap[expectedBalance.Account] = expectedBalance.Balance().String()\n\t}\n\tregistry := protocol.NewRegistry()\n\tacc := account.NewProtocol(rewarding.DepositGas)\n\tr.NoError(acc.Register(registry))\n\trp := rolldpos.NewProtocol(cfg.Genesis.NumCandidateDelegates, cfg.Genesis.NumDelegates, cfg.Genesis.NumSubEpochs)\n\tr.NoError(rp.Register(registry))\n\t// create state factory\n\tvar sf factory.Factory\n\tif cfg.Chain.EnableTrielessStateDB {\n\t\tif cfg.Chain.EnableStateDBCaching {\n\t\t\tsf, err = factory.NewStateDB(cfg, factory.CachedStateDBOption(), factory.RegistryStateDBOption(registry))\n\t\t} else {\n\t\t\tsf, err = factory.NewStateDB(cfg, factory.DefaultStateDBOption(), factory.RegistryStateDBOption(registry))\n\t\t}\n\t} else {\n\t\tsf, err = factory.NewFactory(cfg, factory.InMemTrieOption(), factory.RegistryOption(registry))\n\t}\n\tr.NoError(err)\n\tap, err := actpool.NewActPool(sf, cfg.ActPool)\n\tr.NoError(err)\n\t// create indexer\n\tindexer, err := blockindex.NewIndexer(db.NewMemKVStore(), cfg.Genesis.Hash())\n\tr.NoError(err)\n\t// create BlockDAO\n\tdao := blockdao.NewBlockDAOInMemForTest([]blockdao.BlockIndexer{sf, indexer})\n\tr.NotNil(dao)\n\tbc := blockchain.NewBlockchain(\n\t\tcfg,\n\t\tdao,\n\t\tfactory.NewMinter(sf, ap),\n\t\tblockchain.BlockValidatorOption(block.NewValidator(\n\t\t\tsf,\n\t\t\tprotocol.NewGenericValidator(sf, accountutil.AccountState),\n\t\t)),\n\t)\n\treward := rewarding.NewProtocol(0, 0)\n\tr.NoError(reward.Register(registry))\n\n\tr.NotNil(bc)\n\texecution := NewProtocol(dao.GetBlockHash, rewarding.DepositGas)\n\tr.NoError(execution.Register(registry))\n\tr.NoError(bc.Start(ctx))\n\n\treturn bc, sf, dao, ap\n}\n\nfunc (sct *SmartContractTest) deployContracts(\n\tbc blockchain.Blockchain,\n\tsf factory.Factory,\n\tdao blockdao.BlockDAO,\n\tap actpool.ActPool,\n\tr *require.Assertions,\n) (contractAddresses []string) {\n\tfor i, contract := range sct.Deployments {\n\t\tif contract.AppendContractAddress {\n\t\t\tcontract.ContractAddressToAppend = contractAddresses[contract.ContractIndexToAppend]\n\t\t}\n\t\treceipts, err := runExecutions(bc, sf, dao, ap, []*ExecutionConfig{&contract}, []string{action.EmptyAddress})\n\t\tr.NoError(err)\n\t\tr.Equal(1, len(receipts))\n\t\treceipt := receipts[0]\n\t\tr.NotNil(receipt)\n\t\tif sct.InitGenesis.IsBering {\n\t\t\t// if it is post bering, it compares the status with expected status\n\t\t\tr.Equal(sct.Deployments[i].ExpectedStatus, receipt.Status)\n\t\t\tif receipt.Status != uint64(iotextypes.ReceiptStatus_Success) {\n\t\t\t\treturn []string{}\n\t\t\t}\n\t\t} else {\n\t\t\tif !sct.Deployments[i].Failed {\n\t\t\t\tr.Equal(uint64(iotextypes.ReceiptStatus_Success), receipt.Status, i)\n\t\t\t} else {\n\t\t\t\tr.Equal(uint64(iotextypes.ReceiptStatus_Failure), receipt.Status, i)\n\t\t\t\treturn []string{}\n\t\t\t}\n\t\t}\n\t\tif sct.Deployments[i].ExpectedGasConsumed() != 0 {\n\t\t\tr.Equal(sct.Deployments[i].ExpectedGasConsumed(), receipt.GasConsumed)\n\t\t}\n\n\t\taddr, _ := address.FromString(receipt.ContractAddress)\n\t\tc, err := readCode(sf, addr.Bytes())\n\t\tr.NoError(err)\n\t\tif contract.AppendContractAddress {\n\t\t\tlenOfByteCode := len(contract.ByteCode())\n\t\t\tr.True(bytes.Contains(contract.ByteCode()[:lenOfByteCode-32], c))\n\t\t} else {\n\t\t\tr.True(bytes.Contains(sct.Deployments[i].ByteCode(), c))\n\t\t}\n\t\tcontractAddresses = append(contractAddresses, receipt.ContractAddress)\n\t}\n\treturn\n}\n\nfunc (sct *SmartContractTest) run(r *require.Assertions) {\n\t// prepare blockchain\n\tctx := context.Background()\n\tcfg := config.Default\n\tcfg.Chain.EnableTrielessStateDB = false\n\tbc, sf, dao, ap := sct.prepareBlockchain(ctx, cfg, r)\n\tdefer func() {\n\t\tr.NoError(bc.Stop(ctx))\n\t}()\n\n\t// deploy smart contract\n\tcontractAddresses := sct.deployContracts(bc, sf, dao, ap, r)\n\tif len(contractAddresses) == 0 {\n\t\treturn\n\t}\n\n\t// run executions\n\tfor i, exec := range sct.Executions {\n\t\tcontractAddr := contractAddresses[exec.ContractIndex]\n\t\tif exec.AppendContractAddress {\n\t\t\texec.ContractAddressToAppend = contractAddresses[exec.ContractIndexToAppend]\n\t\t}\n\t\tvar retval []byte\n\t\tvar receipt *action.Receipt\n\t\tvar err error\n\t\tif exec.ReadOnly {\n\t\t\tretval, receipt, err = readExecution(bc, sf, dao, ap, &exec, contractAddr)\n\t\t\tr.NoError(err)\n\t\t\texpected := exec.ExpectedReturnValue()\n\t\t\tif len(expected) == 0 {\n\t\t\t\tr.Equal(0, len(retval))\n\t\t\t} else {\n\t\t\t\tr.Equal(expected, retval)\n\t\t\t}\n\t\t} else {\n\t\t\treceipts, err := runExecutions(bc, sf, dao, ap, []*ExecutionConfig{&exec}, []string{contractAddr})\n\t\t\tr.NoError(err)\n\t\t\tr.Equal(1, len(receipts))\n\t\t\treceipt = receipts[0]\n\t\t\tr.NotNil(receipt)\n\t\t}\n\n\t\tif sct.InitGenesis.IsBering {\n\t\t\t// if it is post bering, it compares the status with expected status\n\t\t\tr.Equal(exec.ExpectedStatus, receipt.Status)\n\t\t} else {\n\t\t\tif exec.Failed {\n\t\t\t\tr.Equal(uint64(iotextypes.ReceiptStatus_Failure), receipt.Status)\n\t\t\t} else {\n\t\t\t\tr.Equal(uint64(iotextypes.ReceiptStatus_Success), receipt.Status)\n\t\t\t}\n\t\t}\n\t\tif exec.ExpectedGasConsumed() != 0 {\n\t\t\tr.Equal(exec.ExpectedGasConsumed(), receipt.GasConsumed, i)\n\t\t}\n\t\tfor _, expectedBalance := range exec.ExpectedBalances {\n\t\t\taccount := expectedBalance.Account\n\t\t\tif account == \"\" {\n\t\t\t\taccount = contractAddr\n\t\t\t}\n\t\t\tstate, err := accountutil.AccountState(sf, account)\n\t\t\tr.NoError(err)\n\t\t\tr.Equal(\n\t\t\t\t0,\n\t\t\t\tstate.Balance.Cmp(expectedBalance.Balance()),\n\t\t\t\t\"balance of account %s is different from expectation, %d vs %d\",\n\t\t\t\taccount,\n\t\t\t\tstate.Balance,\n\t\t\t\texpectedBalance.Balance(),\n\t\t\t)\n\t\t}\n\t\tif receipt.Status == uint64(iotextypes.ReceiptStatus_Success) {\n\t\t\tr.Equal(len(exec.ExpectedLogs), len(receipt.Logs()), i)\n\t\t\t// TODO: check value of logs\n\t\t}\n\t\tif receipt.Status == uint64(iotextypes.ReceiptStatus_ErrExecutionReverted) {\n\t\t\tr.Equal(exec.ExpectedErrorMsg, receipt.ExecutionRevertMsg())\n\t\t}\n\t}\n}\n\nfunc TestProtocol_Validate(t *testing.T) {\n\trequire := require.New(t)\n\tp := NewProtocol(func(uint64) (hash.Hash256, error) {\n\t\treturn hash.ZeroHash256, nil\n\t}, rewarding.DepositGas)\n\tdata := make([]byte, 32769)\n\n\tex, err := action.NewExecution(\"2\", uint64(1), big.NewInt(0), uint64(0), big.NewInt(0), data)\n\trequire.NoError(err)\n\trequire.Equal(action.ErrActPool, errors.Cause(p.Validate(context.Background(), ex, nil)))\n}\n\nfunc TestProtocol_Handle(t *testing.T) {\n\ttestEVM := func(t *testing.T) {\n\t\tlog.S().Info(\"Test EVM\")\n\t\trequire := require.New(t)\n\n\t\tctx := context.Background()\n\t\tcfg := config.Default\n\t\tdefer func() {\n\t\t\tdelete(cfg.Plugins, config.GatewayPlugin)\n\t\t}()\n\n\t\ttestTriePath, err := testutil.PathOfTempFile(\"trie\")\n\t\trequire.NoError(err)\n\t\ttestDBPath, err := testutil.PathOfTempFile(\"db\")\n\t\trequire.NoError(err)\n\t\ttestIndexPath, err := testutil.PathOfTempFile(\"index\")\n\t\trequire.NoError(err)\n\n\t\tcfg.Plugins[config.GatewayPlugin] = true\n\t\tcfg.Chain.TrieDBPath = testTriePath\n\t\tcfg.Chain.ChainDBPath = testDBPath\n\t\tcfg.Chain.IndexDBPath = testIndexPath\n\t\tcfg.Chain.EnableAsyncIndexWrite = false\n\t\tcfg.Genesis.EnableGravityChainVoting = false\n\t\tcfg.ActPool.MinGasPriceStr = \"0\"\n\t\tcfg.Genesis.InitBalanceMap[identityset.Address(27).String()] = unit.ConvertIotxToRau(1000000000).String()\n\t\tregistry := protocol.NewRegistry()\n\t\tacc := account.NewProtocol(rewarding.DepositGas)\n\t\trequire.NoError(acc.Register(registry))\n\t\trp := rolldpos.NewProtocol(cfg.Genesis.NumCandidateDelegates, cfg.Genesis.NumDelegates, cfg.Genesis.NumSubEpochs)\n\t\trequire.NoError(rp.Register(registry))\n\t\t// create state factory\n\t\tsf, err := factory.NewStateDB(cfg, factory.CachedStateDBOption(), factory.RegistryStateDBOption(registry))\n\t\trequire.NoError(err)\n\t\tap, err := actpool.NewActPool(sf, cfg.ActPool)\n\t\trequire.NoError(err)\n\t\t// create indexer\n\t\tcfg.DB.DbPath = cfg.Chain.IndexDBPath\n\t\tindexer, err := blockindex.NewIndexer(db.NewBoltDB(cfg.DB), hash.ZeroHash256)\n\t\trequire.NoError(err)\n\t\t// create BlockDAO\n\t\tcfg.DB.DbPath = cfg.Chain.ChainDBPath\n\t\tdao := blockdao.NewBlockDAOInMemForTest([]blockdao.BlockIndexer{sf, indexer})\n\t\trequire.NotNil(dao)\n\t\tbc := blockchain.NewBlockchain(\n\t\t\tcfg,\n\t\t\tdao,\n\t\t\tfactory.NewMinter(sf, ap),\n\t\t\tblockchain.BlockValidatorOption(block.NewValidator(\n\t\t\t\tsf,\n\t\t\t\tprotocol.NewGenericValidator(sf, accountutil.AccountState),\n\t\t\t)),\n\t\t)\n\t\texeProtocol := NewProtocol(dao.GetBlockHash, rewarding.DepositGas)\n\t\trequire.NoError(exeProtocol.Register(registry))\n\t\trequire.NoError(bc.Start(ctx))\n\t\trequire.NotNil(bc)\n\t\tdefer func() {\n\t\t\trequire.NoError(bc.Stop(ctx))\n\t\t}()\n\n\t\tdata, _ := hex.DecodeString(\"608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a7230582002faabbefbbda99b20217cf33cb8ab8100caf1542bf1f48117d72e2c59139aea0029\")\n\t\texecution, err := action.NewExecution(action.EmptyAddress, 1, big.NewInt(0), uint64(100000), big.NewInt(0), data)\n\t\trequire.NoError(err)\n\n\t\tbd := &action.EnvelopeBuilder{}\n\t\telp := bd.SetAction(execution).\n\t\t\tSetNonce(1).\n\t\t\tSetGasLimit(100000).Build()\n\t\tselp, err := action.Sign(elp, identityset.PrivateKey(27))\n\t\trequire.NoError(err)\n\n\t\trequire.NoError(ap.Add(context.Background(), selp))\n\t\tblk, err := bc.MintNewBlock(testutil.TimestampNow())\n\t\trequire.NoError(err)\n\t\trequire.NoError(bc.CommitBlock(blk))\n\t\trequire.Equal(1, len(blk.Receipts))\n\n\t\teHash, err := selp.Hash()\n\t\trequire.NoError(err)\n\t\tr, _ := dao.GetReceiptByActionHash(eHash, blk.Height())\n\t\trequire.NotNil(r)\n\t\trequire.Equal(eHash, r.ActionHash)\n\t\tcontract, err := address.FromString(r.ContractAddress)\n\t\trequire.NoError(err)\n\n\t\t// test IsContract\n\t\tstate, err := accountutil.AccountState(sf, contract.String())\n\t\trequire.NoError(err)\n\t\trequire.True(state.IsContract())\n\n\t\tc, err := readCode(sf, contract.Bytes())\n\t\trequire.NoError(err)\n\t\trequire.Equal(data[31:], c)\n\n\t\texe, _, err := dao.GetActionByActionHash(eHash, blk.Height())\n\t\trequire.NoError(err)\n\t\texeHash, err := exe.Hash()\n\t\trequire.NoError(err)\n\t\trequire.Equal(eHash, exeHash)\n\n\t\taddr27 := hash.BytesToHash160(identityset.Address(27).Bytes())\n\t\ttotal, err := indexer.GetActionCountByAddress(addr27)\n\t\trequire.NoError(err)\n\t\texes, err := indexer.GetActionsByAddress(addr27, 0, total)\n\t\trequire.NoError(err)\n\t\trequire.Equal(1, len(exes))\n\t\trequire.Equal(eHash[:], exes[0])\n\n\t\tactIndex, err := indexer.GetActionIndex(eHash[:])\n\t\trequire.NoError(err)\n\t\tblkHash, err := dao.GetBlockHash(actIndex.BlockHeight())\n\t\trequire.NoError(err)\n\t\trequire.Equal(blk.HashBlock(), blkHash)\n\n\t\t// store to key 0\n\t\tdata, _ = hex.DecodeString(\"60fe47b1000000000000000000000000000000000000000000000000000000000000000f\")\n\t\texecution, err = action.NewExecution(r.ContractAddress, 2, big.NewInt(0), uint64(120000), big.NewInt(0), data)\n\t\trequire.NoError(err)\n\n\t\tbd = &action.EnvelopeBuilder{}\n\t\telp = bd.SetAction(execution).\n\t\t\tSetNonce(2).\n\t\t\tSetGasLimit(120000).Build()\n\t\tselp, err = action.Sign(elp, identityset.PrivateKey(27))\n\t\trequire.NoError(err)\n\n\t\tlog.S().Infof(\"execution %+v\", execution)\n\n\t\trequire.NoError(ap.Add(context.Background(), selp))\n\t\tblk, err = bc.MintNewBlock(testutil.TimestampNow())\n\t\trequire.NoError(err)\n\t\trequire.NoError(bc.CommitBlock(blk))\n\t\trequire.Equal(1, len(blk.Receipts))\n\n\t\t// TODO (zhi): reenable the unit test\n\t\t/*\n\t\t\tws, err = sf.NewWorkingSet()\n\t\t\trequire.NoError(err)\n\t\t\tstateDB = evm.NewStateDBAdapter(ws, uint64(0), true, hash.ZeroHash256)\n\t\t\tvar emptyEVMHash common.Hash\n\t\t\tv := stateDB.GetState(evmContractAddrHash, emptyEVMHash)\n\t\t\trequire.Equal(byte(15), v[31])\n\t\t*/\n\t\teHash, err = selp.Hash()\n\t\trequire.NoError(err)\n\t\tr, err = dao.GetReceiptByActionHash(eHash, blk.Height())\n\t\trequire.NoError(err)\n\t\trequire.Equal(eHash, r.ActionHash)\n\n\t\t// read from key 0\n\t\tdata, err = hex.DecodeString(\"6d4ce63c\")\n\t\trequire.NoError(err)\n\t\texecution, err = action.NewExecution(r.ContractAddress, 3, big.NewInt(0), uint64(120000), big.NewInt(0), data)\n\t\trequire.NoError(err)\n\n\t\tbd = &action.EnvelopeBuilder{}\n\t\telp = bd.SetAction(execution).\n\t\t\tSetNonce(3).\n\t\t\tSetGasLimit(120000).Build()\n\t\tselp, err = action.Sign(elp, identityset.PrivateKey(27))\n\t\trequire.NoError(err)\n\n\t\tlog.S().Infof(\"execution %+v\", execution)\n\t\trequire.NoError(ap.Add(context.Background(), selp))\n\t\tblk, err = bc.MintNewBlock(testutil.TimestampNow())\n\t\trequire.NoError(err)\n\t\trequire.NoError(bc.CommitBlock(blk))\n\t\trequire.Equal(1, len(blk.Receipts))\n\n\t\teHash, err = selp.Hash()\n\t\trequire.NoError(err)\n\t\tr, err = dao.GetReceiptByActionHash(eHash, blk.Height())\n\t\trequire.NoError(err)\n\t\trequire.Equal(eHash, r.ActionHash)\n\n\t\tdata, _ = hex.DecodeString(\"608060405234801561001057600080fd5b5060df8061001f6000396000f3006080604052600436106049576000357c0100000000000000000000000000000000000000000000000000000000900463ffffffff16806360fe47b114604e5780636d4ce63c146078575b600080fd5b348015605957600080fd5b5060766004803603810190808035906020019092919050505060a0565b005b348015608357600080fd5b50608a60aa565b6040518082815260200191505060405180910390f35b8060008190555050565b600080549050905600a165627a7a7230582002faabbefbbda99b20217cf33cb8ab8100caf1542bf1f48117d72e2c59139aea0029\")\n\t\texecution1, err := action.NewExecution(action.EmptyAddress, 4, big.NewInt(0), uint64(100000), big.NewInt(10), data)\n\t\trequire.NoError(err)\n\t\tbd = &action.EnvelopeBuilder{}\n\n\t\telp = bd.SetAction(execution1).\n\t\t\tSetNonce(4).\n\t\t\tSetGasLimit(100000).SetGasPrice(big.NewInt(10)).Build()\n\t\tselp, err = action.Sign(elp, identityset.PrivateKey(27))\n\t\trequire.NoError(err)\n\n\t\trequire.NoError(ap.Add(context.Background(), selp))\n\t\tblk, err = bc.MintNewBlock(testutil.TimestampNow())\n\t\trequire.NoError(err)\n\t\trequire.NoError(bc.CommitBlock(blk))\n\t\trequire.Equal(1, len(blk.Receipts))\n\t}\n\n\tt.Run(\"EVM\", func(t *testing.T) {\n\t\ttestEVM(t)\n\t})\n\t/**\n\t * source of smart contract: https://etherscan.io/address/0x6fb3e0a217407efff7ca062d46c26e5d60a14d69#code\n\t */\n\tt.Run(\"ERC20\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/erc20.json\")\n\t})\n\t/**\n\t * Source of smart contract: https://etherscan.io/address/0x8dd5fbce2f6a956c3022ba3663759011dd51e73e#code\n\t */\n\tt.Run(\"DelegateERC20\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/delegate_erc20.json\")\n\t})\n\t/*\n\t * Source code: https://kovan.etherscan.io/address/0x81f85886749cbbf3c2ec742db7255c6b07c63c69\n\t */\n\tt.Run(\"InfiniteLoop\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/infiniteloop.json\")\n\t})\n\t// RollDice\n\tt.Run(\"RollDice\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/rolldice.json\")\n\t})\n\t// ChangeState\n\tt.Run(\"ChangeState\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/changestate.json\")\n\t})\n\t// array-return\n\tt.Run(\"ArrayReturn\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/array-return.json\")\n\t})\n\t// basic-token\n\tt.Run(\"BasicToken\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/basic-token.json\")\n\t})\n\t// call-dynamic\n\tt.Run(\"CallDynamic\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/call-dynamic.json\")\n\t})\n\t// factory\n\tt.Run(\"Factory\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/factory.json\")\n\t})\n\t// mapping-delete\n\tt.Run(\"MappingDelete\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/mapping-delete.json\")\n\t})\n\t// f.value\n\tt.Run(\"F.value\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/f.value.json\")\n\t})\n\t// proposal\n\tt.Run(\"Proposal\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/proposal.json\")\n\t})\n\t// public-length\n\tt.Run(\"PublicLength\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/public-length.json\")\n\t})\n\t// public-mapping\n\tt.Run(\"PublicMapping\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/public-mapping.json\")\n\t})\n\t// no-variable-length-returns\n\tt.Run(\"NoVariableLengthReturns\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/no-variable-length-returns.json\")\n\t})\n\t// tuple\n\tt.Run(\"Tuple\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/tuple.json\")\n\t})\n\t// tail-recursion\n\tt.Run(\"TailRecursion\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/tail-recursion.json\")\n\t})\n\t// sha3\n\tt.Run(\"Sha3\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/sha3.json\")\n\t})\n\t// remove-from-array\n\tt.Run(\"RemoveFromArray\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/remove-from-array.json\")\n\t})\n\t// send-eth\n\tt.Run(\"SendEth\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/send-eth.json\")\n\t})\n\t// modifier\n\tt.Run(\"Modifier\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/modifiers.json\")\n\t})\n\t// multisend\n\tt.Run(\"Multisend\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/multisend.json\")\n\t})\n\tt.Run(\"Multisend2\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/multisend2.json\")\n\t})\n\t// reentry\n\tt.Run(\"reentry-attack\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/reentry-attack.json\")\n\t})\n\t// cashier\n\tt.Run(\"cashier\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/cashier.json\")\n\t})\n\t// wireconnection\n\t// [Issue #1422] This unit test proves that there is no problem when we want to deploy and execute the contract\n\t// which inherits abstract contract and implements abstract functions and call each other (Utterance() calls utterance())\n\tt.Run(\"wireconnection\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/wireconnection.json\")\n\t})\n\t// gas-test\n\tt.Run(\"gas-test\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/gas-test.json\")\n\t})\n\t// storage-test\n\tt.Run(\"storage-test\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/storage-test.json\")\n\t})\n\t// cashier-bering\n\tt.Run(\"cashier-bering\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/cashier-bering.json\")\n\t})\n\t// infiniteloop-bering\n\tt.Run(\"infiniteloop-bering\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/infiniteloop-bering.json\")\n\t})\n\t// self-destruct\n\tt.Run(\"self-destruct\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/self-destruct.json\")\n\t})\n}\n\nfunc TestMaxTime(t *testing.T) {\n\tt.Run(\"max-time\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/maxtime.json\")\n\t})\n\n\tt.Run(\"max-time-2\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata/maxtime2.json\")\n\t})\n}\n\nfunc TestIstanbulEVM(t *testing.T) {\n\tcfg := config.Default\n\tconfig.SetEVMNetworkID(cfg.Chain.EVMNetworkID)\n\tt.Run(\"ArrayReturn\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/array-return.json\")\n\t})\n\tt.Run(\"BasicToken\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/basic-token.json\")\n\t})\n\tt.Run(\"CallDynamic\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/call-dynamic.json\")\n\t})\n\tt.Run(\"chainid-selfbalance\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/chainid-selfbalance.json\")\n\t})\n\tt.Run(\"ChangeState\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/changestate.json\")\n\t})\n\tt.Run(\"F.value\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/f.value.json\")\n\t})\n\tt.Run(\"Gas-test\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/gas-test.json\")\n\t})\n\tt.Run(\"InfiniteLoop\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/infiniteloop.json\")\n\t})\n\tt.Run(\"MappingDelete\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/mapping-delete.json\")\n\t})\n\tt.Run(\"max-time\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/maxtime.json\")\n\t})\n\tt.Run(\"Modifier\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/modifiers.json\")\n\t})\n\tt.Run(\"Multisend\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/multisend.json\")\n\t})\n\tt.Run(\"NoVariableLengthReturns\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/no-variable-length-returns.json\")\n\t})\n\tt.Run(\"PublicMapping\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/public-mapping.json\")\n\t})\n\tt.Run(\"reentry-attack\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/reentry-attack.json\")\n\t})\n\tt.Run(\"RemoveFromArray\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/remove-from-array.json\")\n\t})\n\tt.Run(\"SendEth\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/send-eth.json\")\n\t})\n\tt.Run(\"Sha3\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/sha3.json\")\n\t})\n\tt.Run(\"storage-test\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/storage-test.json\")\n\t})\n\tt.Run(\"TailRecursion\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/tail-recursion.json\")\n\t})\n\tt.Run(\"Tuple\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/tuple.json\")\n\t})\n\tt.Run(\"wireconnection\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/wireconnection.json\")\n\t})\n\tt.Run(\"self-destruct\", func(t *testing.T) {\n\t\tNewSmartContractTest(t, \"testdata-istanbul/self-destruct.json\")\n\t})\n}\n\nfunc benchmarkHotContractWithFactory(b *testing.B, async bool) {\n\tsct := SmartContractTest{\n\t\tInitBalances: []ExpectedBalance{\n\t\t\t{\n\t\t\t\tAccount:    \"io1mflp9m6hcgm2qcghchsdqj3z3eccrnekx9p0ms\",\n\t\t\t\tRawBalance: \"1000000000000000000000000000\",\n\t\t\t},\n\t\t},\n\t\tDeployments: []ExecutionConfig{\n\t\t\t{\n\t\t\t\tContractIndex: 0,\n\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\tRawByteCode:   \"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\",\n\t\t\t\tRawAmount:     \"0\",\n\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t},\n\t\t},\n\t}\n\tr := require.New(b)\n\tctx := context.Background()\n\tcfg := config.Default\n\tcfg.Genesis.NumSubEpochs = uint64(b.N)\n\tcfg.Chain.EnableTrielessStateDB = false\n\tif async {\n\t\tcfg.Genesis.GreenlandBlockHeight = 0\n\t} else {\n\t\tcfg.Genesis.GreenlandBlockHeight = 10000000000\n\t}\n\tbc, sf, dao, ap := sct.prepareBlockchain(ctx, cfg, r)\n\tdefer func() {\n\t\tr.NoError(bc.Stop(ctx))\n\t}()\n\tcontractAddresses := sct.deployContracts(bc, sf, dao, ap, r)\n\tr.Equal(1, len(contractAddresses))\n\tcontractAddr := contractAddresses[0]\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\treceipts, err := runExecutions(\n\t\t\tbc, sf, dao, ap, []*ExecutionConfig{\n\t\t\t\t{\n\t\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\t\tRawByteCode:   \"1249c58b\",\n\t\t\t\t\tRawAmount:     \"0\",\n\t\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t\t\tFailed:        false,\n\t\t\t\t\tComment:       \"mint token\",\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{contractAddr},\n\t\t)\n\t\tr.NoError(err)\n\t\tr.Equal(1, len(receipts))\n\t\tr.Equal(uint64(1), receipts[0].Status)\n\t\tecfgs := []*ExecutionConfig{}\n\t\tcontractAddrs := []string{}\n\t\tfor j := 0; j < 100; j++ {\n\t\t\tecfgs = append(ecfgs, &ExecutionConfig{\n\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\tRawByteCode:   fmt.Sprintf(\"a9059cbb000000000000000000000000123456789012345678900987%016x0000000000000000000000000000000000000000000000000000000000000039\", 100*i+j),\n\t\t\t\tRawAmount:     \"0\",\n\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t\tFailed:        false,\n\t\t\t\tComment:       \"send token\",\n\t\t\t})\n\t\t\tcontractAddrs = append(contractAddrs, contractAddr)\n\t\t}\n\t\treceipts, err = runExecutions(bc, sf, dao, ap, ecfgs, contractAddrs)\n\t\tr.NoError(err)\n\t\tfor _, receipt := range receipts {\n\t\t\tr.Equal(uint64(1), receipt.Status)\n\t\t}\n\t}\n\tb.StopTimer()\n}\n\nfunc benchmarkHotContractWithStateDB(b *testing.B, cachedStateDBOption bool) {\n\tsct := SmartContractTest{\n\t\tInitBalances: []ExpectedBalance{\n\t\t\t{\n\t\t\t\tAccount:    \"io1mflp9m6hcgm2qcghchsdqj3z3eccrnekx9p0ms\",\n\t\t\t\tRawBalance: \"1000000000000000000000000000\",\n\t\t\t},\n\t\t},\n\t\tDeployments: []ExecutionConfig{\n\t\t\t{\n\t\t\t\tContractIndex: 0,\n\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\tRawByteCode:   \"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\",\n\t\t\t\tRawAmount:     \"0\",\n\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t},\n\t\t},\n\t}\n\tr := require.New(b)\n\tctx := context.Background()\n\tcfg := config.Default\n\tcfg.Genesis.NumSubEpochs = uint64(b.N)\n\tif cachedStateDBOption {\n\t\tcfg.Chain.EnableStateDBCaching = true\n\t} else {\n\t\tcfg.Chain.EnableStateDBCaching = false\n\t}\n\tbc, sf, dao, ap := sct.prepareBlockchain(ctx, cfg, r)\n\tdefer func() {\n\t\tr.NoError(bc.Stop(ctx))\n\t}()\n\tcontractAddresses := sct.deployContracts(bc, sf, dao, ap, r)\n\tr.Equal(1, len(contractAddresses))\n\tcontractAddr := contractAddresses[0]\n\tb.ResetTimer()\n\tfor i := 0; i < b.N; i++ {\n\t\treceipts, err := runExecutions(\n\t\t\tbc, sf, dao, ap, []*ExecutionConfig{\n\t\t\t\t{\n\t\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\t\tRawByteCode:   \"1249c58b\",\n\t\t\t\t\tRawAmount:     \"0\",\n\t\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t\t\tFailed:        false,\n\t\t\t\t\tComment:       \"mint token\",\n\t\t\t\t},\n\t\t\t},\n\t\t\t[]string{contractAddr},\n\t\t)\n\t\tr.NoError(err)\n\t\tr.Equal(1, len(receipts))\n\t\tr.Equal(uint64(1), receipts[0].Status)\n\t\tecfgs := []*ExecutionConfig{}\n\t\tcontractAddrs := []string{}\n\t\tfor j := 0; j < 100; j++ {\n\t\t\tecfgs = append(ecfgs, &ExecutionConfig{\n\t\t\t\tRawPrivateKey: \"cfa6ef757dee2e50351620dca002d32b9c090cfda55fb81f37f1d26b273743f1\",\n\t\t\t\tRawByteCode:   fmt.Sprintf(\"a9059cbb000000000000000000000000123456789012345678900987%016x0000000000000000000000000000000000000000000000000000000000000039\", 100*i+j),\n\t\t\t\tRawAmount:     \"0\",\n\t\t\t\tRawGasLimit:   5000000,\n\t\t\t\tRawGasPrice:   \"0\",\n\t\t\t\tFailed:        false,\n\t\t\t\tComment:       \"send token\",\n\t\t\t})\n\t\t\tcontractAddrs = append(contractAddrs, contractAddr)\n\t\t}\n\t\treceipts, err = runExecutions(bc, sf, dao, ap, ecfgs, contractAddrs)\n\t\tr.NoError(err)\n\t\tfor _, receipt := range receipts {\n\t\t\tr.Equal(uint64(1), receipt.Status)\n\t\t}\n\t}\n\tb.StopTimer()\n}\n\nfunc BenchmarkHotContract(b *testing.B) {\n\tb.Run(\"async mode\", func(b *testing.B) {\n\t\tbenchmarkHotContractWithFactory(b, true)\n\t})\n\tb.Run(\"sync mode\", func(b *testing.B) {\n\t\tbenchmarkHotContractWithFactory(b, false)\n\t})\n\tb.Run(\"cachedStateDB\", func(b *testing.B) {\n\t\tbenchmarkHotContractWithStateDB(b, true)\n\t})\n\tb.Run(\"defaultStateDB\", func(b *testing.B) {\n\t\tbenchmarkHotContractWithStateDB(b, false)\n\t})\n}\n", "idx": 4, "id": 23785, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go"}
h": "@@ -32,5 +32,24 @@ module Bolt\n       fs = ::WinRM::FS::FileManager.new(@connection)\n       fs.upload(source, destination)\n     end\n+\n+    def make_tempdir\n+      execute(<<-EOS).stdout.chomp\n+$parent = [System.IO.Path]::GetTempPath()\n+$name = [System.IO.Path]::GetRandomFileName()\n+$path = Join-Path $parent $name\n+New-Item -ItemType Directory -Path $path | Out-Null\n+$path\n+EOS\n+    end\n+\n+    def run_script(script)\n+      dir = make_tempdir\n+      remote_path = \"#{dir}\\\\#{File.basename(script, File.extname(script))}.ps1\"\n+      copy(script, remote_path)\n+      args = '-NoProfile -NonInteractive -NoLogo -ExecutionPolicy Bypass'\n+      execute(\"powershell.exe #{args} -File '#{remote_path}'\")\n+      execute(\"Remove-Item -force -recurse '#{dir}'\")\n+    end\n   end\n end", "y": 1, "oldf": "require 'winrm'\nrequire 'winrm-fs'\n\nmodule Bolt\n  class WinRM < Node\n    def initialize(endpoint, user, password, shell = :powershell)\n      @endpoint = endpoint\n      @user = user\n      @password = password\n      @shell = shell\n      @connection = ::WinRM::Connection.new(endpoint: @endpoint,\n                                            user: @user,\n                                            password: @password)\n    end\n\n    def connect\n      @session = @connection.shell(@shell)\n    end\n\n    def disconnect\n      @session.close if @session\n    end\n\n    def execute(command)\n      @session.run(command) do |stdout, stderr|\n        print stdout\n        print stderr\n      end\n    end\n\n    def copy(source, destination)\n      fs = ::WinRM::FS::FileManager.new(@connection)\n      fs.upload(source, destination)\n    end\n  end\nend\n", "idx": 1, "id": 6376, "msg": "Do we intend to support PS2? If not, then the `System.` can be omitted.", "proj": "puppetlabs-bolt", "lang": "rb"}
{"patch": "@@ -117,6 +117,10 @@ public class DistributorStatus {\n       return up;\n     }\n \n+    public boolean isDocker() {\n+      return up;\n+    }\n+\n     public int getMaxSessionCount() {\n       return maxSessionCount;\n     }", "y": 1, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.selenium.grid.data;\n\nimport static com.google.common.collect.ImmutableList.toImmutableList;\n\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableSet;\nimport com.google.common.reflect.TypeToken;\n\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.internal.Require;\nimport org.openqa.selenium.json.JsonInput;\n\nimport java.lang.reflect.Type;\nimport java.net.URI;\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Set;\nimport java.util.UUID;\n\npublic class DistributorStatus {\n\n  private static final Type SUMMARIES_TYPES = new TypeToken<Set<NodeSummary>>() {\n  }.getType();\n\n  private final Set<NodeSummary> allNodes;\n\n  public DistributorStatus(Collection<NodeSummary> allNodes) {\n    this.allNodes = ImmutableSet.copyOf(allNodes);\n  }\n\n  public boolean hasCapacity() {\n    return getNodes().stream()\n        .map(summary -> summary.isUp() && summary.hasCapacity())\n        .reduce(Boolean::logicalOr)\n        .orElse(false);\n  }\n\n  public Set<NodeSummary> getNodes() {\n    return allNodes;\n  }\n\n  private Map<String, Object> toJson() {\n    return ImmutableMap.of(\n        \"nodes\", getNodes());\n  }\n\n  private static DistributorStatus fromJson(JsonInput input) {\n    Set<NodeSummary> nodes = null;\n\n    input.beginObject();\n    while (input.hasNext()) {\n      switch (input.nextName()) {\n        case \"nodes\":\n          nodes = input.read(SUMMARIES_TYPES);\n          break;\n\n        default:\n          input.skipValue();\n      }\n    }\n    input.endObject();\n\n    return new DistributorStatus(nodes);\n  }\n\n  public static class NodeSummary {\n\n    private final UUID nodeId;\n    private final URI uri;\n    private final boolean up;\n    private final int maxSessionCount;\n    private final Map<Capabilities, Integer> stereotypes;\n    private final Map<Capabilities, Integer> used;\n\n    public NodeSummary(\n        UUID nodeId,\n        URI uri,\n        boolean up,\n        int maxSessionCount,\n        Map<Capabilities, Integer> stereotypes,\n        Map<Capabilities, Integer> usedStereotypes) {\n      this.nodeId = Require.nonNull(\"Node id\", nodeId);\n      this.uri = Require.nonNull(\"URI\", uri);\n      this.up = up;\n      this.maxSessionCount = maxSessionCount;\n      this.stereotypes = ImmutableMap.copyOf(Require.nonNull(\"Stereoytpes\", stereotypes));\n      this.used = ImmutableMap.copyOf(Require.nonNull(\"User stereotypes\", usedStereotypes));\n    }\n\n    public UUID getNodeId() {\n      return nodeId;\n    }\n\n    public URI getUri() {\n      return uri;\n    }\n\n    public boolean isUp() {\n      return up;\n    }\n\n    public int getMaxSessionCount() {\n      return maxSessionCount;\n    }\n\n    public Map<Capabilities, Integer> getStereotypes() {\n      return stereotypes;\n    }\n\n    public Map<Capabilities, Integer> getUsedStereotypes() {\n      return used;\n    }\n\n    public boolean hasCapacity() {\n      HashMap<Capabilities, Integer> all = new HashMap<>(stereotypes);\n      used.forEach((caps, count) -> all.computeIfPresent(caps, (ignored, curr) -> curr - count));\n\n      return all.values()\n          .stream()\n          .map(count -> count > 0)\n          .reduce(Boolean::logicalOr)\n          .orElse(false);\n    }\n\n    private Map<String, Object> toJson() {\n      ImmutableMap.Builder<String, Object> builder = ImmutableMap.builder();\n      builder.put(\"nodeId\", getNodeId());\n      builder.put(\"uri\", getUri());\n      builder.put(\"up\", isUp());\n      builder.put(\"maxSessionCount\", getMaxSessionCount());\n      builder.put(\"stereotypes\", getStereotypes().entrySet().stream()\n          .map(entry -> ImmutableMap.of(\n              \"capabilities\", entry.getKey(),\n              \"count\", entry.getValue()))\n          .collect(toImmutableList()));\n      builder.put(\"usedStereotypes\", getUsedStereotypes().entrySet().stream()\n          .map(entry -> ImmutableMap.of(\n              \"capabilities\", entry.getKey(),\n              \"count\", entry.getValue()))\n          .collect(toImmutableList()));\n\n      return builder.build();\n    }\n\n    private static NodeSummary fromJson(JsonInput input) {\n      UUID nodeId = null;\n      URI uri = null;\n      boolean up = false;\n      int maxSessionCount = 0;\n      Map<Capabilities, Integer> stereotypes = new HashMap<>();\n      Map<Capabilities, Integer> used = new HashMap<>();\n\n      input.beginObject();\n      while (input.hasNext()) {\n        switch (input.nextName()) {\n          case \"maxSessionCount\":\n            maxSessionCount = input.nextNumber().intValue();\n            break;\n\n          case \"nodeId\":\n            nodeId = input.read(UUID.class);\n            break;\n\n          case \"stereotypes\":\n            stereotypes = readCapabilityCounts(input);\n            break;\n\n          case \"up\":\n            up = input.nextBoolean();\n            break;\n\n          case \"uri\":\n            uri = input.read(URI.class);\n            break;\n\n          case \"usedStereotypes\":\n            used = readCapabilityCounts(input);\n            break;\n\n          default:\n            input.skipValue();\n            break;\n        }\n      }\n\n      input.endObject();\n\n      return new NodeSummary(nodeId, uri, up, maxSessionCount, stereotypes, used);\n    }\n\n    private static Map<Capabilities, Integer> readCapabilityCounts(JsonInput input) {\n      Map<Capabilities, Integer> toReturn = new HashMap<>();\n\n      input.beginArray();\n      while (input.hasNext()) {\n        Capabilities caps = null;\n        int count = 0;\n        input.beginObject();\n        while (input.hasNext()) {\n          switch (input.nextName()) {\n            case \"capabilities\":\n              caps = input.read(Capabilities.class);\n              break;\n\n            case \"count\":\n              count = input.nextNumber().intValue();\n              break;\n\n            default:\n              input.skipValue();\n              break;\n          }\n        }\n        input.endObject();\n\n        toReturn.put(caps, count);\n      }\n      input.endArray();\n\n      return toReturn;\n    }\n  }\n}\n", "idx": 1, "id": 17772, "msg": "Prefer a human-readable string rather than querying specific technologies. How would I indicate a session is running on BrowserStack? Or some custom thing?", "proj": "SeleniumHQ-selenium", "lang": "py"}
{"patch": "@@ -117,6 +117,19 @@ module Faker\n         regexify(bothify(fetch(key)))\n       end\n \n+      def mercosur_license_plate(legacy_state_abreviation = NOT_GIVEN, state_abreviation: '')\n+        key = 'vehicle.mercosur_license_plate'\n+        if legacy_state_abreviation != NOT_GIVEN\n+          warn_with_uplevel \"Passing `state_abreviation` with the 1st argument of `Vehicle.mercosur_license_plate` is deprecated. Use keyword argument like `Vehicle.mercosur_license_plate(state_abreviation: ...)` instead.\", uplevel: 1\n+          state_abreviation = legacy_state_abreviation\n+        end\n+        \n+        return regexify(bothify(fetch(key))) if state_abreviation.empty?\n+\n+        key = key + '.by_state.' + state_abreviation\n+        regexify(bothify(fetch(key)))\n+      end\n+\n       def singapore_license_plate\n         key = 'vehicle.license_plate'\n         plate_number = regexify(bothify(fetch(key)))", "y": 1, "oldf": "# frozen_string_literal: true\n\nmodule Faker\n  class Vehicle < Base\n    flexible :vehicle\n\n    MILEAGE_MIN = 10_000\n    MILEAGE_MAX = 90_000\n    VIN_LETTERS = 'ABCDEFGHJKLMNPRSTUVWXYZ'\n    VIN_MAP = '0123456789X'\n    VIN_WEIGHTS = '8765432X098765432'\n    VIN_REGEX = /^[A-Z0-9]{3}[A-Z0-9]{5}[A-Z0-9]{1}[A-Z0-9]{1}[A-Z0-0]{1}[A-Z0-9]{1}\\d{5}$/\n    SG_CHECKSUM_WEIGHTS = [3, 14, 2, 12, 2, 11, 1].freeze\n    SG_CHECKSUM_CHARS = 'AYUSPLJGDBZXTRMKHEC'\n\n    class << self\n      def vin\n        regexify(VIN_REGEX)\n      end\n\n      def manufacture\n        fetch('vehicle.manufacture')\n      end\n\n      def make\n        fetch('vehicle.makes')\n      end\n\n      def model(legacy_make_of_model = NOT_GIVEN, make_of_model: '')\n        if legacy_make_of_model != NOT_GIVEN\n          warn_with_uplevel 'Passing `make_of_model` with the 1st argument of `Vehicle.model` is deprecated. Use keyword argument like `Vehicle.model(make_of_model: ...)` instead.', uplevel: 1\n          make_of_model = legacy_make_of_model\n        end\n\n        return fetch(\"vehicle.models_by_make.#{make}\") if make_of_model.empty?\n\n        fetch(\"vehicle.models_by_make.#{make_of_model}\")\n      end\n\n      def make_and_model\n        m = make\n\n        \"#{m} #{model(make_of_model: m)}\"\n      end\n\n      def style\n        fetch('vehicle.styles')\n      end\n\n      def color\n        fetch('vehicle.colors')\n      end\n\n      def transmission\n        fetch('vehicle.transmissions')\n      end\n\n      def drive_type\n        fetch('vehicle.drive_types')\n      end\n\n      def fuel_type\n        fetch('vehicle.fuel_types')\n      end\n\n      def car_type\n        fetch('vehicle.car_types')\n      end\n\n      def engine\n        \"#{sample(fetch_all('vehicle.doors'))} #{fetch('vehicle.cylinder_engine')}\"\n      end\n\n      alias engine_size engine\n\n      def car_options\n        Array.new(rand(5...10)) { fetch('vehicle.car_options') }\n      end\n\n      def standard_specs\n        Array.new(rand(5...10)) { fetch('vehicle.standard_specs') }\n      end\n\n      def doors\n        sample(fetch_all('vehicle.doors'))\n      end\n      alias door_count doors\n\n      def year\n        Faker::Time.backward(days: rand_in_range(365, 5475), period: :all, format: '%Y').to_i\n      end\n\n      def mileage(legacy_min = NOT_GIVEN, legacy_max = NOT_GIVEN, min: MILEAGE_MIN, max: MILEAGE_MAX)\n        if legacy_min != NOT_GIVEN\n          warn_with_uplevel 'Passing `min` with the 1st argument of `Vehicle.mileage` is deprecated. Use keyword argument like `Vehicle.mileage(min: ...)` instead.', uplevel: 1\n          min = legacy_min\n        end\n        if legacy_max != NOT_GIVEN\n          warn_with_uplevel 'Passing `max` with the 2nd argument of `Vehicle.mileage` is deprecated. Use keyword argument like `Vehicle.mileage(max: ...)` instead.', uplevel: 1\n          max = legacy_max\n        end\n\n        rand_in_range(min, max)\n      end\n\n      alias kilometrage mileage\n\n      def license_plate(legacy_state_abreviation = NOT_GIVEN, state_abreviation: '')\n        if legacy_state_abreviation != NOT_GIVEN\n          warn_with_uplevel 'Passing `state_abreviation` with the 1st argument of `Vehicle.license_plate` is deprecated. Use keyword argument like `Vehicle.license_plate(state_abreviation: ...)` instead.', uplevel: 1\n          state_abreviation = legacy_state_abreviation\n        end\n\n        return regexify(bothify(fetch('vehicle.license_plate'))) if state_abreviation.empty?\n\n        key = 'vehicle.license_plate_by_state.' + state_abreviation\n        regexify(bothify(fetch(key)))\n      end\n\n      def singapore_license_plate\n        key = 'vehicle.license_plate'\n        plate_number = regexify(bothify(fetch(key)))\n        \"#{plate_number}#{singapore_checksum(plate_number)}\"\n      end\n\n      private\n\n      def first_eight(number)\n        return number[0...8] unless number.nil?\n\n        regexify(VIN_REGEX)\n      end\n      alias last_eight first_eight\n\n      def calculate_vin_check_digit(vin)\n        sum = 0\n\n        vin.each_char.with_index do |c, i|\n          n = vin_char_to_number(c).to_i\n          weight = VIN_WEIGHTS[i].to_i\n          sum += weight * n\n        end\n\n        mod = sum % 11\n        mod == 10 ? 'X' : mod\n      end\n\n      def vin_char_to_number(char)\n        index = VIN_LETTERS.split('').index(char)\n\n        return char.to_i if index.nil?\n\n        VIN_MAP[index]\n      end\n\n      def singapore_checksum(plate_number)\n        padded_alphabets = format('%3s', plate_number[/^[A-Z]+/]).tr(' ', '-').split('')\n        padded_digits = format('%04d', plate_number[/\\d+/]).split('').map(&:to_i)\n        sum = [*padded_alphabets, *padded_digits].each_with_index.reduce(0) do |memo, (char, i)|\n          value = char.is_a?(Integer) ? char : char.ord - 64\n          memo + (SG_CHECKSUM_WEIGHTS[i] * value)\n        end\n\n        SG_CHECKSUM_CHARS.split('')[sum % 19]\n      end\n    end\n  end\nend\n", "idx": 1, "id": 9317, "msg": "Not sure if the `legacy_state_abreviation` is required in this particular case, since there was no previous implementation of this method, so not including it wouldn't be a breaking change.", "proj": "faker-ruby-faker", "lang": "rb"}
{"patch": "@@ -1258,7 +1258,6 @@ allow:\n \t\t\t.mark = seen_mark,\n \t\t};\n \t\tfwd_fib_set(&fwd, fib);\n-\t\tfwd_fib_set_flags(&fwd, fib_flags);\n \t\treturn fwd;\n \t}\n ", "y": 0, "oldf": "// Project Calico BPF dataplane programs.\n// Copyright (c) 2020 Tigera, Inc. All rights reserved.\n//\n// This program is free software; you can redistribute it and/or modify\n// it under the terms of the GNU General Public License as published by\n// the Free Software Foundation; either version 2 of the License, or\n// (at your option) any later version.\n//\n// This program is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU General Public License for more details.\n//\n// You should have received a copy of the GNU General Public License along\n// with this program; if not, write to the Free Software Foundation, Inc.,\n// 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n\n#include <asm/types.h>\n#include <linux/bpf.h>\n#include <linux/pkt_cls.h>\n#include <linux/ip.h>\n#include <linux/tcp.h>\n#include <linux/icmp.h>\n#include <linux/in.h>\n#include <linux/udp.h>\n#include <linux/if_ether.h>\n#include <iproute2/bpf_elf.h>\n#include <stdbool.h>\n#include <stdint.h>\n#include <stddef.h>\n\n#include \"bpf.h\"\n#include \"log.h\"\n#include \"skb.h\"\n#include \"policy.h\"\n#include \"conntrack.h\"\n#include \"nat.h\"\n#include \"routes.h\"\n#include \"jump.h\"\n#include \"reasons.h\"\n#include \"icmp.h\"\n\n#ifndef CALI_FIB_LOOKUP_ENABLED\n#define CALI_FIB_LOOKUP_ENABLED true\n#endif\n\n#ifndef CALI_DROP_WORKLOAD_TO_HOST\n#define CALI_DROP_WORKLOAD_TO_HOST false\n#endif\n\n#ifdef CALI_DEBUG_ALLOW_ALL\n\n/* If we want to just compile the code without defining any policies and to\n * avoid compiling out code paths that are not reachable if traffic is denied,\n * we can compile it with allow all\n */\nstatic CALI_BPF_INLINE enum calico_policy_result execute_policy_norm(struct __sk_buff *skb,\n\t\t\t\t__u8 ip_proto, __u32 saddr, __u32 daddr, __u16 sport, __u16 dport)\n{\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wunused-label\"\n\n\tRULE_START(0);\n\tRULE_END(0, allow);\n\n\treturn CALI_POL_NO_MATCH;\ndeny:\n\treturn CALI_POL_DENY;\nallow:\n\treturn CALI_POL_ALLOW;\n#pragma clang diagnostic pop\n}\n#else\n\nstatic CALI_BPF_INLINE enum calico_policy_result execute_policy_norm(struct __sk_buff *skb,\n\t\t\t\t__u8 ip_proto, __u32 saddr, __u32 daddr, __u16 sport, __u16 dport)\n{\n#pragma clang diagnostic push\n#pragma clang diagnostic ignored \"-Wunused-label\"\n\n\tRULE_START(0);\n\tRULE_END(0, deny);\n\n\treturn CALI_POL_NO_MATCH;\ndeny:\n\treturn CALI_POL_DENY;\nallow:\n\treturn CALI_POL_ALLOW;\n#pragma clang diagnostic pop\n}\n\n#endif /* CALI_DEBUG_ALLOW_ALL */\n\n__attribute__((section(\"1/0\")))\nint calico_tc_norm_pol_tail(struct __sk_buff *skb)\n{\n\tCALI_DEBUG(\"Entering normal policy tail call\\n\");\n\n\t__u32 key = 0;\n\tstruct cali_tc_state *state = cali_v4_state_lookup_elem(&key);\n\tif (!state) {\n\t        CALI_DEBUG(\"State map lookup failed: DROP\\n\");\n\t        goto deny;\n\t}\n\n\tstate->pol_rc = execute_policy_norm(skb, state->ip_proto, state->ip_src,\n\t\t\t\t\t    state->ip_dst, state->sport, state->dport);\n\n\tbpf_tail_call(skb, &cali_jump, 1);\n\tCALI_DEBUG(\"Tail call to post-policy program failed: DROP\\n\");\n\ndeny:\n\treturn TC_ACT_SHOT;\n}\n\nstruct fwd {\n\tint res;\n\tuint32_t mark;\n\tenum calico_reason reason;\n#if FIB_ENABLED\n\tuint32_t fib_flags;\n\tbool fib;\n#endif\n};\n\n#if FIB_ENABLED\n#define fwd_fib(fwd)\t\t\t((fwd)->fib)\n#define fwd_fib_set(fwd, v)\t\t((fwd)->fib = v)\n#define fwd_fib_set_flags(fwd, flags)\t((fwd)->fib_flags = flags)\n#else\n#define fwd_fib(fwd)\tfalse\n#define fwd_fib_set(fwd, v)\n#define fwd_fib_set_flags(fwd, flags)\n#endif\n\nstatic CALI_BPF_INLINE struct fwd calico_tc_skb_accepted(struct __sk_buff *skb,\n\t\t\t\t\t\t\t struct iphdr *ip_header,\n\t\t\t\t\t\t\t struct cali_tc_state *state,\n\t\t\t\t\t\t\t struct calico_nat_dest *nat_dest);\n\nstatic CALI_BPF_INLINE int skb_nat_l4_csum_ipv4(struct __sk_buff *skb, size_t off,\n\t\t\t\t\t\t__be32 ip_from, __be32 ip_to,\n\t\t\t\t\t\t__u16 port_from, __u16 port_to,\n\t\t\t\t\t\tuint64_t flags)\n{\n\tint ret = 0;\n\n\tif (ip_from != ip_to) {\n\t\tCALI_DEBUG(\"L4 checksum update (csum is at %d) IP from %x to %x\\n\", off,\n\t\t\t\tbe32_to_host(ip_from), be32_to_host(ip_to));\n\t\tret = bpf_l4_csum_replace(skb, off, ip_from, ip_to, flags | BPF_F_PSEUDO_HDR | 4);\n\t\tCALI_DEBUG(\"bpf_l4_csum_replace(IP): %d\\n\", ret);\n\t}\n\tif (port_from != port_to) {\n\t\tCALI_DEBUG(\"L4 checksum update (csum is at %d) port from %d to %d\\n\",\n\t\t\t\toff, be16_to_host(port_from), be16_to_host(port_to));\n\t\tint rc = bpf_l4_csum_replace(skb, off, port_from, port_to, flags | 2);\n\t\tCALI_DEBUG(\"bpf_l4_csum_replace(port): %d\\n\", rc);\n\t\tret |= rc;\n\t}\n\n\treturn ret;\n}\n\nstatic CALI_BPF_INLINE int forward_or_drop(struct __sk_buff *skb,\n\t\t\t\t\t   struct cali_tc_state *state,\n\t\t\t\t\t   struct fwd *fwd)\n{\n\tint rc = fwd->res;\n\tenum calico_reason reason = fwd->reason;\n\n\tif (rc == TC_ACT_SHOT) {\n\t\tgoto deny;\n\t}\n\n\tif (rc == CALI_RES_REDIR_IFINDEX) {\n\t\tint redir_flags = 0;\n\t\tif  (CALI_F_FROM_HOST) {\n\t\t\tredir_flags = BPF_F_INGRESS;\n\t\t}\n\n\t\t/* Revalidate the access to the packet */\n\t\tif ((void *)(long)skb->data + sizeof(struct ethhdr) > (void *)(long)skb->data_end) {\n\t\t\treason = CALI_REASON_SHORT;\n\t\t\tgoto deny;\n\t\t}\n\n\t\t/* Swap the MACs as we are turning it back */\n\t\tstruct ethhdr *eth_hdr = (void *)(long)skb->data;\n\t\tunsigned char mac[ETH_ALEN];\n\t\t__builtin_memcpy(mac, &eth_hdr->h_dest, ETH_ALEN);\n\t\t__builtin_memcpy(&eth_hdr->h_dest, &eth_hdr->h_source, ETH_ALEN);\n\t\t__builtin_memcpy(&eth_hdr->h_source, mac, ETH_ALEN);\n\n\t\trc = bpf_redirect(skb->ifindex, redir_flags);\n\t\tif (rc == TC_ACT_REDIRECT) {\n\t\t\tCALI_DEBUG(\"Redirect to the same interface (%d) succeeded\\n\", skb->ifindex);\n\t\t\tgoto skip_fib;\n\t\t}\n\n\t\tCALI_DEBUG(\"Redirect to the same interface (%d) failed\\n\", skb->ifindex);\n\t\tgoto deny;\n\t}\n\n#if FIB_ENABLED\n\t// Try a short-circuit FIB lookup.\n\tif (fwd_fib(fwd)) {\n\t\t/* XXX we might include the tot_len in the fwd, set it once when\n\t\t * we get the ip_header the first time and only adjust the value\n\t\t * when we modify the packet - to avoid geting the header here\n\t\t * again - it is simpler though.\n\t\t */\n\t\tif (skb_too_short(skb)) {\n\t\t\treason = CALI_REASON_SHORT;\n\t\t\tCALI_DEBUG(\"Too short\\n\");\n\t\t\tgoto deny;\n\t\t}\n\n\t\tstruct iphdr *ip_header = skb_iphdr(skb);\n\t\tstruct bpf_fib_lookup fib_params = {\n\t\t\t.family = 2, /* AF_INET */\n\t\t\t.tot_len = be16_to_host(ip_header->tot_len),\n\t\t\t.ifindex = skb->ingress_ifindex,\n\t\t\t.l4_protocol = state->ip_proto,\n\t\t\t.sport = host_to_be16(state->sport),\n\t\t\t.dport = host_to_be16(state->dport),\n\t\t};\n\n\t\t/* set the ipv4 here, otherwise the ipv4/6 unions do not get\n\t\t * zeroed properly\n\t\t */\n\t\tfib_params.ipv4_src = state->ip_src;\n\t\tfib_params.ipv4_dst = state->ip_dst;\n\n\t\tCALI_DEBUG(\"FIB family=%d\\n\", fib_params.family);\n\t\tCALI_DEBUG(\"FIB tot_len=%d\\n\", fib_params.tot_len);\n\t\tCALI_DEBUG(\"FIB ifindex=%d\\n\", fib_params.ifindex);\n\t\tCALI_DEBUG(\"FIB l4_protocol=%d\\n\", fib_params.l4_protocol);\n\t\tCALI_DEBUG(\"FIB sport=%d\\n\", be16_to_host(fib_params.sport));\n\t\tCALI_DEBUG(\"FIB dport=%d\\n\", be16_to_host(fib_params.dport));\n\t\tCALI_DEBUG(\"FIB ipv4_src=%x\\n\", be32_to_host(fib_params.ipv4_src));\n\t\tCALI_DEBUG(\"FIB ipv4_dst=%x\\n\", be32_to_host(fib_params.ipv4_dst));\n\n\t\tCALI_DEBUG(\"Traffic is towards the host namespace, doing Linux FIB lookup\\n\");\n\t\trc = bpf_fib_lookup(skb, &fib_params, sizeof(fib_params), fwd->fib_flags);\n\t\tif (rc == 0) {\n\t\t\tCALI_DEBUG(\"FIB lookup succeeded\\n\");\n\n\t\t\t/* Since we are going to short circuit the IP stack on\n\t\t\t * forward, check if TTL is still alive. If not, let the\n\t\t\t * IP stack handle it. It was approved by policy, so it\n\t\t\t * is safe.\n\t\t\t */\n\t\t\tif ip_ttl_exceeded(ip_header) {\n\t\t\t\trc = TC_ACT_UNSPEC;\n\t\t\t\tgoto cancel_fib;\n\t\t\t}\n\n\t\t\t// Update the MACs.  NAT may have invalidated pointer into the packet so need to\n\t\t\t// revalidate.\n\t\t\tif ((void *)(long)skb->data + sizeof(struct ethhdr) > (void *)(long)skb->data_end) {\n\t\t\t\treason = CALI_REASON_SHORT;\n\t\t\t\tgoto deny;\n\t\t\t}\n\t\t\tstruct ethhdr *eth_hdr = (void *)(long)skb->data;\n\t\t\t__builtin_memcpy(&eth_hdr->h_source, fib_params.smac, sizeof(eth_hdr->h_source));\n\t\t\t__builtin_memcpy(&eth_hdr->h_dest, fib_params.dmac, sizeof(eth_hdr->h_dest));\n\n\t\t\t// Redirect the packet.\n\t\t\tCALI_DEBUG(\"Got Linux FIB hit, redirecting to iface %d.\\n\", fib_params.ifindex);\n\t\t\trc = bpf_redirect(fib_params.ifindex, 0);\n\t\t\t/* now we know we will bypass IP stack and ip->ttl > 1, decrement it! */\n\t\t\tif (rc == TC_ACT_REDIRECT) {\n\t\t\t\tip_dec_ttl(ip_header);\n\t\t\t}\n\t\t} else if (rc < 0) {\n\t\t\tCALI_DEBUG(\"FIB lookup failed (bad input): %d.\\n\", rc);\n\t\t\trc = TC_ACT_UNSPEC;\n\t\t} else {\n\t\t\tCALI_DEBUG(\"FIB lookup failed (FIB problem): %d.\\n\", rc);\n\t\t\trc = TC_ACT_UNSPEC;\n\t\t}\n\t}\n\ncancel_fib:\n#endif /* FIB_ENABLED */\n\nskip_fib:\n\n\tif (CALI_F_TO_HOST) {\n\t\t/* If we received the packet from the tunnel and we forward it to a\n\t\t * workload we need to skip RPF check since there might be a better path\n\t\t * for the packet if the host has multiple ifaces and might get dropped.\n\t\t *\n\t\t * XXX We should check ourselves that we got our tunnel packets only from\n\t\t * XXX those devices where we expect them before we even decap.\n\t\t */\n\t\tif (CALI_F_FROM_HEP && state->tun_ip != 0) {\n\t\t\tfwd->mark = CALI_SKB_MARK_SKIP_RPF;\n\t\t}\n\t\t/* Packet is towards host namespace, mark it so that downstream\n\t\t * programs know that they're not the first to see the packet.\n\t\t */\n\t\tCALI_DEBUG(\"Traffic is towards host namespace, marking with %x.\\n\", fwd->mark);\n\t\t/* FIXME: this ignores the mask that we should be using.\n\t\t * However, if we mask off the bits, then clang spots that it\n\t\t * can do a 16-bit store instead of a 32-bit load/modify/store,\n\t\t * which trips up the validator.\n\t\t */\n\t\tskb->mark = fwd->mark | CALI_SKB_MARK_SEEN; /* make sure that each pkt has SEEN mark */\n\t}\n\n\tif (CALI_LOG_LEVEL >= CALI_LOG_LEVEL_INFO) {\n\t\tuint64_t prog_end_time = bpf_ktime_get_ns();\n\t\tCALI_INFO(\"Final result=ALLOW (%d). Program execution time: %lluns\\n\",\n\t\t\t\trc, prog_end_time-state->prog_start_time);\n\t}\n\n\treturn rc;\n\ndeny:\n\tif (CALI_LOG_LEVEL >= CALI_LOG_LEVEL_INFO) {\n\t\tuint64_t prog_end_time = bpf_ktime_get_ns();\n\t\tCALI_INFO(\"Final result=DENY (%x). Program execution time: %lluns\\n\",\n\t\t\t\treason, prog_end_time-state->prog_start_time);\n\t}\n\n\treturn TC_ACT_SHOT;\n}\n\nstatic CALI_BPF_INLINE int calico_tc(struct __sk_buff *skb)\n{\n\tstruct cali_tc_state state = {};\n\tstruct fwd fwd = {\n\t\t.res = TC_ACT_UNSPEC,\n\t\t.reason = CALI_REASON_UNKNOWN,\n\t};\n\tstruct calico_nat_dest *nat_dest = NULL;\n\tbool nat_lvl1_drop = 0;\n\n\t/* we assume we do FIB and from this point on, we only set it to false\n\t * if we decide not to do it.\n\t */\n\tfwd_fib_set(&fwd, true);\n\n\tif (CALI_LOG_LEVEL >= CALI_LOG_LEVEL_INFO) {\n\t\tstate.prog_start_time = bpf_ktime_get_ns();\n\t}\n\tstate.tun_ip = 0;\n\n#ifdef CALI_SET_SKB_MARK\n\t/* workaround for test since bpftool run cannot set it in context, wont\n\t * be necessary if fixed in kernel\n\t */\n\tskb->mark = CALI_SET_SKB_MARK;\n#endif\n\n\tif (!CALI_F_TO_HOST && skb->mark == CALI_SKB_MARK_BYPASS) {\n\t\tCALI_DEBUG(\"Packet pre-approved by another hook, allow.\\n\");\n\t\tfwd.reason = CALI_REASON_BYPASS;\n\t\tgoto allow;\n\t}\n\n\tstruct iphdr *ip_header;\n\tif (CALI_F_TO_HEP || CALI_F_TO_WEP) {\n\t\tswitch (skb->mark) {\n\t\tcase CALI_SKB_MARK_BYPASS_FWD:\n\t\t\tCALI_DEBUG(\"Packet approved for forward.\\n\");\n\t\t\tfwd.reason = CALI_REASON_BYPASS;\n\t\t\tgoto allow;\n\t\tcase CALI_SKB_MARK_BYPASS_FWD_SRC_FIXUP:\n\t\t\tCALI_DEBUG(\"Packet approved for forward - src ip fixup\\n\");\n\t\t\tfwd.reason = CALI_REASON_BYPASS;\n\n\t\t\t/* we need to fix up the right src host IP */\n\t\t\tif (skb_too_short(skb)) {\n\t\t\t\tfwd.reason = CALI_REASON_SHORT;\n\t\t\t\tCALI_DEBUG(\"Too short\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\n\t\t\tip_header = skb_iphdr(skb);\n\t\t\t__be32 ip_src = ip_header->saddr;\n\n\t\t\tif (ip_src == HOST_IP) {\n\t\t\t\tCALI_DEBUG(\"src ip fixup not needed %x\\n\", be32_to_host(ip_src));\n\t\t\t\tgoto allow;\n\t\t\t}\n\n\t\t\t/* XXX do a proper CT lookup to find this */\n\t\t\tip_header->saddr = HOST_IP;\n\t\t\tint l3_csum_off = skb_iphdr_offset(skb) + offsetof(struct iphdr, check);\n\n\t\t\tint res = bpf_l3_csum_replace(skb, l3_csum_off, ip_src, HOST_IP, 4);\n\t\t\tif (res) {\n\t\t\t\tfwd.reason = CALI_REASON_CSUM_FAIL;\n\t\t\t\tgoto deny;\n\t\t\t}\n\n\t\t\tgoto allow;\n\t\t}\n\t}\n\n\t// Parse the packet.\n\n\t// TODO Do we need to handle any odd-ball frames here (e.g. with a 0 VLAN header)?\n\tswitch (host_to_be16(skb->protocol)) {\n\tcase ETH_P_IP:\n\t\tbreak;\n\tcase ETH_P_ARP:\n\t\tCALI_DEBUG(\"ARP: allowing packet\\n\");\n\t\tfwd_fib_set(&fwd, false);\n\t\tgoto allow;\n\tcase ETH_P_IPV6:\n\t\tif (CALI_F_WEP) {\n\t\t\tCALI_DEBUG(\"IPv6 from workload: drop\\n\");\n\t\t\treturn TC_ACT_SHOT;\n\t\t} else {\n\t\t\t// FIXME: support IPv6.\n\t\t\tCALI_DEBUG(\"IPv6 on host interface: allow\\n\");\n\t\t\treturn TC_ACT_UNSPEC;\n\t\t}\n\tdefault:\n\t\tif (CALI_F_WEP) {\n\t\t\tCALI_DEBUG(\"Unknown ethertype (%x), drop\\n\", be16_to_host(skb->protocol));\n\t\t\tgoto deny;\n\t\t} else {\n\t\t\tCALI_DEBUG(\"Unknown ethertype on host interface (%x), allow\\n\",\n\t\t\t\t\t\t\t\tbe16_to_host(skb->protocol));\n\t\t\treturn TC_ACT_UNSPEC;\n\t\t}\n\t}\n\n\tif (skb_too_short(skb)) {\n\t\tfwd.reason = CALI_REASON_SHORT;\n\t\tCALI_DEBUG(\"Too short\\n\");\n\t\tgoto deny;\n\t}\n\n\tip_header = skb_iphdr(skb);\n\n\tif (dnat_should_decap() && is_vxlan_tunnel(ip_header)) {\n\t\tstruct udphdr *udp_header = (void*)(ip_header+1);\n\t\t/* decap on host ep only if directly for the node */\n\t\tCALI_DEBUG(\"VXLAN tunnel packet to %x (host IP=%x)\\n\", ip_header->daddr, HOST_IP);\n\t\tif (rt_addr_is_local_host(ip_header->daddr) &&\n\t\t\t\tvxlan_udp_csum_ok(udp_header) &&\n\t\t\t\tvxlan_size_ok(skb, udp_header) &&\n\t\t\t\tvxlan_vni_is_valid(skb, udp_header) &&\n\t\t\t\tvxlan_vni(skb, udp_header) == CALI_VXLAN_VNI) {\n\t\t\tstate.tun_ip = ip_header->saddr;\n\t\t\tCALI_DEBUG(\"vxlan decap\\n\");\n\t\t\tif (vxlan_v4_decap(skb)) {\n\t\t\t\tfwd.reason = CALI_REASON_DECAP_FAIL;\n\t\t\t\tgoto deny;\n\t\t\t}\n\n\t\t\tif (skb_too_short(skb)) {\n\t\t\t\tfwd.reason = CALI_REASON_SHORT;\n\t\t\t\tCALI_DEBUG(\"Too short after VXLAN decap\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\t\t\tip_header = skb_iphdr(skb);\n\n\t\t\tCALI_DEBUG(\"vxlan decap origin %x\\n\", be32_to_host(state.tun_ip));\n\t\t}\n\t}\n\n\t// Drop malformed IP packets\n\tif (ip_header->ihl < 5) {\n\t\tfwd.reason = CALI_REASON_IP_MALFORMED;\n\t\tCALI_DEBUG(\"Drop malformed IP packets\\n\");\n\t\tgoto deny;\n\t} else if (ip_header->ihl > 5) {\n\t\t/* Drop packets with IP options from/to WEP.\n\t\t * Also drop packets with IP options if the dest IP is not host IP\n\t\t */\n\t\tif (CALI_F_WEP || (CALI_F_FROM_HEP && !rt_addr_is_local_host(ip_header->daddr))) {\n\t\t\tfwd.reason = CALI_REASON_IP_OPTIONS;\n\t\t\tCALI_DEBUG(\"Drop packets with IP options\\n\");\n\t\t\tgoto deny;\n\t\t}\n\t\tCALI_DEBUG(\"Allow packets with IP options and dst IP = hostIP\\n\");\n\t\tgoto allow;\n\t}\n\t// Setting all of these up-front to keep the verifier happy.\n\tstruct tcphdr *tcp_header = (void*)(ip_header+1);\n\tstruct udphdr *udp_header = (void*)(ip_header+1);\n\tstruct icmphdr *icmp_header = (void*)(ip_header+1);\n\n\ttc_state_fill_from_iphdr(&state, ip_header);\n\n\tswitch (state.ip_proto) {\n\tcase IPPROTO_TCP:\n\t\t// Re-check buffer space for TCP (has larger headers than UDP).\n\t\tif (!skb_has_data_after(skb, ip_header, sizeof(struct tcphdr))) {\n\t\t\tCALI_DEBUG(\"Too short for TCP: DROP\\n\");\n\t\t\tgoto deny;\n\t\t}\n\t\tstate.sport = be16_to_host(tcp_header->source);\n\t\tstate.dport = be16_to_host(tcp_header->dest);\n\t\tCALI_DEBUG(\"TCP; ports: s=%d d=%d\\n\", state.sport, state.dport);\n\t\tbreak;\n\tcase IPPROTO_UDP:\n\t\tstate.sport = be16_to_host(udp_header->source);\n\t\tstate.dport = be16_to_host(udp_header->dest);\n\t\tCALI_DEBUG(\"UDP; ports: s=%d d=%d\\n\", state.sport, state.dport);\n\t\tbreak;\n\tcase IPPROTO_ICMP:\n\t\ticmp_header = (void*)(ip_header+1);\n\t\tCALI_DEBUG(\"ICMP; type=%d code=%d\\n\",\n\t\t\t\ticmp_header->type, icmp_header->code);\n\t\tbreak;\n\tcase 4:\n\t\t// IPIP\n\t\tif (CALI_F_HEP) {\n\t\t\t// TODO IPIP whitelist.\n\t\t\tCALI_DEBUG(\"IPIP: allow\\n\");\n\t\t\tfwd_fib_set(&fwd, false);\n\t\t\tgoto allow;\n\t\t}\n\tdefault:\n\t\tCALI_DEBUG(\"Unknown protocol (%d), unable to extract ports\\n\", (int)state.ip_proto);\n\t}\n\n\tstate.pol_rc = CALI_POL_NO_MATCH;\n\n\tswitch (state.ip_proto) {\n\tcase IPPROTO_TCP:\n\tcase IPPROTO_UDP:\n\tcase IPPROTO_ICMP:\n\t\tbreak;\n\tdefault:\n\t\tif (CALI_F_HEP) {\n\t\t\t// FIXME: allow unknown protocols through on host endpoints.\n\t\t\tgoto allow;\n\t\t}\n\t\t// FIXME non-port based conntrack.\n\t\tgoto deny;\n\t}\n\n\tstruct ct_ctx ct_lookup_ctx = {\n\t\t.skb = skb,\n\t\t.proto\t= state.ip_proto,\n\t\t.src\t= state.ip_src,\n\t\t.sport\t= state.sport,\n\t\t.dst\t= state.ip_dst,\n\t\t.dport\t= state.dport,\n\t\t.tun_ip = state.tun_ip,\n\t};\n\n\tif (state.ip_proto == IPPROTO_TCP) {\n\t\tif (!skb_has_data_after(skb, ip_header, sizeof(struct tcphdr))) {\n\t\t\tCALI_DEBUG(\"Too short for TCP: DROP\\n\");\n\t\t\tgoto deny;\n\t\t}\n\t\ttcp_header = (void*)(ip_header+1);\n\t\tct_lookup_ctx.tcp = tcp_header;\n\t}\n\n\t/* Do conntrack lookup before anything else */\n\tstate.ct_result = calico_ct_v4_lookup(&ct_lookup_ctx);\n\n\t/* check if someone is trying to spoof a tunnel packet */\n\tif (CALI_F_FROM_HEP && ct_result_tun_src_changed(state.ct_result.rc)) {\n\t\tCALI_DEBUG(\"dropping tunnel pkt with changed source node\\n\");\n\t\tgoto deny;\n\t}\n\n\tif (state.ct_result.flags & CALI_CT_FLAG_NAT_OUT) {\n\t\tstate.flags |= CALI_ST_NAT_OUTGOING;\n\t}\n\n\t/* We are possibly past (D)NAT, but that is ok, we need to let the IP\n\t * stack do the RPF check on the source, dest is not importatnt.\n\t */\n\tif (CALI_F_TO_HOST && ct_result_rpf_failed(state.ct_result.rc)) {\n\t\tfwd_fib_set(&fwd, false);\n\t}\n\n\n\t/* skip policy if we get conntrack hit */\n\tif (ct_result_rc(state.ct_result.rc) != CALI_CT_NEW) {\n\t\tgoto skip_policy;\n\t}\n\n\t/* Unlike from WEP where we can do RPF by comparing to calico routing\n\t * info, we must rely in Linux to do it for us when receiving packets\n\t * from outside of the host. We enforce RPF failed on every new flow.\n\t * This will make it to skip fib in calico_tc_skb_accepted()\n\t */\n\tif (CALI_F_FROM_HEP) {\n\t\tct_result_set_flag(state.ct_result.rc, CALI_CT_RPF_FAILED);\n\t}\n\n\t/* No conntrack entry, check if we should do NAT */\n\tnat_dest = calico_v4_nat_lookup2(state.ip_src, state.ip_dst,\n\t\t\t\t\t state.ip_proto, state.dport,\n\t\t\t\t\t state.tun_ip != 0, &nat_lvl1_drop);\n\n\tif (nat_lvl1_drop) {\n\t\tCALI_DEBUG(\"Packet is from an unauthorised source: DROP\\n\");\n\t\tfwd.reason = CALI_REASON_UNAUTH_SOURCE;\n\t\tgoto deny;\n\t}\n\tif (nat_dest != NULL) {\n\t\tstate.post_nat_ip_dst = nat_dest->addr;\n\t\tstate.post_nat_dport = nat_dest->port;\n\t} else {\n\t\tstate.post_nat_ip_dst = state.ip_dst;\n\t\tstate.post_nat_dport = state.dport;\n\t}\n\n\tif (CALI_F_TO_WEP &&\n\t\t\tskb->mark != CALI_SKB_MARK_SEEN &&\n\t\t\tcali_rt_flags_local_host(cali_rt_lookup_flags(state.ip_src))) {\n\t\t/* Host to workload traffic always allowed.  We discount traffic that was\n\t\t * seen by another program since it must have come in via another interface.\n\t\t */\n\t\tCALI_DEBUG(\"Packet is from the host: ACCEPT\\n\");\n\t\tstate.pol_rc = CALI_POL_ALLOW;\n\t\tgoto skip_policy;\n\t}\n\n\tif (CALI_F_FROM_WEP) {\n\t\t/* Do RPF check since it's our responsibility to police that. */\n\t\tCALI_DEBUG(\"Workload RPF check src=%x skb iface=%d.\\n\",\n\t\t\t\tbe32_to_host(state.ip_src), skb->ifindex);\n\t\tstruct cali_rt *r = cali_rt_lookup(state.ip_src);\n\t\tif (!r) {\n\t\t\tCALI_INFO(\"Workload RPF fail: missing route.\\n\");\n\t\t\tgoto deny;\n\t\t}\n\t\tif (!cali_rt_flags_local_workload(r->flags)) {\n\t\t\tCALI_INFO(\"Workload RPF fail: not a local workload.\\n\");\n\t\t\tgoto deny;\n\t\t}\n\t\tif (r->if_index != skb->ifindex) {\n\t\t\tCALI_INFO(\"Workload RPF fail skb iface (%d) != route iface (%d)\\n\",\n\t\t\t\t\tskb->ifindex, r->if_index);\n\t\t\tgoto deny;\n\t\t}\n\n\t\t// Check whether the workload needs outgoing NAT to this address.\n\t\tif (r->flags & CALI_RT_NAT_OUT) {\n\t\t\tif (!(cali_rt_lookup_flags(state.post_nat_ip_dst) & CALI_RT_IN_POOL)) {\n\t\t\t\tCALI_DEBUG(\"Source is in NAT-outgoing pool \"\n\t\t\t\t\t   \"but dest is not, need to SNAT.\\n\");\n\t\t\t\tstate.flags |= CALI_ST_NAT_OUTGOING;\n\t\t\t}\n\t\t}\n\t}\n\t/* icmp_type and icmp_code share storage with the ports; now we've used\n\t * the ports set to 0 to do the conntrack lookup, we can set the ICMP fields\n\t * for policy.\n\t */\n\tif (state.ip_proto == IPPROTO_ICMP) {\n\t\tstate.icmp_type = icmp_header->type;\n        \tstate.icmp_code = icmp_header->code;\n\t}\n\n\n\t// Set up an entry in the state map and then jump to the normal policy program.\n\tint key = 0;\n\tstruct cali_tc_state *map_state = cali_v4_state_lookup_elem(&key);\n\tif (!map_state) {\n\t\t// Shouldn't be possible; the map is pre-allocated.\n\t\tCALI_INFO(\"State map lookup failed: DROP\\n\");\n\t\tgoto deny;\n\t}\n\n\tstate.pol_rc = CALI_POL_NO_MATCH;\n\tif (nat_dest) {\n\t\tstate.nat_dest.addr = nat_dest->addr;\n\t\tstate.nat_dest.port = nat_dest->port;\n\t} else {\n\t\tstate.nat_dest.addr = 0;\n\t\tstate.nat_dest.port = 0;\n\t}\n\n\t*map_state = state;\n\n\tif (CALI_F_HEP) {\n\t\t/* We don't support host-endpoint policy yet, skip straight to\n\t\t * the epilogue program.\n\t\t * FIXME we really want to just call calico_tc_skb_accepted()\n\t\t * here but that runs out of stack space.\n\t\t */\n\t\tmap_state->pol_rc = CALI_POL_ALLOW;\n\t\tbpf_tail_call(skb, &cali_jump, 1);\n\t\tCALI_DEBUG(\"Tail call to epilogue program failed: ALLOW\\n\");\n\t\treturn TC_ACT_UNSPEC;\n\t}\n\n\tCALI_DEBUG(\"About to jump to policy program; lack of further \"\n\t\t\t\"logs means policy dropped the packet...\\n\");\n\tbpf_tail_call(skb, &cali_jump, 0);\n\tCALI_DEBUG(\"Tail call to policy program failed: DROP\\n\");\n\treturn TC_ACT_SHOT;\n\nskip_policy:\n\tfwd = calico_tc_skb_accepted(skb, ip_header, &state, nat_dest);\n\nallow:\nfinalize:\n\treturn forward_or_drop(skb, &state, &fwd);\ndeny:\n\tfwd.res = TC_ACT_SHOT;\n\tgoto finalize;\n}\n\n__attribute__((section(\"1/1\")))\nint calico_tc_skb_accepted_entrypoint(struct __sk_buff *skb)\n{\n\tCALI_DEBUG(\"Entering calico_tc_skb_accepted_entrypoint\\n\");\n\tstruct iphdr *ip_header = NULL;\n\tif (skb_too_short(skb)) {\n\t\tCALI_DEBUG(\"Too short\\n\");\n\t\tgoto deny;\n\t}\n\tip_header = skb_iphdr(skb);\n\t__u32 key = 0;\n\tstruct cali_tc_state *state = bpf_map_lookup_elem(&cali_v4_state, &key);\n\tif (!state) {\n\t\tCALI_DEBUG(\"State map lookup failed: DROP\\n\");\n\t\tgoto deny;\n\t}\n\n\tstruct calico_nat_dest *nat_dest = NULL;\n\tstruct calico_nat_dest nat_dest_2 = {\n\t\t.addr=state->nat_dest.addr,\n\t\t.port=state->nat_dest.port,\n\t};\n\tif (state->nat_dest.addr != 0) {\n\t\tnat_dest = &nat_dest_2;\n\t}\n\n\tstruct fwd fwd = calico_tc_skb_accepted(skb, ip_header, state, nat_dest);\n\treturn forward_or_drop(skb, state, &fwd);\n\ndeny:\n\treturn TC_ACT_SHOT;\n}\n\nstatic CALI_BPF_INLINE struct fwd calico_tc_skb_accepted(struct __sk_buff *skb,\n\t\t\t\t\t\t\t struct iphdr *ip_header,\n\t\t\t\t\t\t\t struct cali_tc_state *state,\n\t\t\t\t\t\t\t struct calico_nat_dest *nat_dest)\n{\n\tCALI_DEBUG(\"Entering calico_tc_skb_accepted\\n\");\n\n\tenum calico_reason reason = CALI_REASON_UNKNOWN;\n\tint rc = TC_ACT_UNSPEC;\n\tbool fib = false;\n\tstruct ct_ctx ct_nat_ctx = {};\n\tint ct_rc = ct_result_rc(state->ct_result.rc);\n\tbool ct_related = ct_result_is_related(state->ct_result.rc);\n\tuint32_t seen_mark;\n\tsize_t l4_csum_off = 0, l3_csum_off;\n\tuint32_t fib_flags = 0;\n\n\tCALI_DEBUG(\"src=%x dst=%x\\n\", be32_to_host(state->ip_src), be32_to_host(state->ip_dst));\n\tCALI_DEBUG(\"post_nat=%x:%d\\n\", be32_to_host(state->post_nat_ip_dst), state->post_nat_dport);\n\tCALI_DEBUG(\"tun_ip=%x\\n\", state->tun_ip);\n\tCALI_DEBUG(\"pol_rc=%d\\n\", state->pol_rc);\n\tCALI_DEBUG(\"sport=%d\\n\", state->sport);\n\tCALI_DEBUG(\"flags=%x\\n\", state->flags);\n\tCALI_DEBUG(\"ct_rc=%d\\n\", ct_rc);\n\tCALI_DEBUG(\"ct_related=%d\\n\", ct_related);\n\n\t// Set the dport to 0, to make sure conntrack entries for icmp is proper as we use\n\t// dport to hold icmp type and code\n\tif (state->ip_proto == IPPROTO_ICMP) {\n\t\tstate->dport = 0;\n\t}\n\n\tif (CALI_F_FROM_WEP && (state->flags & CALI_ST_NAT_OUTGOING)) {\n\t\tseen_mark = CALI_SKB_MARK_NAT_OUT;\n\t} else {\n\t\t/* XXX we do it here again because doing it in one place only\n\t\t * XXX in calico_tc() irritates the verifier :'(\n\t\t */\n\t\tif (!CALI_F_TO_HOST || !ct_result_rpf_failed(state->ct_result.rc)) {\n\t\t\tfib = true;\n\t\t}\n\t\tseen_mark = CALI_SKB_MARK_SEEN;\n\t}\n\n\t/* We check the ttl here to avoid needing complicated handling of\n\t * related trafic back from the host if we let the host to handle it.\n\t */\n\tCALI_DEBUG(\"ip->ttl %d\\n\", ip_header->ttl);\n\tif (ip_ttl_exceeded(ip_header)) {\n\t\tswitch (ct_rc){\n\t\tcase CALI_CT_NEW:\n\t\t\tif (nat_dest) {\n\t\t\t\tgoto icmp_ttl_exceeded;\n\t\t\t}\n\t\t\tbreak;\n\t\tcase CALI_CT_ESTABLISHED_DNAT:\n\t\tcase CALI_CT_ESTABLISHED_SNAT:\n\t\t\tgoto icmp_ttl_exceeded;\n\t\t}\n\t}\n\n\tl3_csum_off = skb_iphdr_offset(skb) +  offsetof(struct iphdr, check);\n\n\tif (ct_related) {\n\t\tif (ip_header->protocol == IPPROTO_ICMP) {\n\t\t\tstruct icmphdr *icmp;\n\t\t\tbool outer_ip_snat;\n\n\t\t\t/* if we do SNAT ... */\n\t\t\touter_ip_snat = ct_rc == CALI_CT_ESTABLISHED_SNAT;\n\t\t\t/* ... there is a return path to the tunnel ... */\n\t\t\touter_ip_snat = outer_ip_snat && state->ct_result.tun_ip;\n\t\t\t/* ... and should do encap and it is not DSR or it is leaving host\n\t\t\t * and either DSR from WEP or originated at host ... */\n\t\t\touter_ip_snat = outer_ip_snat &&\n\t\t\t\t((dnat_return_should_encap() && !CALI_F_DSR) ||\n\t\t\t\t (CALI_F_TO_HEP &&\n\t\t\t\t  ((CALI_F_DSR && skb_seen(skb)) || !skb_seen(skb))));\n\n\t\t\t/* ... then fix the outer header IP first */\n\t\t\tif (outer_ip_snat) {\n\t\t\t\tip_header->saddr = state->ct_result.nat_ip;\n\t\t\t\tint res = bpf_l3_csum_replace(skb, l3_csum_off,\n\t\t\t\t\t\tstate->ip_src, state->ct_result.nat_ip, 4);\n\t\t\t\tif (res) {\n\t\t\t\t\treason = CALI_REASON_CSUM_FAIL;\n\t\t\t\t\tgoto deny;\n\t\t\t\t}\n\t\t\t\tCALI_DEBUG(\"ICMP related: outer IP SNAT to %x\\n\",\n\t\t\t\t\t\tbe32_to_host(state->ct_result.nat_ip));\n\t\t\t}\n\n\t\t\tif (!icmp_skb_get_hdr(skb, &icmp)) {\n\t\t\t\tCALI_DEBUG(\"Ooops, we already passed one such a check!!!\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\n\t\t\tl3_csum_off += sizeof(*ip_header) + sizeof(*icmp);\n\t\t\tip_header = (struct iphdr *)(icmp + 1); /* skip to inner ip */\n\n\t\t\t/* flip the direction, we need to reverse the original packet */\n\t\t\tswitch (ct_rc) {\n\t\t\tcase CALI_CT_ESTABLISHED_SNAT:\n\t\t\t\t/* handle the DSR case, see CALI_CT_ESTABLISHED_SNAT where nat is done */\n\t\t\t\tif (dnat_return_should_encap() && state->ct_result.tun_ip) {\n\t\t\t\t\tif (CALI_F_DSR) {\n\t\t\t\t\t\t/* SNAT will be done after routing, when leaving HEP */\n\t\t\t\t\t\tCALI_DEBUG(\"DSR enabled, skipping SNAT + encap\\n\");\n\t\t\t\t\t\tgoto allow;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tct_rc = CALI_CT_ESTABLISHED_DNAT;\n\t\t\t\tbreak;\n\t\t\tcase CALI_CT_ESTABLISHED_DNAT:\n\t\t\t\tif (CALI_F_FROM_HEP && state->tun_ip && ct_result_np_node(state->ct_result)) {\n\t\t\t\t\t/* Packet is returning from a NAT tunnel, just forward it. */\n\t\t\t\t\tseen_mark = CALI_SKB_MARK_BYPASS_FWD;\n\t\t\t\t\tCALI_DEBUG(\"ICMP related returned from NAT tunnel\\n\");\n\t\t\t\t\tgoto allow;\n\t\t\t\t}\n\t\t\t\tct_rc = CALI_CT_ESTABLISHED_SNAT;\n\t\t\t\tbreak;\n\t\t\t}\n\t\t}\n\t}\n\n\tstruct tcphdr *tcp_header = (void*)(ip_header+1);\n\tstruct udphdr *udp_header = (void*)(ip_header+1);\n\n\t__u8 ihl = ip_header->ihl * 4;\n\n\tint res = 0;\n\tbool encap_needed = false;\n\n\tif (state->ip_proto == IPPROTO_ICMP && ct_related) {\n\t\t/* do not fix up embedded L4 checksum for related ICMP */\n\t} else {\n\t\tswitch (ip_header->protocol) {\n\t\tcase IPPROTO_TCP:\n\t\t\tl4_csum_off = skb_l4hdr_offset(skb, ihl) + offsetof(struct tcphdr, check);\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tl4_csum_off = skb_l4hdr_offset(skb, ihl) + offsetof(struct udphdr, check);\n\t\t\tbreak;\n\t\t}\n\t}\n\n\tswitch (ct_rc){\n\tcase CALI_CT_NEW:\n\t\tswitch (state->pol_rc) {\n\t\tcase CALI_POL_NO_MATCH:\n\t\t\tCALI_DEBUG(\"Implicitly denied by normal policy: DROP\\n\");\n\t\t\tgoto deny;\n\t\tcase CALI_POL_DENY:\n\t\t\tCALI_DEBUG(\"Denied by normal policy: DROP\\n\");\n\t\t\tgoto deny;\n\t\tcase CALI_POL_ALLOW:\n\t\t\tCALI_DEBUG(\"Allowed by normal policy: ACCEPT\\n\");\n\t\t}\n\n\t\tif (CALI_F_FROM_WEP &&\n\t\t\t\tCALI_DROP_WORKLOAD_TO_HOST &&\n\t\t\t\tcali_rt_flags_local_host(\n\t\t\t\t\tcali_rt_lookup_flags(state->post_nat_ip_dst))) {\n\t\t\tCALI_DEBUG(\"Workload to host traffic blocked by \"\n\t\t\t\t   \"DefaultEndpointToHostAction: DROP\\n\");\n\t\t\tgoto deny;\n\t\t}\n\n\t\tct_nat_ctx.skb = skb;\n\t\tct_nat_ctx.proto = state->ip_proto;\n\t\tct_nat_ctx.src = state->ip_src;\n\t\tct_nat_ctx.sport = state->sport;\n\t\tct_nat_ctx.dst = state->post_nat_ip_dst;\n\t\tct_nat_ctx.dport = state->post_nat_dport;\n\t\tct_nat_ctx.tun_ip = state->tun_ip;\n\t\tif (state->flags & CALI_ST_NAT_OUTGOING) {\n\t\t\tct_nat_ctx.flags |= CALI_CT_FLAG_NAT_OUT;\n\t\t}\n\n\t\tif (state->ip_proto == IPPROTO_TCP) {\n\t\t\tif (!skb_has_data_after(skb, ip_header, sizeof(struct tcphdr))) {\n\t\t\t\tCALI_DEBUG(\"Too short for TCP: DROP\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\t\t\ttcp_header = (void*)(ip_header+1);\n\t\t\tct_nat_ctx.tcp = tcp_header;\n\t\t}\n\n\t\t// If we get here, we've passed policy.\n\n\t\tif (nat_dest == NULL) {\n\t\t\tif (conntrack_create(&ct_nat_ctx, CT_CREATE_NORMAL)) {\n\t\t\t\tCALI_DEBUG(\"Creating normal conntrack failed\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\t\t\tgoto allow;\n\t\t}\n\n\t\tct_nat_ctx.orig_dst = state->ip_dst;\n\t\tct_nat_ctx.orig_dport = state->dport;\n\t\t/* fall through as DNAT is now established */\n\n\tcase CALI_CT_ESTABLISHED_DNAT:\n\t\t/* align with CALI_CT_NEW */\n\t\tif (ct_rc == CALI_CT_ESTABLISHED_DNAT) {\n\t\t\tif (CALI_F_FROM_HEP && state->tun_ip && ct_result_np_node(state->ct_result)) {\n\t\t\t\t/* Packet is returning from a NAT tunnel,\n\t\t\t\t * already SNATed, just forward it.\n\t\t\t\t */\n\t\t\t\tseen_mark = CALI_SKB_MARK_BYPASS_FWD;\n\t\t\t\tCALI_DEBUG(\"returned from NAT tunnel\\n\");\n\t\t\t\tgoto allow;\n\t\t\t}\n\t\t\tstate->post_nat_ip_dst = state->ct_result.nat_ip;\n\t\t\tstate->post_nat_dport = state->ct_result.nat_port;\n\t\t}\n\n\t\tCALI_DEBUG(\"CT: DNAT to %x:%d\\n\",\n\t\t\t\tbe32_to_host(state->post_nat_ip_dst), state->post_nat_dport);\n\n\n\t\tencap_needed = dnat_should_encap();\n\n\t\t/* We have not created the conntrack yet since we did not know\n\t\t * if we need encap or not. Must do before MTU check and before\n\t\t * we jump to do the encap.\n\t\t */\n\t\tif (ct_rc == CALI_CT_NEW) {\n\t\t\tstruct cali_rt * rt;\n\t\t\tint nat_type = CT_CREATE_NAT;\n\n\t\t\tif (encap_needed) {\n\t\t\t\t/* When we need to encap, we need to find out if the backend is\n\t\t\t\t * local or not. If local, we actually do not need the encap.\n\t\t\t\t */\n\t\t\t\trt = cali_rt_lookup(state->post_nat_ip_dst);\n\t\t\t\tif (!rt) {\n\t\t\t\t\treason = CALI_REASON_RT_UNKNOWN;\n\t\t\t\t\tgoto deny;\n\t\t\t\t}\n\t\t\t\tCALI_DEBUG(\"rt found for 0x%x local %d\\n\",\n\t\t\t\t\t\tbe32_to_host(state->post_nat_ip_dst), !!cali_rt_is_local(rt));\n\n\t\t\t\tencap_needed = !cali_rt_is_local(rt);\n\t\t\t\tif (encap_needed) {\n\t\t\t\t\tif (CALI_F_FROM_HEP && state->tun_ip == 0) {\n\t\t\t\t\t\tif (CALI_F_DSR) {\n\t\t\t\t\t\t\tct_nat_ctx.flags |= CALI_CT_FLAG_DSR_FWD;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tct_nat_ctx.flags |= CALI_CT_FLAG_NP_FWD;\n\t\t\t\t\t}\n\n\t\t\t\t\tnat_type = CT_CREATE_NAT_FWD;\n\t\t\t\t\tct_nat_ctx.tun_ip = rt->next_hop;\n\t\t\t\t\tstate->ip_dst = rt->next_hop;\n\t\t\t\t}\n\t\t\t}\n\n\n\t\t\tif (conntrack_create(&ct_nat_ctx, nat_type)) {\n\t\t\t\tCALI_DEBUG(\"Creating NAT conntrack failed\\n\");\n\t\t\t\tgoto deny;\n\t\t\t}\n\t\t} else {\n\t\t\tif (encap_needed && ct_result_np_node(state->ct_result)) {\n\t\t\t\tCALI_DEBUG(\"CT says encap to node %x\\n\", be32_to_host(state->ct_result.tun_ip));\n\t\t\t\tstate->ip_dst = state->ct_result.tun_ip;\n\t\t\t} else {\n\t\t\t\tencap_needed = false;\n\t\t\t}\n\t\t}\n\n\t\tif (encap_needed) {\n\t\t\tif (!(state->ip_proto == IPPROTO_TCP && skb_is_gso(skb)) &&\n\t\t\t\t\tip_is_dnf(ip_header) && vxlan_v4_encap_too_big(skb)) {\n\t\t\t\tCALI_DEBUG(\"Request packet with DNF set is too big\\n\");\n\t\t\t\tgoto icmp_too_big;\n\t\t\t}\n\t\t\tstate->ip_src = HOST_IP;\n\t\t\tseen_mark = CALI_SKB_MARK_SKIP_RPF;\n\n\t\t\t/* We cannot enforce RPF check on encapped traffic, do FIB if you can */\n\t\t\tfib = true;\n\n\t\t\tgoto nat_encap;\n\t\t}\n\n\t\tip_header->daddr = state->post_nat_ip_dst;\n\n\t\tswitch (ip_header->protocol) {\n\t\tcase IPPROTO_TCP:\n\t\t\ttcp_header->dest = host_to_be16(state->post_nat_dport);\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tudp_header->dest = host_to_be16(state->post_nat_dport);\n\t\t\tbreak;\n\t\t}\n\n\t\tCALI_VERB(\"L3 csum at %d L4 csum at %d\\n\", l3_csum_off, l4_csum_off);\n\n\t\tif (l4_csum_off) {\n\t\t\tres = skb_nat_l4_csum_ipv4(skb, l4_csum_off, state->ip_dst,\n\t\t\t\t\tstate->post_nat_ip_dst,\thost_to_be16(state->dport),\n\t\t\t\t\thost_to_be16(state->post_nat_dport),\n\t\t\t\t\tip_header->protocol == IPPROTO_UDP ? BPF_F_MARK_MANGLED_0 : 0);\n\t\t}\n\n\t\tres |= bpf_l3_csum_replace(skb, l3_csum_off, state->ip_dst, state->post_nat_ip_dst, 4);\n\n\t\tif (res) {\n\t\t\treason = CALI_REASON_CSUM_FAIL;\n\t\t\tgoto deny;\n\t\t}\n\n\t\t/* Handle returning ICMP related to tunnel\n\t\t *\n\t\t * N.B. we assume that we can fit in the MTU. Since it is ICMP\n\t\t * and even though Linux sends up to min ipv4 MTU, it is\n\t\t * unlikely that we are anywhere to close the MTU limit. If we\n\t\t * are, we need to fail anyway.\n\t\t */\n\t\tif (ct_related && state->ip_proto == IPPROTO_ICMP\n\t\t\t\t&& state->ct_result.tun_ip\n\t\t\t\t&& !CALI_F_DSR) {\n\t\t\tif (dnat_return_should_encap()) {\n\t\t\t\tCALI_DEBUG(\"Returning related ICMP from workload to tunnel\\n\");\n\t\t\t\tstate->ip_dst = state->ct_result.tun_ip;\n\t\t\t\tseen_mark = CALI_SKB_MARK_BYPASS_FWD_SRC_FIXUP;\n\t\t\t\tgoto nat_encap;\n\t\t\t} else if (CALI_F_TO_HEP) {\n\t\t\t\t/* Special case for ICMP error being returned by the host with the\n\t\t\t\t * backing workload into the tunnel back to the original host. It is\n\t\t\t\t * ICMP related and there is a return tunnel path. We need to change\n\t\t\t\t * both the source and destination at once.\n\t\t\t\t *\n\t\t\t\t * XXX the packet was routed to the original client as if it was XXX\n\t\t\t\t * DSR and we might not be on the right iface!!! Should we XXX try\n\t\t\t\t * to reinject it to fix the routing?\n\t\t\t\t */\n\t\t\t\tCALI_DEBUG(\"Returning related ICMP from host to tunnel\\n\");\n\t\t\t\tstate->ip_src = HOST_IP;\n\t\t\t\tstate->ip_dst = state->ct_result.tun_ip;\n\t\t\t\tgoto nat_encap;\n\t\t\t}\n\t\t}\n\n\t\tstate->dport = state->post_nat_dport;\n\t\tstate->ip_dst = state->post_nat_ip_dst;\n\n\t\tgoto allow;\n\n\tcase CALI_CT_ESTABLISHED_SNAT:\n\t\tCALI_DEBUG(\"CT: SNAT from %x:%d\\n\",\n\t\t\t\tbe32_to_host(state->ct_result.nat_ip), state->ct_result.nat_port);\n\n\t\tif (dnat_return_should_encap() && state->ct_result.tun_ip) {\n\t\t\tif (CALI_F_DSR) {\n\t\t\t\t/* SNAT will be done after routing, when leaving HEP */\n\t\t\t\tCALI_DEBUG(\"DSR enabled, skipping SNAT + encap\\n\");\n\t\t\t\tgoto allow;\n\t\t\t}\n\n\t\t\tif (!(state->ip_proto == IPPROTO_TCP && skb_is_gso(skb)) &&\n\t\t\t\t\tip_is_dnf(ip_header) && vxlan_v4_encap_too_big(skb)) {\n\t\t\t\tCALI_DEBUG(\"Return ICMP mtu is too big\\n\");\n\t\t\t\tgoto icmp_too_big;\n\t\t\t}\n\t\t}\n\n\t\t// Actually do the NAT.\n\t\tip_header->saddr = state->ct_result.nat_ip;\n\n\t\tswitch (ip_header->protocol) {\n\t\tcase IPPROTO_TCP:\n\t\t\ttcp_header->source = host_to_be16(state->ct_result.nat_port);\n\t\t\tbreak;\n\t\tcase IPPROTO_UDP:\n\t\t\tudp_header->source = host_to_be16(state->ct_result.nat_port);\n\t\t\tbreak;\n\t\t}\n\n\t\tCALI_VERB(\"L3 csum at %d L4 csum at %d\\n\", l3_csum_off, l4_csum_off);\n\n\t\tif (l4_csum_off) {\n\t\t\tres = skb_nat_l4_csum_ipv4(skb, l4_csum_off, state->ip_src,\n\t\t\t\t\tstate->ct_result.nat_ip, host_to_be16(state->sport),\n\t\t\t\t\thost_to_be16(state->ct_result.nat_port),\n\t\t\t\t\tip_header->protocol == IPPROTO_UDP ? BPF_F_MARK_MANGLED_0 : 0);\n\t\t}\n\n\t\tCALI_VERB(\"L3 checksum update (csum is at %d) port from %x to %x\\n\",\n\t\t\t\tl3_csum_off, state->ip_src, state->ct_result.nat_ip);\n\n\t\tint csum_rc = bpf_l3_csum_replace(skb, l3_csum_off,\n\t\t\t\t\t\t  state->ip_src, state->ct_result.nat_ip, 4);\n\t\tCALI_VERB(\"bpf_l3_csum_replace(IP): %d\\n\", csum_rc);\n\t\tres |= csum_rc;\n\n\t\tif (res) {\n\t\t\treason = CALI_REASON_CSUM_FAIL;\n\t\t\tgoto deny;\n\t\t}\n\n\t\tif (dnat_return_should_encap() && state->ct_result.tun_ip) {\n\t\t\tstate->ip_dst = state->ct_result.tun_ip;\n\t\t\tseen_mark = CALI_SKB_MARK_BYPASS_FWD_SRC_FIXUP;\n\t\t\tgoto nat_encap;\n\t\t}\n\n\t\tstate->sport = state->ct_result.nat_port;\n\t\tstate->ip_src = state->ct_result.nat_ip;\n\n\t\tgoto allow;\n\n\tcase CALI_CT_ESTABLISHED_BYPASS:\n\t\tseen_mark = CALI_SKB_MARK_BYPASS;\n\t\t// fall through\n\tcase CALI_CT_ESTABLISHED:\n\t\tgoto allow;\n\tdefault:\n\t\tif (CALI_F_FROM_HEP) {\n\t\t\t/* Since we're using the host endpoint program for TC-redirect\n\t\t\t * acceleration for workloads (but we haven't fully implemented\n\t\t\t * host endpoint support yet), we can get an incorrect conntrack\n\t\t\t * invalid for host traffic.\n\t\t\t *\n\t\t\t * FIXME: Properly handle host endpoint conntrack failures\n\t\t\t */\n\t\t\tCALI_DEBUG(\"Traffic is towards host namespace but not conntracked, \"\n\t\t\t\t\"falling through to iptables\\n\");\n\t\t\tfib = false;\n\t\t\tgoto allow;\n\t\t}\n\t\tgoto deny;\n\t}\n\n\tCALI_INFO(\"We should never fall through here\\n\");\n\tgoto deny;\n\nicmp_ttl_exceeded:\n\tif (skb_too_short(skb)) {\n\t\treason = CALI_REASON_SHORT;\n\t\tCALI_DEBUG(\"Too short\\n\");\n\t\tgoto deny;\n\t}\n\n\tip_header = skb_iphdr(skb);\n\t/* we silently drop the packet if things go wrong */\n\n\t/* XXX we should check if it is broadcast or multicast and not respond */\n\n\t/* do not respond to IP fragments except the first */\n\tif (ip_frag_no(ip_header)) {\n\t\tgoto deny;\n\t}\n\n\tif (icmp_v4_ttl_exceeded(skb)) {\n\t\tgoto deny;\n\t}\n\n\t/* we need to allow the reponse for the IP stack to route it back.\n\t * XXX we might want to send it back the same iface\n\t */\n\tgoto icmp_allow;\n\nicmp_too_big:\n\tif (icmp_v4_too_big(skb)) {\n\t\treason = CALI_REASON_ICMP_DF;\n\t\tgoto deny;\n\t}\n\n\t/* XXX we might use skb->ifindex to redirect it straight back\n\t * to where it came from if it is guaranteed to be the path\n\t */\n\tfib_flags |= BPF_FIB_LOOKUP_OUTPUT;\n\tif (CALI_F_FROM_WEP) {\n\t\t/* we know it came from workload, just send it back the same way */\n\t\trc = CALI_RES_REDIR_IFINDEX;\n\t}\n\n\tgoto icmp_allow;\n\nicmp_allow:\n\t/* recheck the size of the packet after it was turned into icmp and set\n\t * state so that it can processed further.\n\t */\n\tif (skb_shorter(skb, ETH_IPV4_UDP_SIZE)) {\n\t\treason = CALI_REASON_SHORT;\n\t\tgoto deny;\n\t}\n\tip_header = skb_iphdr(skb);\n\ttc_state_fill_from_iphdr(state, ip_header);\n\tstate->sport = state->dport = 0;\n\n\t/* packet was created because of approved traffic, treat it as related */\n\tseen_mark = CALI_SKB_MARK_BYPASS_FWD;\n\n\tgoto allow;\n\nnat_encap:\n\tif (vxlan_v4_encap(skb, state->ip_src, state->ip_dst)) {\n\t\treason = CALI_REASON_ENCAP_FAIL;\n\t\tgoto  deny;\n\t}\n\n\tstate->sport = state->dport = CALI_VXLAN_PORT;\n\tstate->ip_proto = IPPROTO_UDP;\n\nallow:\n\t{\n\t\tstruct fwd fwd = {\n\t\t\t.res = rc,\n\t\t\t.mark = seen_mark,\n\t\t};\n\t\tfwd_fib_set(&fwd, fib);\n\t\tfwd_fib_set_flags(&fwd, fib_flags);\n\t\treturn fwd;\n\t}\n\ndeny:\n\t{\n\t\tstruct fwd fwd = {\n\t\t\t.res = TC_ACT_SHOT,\n\t\t\t.reason = reason,\n\t\t};\n\t\treturn fwd;\n\t}\n}\n\n#ifndef CALI_ENTRYPOINT_NAME\n#define CALI_ENTRYPOINT_NAME calico_entrypoint\n#endif\n\n// Entrypoint with definable name.  It's useful to redefine the name for each entrypoint\n// because the name is exposed by bpftool et al.\n__attribute__((section(XSTR(CALI_ENTRYPOINT_NAME))))\nint tc_calico_entry(struct __sk_buff *skb)\n{\n\treturn calico_tc(skb);\n}\n\nchar ____license[] __attribute__((section(\"license\"), used)) = \"GPL\";\n", "idx": 8, "id": 18064, "msg": "", "proj": "projectcalico-felix", "lang": "c"}
{"patch": "@@ -115,13 +115,7 @@ func TestPodTrafficShaping(t *testing.T) {\n \tskipIfIPv6Cluster(t)\n \tnodeName := controlPlaneNodeName()\n \tskipIfMissingKernelModule(t, nodeName, []string{\"ifb\", \"sch_tbf\", \"sch_ingress\"})\n-\tskipIfHasWindowsNodes(t)\n \n-\tdata, err := setupTest(t)\n-\tif err != nil {\n-\t\tt.Fatalf(\"Error when setting up test: %v\", err)\n-\t}\n-\tdefer teardownTest(t, data)\n \ttests := []struct {\n \t\tname string\n \t\t// The bandwidths' unit is Mbits/sec.", "y": 0, "oldf": "// Copyright 2019 Antrea Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage e2e\n\nimport (\n\t\"fmt\"\n\t\"strconv\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/assert\"\n\tv1 \"k8s.io/api/core/v1\"\n)\n\nconst iperfPort = 5201\n\n// TestBenchmarkBandwidthIntraNode runs the bandwidth benchmark between Pods on same node.\nfunc TestBenchmarkBandwidthIntraNode(t *testing.T) {\n\tskipIfNotBenchmarkTest(t)\n\tskipIfNotIPv4Cluster(t)\n\tskipIfHasWindowsNodes(t)\n\n\tdata, err := setupTest(t)\n\tif err != nil {\n\t\tt.Fatalf(\"Error when setting up test: %v\", err)\n\t}\n\tdefer teardownTest(t, data)\n\tif err := data.createPodOnNode(\"perftest-a\", testNamespace, controlPlaneNodeName(), perftoolImage, nil, nil, nil, nil, false, nil); err != nil {\n\t\tt.Fatalf(\"Error when creating the perftest client Pod: %v\", err)\n\t}\n\tif err := data.podWaitForRunning(defaultTimeout, \"perftest-a\", testNamespace); err != nil {\n\t\tt.Fatalf(\"Error when waiting for the perftest client Pod: %v\", err)\n\t}\n\tif err := data.createPodOnNode(\"perftest-b\", testNamespace, controlPlaneNodeName(), perftoolImage, nil, nil, nil, []v1.ContainerPort{{Protocol: v1.ProtocolTCP, ContainerPort: iperfPort}}, false, nil); err != nil {\n\t\tt.Fatalf(\"Error when creating the perftest server Pod: %v\", err)\n\t}\n\tpodBIPs, err := data.podWaitForIPs(defaultTimeout, \"perftest-b\", testNamespace)\n\tif err != nil {\n\t\tt.Fatalf(\"Error when getting the perftest server Pod's IP: %v\", err)\n\t}\n\tpodBIP := podBIPs.ipv4.String()\n\tstdout, _, err := data.runCommandFromPod(testNamespace, \"perftest-a\", \"perftool\", []string{\"bash\", \"-c\", fmt.Sprintf(\"iperf3 -c %s|grep sender|awk '{print $7,$8}'\", podBIP)})\n\tif err != nil {\n\t\tt.Fatalf(\"Error when running iperf3 client: %v\", err)\n\t}\n\tstdout = strings.TrimSpace(stdout)\n\tt.Logf(\"Bandwidth: %s\", stdout)\n}\n\nfunc benchmarkBandwidthService(t *testing.T, endpointNode, clientNode string) {\n\tskipIfHasWindowsNodes(t)\n\n\tdata, err := setupTest(t)\n\tif err != nil {\n\t\tt.Fatalf(\"Error when setting up test: %v\", err)\n\t}\n\tdefer teardownTest(t, data)\n\n\tsvc, err := data.createService(\"perftest-b\", iperfPort, iperfPort, map[string]string{\"antrea-e2e\": \"perftest-b\"}, false, v1.ServiceTypeClusterIP, nil)\n\tif err != nil {\n\t\tt.Fatalf(\"Error when creating perftest service: %v\", err)\n\t}\n\tif err := data.createPodOnNode(\"perftest-a\", testNamespace, clientNode, perftoolImage, nil, nil, nil, nil, false, nil); err != nil {\n\t\tt.Fatalf(\"Error when creating the perftest client Pod: %v\", err)\n\t}\n\tif err := data.podWaitForRunning(defaultTimeout, \"perftest-a\", testNamespace); err != nil {\n\t\tt.Fatalf(\"Error when waiting for the perftest client Pod: %v\", err)\n\t}\n\tif err := data.createPodOnNode(\"perftest-b\", testNamespace, endpointNode, perftoolImage, nil, nil, nil, []v1.ContainerPort{{Protocol: v1.ProtocolTCP, ContainerPort: iperfPort}}, false, nil); err != nil {\n\t\tt.Fatalf(\"Error when creating the perftest server Pod: %v\", err)\n\t}\n\tif err := data.podWaitForRunning(defaultTimeout, \"perftest-b\", testNamespace); err != nil {\n\t\tt.Fatalf(\"Error when getting the perftest server Pod's IP: %v\", err)\n\t}\n\tstdout, stderr, err := data.runCommandFromPod(testNamespace, \"perftest-a\", perftoolContainerName, []string{\"bash\", \"-c\", fmt.Sprintf(\"iperf3 -c %s|grep sender|awk '{print $7,$8}'\", svc.Spec.ClusterIP)})\n\tif err != nil {\n\t\tt.Fatalf(\"Error when running iperf3 client: %v, stderr: %s\", err, stderr)\n\t}\n\tstdout = strings.TrimSpace(stdout)\n\tt.Logf(\"Bandwidth: %s\", stdout)\n}\n\n// TestBenchmarkBandwidthServiceLocalAccess runs the bandwidth benchmark of service\n// traffic between a Pod and an Endpoint on same Node.\nfunc TestBenchmarkBandwidthServiceLocalAccess(t *testing.T) {\n\tskipIfNotBenchmarkTest(t)\n\tbenchmarkBandwidthService(t, controlPlaneNodeName(), controlPlaneNodeName())\n}\n\n// TestBenchmarkBandwidthServiceRemoteAccess runs the bandwidth benchmark of service\n// traffic between a Pod and an Endpoint on different Node.\nfunc TestBenchmarkBandwidthServiceRemoteAccess(t *testing.T) {\n\tskipIfNotBenchmarkTest(t)\n\tskipIfNumNodesLessThan(t, 2)\n\tbenchmarkBandwidthService(t, controlPlaneNodeName(), workerNodeName(1))\n}\n\nfunc TestPodTrafficShaping(t *testing.T) {\n\t// TODO: tc configuration succeeded, however it didn't take effect, need to understand the reason.\n\tskipIfProviderIs(t, \"kind\", \"tc does not work with Kind\")\n\t// Test is flaky on dual-stack clusters: https://github.com/antrea-io/antrea/issues/1543.\n\t// So we disable it except for IPv4 single-stack clusters for now.\n\tskipIfIPv6Cluster(t)\n\tnodeName := controlPlaneNodeName()\n\tskipIfMissingKernelModule(t, nodeName, []string{\"ifb\", \"sch_tbf\", \"sch_ingress\"})\n\tskipIfHasWindowsNodes(t)\n\n\tdata, err := setupTest(t)\n\tif err != nil {\n\t\tt.Fatalf(\"Error when setting up test: %v\", err)\n\t}\n\tdefer teardownTest(t, data)\n\ttests := []struct {\n\t\tname string\n\t\t// The bandwidths' unit is Mbits/sec.\n\t\tclientEgressBandwidth  int\n\t\tserverIngressBandwidth int\n\t\texpectedBandwidth      int\n\t}{\n\t\t{\n\t\t\tname:                   \"limited by egress bandwidth\",\n\t\t\tclientEgressBandwidth:  100,\n\t\t\tserverIngressBandwidth: 200,\n\t\t\texpectedBandwidth:      100,\n\t\t},\n\t\t{\n\t\t\tname:                   \"limited by ingress bandwidth\",\n\t\t\tclientEgressBandwidth:  300,\n\t\t\tserverIngressBandwidth: 200,\n\t\t\texpectedBandwidth:      200,\n\t\t},\n\t}\n\tfor i, tt := range tests {\n\t\tt.Run(tt.name, func(t *testing.T) {\n\t\t\tclientPodName := fmt.Sprintf(\"client-a-%d\", i)\n\t\t\tserverPodName := fmt.Sprintf(\"server-a-%d\", i)\n\t\t\tif err := data.createPodOnNode(clientPodName, testNamespace, nodeName, perftoolImage, nil, nil, nil, nil, false, func(pod *v1.Pod) {\n\t\t\t\tpod.Annotations = map[string]string{\n\t\t\t\t\t\"kubernetes.io/egress-bandwidth\": fmt.Sprintf(\"%dM\", tt.clientEgressBandwidth),\n\t\t\t\t}\n\t\t\t}); err != nil {\n\t\t\t\tt.Fatalf(\"Error when creating the perftest client Pod: %v\", err)\n\t\t\t}\n\t\t\tdefer deletePodWrapper(t, data, clientPodName)\n\t\t\tif err := data.podWaitForRunning(defaultTimeout, clientPodName, testNamespace); err != nil {\n\t\t\t\tt.Fatalf(\"Error when waiting for the perftest client Pod: %v\", err)\n\t\t\t}\n\t\t\tif err := data.createPodOnNode(serverPodName, testNamespace, nodeName, perftoolImage, nil, nil, nil, []v1.ContainerPort{{Protocol: v1.ProtocolTCP, ContainerPort: iperfPort}}, false, func(pod *v1.Pod) {\n\t\t\t\tpod.Annotations = map[string]string{\n\t\t\t\t\t\"kubernetes.io/ingress-bandwidth\": fmt.Sprintf(\"%dM\", tt.serverIngressBandwidth),\n\t\t\t\t}\n\t\t\t}); err != nil {\n\t\t\t\tt.Fatalf(\"Error when creating the perftest server Pod: %v\", err)\n\t\t\t}\n\t\t\tdefer deletePodWrapper(t, data, serverPodName)\n\t\t\tpodIPs, err := data.podWaitForIPs(defaultTimeout, serverPodName, testNamespace)\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"Error when getting the perftest server Pod's IP: %v\", err)\n\t\t\t}\n\n\t\t\trunIperf := func(cmd []string) {\n\t\t\t\tstdout, _, err := data.runCommandFromPod(testNamespace, clientPodName, \"perftool\", cmd)\n\t\t\t\tif err != nil {\n\t\t\t\t\tt.Fatalf(\"Error when running iperf3 client: %v\", err)\n\t\t\t\t}\n\t\t\t\tstdout = strings.TrimSpace(stdout)\n\t\t\t\tactualBandwidth, _ := strconv.ParseFloat(strings.TrimSpace(stdout), 64)\n\t\t\t\tt.Logf(\"Actual bandwidth: %v Mbits/sec\", actualBandwidth)\n\t\t\t\t// Allow a certain deviation.\n\t\t\t\tassert.InEpsilon(t, actualBandwidth, tt.expectedBandwidth, 0.1)\n\t\t\t}\n\t\t\tif podIPs.ipv4 != nil {\n\t\t\t\trunIperf([]string{\"bash\", \"-c\", fmt.Sprintf(\"iperf3 -c %s -f m -O 1|grep sender|awk '{print $7}'\", podIPs.ipv4.String())})\n\t\t\t}\n\t\t\tif podIPs.ipv6 != nil {\n\t\t\t\trunIperf([]string{\"bash\", \"-c\", fmt.Sprintf(\"iperf3 -6 -c %s -f m -O 1|grep sender|awk '{print $7}'\", podIPs.ipv6.String())})\n\t\t\t}\n\t\t})\n\t}\n}\n", "idx": 4, "id": 39093, "msg": "", "proj": "antrea-io-antrea", "lang": "go"}
{"patch": "@@ -43,6 +43,28 @@ module Windows\n       @ssh_server\n     end\n \n+    # Gets the path & file name for the puppet agent dev package on Windows\n+    #\n+    # @param [String] puppet_collection Name of the puppet collection to use\n+    # @param [String] puppet_agent_version Version of puppet agent to get\n+    # @param [Hash{Symbol=>String}] opts Options hash to provide extra values\n+    #\n+    # @note Windows only uses the 'install_32' option of the opts hash at this\n+    #   time. Note that it will not fail if not provided, however\n+    #\n+    # @return [String, String] Path to the directory and filename of the package, respectively\n+    def puppet_agent_dev_package_info( puppet_collection = nil, puppet_agent_version = nil, opts = {} )\n+      release_path_end = 'windows'\n+      is_config_32 = self['ruby_arch'] == 'x86' || self['install_32'] || opts['install_32']\n+      should_install_64bit = self.is_x86_64? && !is_config_32\n+      # only install 64bit builds if\n+      # - we do not have install_32 set on host\n+      # - we do not have install_32 set globally\n+      arch_suffix = should_install_64bit ? '64' : '86'\n+      release_file = \"puppet-agent-x#{arch_suffix}.msi\"\n+      return release_path_end, release_file\n+    end\n+\n     attr_reader :scp_separator\n     def initialize name, host_hash, options\n       super", "y": 1, "oldf": "[ 'host', 'command_factory', 'command', 'options' ].each do |lib|\n      require \"beaker/#{lib}\"\nend\n\nmodule Windows\n  # A windows host with cygwin tools installed\n  class Host < Unix::Host\n    [ 'user', 'group', 'exec', 'pkg', 'file' ].each do |lib|\n          require \"beaker/host/windows/#{lib}\"\n    end\n\n    include Windows::User\n    include Windows::Group\n    include Windows::File\n    include Windows::Exec\n    include Windows::Pkg\n\n    def platform_defaults\n      h = Beaker::Options::OptionsHash.new\n      h.merge({\n        'user'          => 'Administrator',\n        'group'         => 'Administrators',\n        'pathseparator' => ';',\n      })\n    end\n\n    def external_copy_base\n      return @external_copy_base if @external_copy_base\n      @external_copy_base = execute('echo `cygpath -smF 35`/')\n      @external_copy_base\n    end\n\n    # Determines which SSH Server is in use on this host\n    #\n    # @return [Symbol] Value for the SSH Server in use\n    #   (:bitvise or :openssh at this point).\n    def determine_ssh_server\n      return @ssh_server if @ssh_server\n      @ssh_server = :openssh\n      status = execute('cmd.exe /c sc query BvSshServer', :accept_all_exit_codes => true)\n      @ssh_server = :bitvise if status =~ /4  RUNNING/\n      logger.debug(\"windows.rb:determine_ssh_server: determined ssh server: '#{@ssh_server}'\")\n      @ssh_server\n    end\n\n    attr_reader :scp_separator\n    def initialize name, host_hash, options\n      super\n\n      @ssh_server         = nil\n      @scp_separator      = '\\\\'\n      @external_copy_base = nil\n    end\n\n  end\nend\n", "idx": 1, "id": 12076, "msg": "Should `puppet_collection` or `puppet_agent_version` be used anywhere below?", "proj": "voxpupuli-beaker", "lang": "rb"}
{"patch": "@@ -0,0 +1,33 @@\n+//+build !go1.9\n+\n+package fstests\n+\n+func leadingZeros64(x uint64) int {\n+\tvar n uint64 = 64\n+\n+\tif y := x >> 32; y != 0 {\n+\t\tn = n - 32\n+\t\tx = y\n+\t}\n+\tif y := x >> 16; y != 0 {\n+\t\tn = n - 16\n+\t\tx = y\n+\t}\n+\tif y := x >> 8; y != 0 {\n+\t\tn = n - 8\n+\t\tx = y\n+\t}\n+\tif y := x >> 4; y != 0 {\n+\t\tn = n - 4\n+\t\tx = y\n+\t}\n+\tif y := x >> 2; y != 0 {\n+\t\tn = n - 2\n+\t\tx = y\n+\t}\n+\tif y := x >> 1; y != 0 {\n+\t\treturn int(n - 2)\n+\t}\n+\n+\treturn int(n - x)\n+}", "y": 1, "oldf": "", "idx": 1, "id": 7718, "msg": "How is this different from math.LeadingZeros64?", "proj": "rclone-rclone", "lang": "go"}
{"patch": "@@ -3402,7 +3402,7 @@ void nano::active_transactions::request_confirm (std::unique_lock<std::mutex> &\n \t\t\t\t\t\tfor (auto & rep : *reps)\n \t\t\t\t\t\t{\n \t\t\t\t\t\t\tauto rep_request (requests_bundle.find (rep.endpoint));\n-\t\t\t\t\t\t\tauto block (i->election->status.winner);\n+\t\t\t\t\t\t\tauto block (election_l->status.winner);\n \t\t\t\t\t\t\tauto root_hash (std::make_pair (block->hash (), block->root ()));\n \t\t\t\t\t\t\tif (rep_request == requests_bundle.end ())\n \t\t\t\t\t\t\t{", "y": 0, "oldf": "#include <nano/node/node.hpp>\n\n#include <nano/lib/interface.h>\n#include <nano/lib/timer.hpp>\n#include <nano/lib/utility.hpp>\n#include <nano/node/common.hpp>\n#include <nano/node/rpc.hpp>\n\n#include <algorithm>\n#include <cstdlib>\n#include <future>\n#include <sstream>\n\n#include <boost/polymorphic_cast.hpp>\n#include <boost/property_tree/json_parser.hpp>\n\ndouble constexpr nano::node::price_max;\ndouble constexpr nano::node::free_cutoff;\nstd::chrono::seconds constexpr nano::node::period;\nstd::chrono::seconds constexpr nano::node::cutoff;\nstd::chrono::seconds constexpr nano::node::syn_cookie_cutoff;\nstd::chrono::minutes constexpr nano::node::backup_interval;\nstd::chrono::seconds constexpr nano::node::search_pending_interval;\nstd::chrono::seconds constexpr nano::node::peer_interval;\nstd::chrono::hours constexpr nano::node::unchecked_cleaning_interval;\nstd::chrono::milliseconds constexpr nano::node::process_confirmed_interval;\n\nint constexpr nano::port_mapping::mapping_timeout;\nint constexpr nano::port_mapping::check_timeout;\nunsigned constexpr nano::active_transactions::request_interval_ms;\nsize_t constexpr nano::active_transactions::max_broadcast_queue;\nsize_t constexpr nano::block_arrival::arrival_size_min;\nstd::chrono::seconds constexpr nano::block_arrival::arrival_time_min;\nuint64_t constexpr nano::online_reps::weight_period;\nuint64_t constexpr nano::online_reps::weight_samples;\n\nnamespace nano\n{\nextern unsigned char nano_bootstrap_weights[];\nextern size_t nano_bootstrap_weights_size;\n}\n\nnano::network::network (nano::node & node_a, uint16_t port) :\nbuffer_container (node_a.stats, nano::network::buffer_size, 4096), // 2Mb receive buffer\nsocket (node_a.io_ctx, nano::endpoint (boost::asio::ip::address_v6::any (), port)),\nresolver (node_a.io_ctx),\nnode (node_a),\non (true)\n{\n\tboost::thread::attributes attrs;\n\tnano::thread_attributes::set (attrs);\n\tfor (size_t i = 0; i < node.config.network_threads; ++i)\n\t{\n\t\tpacket_processing_threads.push_back (boost::thread (attrs, [this]() {\n\t\t\tnano::thread_role::set (nano::thread_role::name::packet_processing);\n\t\t\ttry\n\t\t\t{\n\t\t\t\tprocess_packets ();\n\t\t\t}\n\t\t\tcatch (boost::system::error_code & ec)\n\t\t\t{\n\t\t\t\tBOOST_LOG (this->node.log) << FATAL_LOG_PREFIX << ec.message ();\n\t\t\t\trelease_assert (false);\n\t\t\t}\n\t\t\tcatch (std::error_code & ec)\n\t\t\t{\n\t\t\t\tBOOST_LOG (this->node.log) << FATAL_LOG_PREFIX << ec.message ();\n\t\t\t\trelease_assert (false);\n\t\t\t}\n\t\t\tcatch (std::runtime_error & err)\n\t\t\t{\n\t\t\t\tBOOST_LOG (this->node.log) << FATAL_LOG_PREFIX << err.what ();\n\t\t\t\trelease_assert (false);\n\t\t\t}\n\t\t\tcatch (...)\n\t\t\t{\n\t\t\t\tBOOST_LOG (this->node.log) << FATAL_LOG_PREFIX << \"Unknown exception\";\n\t\t\t\trelease_assert (false);\n\t\t\t}\n\t\t\tif (this->node.config.logging.network_packet_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (this->node.log) << \"Exiting packet processing thread\";\n\t\t\t}\n\t\t}));\n\t}\n}\n\nnano::network::~network ()\n{\n\tfor (auto & thread : packet_processing_threads)\n\t{\n\t\tthread.join ();\n\t}\n}\n\nvoid nano::network::start ()\n{\n\tfor (size_t i = 0; i < node.config.io_threads; ++i)\n\t{\n\t\treceive ();\n\t}\n}\n\nvoid nano::network::receive ()\n{\n\tif (node.config.logging.network_packet_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << \"Receiving packet\";\n\t}\n\tstd::unique_lock<std::mutex> lock (socket_mutex);\n\tauto data (buffer_container.allocate ());\n\tsocket.async_receive_from (boost::asio::buffer (data->buffer, nano::network::buffer_size), data->endpoint, [this, data](boost::system::error_code const & error, size_t size_a) {\n\t\tif (!error && this->on)\n\t\t{\n\t\t\tdata->size = size_a;\n\t\t\tthis->buffer_container.enqueue (data);\n\t\t\tthis->receive ();\n\t\t}\n\t\telse\n\t\t{\n\t\t\tthis->buffer_container.release (data);\n\t\t\tif (error)\n\t\t\t{\n\t\t\t\tif (this->node.config.logging.network_logging ())\n\t\t\t\t{\n\t\t\t\t\tBOOST_LOG (this->node.log) << boost::str (boost::format (\"UDP Receive error: %1%\") % error.message ());\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (this->on)\n\t\t\t{\n\t\t\t\tthis->node.alarm.add (std::chrono::steady_clock::now () + std::chrono::seconds (5), [this]() { this->receive (); });\n\t\t\t}\n\t\t}\n\t});\n}\n\nvoid nano::network::process_packets ()\n{\n\twhile (on.load ())\n\t{\n\t\tauto data (buffer_container.dequeue ());\n\t\tif (data == nullptr)\n\t\t{\n\t\t\tbreak;\n\t\t}\n\t\t//std::cerr << data->endpoint.address ().to_string ();\n\t\treceive_action (data);\n\t\tbuffer_container.release (data);\n\t}\n}\n\nvoid nano::network::stop ()\n{\n\ton = false;\n\tsocket.close ();\n\tresolver.cancel ();\n\tbuffer_container.stop ();\n}\n\nvoid nano::network::send_keepalive (nano::endpoint const & endpoint_a)\n{\n\tassert (endpoint_a.address ().is_v6 ());\n\tnano::keepalive message;\n\tnode.peers.random_fill (message.peers);\n\tauto bytes = message.to_bytes ();\n\tif (node.config.logging.network_keepalive_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Keepalive req sent to %1%\") % endpoint_a);\n\t}\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tsend_buffer (bytes->data (), bytes->size (), endpoint_a, [bytes, node_w, endpoint_a](boost::system::error_code const & ec, size_t) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_keepalive_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error sending keepalive to %1%: %2%\") % endpoint_a % ec.message ());\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tnode_l->stats.inc (nano::stat::type::message, nano::stat::detail::keepalive, nano::stat::dir::out);\n\t\t\t}\n\t\t}\n\t});\n}\n\nvoid nano::node::keepalive (std::string const & address_a, uint16_t port_a, bool preconfigured_peer_a)\n{\n\tauto node_l (shared_from_this ());\n\tnetwork.resolver.async_resolve (boost::asio::ip::udp::resolver::query (address_a, std::to_string (port_a)), [node_l, address_a, port_a, preconfigured_peer_a](boost::system::error_code const & ec, boost::asio::ip::udp::resolver::iterator i_a) {\n\t\tif (!ec)\n\t\t{\n\t\t\tfor (auto i (i_a), n (boost::asio::ip::udp::resolver::iterator{}); i != n; ++i)\n\t\t\t{\n\t\t\t\tauto endpoint (nano::map_endpoint_to_v6 (i->endpoint ()));\n\t\t\t\tnode_l->send_keepalive (endpoint);\n\t\t\t\tif (preconfigured_peer_a)\n\t\t\t\t{\n\t\t\t\t\tnode_l->peers.insert (endpoint, nano::protocol_version, true);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error resolving address: %1%:%2%: %3%\") % address_a % port_a % ec.message ());\n\t\t}\n\t});\n}\n\nvoid nano::network::send_node_id_handshake (nano::endpoint const & endpoint_a, boost::optional<nano::uint256_union> const & query, boost::optional<nano::uint256_union> const & respond_to)\n{\n\tassert (endpoint_a.address ().is_v6 ());\n\tboost::optional<std::pair<nano::account, nano::signature>> response (boost::none);\n\tif (respond_to)\n\t{\n\t\tresponse = std::make_pair (node.node_id.pub, nano::sign_message (node.node_id.prv, node.node_id.pub, *respond_to));\n\t\tassert (!nano::validate_message (response->first, *respond_to, response->second));\n\t}\n\tnano::node_id_handshake message (query, response);\n\tauto bytes = message.to_bytes ();\n\tif (node.config.logging.network_node_id_handshake_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Node ID handshake sent with node ID %1% to %2%: query %3%, respond_to %4% (signature %5%)\") % node.node_id.pub.to_account () % endpoint_a % (query ? query->to_string () : std::string (\"[none]\")) % (respond_to ? respond_to->to_string () : std::string (\"[none]\")) % (response ? response->second.to_string () : std::string (\"[none]\")));\n\t}\n\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::node_id_handshake, nano::stat::dir::out);\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tsend_buffer (bytes->data (), bytes->size (), endpoint_a, [bytes, node_w, endpoint_a](boost::system::error_code const & ec, size_t) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_node_id_handshake_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error sending node ID handshake to %1% %2%\") % endpoint_a % ec.message ());\n\t\t\t}\n\t\t}\n\t});\n}\n\nvoid nano::network::republish (nano::block_hash const & hash_a, std::shared_ptr<std::vector<uint8_t>> buffer_a, nano::endpoint endpoint_a)\n{\n\tif (node.config.logging.network_publish_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Publishing %1% to %2%\") % hash_a.to_string () % endpoint_a);\n\t}\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tsend_buffer (buffer_a->data (), buffer_a->size (), endpoint_a, [node_w, endpoint_a](boost::system::error_code const & ec, size_t size) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error sending publish to %1%: %2%\") % endpoint_a % ec.message ());\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tnode_l->stats.inc (nano::stat::type::message, nano::stat::detail::publish, nano::stat::dir::out);\n\t\t\t}\n\t\t}\n\t});\n}\n\ntemplate <typename T>\nbool confirm_block (nano::transaction const & transaction_a, nano::node & node_a, T & list_a, std::shared_ptr<nano::block> block_a, bool also_publish)\n{\n\tbool result (false);\n\tif (node_a.config.enable_voting)\n\t{\n\t\tauto hash (block_a->hash ());\n\t\t// Search in cache\n\t\tauto votes (node_a.votes_cache.find (hash));\n\t\tif (votes.empty ())\n\t\t{\n\t\t\t// Generate new vote\n\t\t\tnode_a.wallets.foreach_representative (transaction_a, [&result, &list_a, &node_a, &transaction_a, &hash](nano::public_key const & pub_a, nano::raw_key const & prv_a) {\n\t\t\t\tresult = true;\n\t\t\t\tauto vote (node_a.store.vote_generate (transaction_a, pub_a, prv_a, std::vector<nano::block_hash> (1, hash)));\n\t\t\t\tnano::confirm_ack confirm (vote);\n\t\t\t\tauto vote_bytes = confirm.to_bytes ();\n\t\t\t\tfor (auto j (list_a.begin ()), m (list_a.end ()); j != m; ++j)\n\t\t\t\t{\n\t\t\t\t\tnode_a.network.confirm_send (confirm, vote_bytes, *j);\n\t\t\t\t}\n\t\t\t\tnode_a.votes_cache.add (vote);\n\t\t\t});\n\t\t}\n\t\telse\n\t\t{\n\t\t\t// Send from cache\n\t\t\tfor (auto & vote : votes)\n\t\t\t{\n\t\t\t\tnano::confirm_ack confirm (vote);\n\t\t\t\tauto vote_bytes = confirm.to_bytes ();\n\t\t\t\tfor (auto j (list_a.begin ()), m (list_a.end ()); j != m; ++j)\n\t\t\t\t{\n\t\t\t\t\tnode_a.network.confirm_send (confirm, vote_bytes, *j);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Republish if required\n\t\tif (also_publish)\n\t\t{\n\t\t\tnano::publish publish (block_a);\n\t\t\tstd::shared_ptr<std::vector<uint8_t>> publish_bytes;\n\t\t\tpublish_bytes = publish.to_bytes ();\n\t\t\tfor (auto j (list_a.begin ()), m (list_a.end ()); j != m; ++j)\n\t\t\t{\n\t\t\t\tnode_a.network.republish (hash, publish_bytes, *j);\n\t\t\t}\n\t\t}\n\t}\n\treturn result;\n}\n\nbool confirm_block (nano::transaction const & transaction_a, nano::node & node_a, nano::endpoint & peer_a, std::shared_ptr<nano::block> block_a, bool also_publish)\n{\n\tstd::array<nano::endpoint, 1> endpoints;\n\tendpoints[0] = peer_a;\n\tauto result (confirm_block (transaction_a, node_a, endpoints, std::move (block_a), also_publish));\n\treturn result;\n}\n\nvoid nano::network::confirm_hashes (nano::transaction const & transaction_a, nano::endpoint const & peer_a, std::vector<nano::block_hash> blocks_bundle_a)\n{\n\tif (node.config.enable_voting)\n\t{\n\t\tnode.wallets.foreach_representative (transaction_a, [this, &blocks_bundle_a, &peer_a, &transaction_a](nano::public_key const & pub_a, nano::raw_key const & prv_a) {\n\t\t\tauto vote (this->node.store.vote_generate (transaction_a, pub_a, prv_a, blocks_bundle_a));\n\t\t\tnano::confirm_ack confirm (vote);\n\t\t\tstd::shared_ptr<std::vector<uint8_t>> bytes (new std::vector<uint8_t>);\n\t\t\t{\n\t\t\t\tnano::vectorstream stream (*bytes);\n\t\t\t\tconfirm.serialize (stream);\n\t\t\t}\n\t\t\tthis->node.network.confirm_send (confirm, bytes, peer_a);\n\t\t\tthis->node.votes_cache.add (vote);\n\t\t});\n\t}\n}\n\nbool nano::network::send_votes_cache (nano::block_hash const & hash_a, nano::endpoint const & peer_a)\n{\n\t// Search in cache\n\tauto votes (node.votes_cache.find (hash_a));\n\t// Send from cache\n\tfor (auto & vote : votes)\n\t{\n\t\tnano::confirm_ack confirm (vote);\n\t\tauto vote_bytes = confirm.to_bytes ();\n\t\tconfirm_send (confirm, vote_bytes, peer_a);\n\t}\n\t// Returns true if votes were sent\n\tbool result (!votes.empty ());\n\treturn result;\n}\n\nvoid nano::network::republish_block (std::shared_ptr<nano::block> block)\n{\n\tauto hash (block->hash ());\n\tauto list (node.peers.list_fanout ());\n\tnano::publish message (block);\n\tauto bytes = message.to_bytes ();\n\tfor (auto i (list.begin ()), n (list.end ()); i != n; ++i)\n\t{\n\t\trepublish (hash, bytes, *i);\n\t}\n\tif (node.config.logging.network_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Block %1% was republished to peers\") % hash.to_string ());\n\t}\n}\n\nvoid nano::network::republish_block (std::shared_ptr<nano::block> block, nano::endpoint const & peer_a)\n{\n\tauto hash (block->hash ());\n\tnano::publish message (block);\n\tstd::vector<uint8_t> bytes;\n\t{\n\t\tnano::vectorstream stream (bytes);\n\t\tmessage.serialize (stream);\n\t}\n\trepublish (hash, std::make_shared<std::vector<uint8_t>> (bytes), peer_a);\n\tif (node.config.logging.network_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Block %1% was republished to peer\") % hash.to_string ());\n\t}\n}\n\nvoid nano::network::republish_block_batch (std::deque<std::shared_ptr<nano::block>> blocks_a, unsigned delay_a)\n{\n\tauto block (blocks_a.front ());\n\tblocks_a.pop_front ();\n\trepublish_block (block);\n\tif (!blocks_a.empty ())\n\t{\n\t\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\t\tnode.alarm.add (std::chrono::steady_clock::now () + std::chrono::milliseconds (delay_a + std::rand () % delay_a), [node_w, blocks_a, delay_a]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->network.republish_block_batch (blocks_a, delay_a);\n\t\t\t}\n\t\t});\n\t}\n}\n\n// In order to rate limit network traffic we republish:\n// 1) Only if they are a non-replay vote of a block that's actively settling. Settling blocks are limited by block PoW\n// 2) The rep has a weight > Y to prevent creating a lot of small-weight accounts to send out votes\n// 3) Only if a vote for this block from this representative hasn't been received in the previous X second.\n//    This prevents rapid publishing of votes with increasing sequence numbers.\n//\n// These rules are implemented by the caller, not this function.\nvoid nano::network::republish_vote (std::shared_ptr<nano::vote> vote_a)\n{\n\tnano::confirm_ack confirm (vote_a);\n\tauto bytes = confirm.to_bytes ();\n\tauto list (node.peers.list_fanout ());\n\tfor (auto j (list.begin ()), m (list.end ()); j != m; ++j)\n\t{\n\t\tnode.network.confirm_send (confirm, bytes, *j);\n\t}\n}\n\nvoid nano::network::broadcast_confirm_req (std::shared_ptr<nano::block> block_a)\n{\n\tauto list (std::make_shared<std::vector<nano::peer_information>> (node.peers.representatives (std::numeric_limits<size_t>::max ())));\n\tif (list->empty () || node.peers.total_weight () < node.config.online_weight_minimum.number ())\n\t{\n\t\t// broadcast request to all peers (with max limit 2 * sqrt (peers count))\n\t\tlist = std::make_shared<std::vector<nano::peer_information>> (node.peers.list_vector (std::min (static_cast<size_t> (100), 2 * node.peers.size_sqrt ())));\n\t}\n\n\t/*\n\t * In either case (broadcasting to all representatives, or broadcasting to\n\t * all peers because there are not enough connected representatives),\n\t * limit each instance to a single random up-to-32 selection.  The invoker\n\t * of \"broadcast_confirm_req\" will be responsible for calling it again\n\t * if the votes for a block have not arrived in time.\n\t */\n\tconst size_t max_endpoints = 32;\n\trandom_pool.Shuffle (list->begin (), list->end ());\n\tif (list->size () > max_endpoints)\n\t{\n\t\tlist->erase (list->begin () + max_endpoints, list->end ());\n\t}\n\n\tbroadcast_confirm_req_base (block_a, list, 0);\n}\n\nvoid nano::network::broadcast_confirm_req_base (std::shared_ptr<nano::block> block_a, std::shared_ptr<std::vector<nano::peer_information>> endpoints_a, unsigned delay_a, bool resumption)\n{\n\tconst size_t max_reps = 10;\n\tif (!resumption && node.config.logging.network_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Broadcasting confirm req for block %1% to %2% representatives\") % block_a->hash ().to_string () % endpoints_a->size ());\n\t}\n\tauto count (0);\n\twhile (!endpoints_a->empty () && count < max_reps)\n\t{\n\t\tsend_confirm_req (endpoints_a->back ().endpoint, block_a);\n\t\tendpoints_a->pop_back ();\n\t\tcount++;\n\t}\n\tif (!endpoints_a->empty ())\n\t{\n\t\tdelay_a += std::rand () % broadcast_interval_ms;\n\n\t\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\t\tnode.alarm.add (std::chrono::steady_clock::now () + std::chrono::milliseconds (delay_a), [node_w, block_a, endpoints_a, delay_a]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->network.broadcast_confirm_req_base (block_a, endpoints_a, delay_a, true);\n\t\t\t}\n\t\t});\n\t}\n}\n\nvoid nano::network::broadcast_confirm_req_batch (std::unordered_map<nano::endpoint, std::vector<std::pair<nano::block_hash, nano::block_hash>>> request_bundle_a, unsigned delay_a, bool resumption)\n{\n\tconst size_t max_reps = 10;\n\tif (!resumption && node.config.logging.network_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Broadcasting batch confirm req to %1% representatives\") % request_bundle_a.size ());\n\t}\n\tauto count (0);\n\twhile (!request_bundle_a.empty () && count < max_reps)\n\t{\n\t\tauto j (request_bundle_a.begin ());\n\t\tcount++;\n\t\tstd::vector<std::pair<nano::block_hash, nano::block_hash>> roots_hashes;\n\t\t// Limit max request size hash + root to 6 pairs\n\t\twhile (roots_hashes.size () <= confirm_req_hashes_max && !j->second.empty ())\n\t\t{\n\t\t\troots_hashes.push_back (j->second.back ());\n\t\t\tj->second.pop_back ();\n\t\t}\n\t\tsend_confirm_req_hashes (j->first, roots_hashes);\n\t\tif (j->second.empty ())\n\t\t{\n\t\t\trequest_bundle_a.erase (j);\n\t\t}\n\t}\n\tif (!request_bundle_a.empty ())\n\t{\n\t\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\t\tnode.alarm.add (std::chrono::steady_clock::now () + std::chrono::milliseconds (delay_a), [node_w, request_bundle_a, delay_a]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->network.broadcast_confirm_req_batch (request_bundle_a, delay_a + 50, true);\n\t\t\t}\n\t\t});\n\t}\n}\n\nvoid nano::network::broadcast_confirm_req_batch (std::deque<std::pair<std::shared_ptr<nano::block>, std::shared_ptr<std::vector<nano::peer_information>>>> deque_a, unsigned delay_a)\n{\n\tauto pair (deque_a.front ());\n\tdeque_a.pop_front ();\n\tauto block (pair.first);\n\t// confirm_req to representatives\n\tauto endpoints (pair.second);\n\tif (!endpoints->empty ())\n\t{\n\t\tbroadcast_confirm_req_base (block, endpoints, delay_a);\n\t}\n\t/* Continue while blocks remain\n\tBroadcast with random delay between delay_a & 2*delay_a */\n\tif (!deque_a.empty ())\n\t{\n\t\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\t\tnode.alarm.add (std::chrono::steady_clock::now () + std::chrono::milliseconds (delay_a + std::rand () % delay_a), [node_w, deque_a, delay_a]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->network.broadcast_confirm_req_batch (deque_a, delay_a);\n\t\t\t}\n\t\t});\n\t}\n}\n\nvoid nano::network::send_confirm_req (nano::endpoint const & endpoint_a, std::shared_ptr<nano::block> block)\n{\n\tnano::confirm_req message (block);\n\tauto bytes = message.to_bytes ();\n\tif (node.config.logging.network_message_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Sending confirm req to %1%\") % endpoint_a);\n\t}\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::confirm_req, nano::stat::dir::out);\n\tsend_buffer (bytes->data (), bytes->size (), endpoint_a, [bytes, node_w](boost::system::error_code const & ec, size_t size) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error sending confirm request: %1%\") % ec.message ());\n\t\t\t}\n\t\t}\n\t});\n}\n\nvoid nano::network::send_confirm_req_hashes (nano::endpoint const & endpoint_a, std::vector<std::pair<nano::block_hash, nano::block_hash>> const & roots_hashes_a)\n{\n\tnano::confirm_req message (roots_hashes_a);\n\tstd::vector<uint8_t> bytes;\n\t{\n\t\tnano::vectorstream stream (bytes);\n\t\tmessage.serialize (stream);\n\t}\n\tif (node.config.logging.network_message_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Sending confirm req hashes to %1%\") % endpoint_a);\n\t}\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::confirm_req, nano::stat::dir::out);\n\tsend_buffer (bytes.data (), bytes.size (), endpoint_a, [node_w](boost::system::error_code const & ec, size_t size) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error sending confirm request: %1%\") % ec.message ());\n\t\t\t}\n\t\t}\n\t});\n}\n\ntemplate <typename T>\nvoid rep_query (nano::node & node_a, T const & peers_a)\n{\n\tauto transaction (node_a.store.tx_begin_read ());\n\tstd::shared_ptr<nano::block> block (node_a.store.block_random (transaction));\n\tauto hash (block->hash ());\n\tnode_a.rep_crawler.add (hash);\n\tfor (auto i (peers_a.begin ()), n (peers_a.end ()); i != n; ++i)\n\t{\n\t\tnode_a.peers.rep_request (*i);\n\t\tnode_a.network.send_confirm_req (*i, block);\n\t}\n\tstd::weak_ptr<nano::node> node_w (node_a.shared ());\n\tnode_a.alarm.add (std::chrono::steady_clock::now () + std::chrono::seconds (5), [node_w, hash]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->rep_crawler.remove (hash);\n\t\t}\n\t});\n}\n\nvoid rep_query (nano::node & node_a, nano::endpoint const & peers_a)\n{\n\tstd::array<nano::endpoint, 1> peers;\n\tpeers[0] = peers_a;\n\trep_query (node_a, peers);\n}\n\nnamespace\n{\nclass network_message_visitor : public nano::message_visitor\n{\npublic:\n\tnetwork_message_visitor (nano::node & node_a, nano::endpoint const & sender_a) :\n\tnode (node_a),\n\tsender (sender_a)\n\t{\n\t}\n\tvirtual ~network_message_visitor () = default;\n\tvoid keepalive (nano::keepalive const & message_a) override\n\t{\n\t\tif (node.config.logging.network_keepalive_logging ())\n\t\t{\n\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Received keepalive message from %1%\") % sender);\n\t\t}\n\t\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::keepalive, nano::stat::dir::in);\n\t\tif (node.peers.contacted (sender, message_a.header.version_using))\n\t\t{\n\t\t\tauto endpoint_l (nano::map_endpoint_to_v6 (sender));\n\t\t\tauto cookie (node.peers.assign_syn_cookie (endpoint_l));\n\t\t\tif (cookie)\n\t\t\t{\n\t\t\t\tnode.network.send_node_id_handshake (endpoint_l, *cookie, boost::none);\n\t\t\t}\n\t\t}\n\t\tnode.network.merge_peers (message_a.peers);\n\t}\n\tvoid publish (nano::publish const & message_a) override\n\t{\n\t\tif (node.config.logging.network_message_logging ())\n\t\t{\n\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Publish message from %1% for %2%\") % sender % message_a.block->hash ().to_string ());\n\t\t}\n\t\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::publish, nano::stat::dir::in);\n\t\tnode.peers.contacted (sender, message_a.header.version_using);\n\t\tif (!node.block_processor.full ())\n\t\t{\n\t\t\tnode.process_active (message_a.block);\n\t\t}\n\t\tnode.active.publish (message_a.block);\n\t}\n\tvoid confirm_req (nano::confirm_req const & message_a) override\n\t{\n\t\tif (node.config.logging.network_message_logging ())\n\t\t{\n\t\t\tif (!message_a.roots_hashes.empty ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Confirm_req message from %1% for hashes:roots %2%\") % sender % message_a.roots_string ());\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Confirm_req message from %1% for %2%\") % sender % message_a.block->hash ().to_string ());\n\t\t\t}\n\t\t}\n\t\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::confirm_req, nano::stat::dir::in);\n\t\tnode.peers.contacted (sender, message_a.header.version_using);\n\t\t// Don't load nodes with disabled voting\n\t\tif (node.config.enable_voting && node.wallets.reps_count)\n\t\t{\n\t\t\tif (message_a.block != nullptr)\n\t\t\t{\n\t\t\t\tauto hash (message_a.block->hash ());\n\t\t\t\tif (!node.network.send_votes_cache (hash, sender))\n\t\t\t\t{\n\t\t\t\t\tauto transaction (node.store.tx_begin_read ());\n\t\t\t\t\tauto successor (node.ledger.successor (transaction, nano::uint512_union (message_a.block->previous (), message_a.block->root ())));\n\t\t\t\t\tif (successor != nullptr)\n\t\t\t\t\t{\n\t\t\t\t\t\tauto same_block (successor->hash () == hash);\n\t\t\t\t\t\tconfirm_block (transaction, node, sender, std::move (successor), !same_block);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (!message_a.roots_hashes.empty ())\n\t\t\t{\n\t\t\t\tauto transaction (node.store.tx_begin_read ());\n\t\t\t\tstd::vector<nano::block_hash> blocks_bundle;\n\t\t\t\tfor (auto & root_hash : message_a.roots_hashes)\n\t\t\t\t{\n\t\t\t\t\tif (!node.network.send_votes_cache (root_hash.first, sender) && node.store.block_exists (transaction, root_hash.first))\n\t\t\t\t\t{\n\t\t\t\t\t\tblocks_bundle.push_back (root_hash.first);\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tnano::block_hash successor (0);\n\t\t\t\t\t\t// Search for block root\n\t\t\t\t\t\tsuccessor = node.store.block_successor (transaction, root_hash.second);\n\t\t\t\t\t\t// Search for account root\n\t\t\t\t\t\tif (successor.is_zero () && node.store.account_exists (transaction, root_hash.second))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tnano::account_info info;\n\t\t\t\t\t\t\tauto error (node.store.account_get (transaction, root_hash.second, info));\n\t\t\t\t\t\t\tassert (!error);\n\t\t\t\t\t\t\tsuccessor = info.open_block;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif (!successor.is_zero ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif (!node.network.send_votes_cache (successor, sender))\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tblocks_bundle.push_back (successor);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tauto successor_block (node.store.block_get (transaction, successor));\n\t\t\t\t\t\t\tassert (successor_block != nullptr);\n\t\t\t\t\t\t\tnode.network.republish_block (std::move (successor_block), sender);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif (!blocks_bundle.empty ())\n\t\t\t\t{\n\t\t\t\t\tnode.network.confirm_hashes (transaction, sender, blocks_bundle);\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tvoid confirm_ack (nano::confirm_ack const & message_a) override\n\t{\n\t\tif (node.config.logging.network_message_logging ())\n\t\t{\n\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Received confirm_ack message from %1% for %2%sequence %3%\") % sender % message_a.vote->hashes_string () % std::to_string (message_a.vote->sequence));\n\t\t}\n\t\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::confirm_ack, nano::stat::dir::in);\n\t\tnode.peers.contacted (sender, message_a.header.version_using);\n\t\tfor (auto & vote_block : message_a.vote->blocks)\n\t\t{\n\t\t\tif (!vote_block.which ())\n\t\t\t{\n\t\t\t\tauto block (boost::get<std::shared_ptr<nano::block>> (vote_block));\n\t\t\t\tif (!node.block_processor.full ())\n\t\t\t\t{\n\t\t\t\t\tnode.process_active (block);\n\t\t\t\t}\n\t\t\t\tnode.active.publish (block);\n\t\t\t}\n\t\t}\n\t\tnode.vote_processor.vote (message_a.vote, sender);\n\t}\n\tvoid bulk_pull (nano::bulk_pull const &) override\n\t{\n\t\tassert (false);\n\t}\n\tvoid bulk_pull_account (nano::bulk_pull_account const &) override\n\t{\n\t\tassert (false);\n\t}\n\tvoid bulk_push (nano::bulk_push const &) override\n\t{\n\t\tassert (false);\n\t}\n\tvoid frontier_req (nano::frontier_req const &) override\n\t{\n\t\tassert (false);\n\t}\n\tvoid node_id_handshake (nano::node_id_handshake const & message_a) override\n\t{\n\t\tif (node.config.logging.network_node_id_handshake_logging ())\n\t\t{\n\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Received node_id_handshake message from %1% with query %2% and response account %3%\") % sender % (message_a.query ? message_a.query->to_string () : std::string (\"[none]\")) % (message_a.response ? message_a.response->first.to_account () : std::string (\"[none]\")));\n\t\t}\n\t\tauto endpoint_l (nano::map_endpoint_to_v6 (sender));\n\t\tboost::optional<nano::uint256_union> out_query;\n\t\tboost::optional<nano::uint256_union> out_respond_to;\n\t\tif (message_a.query)\n\t\t{\n\t\t\tout_respond_to = message_a.query;\n\t\t}\n\t\tauto validated_response (false);\n\t\tif (message_a.response)\n\t\t{\n\t\t\tif (!node.peers.validate_syn_cookie (endpoint_l, message_a.response->first, message_a.response->second))\n\t\t\t{\n\t\t\t\tvalidated_response = true;\n\t\t\t\tif (message_a.response->first != node.node_id.pub)\n\t\t\t\t{\n\t\t\t\t\tnode.peers.insert (endpoint_l, message_a.header.version_using, false, message_a.response->first);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse if (node.config.logging.network_node_id_handshake_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Failed to validate syn cookie signature %1% by %2%\") % message_a.response->second.to_string () % message_a.response->first.to_account ());\n\t\t\t}\n\t\t}\n\t\tif (!validated_response && !node.peers.known_peer (endpoint_l))\n\t\t{\n\t\t\tout_query = node.peers.assign_syn_cookie (endpoint_l);\n\t\t}\n\t\tif (out_query || out_respond_to)\n\t\t{\n\t\t\tnode.network.send_node_id_handshake (sender, out_query, out_respond_to);\n\t\t}\n\t\tnode.stats.inc (nano::stat::type::message, nano::stat::detail::node_id_handshake, nano::stat::dir::in);\n\t}\n\tnano::node & node;\n\tnano::endpoint sender;\n};\n}\n\nvoid nano::network::receive_action (nano::udp_data * data_a)\n{\n\tauto allowed_sender (true);\n\tif (data_a->endpoint == endpoint ())\n\t{\n\t\tallowed_sender = false;\n\t}\n\telse if (nano::reserved_address (data_a->endpoint, false) && !node.config.allow_local_peers)\n\t{\n\t\tallowed_sender = false;\n\t}\n\tif (allowed_sender)\n\t{\n\t\tnetwork_message_visitor visitor (node, data_a->endpoint);\n\t\tnano::message_parser parser (node.block_uniquer, node.vote_uniquer, visitor, node.work);\n\t\tparser.deserialize_buffer (data_a->buffer, data_a->size);\n\t\tif (parser.status != nano::message_parser::parse_status::success)\n\t\t{\n\t\t\tnode.stats.inc (nano::stat::type::error);\n\n\t\t\tswitch (parser.status)\n\t\t\t{\n\t\t\t\tcase nano::message_parser::parse_status::insufficient_work:\n\t\t\t\t\t// We've already increment error count, update detail only\n\t\t\t\t\tnode.stats.inc_detail_only (nano::stat::type::error, nano::stat::detail::insufficient_work);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_magic:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_magic);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_network:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_network);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_header:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_header);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_message_type:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_message_type);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_keepalive_message:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_keepalive_message);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_publish_message:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_publish_message);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_confirm_req_message:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_confirm_req_message);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_confirm_ack_message:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_confirm_ack_message);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::invalid_node_id_handshake_message:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::invalid_node_id_handshake_message);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::outdated_version:\n\t\t\t\t\tnode.stats.inc (nano::stat::type::udp, nano::stat::detail::outdated_version);\n\t\t\t\t\tbreak;\n\t\t\t\tcase nano::message_parser::parse_status::success:\n\t\t\t\t\t/* Already checked, unreachable */\n\t\t\t\t\tbreak;\n\t\t\t}\n\n\t\t\tif (node.config.logging.network_logging () && parser.status != nano::message_parser::parse_status::outdated_version)\n\t\t\t{\n\t\t\t\tBOOST_LOG (node.log) << \"Could not parse message.  Error: \" << parser.status_string ();\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tnode.stats.add (nano::stat::type::traffic, nano::stat::dir::in, data_a->size);\n\t\t}\n\t}\n\telse\n\t{\n\t\tif (node.config.logging.network_logging ())\n\t\t{\n\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Reserved sender %1%\") % data_a->endpoint.address ().to_string ());\n\t\t}\n\n\t\tnode.stats.inc_detail_only (nano::stat::type::error, nano::stat::detail::bad_sender);\n\t}\n}\n\n// Send keepalives to all the peers we've been notified of\nvoid nano::network::merge_peers (std::array<nano::endpoint, 8> const & peers_a)\n{\n\tfor (auto i (peers_a.begin ()), j (peers_a.end ()); i != j; ++i)\n\t{\n\t\tif (!node.peers.reachout (*i))\n\t\t{\n\t\t\tsend_keepalive (*i);\n\t\t}\n\t}\n}\n\nbool nano::operation::operator> (nano::operation const & other_a) const\n{\n\treturn wakeup > other_a.wakeup;\n}\n\nnano::alarm::alarm (boost::asio::io_context & io_ctx_a) :\nio_ctx (io_ctx_a),\nthread ([this]() {\n\tnano::thread_role::set (nano::thread_role::name::alarm);\n\trun ();\n})\n{\n}\n\nnano::alarm::~alarm ()\n{\n\tadd (std::chrono::steady_clock::now (), nullptr);\n\tthread.join ();\n}\n\nvoid nano::alarm::run ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\tauto done (false);\n\twhile (!done)\n\t{\n\t\tif (!operations.empty ())\n\t\t{\n\t\t\tauto & operation (operations.top ());\n\t\t\tif (operation.function)\n\t\t\t{\n\t\t\t\tif (operation.wakeup <= std::chrono::steady_clock::now ())\n\t\t\t\t{\n\t\t\t\t\tio_ctx.post (operation.function);\n\t\t\t\t\toperations.pop ();\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tauto wakeup (operation.wakeup);\n\t\t\t\t\tcondition.wait_until (lock, wakeup);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tdone = true;\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tcondition.wait (lock);\n\t\t}\n\t}\n}\n\nvoid nano::alarm::add (std::chrono::steady_clock::time_point const & wakeup_a, std::function<void()> const & operation)\n{\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\toperations.push (nano::operation ({ wakeup_a, operation }));\n\t}\n\tcondition.notify_all ();\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (alarm & alarm, const std::string & name)\n{\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tauto count = alarm.operations.size ();\n\tauto sizeof_element = sizeof (decltype (alarm.operations)::value_type);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"operations\", count, sizeof_element }));\n\treturn composite;\n}\n}\n\nnano::node_init::node_init () :\nblock_store_init (false),\nwallet_init (false)\n{\n}\n\nbool nano::node_init::error ()\n{\n\treturn block_store_init || wallet_init || wallets_store_init;\n}\n\nnano::vote_processor::vote_processor (nano::node & node_a) :\nnode (node_a),\nstarted (false),\nstopped (false),\nactive (false),\nthread ([this]() {\n\tnano::thread_role::set (nano::thread_role::name::vote_processing);\n\tprocess_loop ();\n})\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (!started)\n\t{\n\t\tcondition.wait (lock);\n\t}\n}\n\nvoid nano::vote_processor::process_loop ()\n{\n\tstd::chrono::steady_clock::time_point start_time, end_time;\n\tstd::chrono::steady_clock::duration elapsed_time;\n\tstd::chrono::milliseconds elapsed_time_ms;\n\tuint64_t elapsed_time_ms_int;\n\tbool log_this_iteration;\n\n\tstd::unique_lock<std::mutex> lock (mutex);\n\tstarted = true;\n\n\tlock.unlock ();\n\tcondition.notify_all ();\n\tlock.lock ();\n\n\twhile (!stopped)\n\t{\n\t\tif (!votes.empty ())\n\t\t{\n\t\t\tstd::deque<std::pair<std::shared_ptr<nano::vote>, nano::endpoint>> votes_l;\n\t\t\tvotes_l.swap (votes);\n\n\t\t\tlog_this_iteration = false;\n\t\t\tif (node.config.logging.network_logging () && votes_l.size () > 50)\n\t\t\t{\n\t\t\t\t/*\n\t\t\t\t * Only log the timing information for this iteration if\n\t\t\t\t * there are a sufficient number of items for it to be relevant\n\t\t\t\t */\n\t\t\t\tlog_this_iteration = true;\n\t\t\t\tstart_time = std::chrono::steady_clock::now ();\n\t\t\t}\n\t\t\tactive = true;\n\t\t\tlock.unlock ();\n\t\t\tverify_votes (votes_l);\n\t\t\t{\n\t\t\t\tstd::unique_lock<std::mutex> active_single_lock (node.active.mutex);\n\t\t\t\tauto transaction (node.store.tx_begin_read ());\n\t\t\t\tuint64_t count (1);\n\t\t\t\tfor (auto & i : votes_l)\n\t\t\t\t{\n\t\t\t\t\tvote_blocking (transaction, i.first, i.second, true);\n\t\t\t\t\t// Free active_transactions mutex each 100 processed votes\n\t\t\t\t\tif (count % 100 == 0)\n\t\t\t\t\t{\n\t\t\t\t\t\tactive_single_lock.unlock ();\n\t\t\t\t\t\tactive_single_lock.lock ();\n\t\t\t\t\t}\n\t\t\t\t\tcount++;\n\t\t\t\t}\n\t\t\t}\n\t\t\tlock.lock ();\n\t\t\tactive = false;\n\n\t\t\tlock.unlock ();\n\t\t\tcondition.notify_all ();\n\t\t\tlock.lock ();\n\n\t\t\tif (log_this_iteration)\n\t\t\t{\n\t\t\t\tend_time = std::chrono::steady_clock::now ();\n\t\t\t\telapsed_time = end_time - start_time;\n\t\t\t\telapsed_time_ms = std::chrono::duration_cast<std::chrono::milliseconds> (elapsed_time);\n\t\t\t\telapsed_time_ms_int = elapsed_time_ms.count ();\n\n\t\t\t\tif (elapsed_time_ms_int >= 100)\n\t\t\t\t{\n\t\t\t\t\t/*\n\t\t\t\t\t * If the time spent was less than 100ms then\n\t\t\t\t\t * the results are probably not useful as well,\n\t\t\t\t\t * so don't spam the logs.\n\t\t\t\t\t */\n\t\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Processed %1% votes in %2% milliseconds (rate of %3% votes per second)\") % votes_l.size () % elapsed_time_ms_int % ((votes_l.size () * 1000ULL) / elapsed_time_ms_int));\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tcondition.wait (lock);\n\t\t}\n\t}\n}\n\nvoid nano::vote_processor::vote (std::shared_ptr<nano::vote> vote_a, nano::endpoint endpoint_a)\n{\n\tassert (endpoint_a.address ().is_v6 ());\n\tstd::unique_lock<std::mutex> lock (mutex);\n\tif (!stopped)\n\t{\n\t\tbool process (false);\n\t\t/* Random early delection levels\n\t\tAlways process votes for test network (process = true)\n\t\tStop processing with max 144 * 1024 votes */\n\t\tif (!nano::is_test_network)\n\t\t{\n\t\t\t// Level 0 (< 0.1%)\n\t\t\tif (votes.size () < 96 * 1024)\n\t\t\t{\n\t\t\t\tprocess = true;\n\t\t\t}\n\t\t\t// Level 1 (0.1-1%)\n\t\t\telse if (votes.size () < 112 * 1024)\n\t\t\t{\n\t\t\t\tprocess = (representatives_1.find (vote_a->account) != representatives_1.end ());\n\t\t\t}\n\t\t\t// Level 2 (1-5%)\n\t\t\telse if (votes.size () < 128 * 1024)\n\t\t\t{\n\t\t\t\tprocess = (representatives_2.find (vote_a->account) != representatives_2.end ());\n\t\t\t}\n\t\t\t// Level 3 (> 5%)\n\t\t\telse if (votes.size () < 144 * 1024)\n\t\t\t{\n\t\t\t\tprocess = (representatives_3.find (vote_a->account) != representatives_3.end ());\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\t// Process for test network\n\t\t\tprocess = true;\n\t\t}\n\t\tif (process)\n\t\t{\n\t\t\tvotes.push_back (std::make_pair (vote_a, endpoint_a));\n\n\t\t\tlock.unlock ();\n\t\t\tcondition.notify_all ();\n\t\t\tlock.lock ();\n\t\t}\n\t\telse\n\t\t{\n\t\t\tnode.stats.inc (nano::stat::type::vote, nano::stat::detail::vote_overflow);\n\t\t\tif (node.config.logging.vote_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node.log) << \"Votes overflow\";\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid nano::vote_processor::verify_votes (std::deque<std::pair<std::shared_ptr<nano::vote>, nano::endpoint>> & votes_a)\n{\n\tauto size (votes_a.size ());\n\tstd::vector<unsigned char const *> messages;\n\tmessages.reserve (size);\n\tstd::vector<nano::uint256_union> hashes;\n\thashes.reserve (size);\n\tstd::vector<size_t> lengths (size, sizeof (nano::uint256_union));\n\tstd::vector<unsigned char const *> pub_keys;\n\tpub_keys.reserve (size);\n\tstd::vector<unsigned char const *> signatures;\n\tsignatures.reserve (size);\n\tstd::vector<int> verifications;\n\tverifications.resize (size);\n\tfor (auto & vote : votes_a)\n\t{\n\t\thashes.push_back (vote.first->hash ());\n\t\tmessages.push_back (hashes.back ().bytes.data ());\n\t\tpub_keys.push_back (vote.first->account.bytes.data ());\n\t\tsignatures.push_back (vote.first->signature.bytes.data ());\n\t}\n\tnano::signature_check_set check = { size, messages.data (), lengths.data (), pub_keys.data (), signatures.data (), verifications.data () };\n\tnode.checker.verify (check);\n\tstd::remove_reference_t<decltype (votes_a)> result;\n\tauto i (0);\n\tfor (auto & vote : votes_a)\n\t{\n\t\tassert (verifications[i] == 1 || verifications[i] == 0);\n\t\tif (verifications[i] == 1)\n\t\t{\n\t\t\tresult.push_back (vote);\n\t\t}\n\t\t++i;\n\t}\n\tvotes_a.swap (result);\n}\n\n// node.active.mutex lock required\nnano::vote_code nano::vote_processor::vote_blocking (nano::transaction const & transaction_a, std::shared_ptr<nano::vote> vote_a, nano::endpoint endpoint_a, bool validated)\n{\n\tassert (endpoint_a.address ().is_v6 ());\n\tassert (!node.active.mutex.try_lock ());\n\tauto result (nano::vote_code::invalid);\n\tif (validated || !vote_a->validate ())\n\t{\n\t\tauto max_vote (node.store.vote_max (transaction_a, vote_a));\n\t\tresult = nano::vote_code::replay;\n\t\tif (!node.active.vote (vote_a, true))\n\t\t{\n\t\t\tresult = nano::vote_code::vote;\n\t\t}\n\t\tswitch (result)\n\t\t{\n\t\t\tcase nano::vote_code::vote:\n\t\t\t\tnode.observers.vote.notify (transaction_a, vote_a, endpoint_a);\n\t\t\tcase nano::vote_code::replay:\n\t\t\t\t// This tries to assist rep nodes that have lost track of their highest sequence number by replaying our highest known vote back to them\n\t\t\t\t// Only do this if the sequence number is significantly different to account for network reordering\n\t\t\t\t// Amplify attack considerations: We're sending out a confirm_ack in response to a confirm_ack for no net traffic increase\n\t\t\t\tif (max_vote->sequence > vote_a->sequence + 10000)\n\t\t\t\t{\n\t\t\t\t\tnano::confirm_ack confirm (max_vote);\n\t\t\t\t\tnode.network.confirm_send (confirm, confirm.to_bytes (), endpoint_a);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase nano::vote_code::invalid:\n\t\t\t\tassert (false);\n\t\t\t\tbreak;\n\t\t}\n\t}\n\tstd::string status;\n\tswitch (result)\n\t{\n\t\tcase nano::vote_code::invalid:\n\t\t\tstatus = \"Invalid\";\n\t\t\tnode.stats.inc (nano::stat::type::vote, nano::stat::detail::vote_invalid);\n\t\t\tbreak;\n\t\tcase nano::vote_code::replay:\n\t\t\tstatus = \"Replay\";\n\t\t\tnode.stats.inc (nano::stat::type::vote, nano::stat::detail::vote_replay);\n\t\t\tbreak;\n\t\tcase nano::vote_code::vote:\n\t\t\tstatus = \"Vote\";\n\t\t\tnode.stats.inc (nano::stat::type::vote, nano::stat::detail::vote_valid);\n\t\t\tbreak;\n\t}\n\tif (node.config.logging.vote_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Vote from: %1% sequence: %2% block(s): %3%status: %4%\") % vote_a->account.to_account () % std::to_string (vote_a->sequence) % vote_a->hashes_string () % status);\n\t}\n\treturn result;\n}\n\nvoid nano::vote_processor::stop ()\n{\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\tstopped = true;\n\t}\n\tcondition.notify_all ();\n\tif (thread.joinable ())\n\t{\n\t\tthread.join ();\n\t}\n}\n\nvoid nano::vote_processor::flush ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (active || !votes.empty ())\n\t{\n\t\tcondition.wait (lock);\n\t}\n}\n\nvoid nano::vote_processor::calculate_weights ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\tif (!stopped)\n\t{\n\t\trepresentatives_1.clear ();\n\t\trepresentatives_2.clear ();\n\t\trepresentatives_3.clear ();\n\t\tauto supply (node.online_reps.online_stake ());\n\t\tauto transaction (node.store.tx_begin_read ());\n\t\tfor (auto i (node.store.representation_begin (transaction)), n (node.store.representation_end ()); i != n; ++i)\n\t\t{\n\t\t\tnano::account representative (i->first);\n\t\t\tauto weight (node.ledger.weight (transaction, representative));\n\t\t\tif (weight > supply / 1000) // 0.1% or above (level 1)\n\t\t\t{\n\t\t\t\trepresentatives_1.insert (representative);\n\t\t\t\tif (weight > supply / 100) // 1% or above (level 2)\n\t\t\t\t{\n\t\t\t\t\trepresentatives_2.insert (representative);\n\t\t\t\t\tif (weight > supply / 20) // 5% or above (level 3)\n\t\t\t\t\t{\n\t\t\t\t\t\trepresentatives_3.insert (representative);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (node_observers & node_observers, const std::string & name)\n{\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (collect_seq_con_info (node_observers.blocks, \"blocks\"));\n\tcomposite->add_component (collect_seq_con_info (node_observers.wallet, \"wallet\"));\n\tcomposite->add_component (collect_seq_con_info (node_observers.vote, \"vote\"));\n\tcomposite->add_component (collect_seq_con_info (node_observers.account_balance, \"account_balance\"));\n\tcomposite->add_component (collect_seq_con_info (node_observers.endpoint, \"endpoint\"));\n\tcomposite->add_component (collect_seq_con_info (node_observers.disconnect, \"disconnect\"));\n\treturn composite;\n}\n\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (vote_processor & vote_processor, const std::string & name)\n{\n\tsize_t votes_count = 0;\n\tsize_t representatives_1_count = 0;\n\tsize_t representatives_2_count = 0;\n\tsize_t representatives_3_count = 0;\n\n\t{\n\t\tstd::lock_guard<std::mutex> (vote_processor.mutex);\n\t\tvotes_count = vote_processor.votes.size ();\n\t\trepresentatives_1_count = vote_processor.representatives_1.size ();\n\t\trepresentatives_2_count = vote_processor.representatives_2.size ();\n\t\trepresentatives_3_count = vote_processor.representatives_3.size ();\n\t}\n\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"votes\", votes_count, sizeof (decltype (vote_processor.votes)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"representatives_1\", representatives_1_count, sizeof (decltype (vote_processor.representatives_1)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"representatives_2\", representatives_2_count, sizeof (decltype (vote_processor.representatives_2)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"representatives_3\", representatives_3_count, sizeof (decltype (vote_processor.representatives_3)::value_type) }));\n\treturn composite;\n}\n}\n\nvoid nano::rep_crawler::add (nano::block_hash const & hash_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tactive.insert (hash_a);\n}\n\nvoid nano::rep_crawler::remove (nano::block_hash const & hash_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tactive.erase (hash_a);\n}\n\nbool nano::rep_crawler::exists (nano::block_hash const & hash_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\treturn active.count (hash_a) != 0;\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (rep_crawler & rep_crawler, const std::string & name)\n{\n\tsize_t count = 0;\n\t{\n\t\tstd::lock_guard<std::mutex> guard (rep_crawler.mutex);\n\t\tcount = rep_crawler.active.size ();\n\t}\n\n\tauto sizeof_element = sizeof (decltype (rep_crawler.active)::value_type);\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"active\", count, sizeof_element }));\n\treturn composite;\n}\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (block_processor & block_processor, const std::string & name)\n{\n\tsize_t state_blocks_count = 0;\n\tsize_t blocks_count = 0;\n\tsize_t blocks_hashes_count = 0;\n\tsize_t forced_count = 0;\n\tsize_t rolled_back_count = 0;\n\n\t{\n\t\tstd::lock_guard<std::mutex> guard (block_processor.mutex);\n\t\tstate_blocks_count = block_processor.state_blocks.size ();\n\t\tblocks_count = block_processor.blocks.size ();\n\t\tblocks_hashes_count = block_processor.blocks_hashes.size ();\n\t\tforced_count = block_processor.forced.size ();\n\t\trolled_back_count = block_processor.rolled_back.size ();\n\t}\n\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"state_blocks\", state_blocks_count, sizeof (decltype (block_processor.state_blocks)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"blocks\", blocks_count, sizeof (decltype (block_processor.blocks)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"blocks_hashes\", blocks_hashes_count, sizeof (decltype (block_processor.blocks_hashes)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"forced\", forced_count, sizeof (decltype (block_processor.forced)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"rolled_back\", rolled_back_count, sizeof (decltype (block_processor.rolled_back)::value_type) }));\n\tcomposite->add_component (collect_seq_con_info (block_processor.generator, \"generator\"));\n\treturn composite;\n}\n}\n\nnano::node::node (nano::node_init & init_a, boost::asio::io_context & io_ctx_a, uint16_t peering_port_a, boost::filesystem::path const & application_path_a, nano::alarm & alarm_a, nano::logging const & logging_a, nano::work_pool & work_a) :\nnode (init_a, io_ctx_a, application_path_a, alarm_a, nano::node_config (peering_port_a, logging_a), work_a)\n{\n}\n\nnano::node::node (nano::node_init & init_a, boost::asio::io_context & io_ctx_a, boost::filesystem::path const & application_path_a, nano::alarm & alarm_a, nano::node_config const & config_a, nano::work_pool & work_a, nano::node_flags flags_a) :\nio_ctx (io_ctx_a),\nconfig (config_a),\nflags (flags_a),\nalarm (alarm_a),\nwork (work_a),\nstore_impl (std::make_unique<nano::mdb_store> (init_a.block_store_init, config.logging, application_path_a / \"data.ldb\", config_a.lmdb_max_dbs, !flags.disable_unchecked_drop, flags.sideband_batch_size)),\nstore (*store_impl),\nwallets_store_impl (std::make_unique<nano::mdb_wallets_store> (init_a.wallets_store_init, application_path_a / \"wallets.ldb\", config_a.lmdb_max_dbs)),\nwallets_store (*wallets_store_impl),\ngap_cache (*this),\nledger (store, stats, config.epoch_block_link, config.epoch_block_signer),\nactive (*this),\nnetwork (*this, config.peering_port),\nbootstrap_initiator (*this),\nbootstrap (io_ctx_a, config.peering_port, *this),\npeers (network.endpoint ()),\napplication_path (application_path_a),\nwallets (init_a.wallet_init, *this),\nport_mapping (*this),\nchecker (config.signature_checker_threads),\nvote_processor (*this),\nwarmed_up (0),\nblock_processor (*this),\nblock_processor_thread ([this]() {\n\tnano::thread_role::set (nano::thread_role::name::block_processing);\n\tthis->block_processor.process_blocks ();\n}),\nonline_reps (ledger, config.online_weight_minimum.number ()),\nstats (config.stat_config),\nvote_uniquer (block_uniquer),\nstartup_time (std::chrono::steady_clock::now ())\n{\n\twallets.observer = [this](bool active) {\n\t\tobservers.wallet.notify (active);\n\t};\n\tpeers.peer_observer = [this](nano::endpoint const & endpoint_a) {\n\t\tobservers.endpoint.notify (endpoint_a);\n\t};\n\tpeers.disconnect_observer = [this]() {\n\t\tobservers.disconnect.notify ();\n\t};\n\tif (!config.callback_address.empty ())\n\t{\n\t\tobservers.blocks.add ([this](std::shared_ptr<nano::block> block_a, nano::account const & account_a, nano::amount const & amount_a, bool is_state_send_a) {\n\t\t\tif (this->block_arrival.recent (block_a->hash ()))\n\t\t\t{\n\t\t\t\tauto node_l (shared_from_this ());\n\t\t\t\tbackground ([node_l, block_a, account_a, amount_a, is_state_send_a]() {\n\t\t\t\t\tboost::property_tree::ptree event;\n\t\t\t\t\tevent.add (\"account\", account_a.to_account ());\n\t\t\t\t\tevent.add (\"hash\", block_a->hash ().to_string ());\n\t\t\t\t\tstd::string block_text;\n\t\t\t\t\tblock_a->serialize_json (block_text);\n\t\t\t\t\tevent.add (\"block\", block_text);\n\t\t\t\t\tevent.add (\"amount\", amount_a.to_string_dec ());\n\t\t\t\t\tif (is_state_send_a)\n\t\t\t\t\t{\n\t\t\t\t\t\tevent.add (\"is_send\", is_state_send_a);\n\t\t\t\t\t}\n\t\t\t\t\tstd::stringstream ostream;\n\t\t\t\t\tboost::property_tree::write_json (ostream, event);\n\t\t\t\t\tostream.flush ();\n\t\t\t\t\tauto body (std::make_shared<std::string> (ostream.str ()));\n\t\t\t\t\tauto address (node_l->config.callback_address);\n\t\t\t\t\tauto port (node_l->config.callback_port);\n\t\t\t\t\tauto target (std::make_shared<std::string> (node_l->config.callback_target));\n\t\t\t\t\tauto resolver (std::make_shared<boost::asio::ip::tcp::resolver> (node_l->io_ctx));\n\t\t\t\t\tresolver->async_resolve (boost::asio::ip::tcp::resolver::query (address, std::to_string (port)), [node_l, address, port, target, body, resolver](boost::system::error_code const & ec, boost::asio::ip::tcp::resolver::iterator i_a) {\n\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tnode_l->do_rpc_callback (i_a, address, port, target, body, resolver);\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif (node_l->config.logging.callback_logging ())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error resolving callback: %1%:%2%: %3%\") % address % port % ec.message ());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tnode_l->stats.inc (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t});\n\t\t\t}\n\t\t});\n\t}\n\tobservers.endpoint.add ([this](nano::endpoint const & endpoint_a) {\n\t\tthis->network.send_keepalive (endpoint_a);\n\t\trep_query (*this, endpoint_a);\n\t});\n\tobservers.vote.add ([this](nano::transaction const & transaction, std::shared_ptr<nano::vote> vote_a, nano::endpoint const & endpoint_a) {\n\t\tassert (endpoint_a.address ().is_v6 ());\n\t\tthis->gap_cache.vote (vote_a);\n\t\tthis->online_reps.observe (vote_a->account);\n\t\tnano::uint128_t rep_weight;\n\t\tnano::uint128_t min_rep_weight;\n\t\t{\n\t\t\trep_weight = ledger.weight (transaction, vote_a->account);\n\t\t\tmin_rep_weight = online_reps.online_stake () / 1000;\n\t\t}\n\t\tif (rep_weight > min_rep_weight)\n\t\t{\n\t\t\tbool rep_crawler_exists (false);\n\t\t\tfor (auto hash : *vote_a)\n\t\t\t{\n\t\t\t\tif (this->rep_crawler.exists (hash))\n\t\t\t\t{\n\t\t\t\t\trep_crawler_exists = true;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (rep_crawler_exists)\n\t\t\t{\n\t\t\t\t// We see a valid non-replay vote for a block we requested, this node is probably a representative\n\t\t\t\tif (this->peers.rep_response (endpoint_a, vote_a->account, rep_weight))\n\t\t\t\t{\n\t\t\t\t\tBOOST_LOG (log) << boost::str (boost::format (\"Found a representative at %1%\") % endpoint_a);\n\t\t\t\t\t// Rebroadcasting all active votes to new representative\n\t\t\t\t\tauto blocks (this->active.list_blocks (true));\n\t\t\t\t\tfor (auto i (blocks.begin ()), n (blocks.end ()); i != n; ++i)\n\t\t\t\t\t{\n\t\t\t\t\t\tif (*i != nullptr)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tthis->network.send_confirm_req (endpoint_a, *i);\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t});\n\tif (NANO_VERSION_PATCH == 0)\n\t{\n\t\tBOOST_LOG (log) << \"Node starting, version: \" << NANO_MAJOR_MINOR_VERSION;\n\t}\n\telse\n\t{\n\t\tBOOST_LOG (log) << \"Node starting, version: \" << NANO_MAJOR_MINOR_RC_VERSION;\n\t}\n\n\tBOOST_LOG (log) << boost::str (boost::format (\"Work pool running %1% threads\") % work.threads.size ());\n\tif (!init_a.error ())\n\t{\n\t\tif (config.logging.node_lifetime_tracing ())\n\t\t{\n\t\t\tBOOST_LOG (log) << \"Constructing node\";\n\t\t}\n\t\tnano::genesis genesis;\n\t\tauto transaction (store.tx_begin_write ());\n\t\tif (store.latest_begin (transaction) == store.latest_end ())\n\t\t{\n\t\t\t// Store was empty meaning we just created it, add the genesis block\n\t\t\tstore.initialize (transaction, genesis);\n\t\t}\n\t\tif (!store.block_exists (transaction, genesis.hash ()))\n\t\t{\n\t\t\tBOOST_LOG (log) << \"Genesis block not found. Make sure the node network ID is correct.\";\n\t\t\tstd::exit (1);\n\t\t}\n\n\t\tnode_id = nano::keypair (store.get_node_id (transaction));\n\t\tBOOST_LOG (log) << \"Node ID: \" << node_id.pub.to_account ();\n\t}\n\tpeers.online_weight_minimum = config.online_weight_minimum.number ();\n\tif (nano::is_live_network || nano::is_beta_network)\n\t{\n\t\tnano::bufferstream weight_stream ((const uint8_t *)nano_bootstrap_weights, nano_bootstrap_weights_size);\n\t\tnano::uint128_union block_height;\n\t\tif (!nano::try_read (weight_stream, block_height))\n\t\t{\n\t\t\tauto max_blocks = (uint64_t)block_height.number ();\n\t\t\tauto transaction (store.tx_begin_read ());\n\t\t\tif (ledger.store.block_count (transaction).sum () < max_blocks)\n\t\t\t{\n\t\t\t\tledger.bootstrap_weight_max_blocks = max_blocks;\n\t\t\t\twhile (true)\n\t\t\t\t{\n\t\t\t\t\tnano::account account;\n\t\t\t\t\tif (nano::try_read (weight_stream, account.bytes))\n\t\t\t\t\t{\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tnano::amount weight;\n\t\t\t\t\tif (nano::try_read (weight_stream, weight.bytes))\n\t\t\t\t\t{\n\t\t\t\t\t\tbreak;\n\t\t\t\t\t}\n\t\t\t\t\tBOOST_LOG (log) << \"Using bootstrap rep weight: \" << account.to_account () << \" -> \" << weight.format_balance (Mxrb_ratio, 0, true) << \" XRB\";\n\t\t\t\t\tledger.bootstrap_weights[account] = weight.number ();\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nnano::node::~node ()\n{\n\tif (config.logging.node_lifetime_tracing ())\n\t{\n\t\tBOOST_LOG (log) << \"Destructing node\";\n\t}\n\tstop ();\n}\n\nvoid nano::node::do_rpc_callback (boost::asio::ip::tcp::resolver::iterator i_a, std::string const & address, uint16_t port, std::shared_ptr<std::string> target, std::shared_ptr<std::string> body, std::shared_ptr<boost::asio::ip::tcp::resolver> resolver)\n{\n\tif (i_a != boost::asio::ip::tcp::resolver::iterator{})\n\t{\n\t\tauto node_l (shared_from_this ());\n\t\tauto sock (std::make_shared<boost::asio::ip::tcp::socket> (node_l->io_ctx));\n\t\tsock->async_connect (i_a->endpoint (), [node_l, target, body, sock, address, port, i_a, resolver](boost::system::error_code const & ec) mutable {\n\t\t\tif (!ec)\n\t\t\t{\n\t\t\t\tauto req (std::make_shared<boost::beast::http::request<boost::beast::http::string_body>> ());\n\t\t\t\treq->method (boost::beast::http::verb::post);\n\t\t\t\treq->target (*target);\n\t\t\t\treq->version (11);\n\t\t\t\treq->insert (boost::beast::http::field::host, address);\n\t\t\t\treq->insert (boost::beast::http::field::content_type, \"application/json\");\n\t\t\t\treq->body () = *body;\n\t\t\t\treq->prepare_payload ();\n\t\t\t\tboost::beast::http::async_write (*sock, *req, [node_l, sock, address, port, req, i_a, target, body, resolver](boost::system::error_code const & ec, size_t bytes_transferred) mutable {\n\t\t\t\t\tif (!ec)\n\t\t\t\t\t{\n\t\t\t\t\t\tauto sb (std::make_shared<boost::beast::flat_buffer> ());\n\t\t\t\t\t\tauto resp (std::make_shared<boost::beast::http::response<boost::beast::http::string_body>> ());\n\t\t\t\t\t\tboost::beast::http::async_read (*sock, *sb, *resp, [node_l, sb, resp, sock, address, port, i_a, target, body, resolver](boost::system::error_code const & ec, size_t bytes_transferred) mutable {\n\t\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (resp->result () == boost::beast::http::status::ok)\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tnode_l->stats.inc (nano::stat::type::http_callback, nano::stat::detail::initiate, nano::stat::dir::out);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tif (node_l->config.logging.callback_logging ())\n\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Callback to %1%:%2% failed with status: %3%\") % address % port % resp->result ());\n\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\tnode_l->stats.inc (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (node_l->config.logging.callback_logging ())\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Unable complete callback: %1%:%2%: %3%\") % address % port % ec.message ());\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tnode_l->stats.inc (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out);\n\t\t\t\t\t\t\t};\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tif (node_l->config.logging.callback_logging ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Unable to send callback: %1%:%2%: %3%\") % address % port % ec.message ());\n\t\t\t\t\t\t}\n\t\t\t\t\t\tnode_l->stats.inc (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out);\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tif (node_l->config.logging.callback_logging ())\n\t\t\t\t{\n\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Unable to connect to callback address: %1%:%2%: %3%\") % address % port % ec.message ());\n\t\t\t\t}\n\t\t\t\tnode_l->stats.inc (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out);\n\t\t\t\t++i_a;\n\t\t\t\tnode_l->do_rpc_callback (i_a, address, port, target, body, resolver);\n\t\t\t}\n\t\t});\n\t}\n}\n\nbool nano::node::copy_with_compaction (boost::filesystem::path const & destination_file)\n{\n\treturn !mdb_env_copy2 (boost::polymorphic_downcast<nano::mdb_store *> (store_impl.get ())->env.environment, destination_file.string ().c_str (), MDB_CP_COMPACT);\n}\n\nvoid nano::node::send_keepalive (nano::endpoint const & endpoint_a)\n{\n\tnetwork.send_keepalive (nano::map_endpoint_to_v6 (endpoint_a));\n}\n\nvoid nano::node::process_fork (nano::transaction const & transaction_a, std::shared_ptr<nano::block> block_a)\n{\n\tauto root (block_a->root ());\n\tif (!store.block_exists (transaction_a, block_a->type (), block_a->hash ()) && store.root_exists (transaction_a, block_a->root ()))\n\t{\n\t\tstd::shared_ptr<nano::block> ledger_block (ledger.forked_block (transaction_a, *block_a));\n\t\tif (ledger_block)\n\t\t{\n\t\t\tstd::weak_ptr<nano::node> this_w (shared_from_this ());\n\t\t\tif (!active.start (ledger_block, [this_w, root](std::shared_ptr<nano::block>) {\n\t\t\t\t    if (auto this_l = this_w.lock ())\n\t\t\t\t    {\n\t\t\t\t\t    auto attempt (this_l->bootstrap_initiator.current_attempt ());\n\t\t\t\t\t    if (attempt && attempt->mode == nano::bootstrap_mode::legacy)\n\t\t\t\t\t    {\n\t\t\t\t\t\t    auto transaction (this_l->store.tx_begin_read ());\n\t\t\t\t\t\t    auto account (this_l->ledger.store.frontier_get (transaction, root));\n\t\t\t\t\t\t    if (!account.is_zero ())\n\t\t\t\t\t\t    {\n\t\t\t\t\t\t\t    attempt->requeue_pull (nano::pull_info (account, root, root));\n\t\t\t\t\t\t    }\n\t\t\t\t\t\t    else if (this_l->ledger.store.account_exists (transaction, root))\n\t\t\t\t\t\t    {\n\t\t\t\t\t\t\t    attempt->requeue_pull (nano::pull_info (root, nano::block_hash (0), nano::block_hash (0)));\n\t\t\t\t\t\t    }\n\t\t\t\t\t    }\n\t\t\t\t    }\n\t\t\t    }))\n\t\t\t{\n\t\t\t\tBOOST_LOG (log) << boost::str (boost::format (\"Resolving fork between our block: %1% and block %2% both with root %3%\") % ledger_block->hash ().to_string () % block_a->hash ().to_string () % block_a->root ().to_string ());\n\t\t\t\tnetwork.broadcast_confirm_req (ledger_block);\n\t\t\t}\n\t\t}\n\t}\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (node & node, const std::string & name)\n{\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (collect_seq_con_info (node.alarm, \"alarm\"));\n\tcomposite->add_component (collect_seq_con_info (node.work, \"work\"));\n\tcomposite->add_component (collect_seq_con_info (node.gap_cache, \"gap_cache\"));\n\tcomposite->add_component (collect_seq_con_info (node.ledger, \"ledger\"));\n\tcomposite->add_component (collect_seq_con_info (node.active, \"active\"));\n\tcomposite->add_component (collect_seq_con_info (node.bootstrap_initiator, \"bootstrap_initiator\"));\n\tcomposite->add_component (collect_seq_con_info (node.bootstrap, \"bootstrap\"));\n\tcomposite->add_component (collect_seq_con_info (node.peers, \"peers\"));\n\tcomposite->add_component (collect_seq_con_info (node.observers, \"observers\"));\n\tcomposite->add_component (collect_seq_con_info (node.wallets, \"wallets\"));\n\tcomposite->add_component (collect_seq_con_info (node.vote_processor, \"vote_processor\"));\n\tcomposite->add_component (collect_seq_con_info (node.rep_crawler, \"rep_crawler\"));\n\tcomposite->add_component (collect_seq_con_info (node.block_processor, \"block_processor\"));\n\tcomposite->add_component (collect_seq_con_info (node.block_arrival, \"block_arrival\"));\n\tcomposite->add_component (collect_seq_con_info (node.online_reps, \"online_reps\"));\n\tcomposite->add_component (collect_seq_con_info (node.votes_cache, \"votes_cache\"));\n\tcomposite->add_component (collect_seq_con_info (node.block_uniquer, \"block_uniquer\"));\n\tcomposite->add_component (collect_seq_con_info (node.vote_uniquer, \"vote_uniquer\"));\n\treturn composite;\n}\n}\n\nnano::gap_cache::gap_cache (nano::node & node_a) :\nnode (node_a)\n{\n}\n\nvoid nano::gap_cache::add (nano::transaction const & transaction_a, nano::block_hash const & hash_a, std::chrono::steady_clock::time_point time_point_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto existing (blocks.get<1> ().find (hash_a));\n\tif (existing != blocks.get<1> ().end ())\n\t{\n\t\tblocks.get<1> ().modify (existing, [time_point_a](nano::gap_information & info) {\n\t\t\tinfo.arrival = time_point_a;\n\t\t});\n\t}\n\telse\n\t{\n\t\tblocks.insert ({ time_point_a, hash_a, std::unordered_set<nano::account> () });\n\t\tif (blocks.size () > max)\n\t\t{\n\t\t\tblocks.get<0> ().erase (blocks.get<0> ().begin ());\n\t\t}\n\t}\n}\n\nvoid nano::gap_cache::vote (std::shared_ptr<nano::vote> vote_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto transaction (node.store.tx_begin_read ());\n\tfor (auto hash : *vote_a)\n\t{\n\t\tauto existing (blocks.get<1> ().find (hash));\n\t\tif (existing != blocks.get<1> ().end ())\n\t\t{\n\t\t\tauto is_new (false);\n\t\t\tblocks.get<1> ().modify (existing, [&](nano::gap_information & info) { is_new = info.voters.insert (vote_a->account).second; });\n\t\t\tif (is_new)\n\t\t\t{\n\t\t\t\tuint128_t tally;\n\t\t\t\tfor (auto & voter : existing->voters)\n\t\t\t\t{\n\t\t\t\t\ttally += node.ledger.weight (transaction, voter);\n\t\t\t\t}\n\t\t\t\tbool start_bootstrap (false);\n\t\t\t\tif (!node.flags.disable_lazy_bootstrap)\n\t\t\t\t{\n\t\t\t\t\tif (tally >= node.config.online_weight_minimum.number ())\n\t\t\t\t\t{\n\t\t\t\t\t\tstart_bootstrap = true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse if (!node.flags.disable_legacy_bootstrap && tally > bootstrap_threshold (transaction))\n\t\t\t\t{\n\t\t\t\t\tstart_bootstrap = true;\n\t\t\t\t}\n\t\t\t\tif (start_bootstrap)\n\t\t\t\t{\n\t\t\t\t\tauto node_l (node.shared ());\n\t\t\t\t\tauto now (std::chrono::steady_clock::now ());\n\t\t\t\t\tnode.alarm.add (nano::is_test_network ? now + std::chrono::milliseconds (5) : now + std::chrono::seconds (5), [node_l, hash]() {\n\t\t\t\t\t\tauto transaction (node_l->store.tx_begin_read ());\n\t\t\t\t\t\tif (!node_l->store.block_exists (transaction, hash))\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tif (!node_l->bootstrap_initiator.in_progress ())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Missing block %1% which has enough votes to warrant lazy bootstrapping it\") % hash.to_string ());\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tif (!node_l->flags.disable_lazy_bootstrap)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tnode_l->bootstrap_initiator.bootstrap_lazy (hash);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse if (!node_l->flags.disable_legacy_bootstrap)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tnode_l->bootstrap_initiator.bootstrap ();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nnano::uint128_t nano::gap_cache::bootstrap_threshold (nano::transaction const & transaction_a)\n{\n\tauto result ((node.online_reps.online_stake () / 256) * node.config.bootstrap_fraction_numerator);\n\treturn result;\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (gap_cache & gap_cache, const std::string & name)\n{\n\tsize_t count = 0;\n\t{\n\t\tstd::lock_guard<std::mutex> (gap_cache.mutex);\n\t\tcount = gap_cache.blocks.size ();\n\t}\n\n\tauto sizeof_element = sizeof (decltype (gap_cache.blocks)::value_type);\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"blocks\", count, sizeof_element }));\n\treturn composite;\n}\n}\n\nvoid nano::network::confirm_send (nano::confirm_ack const & confirm_a, std::shared_ptr<std::vector<uint8_t>> bytes_a, nano::endpoint const & endpoint_a)\n{\n\tif (node.config.logging.network_publish_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Sending confirm_ack for block(s) %1%to %2% sequence %3%\") % confirm_a.vote->hashes_string () % endpoint_a % std::to_string (confirm_a.vote->sequence));\n\t}\n\tstd::weak_ptr<nano::node> node_w (node.shared ());\n\tnode.network.send_buffer (bytes_a->data (), bytes_a->size (), endpoint_a, [bytes_a, node_w, endpoint_a](boost::system::error_code const & ec, size_t size_a) {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tif (ec && node_l->config.logging.network_logging ())\n\t\t\t{\n\t\t\t\tBOOST_LOG (node_l->log) << boost::str (boost::format (\"Error broadcasting confirm_ack to %1%: %2%\") % endpoint_a % ec.message ());\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tnode_l->stats.inc (nano::stat::type::message, nano::stat::detail::confirm_ack, nano::stat::dir::out);\n\t\t\t}\n\t\t}\n\t});\n}\n\nvoid nano::node::process_active (std::shared_ptr<nano::block> incoming)\n{\n\tblock_arrival.add (incoming->hash ());\n\tblock_processor.add (incoming, nano::seconds_since_epoch ());\n}\n\nnano::process_return nano::node::process (nano::block const & block_a)\n{\n\tauto transaction (store.tx_begin_write ());\n\tauto result (ledger.process (transaction, block_a));\n\treturn result;\n}\n\nvoid nano::node::start ()\n{\n\tnetwork.start ();\n\tadd_initial_peers ();\n\tongoing_keepalive ();\n\tongoing_syn_cookie_cleanup ();\n\tif (!flags.disable_legacy_bootstrap)\n\t{\n\t\tongoing_bootstrap ();\n\t}\n\tongoing_store_flush ();\n\tongoing_rep_crawl ();\n\tongoing_rep_calculation ();\n\tongoing_peer_store ();\n\tongoing_online_weight_calculation_queue ();\n\tif (!flags.disable_bootstrap_listener)\n\t{\n\t\tbootstrap.start ();\n\t}\n\tif (!flags.disable_backup)\n\t{\n\t\tbackup_wallet ();\n\t}\n\tsearch_pending ();\n\tif (!flags.disable_wallet_bootstrap)\n\t{\n\t\t// Delay to start wallet lazy bootstrap\n\t\tauto this_l (shared ());\n\t\talarm.add (std::chrono::steady_clock::now () + std::chrono::minutes (1), [this_l]() {\n\t\t\tthis_l->bootstrap_wallet ();\n\t\t});\n\t}\n\tport_mapping.start ();\n\tif (!flags.disable_unchecked_cleaning)\n\t{\n\t\tunchecked_cleaning ();\n\t}\n}\n\nvoid nano::node::stop ()\n{\n\tBOOST_LOG (log) << \"Node stopping\";\n\tblock_processor.stop ();\n\tif (block_processor_thread.joinable ())\n\t{\n\t\tblock_processor_thread.join ();\n\t}\n\tvote_processor.stop ();\n\tactive.stop ();\n\tnetwork.stop ();\n\tbootstrap_initiator.stop ();\n\tbootstrap.stop ();\n\tport_mapping.stop ();\n\tchecker.stop ();\n\twallets.stop ();\n}\n\nvoid nano::node::keepalive_preconfigured (std::vector<std::string> const & peers_a)\n{\n\tfor (auto i (peers_a.begin ()), n (peers_a.end ()); i != n; ++i)\n\t{\n\t\tkeepalive (*i, nano::network::node_port, true);\n\t}\n}\n\nnano::block_hash nano::node::latest (nano::account const & account_a)\n{\n\tauto transaction (store.tx_begin_read ());\n\treturn ledger.latest (transaction, account_a);\n}\n\nnano::uint128_t nano::node::balance (nano::account const & account_a)\n{\n\tauto transaction (store.tx_begin_read ());\n\treturn ledger.account_balance (transaction, account_a);\n}\n\nstd::shared_ptr<nano::block> nano::node::block (nano::block_hash const & hash_a)\n{\n\tauto transaction (store.tx_begin_read ());\n\treturn store.block_get (transaction, hash_a);\n}\n\nstd::pair<nano::uint128_t, nano::uint128_t> nano::node::balance_pending (nano::account const & account_a)\n{\n\tstd::pair<nano::uint128_t, nano::uint128_t> result;\n\tauto transaction (store.tx_begin_read ());\n\tresult.first = ledger.account_balance (transaction, account_a);\n\tresult.second = ledger.account_pending (transaction, account_a);\n\treturn result;\n}\n\nnano::uint128_t nano::node::weight (nano::account const & account_a)\n{\n\tauto transaction (store.tx_begin_read ());\n\treturn ledger.weight (transaction, account_a);\n}\n\nnano::account nano::node::representative (nano::account const & account_a)\n{\n\tauto transaction (store.tx_begin_read ());\n\tnano::account_info info;\n\tnano::account result (0);\n\tif (!store.account_get (transaction, account_a, info))\n\t{\n\t\tresult = info.rep_block;\n\t}\n\treturn result;\n}\n\nvoid nano::node::ongoing_keepalive ()\n{\n\tkeepalive_preconfigured (config.preconfigured_peers);\n\tauto peers_l (peers.purge_list (std::chrono::steady_clock::now () - cutoff));\n\tfor (auto i (peers_l.begin ()), j (peers_l.end ()); i != j && std::chrono::steady_clock::now () - i->last_attempt > period; ++i)\n\t{\n\t\tnetwork.send_keepalive (i->endpoint);\n\t}\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + period, [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_keepalive ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_syn_cookie_cleanup ()\n{\n\tpeers.purge_syn_cookies (std::chrono::steady_clock::now () - syn_cookie_cutoff);\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + (syn_cookie_cutoff * 2), [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_syn_cookie_cleanup ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_rep_crawl ()\n{\n\tauto now (std::chrono::steady_clock::now ());\n\tauto peers_l (peers.rep_crawl ());\n\trep_query (*this, peers_l);\n\tif (network.on)\n\t{\n\t\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\t\talarm.add (now + std::chrono::seconds (4), [node_w]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->ongoing_rep_crawl ();\n\t\t\t}\n\t\t});\n\t}\n}\n\nvoid nano::node::ongoing_rep_calculation ()\n{\n\tauto now (std::chrono::steady_clock::now ());\n\tvote_processor.calculate_weights ();\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (now + std::chrono::minutes (10), [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_rep_calculation ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_bootstrap ()\n{\n\tauto next_wakeup (300);\n\tif (warmed_up < 3)\n\t{\n\t\t// Re-attempt bootstrapping more aggressively on startup\n\t\tnext_wakeup = 5;\n\t\tif (!bootstrap_initiator.in_progress () && !peers.empty ())\n\t\t{\n\t\t\t++warmed_up;\n\t\t}\n\t}\n\tbootstrap_initiator.bootstrap ();\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + std::chrono::seconds (next_wakeup), [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_bootstrap ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_store_flush ()\n{\n\t{\n\t\tauto transaction (store.tx_begin_write ());\n\t\tstore.flush (transaction);\n\t}\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + std::chrono::seconds (5), [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_store_flush ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_peer_store ()\n{\n\tauto endpoint_peers = peers.list ();\n\tif (!endpoint_peers.empty ())\n\t{\n\t\t// Clear all peers then refresh with the current list of peers\n\t\tauto transaction (store.tx_begin_write ());\n\t\tstore.peer_clear (transaction);\n\t\tfor (const auto & endpoint : endpoint_peers)\n\t\t{\n\t\t\tnano::endpoint_key endpoint_key (endpoint.address ().to_v6 ().to_bytes (), endpoint.port ());\n\t\t\tstore.peer_put (transaction, std::move (endpoint_key));\n\t\t}\n\t}\n\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + peer_interval, [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_peer_store ();\n\t\t}\n\t});\n}\n\nvoid nano::node::backup_wallet ()\n{\n\tauto transaction (wallets.tx_begin_read ());\n\tfor (auto i (wallets.items.begin ()), n (wallets.items.end ()); i != n; ++i)\n\t{\n\t\tboost::system::error_code error_chmod;\n\t\tauto backup_path (application_path / \"backup\");\n\n\t\tboost::filesystem::create_directories (backup_path);\n\t\tnano::set_secure_perm_directory (backup_path, error_chmod);\n\t\ti->second->store.write_backup (transaction, backup_path / (i->first.to_string () + \".json\"));\n\t}\n\tauto this_l (shared ());\n\talarm.add (std::chrono::steady_clock::now () + backup_interval, [this_l]() {\n\t\tthis_l->backup_wallet ();\n\t});\n}\n\nvoid nano::node::search_pending ()\n{\n\t// Reload wallets from disk\n\twallets.reload ();\n\t// Search pending\n\twallets.search_pending_all ();\n\tauto this_l (shared ());\n\talarm.add (std::chrono::steady_clock::now () + search_pending_interval, [this_l]() {\n\t\tthis_l->search_pending ();\n\t});\n}\n\nvoid nano::node::bootstrap_wallet ()\n{\n\tstd::deque<nano::account> accounts;\n\t{\n\t\tstd::lock_guard<std::mutex> lock (wallets.mutex);\n\t\tauto transaction (wallets.tx_begin_read ());\n\t\tfor (auto i (wallets.items.begin ()), n (wallets.items.end ()); i != n && accounts.size () < 128; ++i)\n\t\t{\n\t\t\tauto & wallet (*i->second);\n\t\t\tstd::lock_guard<std::recursive_mutex> wallet_lock (wallet.store.mutex);\n\t\t\tfor (auto j (wallet.store.begin (transaction)), m (wallet.store.end ()); j != m && accounts.size () < 128; ++j)\n\t\t\t{\n\t\t\t\tnano::account account (j->first);\n\t\t\t\taccounts.push_back (account);\n\t\t\t}\n\t\t}\n\t}\n\tbootstrap_initiator.bootstrap_wallet (accounts);\n}\n\nvoid nano::node::unchecked_cleaning ()\n{\n\tstd::deque<nano::unchecked_key> cleaning;\n\t// Collect old unchecked keys\n\tif (!bootstrap_initiator.in_progress ())\n\t{\n\t\tauto now (nano::seconds_since_epoch ());\n\t\tauto transaction (store.tx_begin_read ());\n\t\t// Max 32k records to clean, max 60 seconds reading to prevent slow i/o systems start issues\n\t\tfor (auto i (store.unchecked_begin (transaction)), n (store.unchecked_end ()); i != n && cleaning.size () < 64 * 1024 && nano::seconds_since_epoch () - now < 60; ++i)\n\t\t{\n\t\t\tnano::unchecked_key key (i->first);\n\t\t\tnano::unchecked_info info (i->second);\n\t\t\tif ((now - info.modified) > unchecked_cutoff.count ())\n\t\t\t{\n\t\t\t\tcleaning.push_back (key);\n\t\t\t}\n\t\t}\n\t}\n\t// Delete old unchecked keys in batches\n\twhile (!cleaning.empty () && !bootstrap_initiator.in_progress ())\n\t{\n\t\tsize_t deleted_count (0);\n\t\tauto transaction (store.tx_begin_write ());\n\t\twhile (deleted_count++ < 2 * 1024 && !cleaning.empty ())\n\t\t{\n\t\t\tauto key (cleaning.front ());\n\t\t\tcleaning.pop_front ();\n\t\t\tstore.unchecked_del (transaction, key);\n\t\t}\n\t}\n\tcleaning.clear ();\n\tauto this_l (shared ());\n\talarm.add (std::chrono::steady_clock::now () + unchecked_cleaning_interval, [this_l]() {\n\t\tthis_l->unchecked_cleaning ();\n\t});\n}\n\nint nano::node::price (nano::uint128_t const & balance_a, int amount_a)\n{\n\tassert (balance_a >= amount_a * nano::Gxrb_ratio);\n\tauto balance_l (balance_a);\n\tdouble result (0.0);\n\tfor (auto i (0); i < amount_a; ++i)\n\t{\n\t\tbalance_l -= nano::Gxrb_ratio;\n\t\tauto balance_scaled ((balance_l / nano::Mxrb_ratio).convert_to<double> ());\n\t\tauto units (balance_scaled / 1000.0);\n\t\tauto unit_price (((free_cutoff - units) / free_cutoff) * price_max);\n\t\tresult += std::min (std::max (0.0, unit_price), price_max);\n\t}\n\treturn static_cast<int> (result * 100.0);\n}\n\nnamespace\n{\nclass work_request\n{\npublic:\n\twork_request (boost::asio::io_context & io_ctx_a, boost::asio::ip::address address_a, uint16_t port_a) :\n\taddress (address_a),\n\tport (port_a),\n\tsocket (io_ctx_a)\n\t{\n\t}\n\tboost::asio::ip::address address;\n\tuint16_t port;\n\tboost::beast::flat_buffer buffer;\n\tboost::beast::http::response<boost::beast::http::string_body> response;\n\tboost::asio::ip::tcp::socket socket;\n};\nclass distributed_work : public std::enable_shared_from_this<distributed_work>\n{\npublic:\n\tdistributed_work (std::shared_ptr<nano::node> const & node_a, nano::block_hash const & root_a, std::function<void(uint64_t)> callback_a, uint64_t difficulty_a) :\n\tdistributed_work (1, node_a, root_a, callback_a, difficulty_a)\n\t{\n\t\tassert (node_a != nullptr);\n\t}\n\tdistributed_work (unsigned int backoff_a, std::shared_ptr<nano::node> const & node_a, nano::block_hash const & root_a, std::function<void(uint64_t)> callback_a, uint64_t difficulty_a) :\n\tcallback (callback_a),\n\tbackoff (backoff_a),\n\tnode (node_a),\n\troot (root_a),\n\tneed_resolve (node_a->config.work_peers),\n\tdifficulty (difficulty_a)\n\t{\n\t\tassert (node_a != nullptr);\n\t\tcompleted.clear ();\n\t}\n\tvoid start ()\n\t{\n\t\tif (need_resolve.empty ())\n\t\t{\n\t\t\tstart_work ();\n\t\t}\n\t\telse\n\t\t{\n\t\t\tauto current (need_resolve.back ());\n\t\t\tneed_resolve.pop_back ();\n\t\t\tauto this_l (shared_from_this ());\n\t\t\tboost::system::error_code ec;\n\t\t\tauto parsed_address (boost::asio::ip::address_v6::from_string (current.first, ec));\n\t\t\tif (!ec)\n\t\t\t{\n\t\t\t\toutstanding[parsed_address] = current.second;\n\t\t\t\tstart ();\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tnode->network.resolver.async_resolve (boost::asio::ip::udp::resolver::query (current.first, std::to_string (current.second)), [current, this_l](boost::system::error_code const & ec, boost::asio::ip::udp::resolver::iterator i_a) {\n\t\t\t\t\tif (!ec)\n\t\t\t\t\t{\n\t\t\t\t\t\tfor (auto i (i_a), n (boost::asio::ip::udp::resolver::iterator{}); i != n; ++i)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto endpoint (i->endpoint ());\n\t\t\t\t\t\t\tthis_l->outstanding[endpoint.address ()] = endpoint.port ();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tBOOST_LOG (this_l->node->log) << boost::str (boost::format (\"Error resolving work peer: %1%:%2%: %3%\") % current.first % current.second % ec.message ());\n\t\t\t\t\t}\n\t\t\t\t\tthis_l->start ();\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\t}\n\tvoid start_work ()\n\t{\n\t\tif (!outstanding.empty ())\n\t\t{\n\t\t\tauto this_l (shared_from_this ());\n\t\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\t\tfor (auto const & i : outstanding)\n\t\t\t{\n\t\t\t\tauto host (i.first);\n\t\t\t\tauto service (i.second);\n\t\t\t\tnode->background ([this_l, host, service]() {\n\t\t\t\t\tauto connection (std::make_shared<work_request> (this_l->node->io_ctx, host, service));\n\t\t\t\t\tconnection->socket.async_connect (nano::tcp_endpoint (host, service), [this_l, connection](boost::system::error_code const & ec) {\n\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tstd::string request_string;\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tboost::property_tree::ptree request;\n\t\t\t\t\t\t\t\trequest.put (\"action\", \"work_generate\");\n\t\t\t\t\t\t\t\trequest.put (\"hash\", this_l->root.to_string ());\n\t\t\t\t\t\t\t\tstd::stringstream ostream;\n\t\t\t\t\t\t\t\tboost::property_tree::write_json (ostream, request);\n\t\t\t\t\t\t\t\trequest_string = ostream.str ();\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\tauto request (std::make_shared<boost::beast::http::request<boost::beast::http::string_body>> ());\n\t\t\t\t\t\t\trequest->method (boost::beast::http::verb::post);\n\t\t\t\t\t\t\trequest->target (\"/\");\n\t\t\t\t\t\t\trequest->version (11);\n\t\t\t\t\t\t\trequest->body () = request_string;\n\t\t\t\t\t\t\trequest->prepare_payload ();\n\t\t\t\t\t\t\tboost::beast::http::async_write (connection->socket, *request, [this_l, connection, request](boost::system::error_code const & ec, size_t bytes_transferred) {\n\t\t\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tboost::beast::http::async_read (connection->socket, connection->buffer, connection->response, [this_l, connection](boost::system::error_code const & ec, size_t bytes_transferred) {\n\t\t\t\t\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tif (connection->response.result () == boost::beast::http::status::ok)\n\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\tthis_l->success (connection->response.body (), connection->address);\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\t\tBOOST_LOG (this_l->node->log) << boost::str (boost::format (\"Work peer responded with an error %1% %2%: %3%\") % connection->address % connection->port % connection->response.result ());\n\t\t\t\t\t\t\t\t\t\t\t\tthis_l->failure (connection->address);\n\t\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\t\t\tBOOST_LOG (this_l->node->log) << boost::str (boost::format (\"Unable to read from work_peer %1% %2%: %3% (%4%)\") % connection->address % connection->port % ec.message () % ec.value ());\n\t\t\t\t\t\t\t\t\t\t\tthis_l->failure (connection->address);\n\t\t\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\t\t});\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tBOOST_LOG (this_l->node->log) << boost::str (boost::format (\"Unable to write to work_peer %1% %2%: %3% (%4%)\") % connection->address % connection->port % ec.message () % ec.value ());\n\t\t\t\t\t\t\t\t\tthis_l->failure (connection->address);\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t});\n\t\t\t\t\t\t}\n\t\t\t\t\t\telse\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tBOOST_LOG (this_l->node->log) << boost::str (boost::format (\"Unable to connect to work_peer %1% %2%: %3% (%4%)\") % connection->address % connection->port % ec.message () % ec.value ());\n\t\t\t\t\t\t\tthis_l->failure (connection->address);\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t});\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\thandle_failure (true);\n\t\t}\n\t}\n\tvoid stop ()\n\t{\n\t\tauto this_l (shared_from_this ());\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\tfor (auto const & i : outstanding)\n\t\t{\n\t\t\tauto host (i.first);\n\t\t\tnode->background ([this_l, host]() {\n\t\t\t\tstd::string request_string;\n\t\t\t\t{\n\t\t\t\t\tboost::property_tree::ptree request;\n\t\t\t\t\trequest.put (\"action\", \"work_cancel\");\n\t\t\t\t\trequest.put (\"hash\", this_l->root.to_string ());\n\t\t\t\t\tstd::stringstream ostream;\n\t\t\t\t\tboost::property_tree::write_json (ostream, request);\n\t\t\t\t\trequest_string = ostream.str ();\n\t\t\t\t}\n\t\t\t\tboost::beast::http::request<boost::beast::http::string_body> request;\n\t\t\t\trequest.method (boost::beast::http::verb::post);\n\t\t\t\trequest.target (\"/\");\n\t\t\t\trequest.version (11);\n\t\t\t\trequest.body () = request_string;\n\t\t\t\trequest.prepare_payload ();\n\t\t\t\tauto socket (std::make_shared<boost::asio::ip::tcp::socket> (this_l->node->io_ctx));\n\t\t\t\tboost::beast::http::async_write (*socket, request, [socket](boost::system::error_code const & ec, size_t bytes_transferred) {\n\t\t\t\t});\n\t\t\t});\n\t\t}\n\t\toutstanding.clear ();\n\t}\n\tvoid success (std::string const & body_a, boost::asio::ip::address const & address)\n\t{\n\t\tauto last (remove (address));\n\t\tstd::stringstream istream (body_a);\n\t\ttry\n\t\t{\n\t\t\tboost::property_tree::ptree result;\n\t\t\tboost::property_tree::read_json (istream, result);\n\t\t\tauto work_text (result.get<std::string> (\"work\"));\n\t\t\tuint64_t work;\n\t\t\tif (!nano::from_string_hex (work_text, work))\n\t\t\t{\n\t\t\t\tif (!nano::work_validate (root, work))\n\t\t\t\t{\n\t\t\t\t\tset_once (work);\n\t\t\t\t\tstop ();\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tBOOST_LOG (node->log) << boost::str (boost::format (\"Incorrect work response from %1% for root %2%: %3%\") % address % root.to_string () % work_text);\n\t\t\t\t\thandle_failure (last);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tBOOST_LOG (node->log) << boost::str (boost::format (\"Work response from %1% wasn't a number: %2%\") % address % work_text);\n\t\t\t\thandle_failure (last);\n\t\t\t}\n\t\t}\n\t\tcatch (...)\n\t\t{\n\t\t\tBOOST_LOG (node->log) << boost::str (boost::format (\"Work response from %1% wasn't parsable: %2%\") % address % body_a);\n\t\t\thandle_failure (last);\n\t\t}\n\t}\n\tvoid set_once (uint64_t work_a)\n\t{\n\t\tif (!completed.test_and_set ())\n\t\t{\n\t\t\tcallback (work_a);\n\t\t}\n\t}\n\tvoid failure (boost::asio::ip::address const & address)\n\t{\n\t\tauto last (remove (address));\n\t\thandle_failure (last);\n\t}\n\tvoid handle_failure (bool last)\n\t{\n\t\tif (last)\n\t\t{\n\t\t\tif (!completed.test_and_set ())\n\t\t\t{\n\t\t\t\tif (node->config.work_threads != 0 || node->work.opencl)\n\t\t\t\t{\n\t\t\t\t\tauto callback_l (callback);\n\t\t\t\t\t// clang-format off\n\t\t\t\t\tnode->work.generate (root, [callback_l](boost::optional<uint64_t> const & work_a) {\n\t\t\t\t\t\tcallback_l (work_a.value ());\n\t\t\t\t\t},\n\t\t\t\t\tdifficulty);\n\t\t\t\t\t// clang-format on\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tif (backoff == 1 && node->config.logging.work_generation_time ())\n\t\t\t\t\t{\n\t\t\t\t\t\tBOOST_LOG (node->log) << \"Work peer(s) failed to generate work for root \" << root.to_string () << \", retrying...\";\n\t\t\t\t\t}\n\t\t\t\t\tauto now (std::chrono::steady_clock::now ());\n\t\t\t\t\tauto root_l (root);\n\t\t\t\t\tauto callback_l (callback);\n\t\t\t\t\tstd::weak_ptr<nano::node> node_w (node);\n\t\t\t\t\tauto next_backoff (std::min (backoff * 2, (unsigned int)60 * 5));\n\t\t\t\t\t// clang-format off\n\t\t\t\t\tnode->alarm.add (now + std::chrono::seconds (backoff), [ node_w, root_l, callback_l, next_backoff, difficulty = difficulty ] {\n\t\t\t\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto work_generation (std::make_shared<distributed_work> (next_backoff, node_l, root_l, callback_l, difficulty));\n\t\t\t\t\t\t\twork_generation->start ();\n\t\t\t\t\t\t}\n\t\t\t\t\t});\n\t\t\t\t\t// clang-format on\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tbool remove (boost::asio::ip::address const & address)\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\toutstanding.erase (address);\n\t\treturn outstanding.empty ();\n\t}\n\tstd::function<void(uint64_t)> callback;\n\tunsigned int backoff; // in seconds\n\tstd::shared_ptr<nano::node> node;\n\tnano::block_hash root;\n\tstd::mutex mutex;\n\tstd::map<boost::asio::ip::address, uint16_t> outstanding;\n\tstd::vector<std::pair<std::string, uint16_t>> need_resolve;\n\tstd::atomic_flag completed;\n\tuint64_t difficulty;\n};\n}\n\nvoid nano::node::work_generate_blocking (nano::block & block_a, uint64_t difficulty_a)\n{\n\tblock_a.block_work_set (work_generate_blocking (block_a.root (), difficulty_a));\n}\n\nvoid nano::node::work_generate (nano::uint256_union const & hash_a, std::function<void(uint64_t)> callback_a, uint64_t difficulty_a)\n{\n\tauto work_generation (std::make_shared<distributed_work> (shared (), hash_a, callback_a, difficulty_a));\n\twork_generation->start ();\n}\n\nuint64_t nano::node::work_generate_blocking (nano::uint256_union const & hash_a, uint64_t difficulty_a)\n{\n\tstd::promise<uint64_t> promise;\n\tstd::future<uint64_t> future = promise.get_future ();\n\t// clang-format off\n\twork_generate (hash_a, [&promise](uint64_t work_a) {\n\t\tpromise.set_value (work_a);\n\t},\n\tdifficulty_a);\n\t// clang-format on\n\treturn future.get ();\n}\n\nvoid nano::node::add_initial_peers ()\n{\n\tauto transaction (store.tx_begin_read ());\n\tfor (auto i (store.peers_begin (transaction)), n (store.peers_end ()); i != n; ++i)\n\t{\n\t\tnano::endpoint endpoint (boost::asio::ip::address_v6 (i->first.address_bytes ()), i->first.port ());\n\t\tif (!peers.reachout (endpoint))\n\t\t{\n\t\t\tsend_keepalive (endpoint);\n\t\t}\n\t}\n}\n\nvoid nano::node::block_confirm (std::shared_ptr<nano::block> block_a)\n{\n\tactive.start (block_a);\n\tnetwork.broadcast_confirm_req (block_a);\n\t// Calculate votes for local representatives\n\tif (config.enable_voting && active.active (*block_a))\n\t{\n\t\tblock_processor.generator.add (block_a->hash ());\n\t}\n}\n\nnano::uint128_t nano::node::delta ()\n{\n\tauto result ((online_reps.online_stake () / 100) * config.online_weight_quorum);\n\treturn result;\n}\n\nvoid nano::node::ongoing_online_weight_calculation_queue ()\n{\n\tstd::weak_ptr<nano::node> node_w (shared_from_this ());\n\talarm.add (std::chrono::steady_clock::now () + (std::chrono::seconds (nano::online_reps::weight_period)), [node_w]() {\n\t\tif (auto node_l = node_w.lock ())\n\t\t{\n\t\t\tnode_l->ongoing_online_weight_calculation ();\n\t\t}\n\t});\n}\n\nvoid nano::node::ongoing_online_weight_calculation ()\n{\n\tonline_reps.sample ();\n\tongoing_online_weight_calculation_queue ();\n}\n\nnamespace\n{\nclass confirmed_visitor : public nano::block_visitor\n{\npublic:\n\tconfirmed_visitor (nano::transaction const & transaction_a, nano::node & node_a, std::shared_ptr<nano::block> block_a, nano::block_hash const & hash_a) :\n\ttransaction (transaction_a),\n\tnode (node_a),\n\tblock (block_a),\n\thash (hash_a)\n\t{\n\t}\n\tvirtual ~confirmed_visitor () = default;\n\tvoid scan_receivable (nano::account const & account_a)\n\t{\n\t\tfor (auto i (node.wallets.items.begin ()), n (node.wallets.items.end ()); i != n; ++i)\n\t\t{\n\t\t\tauto wallet (i->second);\n\t\t\tauto transaction_l (node.wallets.tx_begin_read ());\n\t\t\tif (wallet->store.exists (transaction_l, account_a))\n\t\t\t{\n\t\t\t\tnano::account representative;\n\t\t\t\tnano::pending_info pending;\n\t\t\t\trepresentative = wallet->store.representative (transaction_l);\n\t\t\t\tauto error (node.store.pending_get (transaction, nano::pending_key (account_a, hash), pending));\n\t\t\t\tif (!error)\n\t\t\t\t{\n\t\t\t\t\tauto node_l (node.shared ());\n\t\t\t\t\tauto amount (pending.amount.number ());\n\t\t\t\t\twallet->receive_async (block, representative, amount, [](std::shared_ptr<nano::block>) {});\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tif (!node.store.block_exists (transaction, hash))\n\t\t\t\t\t{\n\t\t\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Confirmed block is missing:  %1%\") % hash.to_string ());\n\t\t\t\t\t\tassert (false && \"Confirmed block is missing\");\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Block %1% has already been received\") % hash.to_string ());\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tvoid state_block (nano::state_block const & block_a) override\n\t{\n\t\tscan_receivable (block_a.hashables.link);\n\t}\n\tvoid send_block (nano::send_block const & block_a) override\n\t{\n\t\tscan_receivable (block_a.hashables.destination);\n\t}\n\tvoid receive_block (nano::receive_block const &) override\n\t{\n\t}\n\tvoid open_block (nano::open_block const &) override\n\t{\n\t}\n\tvoid change_block (nano::change_block const &) override\n\t{\n\t}\n\tnano::transaction const & transaction;\n\tnano::node & node;\n\tstd::shared_ptr<nano::block> block;\n\tnano::block_hash const & hash;\n};\n}\n\nvoid nano::node::process_confirmed (std::shared_ptr<nano::block> block_a, uint8_t iteration)\n{\n\tauto hash (block_a->hash ());\n\tif (ledger.block_exists (block_a->type (), hash))\n\t{\n\t\tauto transaction (store.tx_begin_read ());\n\t\tconfirmed_visitor visitor (transaction, *this, block_a, hash);\n\t\tblock_a->visit (visitor);\n\t\tauto account (ledger.account (transaction, hash));\n\t\tauto amount (ledger.amount (transaction, hash));\n\t\tbool is_state_send (false);\n\t\tnano::account pending_account (0);\n\t\tif (auto state = dynamic_cast<nano::state_block *> (block_a.get ()))\n\t\t{\n\t\t\tis_state_send = ledger.is_send (transaction, *state);\n\t\t\tpending_account = state->hashables.link;\n\t\t}\n\t\tif (auto send = dynamic_cast<nano::send_block *> (block_a.get ()))\n\t\t{\n\t\t\tpending_account = send->hashables.destination;\n\t\t}\n\t\tobservers.blocks.notify (block_a, account, amount, is_state_send);\n\t\tif (amount > 0)\n\t\t{\n\t\t\tobservers.account_balance.notify (account, false);\n\t\t\tif (!pending_account.is_zero ())\n\t\t\t{\n\t\t\t\tobservers.account_balance.notify (pending_account, true);\n\t\t\t}\n\t\t}\n\t}\n\t// Limit to 0.5 * 20 = 10 seconds (more than max block_processor::process_batch finish time)\n\telse if (iteration < 20)\n\t{\n\t\titeration++;\n\t\tstd::weak_ptr<nano::node> node_w (shared ());\n\t\talarm.add (std::chrono::steady_clock::now () + process_confirmed_interval, [node_w, block_a, iteration]() {\n\t\t\tif (auto node_l = node_w.lock ())\n\t\t\t{\n\t\t\t\tnode_l->process_confirmed (block_a, iteration);\n\t\t\t}\n\t\t});\n\t}\n}\n\nvoid nano::node::process_message (nano::message & message_a, nano::endpoint const & sender_a)\n{\n\tnetwork_message_visitor visitor (*this, sender_a);\n\tmessage_a.visit (visitor);\n}\n\nnano::endpoint nano::network::endpoint ()\n{\n\tboost::system::error_code ec;\n\tauto port (socket.local_endpoint (ec).port ());\n\tif (ec)\n\t{\n\t\tBOOST_LOG (node.log) << \"Unable to retrieve port: \" << ec.message ();\n\t}\n\treturn nano::endpoint (boost::asio::ip::address_v6::loopback (), port);\n}\n\nbool nano::block_arrival::add (nano::block_hash const & hash_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto now (std::chrono::steady_clock::now ());\n\tauto inserted (arrival.insert (nano::block_arrival_info{ now, hash_a }));\n\tauto result (!inserted.second);\n\treturn result;\n}\n\nbool nano::block_arrival::recent (nano::block_hash const & hash_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto now (std::chrono::steady_clock::now ());\n\twhile (arrival.size () > arrival_size_min && arrival.begin ()->arrival + arrival_time_min < now)\n\t{\n\t\tarrival.erase (arrival.begin ());\n\t}\n\treturn arrival.get<1> ().find (hash_a) != arrival.get<1> ().end ();\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (block_arrival & block_arrival, const std::string & name)\n{\n\tsize_t count = 0;\n\t{\n\t\tstd::lock_guard<std::mutex> guard (block_arrival.mutex);\n\t\tcount = block_arrival.arrival.size ();\n\t}\n\n\tauto sizeof_element = sizeof (decltype (block_arrival.arrival)::value_type);\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"arrival\", count, sizeof_element }));\n\treturn composite;\n}\n}\n\nnano::online_reps::online_reps (nano::ledger & ledger_a, nano::uint128_t minimum_a) :\nledger (ledger_a),\nminimum (minimum_a)\n{\n\tauto transaction (ledger_a.store.tx_begin_read ());\n\tonline = trend (transaction);\n}\n\nvoid nano::online_reps::observe (nano::account const & rep_a)\n{\n\tauto transaction (ledger.store.tx_begin_read ());\n\tif (ledger.weight (transaction, rep_a) > nano::Gxrb_ratio)\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\treps.insert (rep_a);\n\t}\n}\n\nvoid nano::online_reps::sample ()\n{\n\tauto transaction (ledger.store.tx_begin_write ());\n\t// Discard oldest entries\n\twhile (ledger.store.online_weight_count (transaction) >= weight_samples)\n\t{\n\t\tauto oldest (ledger.store.online_weight_begin (transaction));\n\t\tassert (oldest != ledger.store.online_weight_end ());\n\t\tledger.store.online_weight_del (transaction, oldest->first);\n\t}\n\t// Calculate current active rep weight\n\tnano::uint128_t current;\n\tstd::unordered_set<nano::account> reps_copy;\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\treps_copy.swap (reps);\n\t}\n\tfor (auto & i : reps_copy)\n\t{\n\t\tcurrent += ledger.weight (transaction, i);\n\t}\n\tledger.store.online_weight_put (transaction, std::chrono::system_clock::now ().time_since_epoch ().count (), current);\n\tauto trend_l (trend (transaction));\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tonline = trend_l;\n}\n\nnano::uint128_t nano::online_reps::trend (nano::transaction & transaction_a)\n{\n\tstd::vector<nano::uint128_t> items;\n\titems.reserve (weight_samples + 1);\n\titems.push_back (minimum);\n\tfor (auto i (ledger.store.online_weight_begin (transaction_a)), n (ledger.store.online_weight_end ()); i != n; ++i)\n\t{\n\t\titems.push_back (i->second.number ());\n\t}\n\n\t// Pick median value for our target vote weight\n\tauto median_idx = items.size () / 2;\n\tnth_element (items.begin (), items.begin () + median_idx, items.end ());\n\treturn nano::uint128_t{ items[median_idx] };\n}\n\nnano::uint128_t nano::online_reps::online_stake ()\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\treturn std::max (online, minimum);\n}\n\nstd::vector<nano::account> nano::online_reps::list ()\n{\n\tstd::vector<nano::account> result;\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tfor (auto & i : reps)\n\t{\n\t\tresult.push_back (i);\n\t}\n\treturn result;\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (online_reps & online_reps, const std::string & name)\n{\n\tsize_t count = 0;\n\t{\n\t\tstd::lock_guard<std::mutex> guard (online_reps.mutex);\n\t\tcount = online_reps.reps.size ();\n\t}\n\n\tauto sizeof_element = sizeof (decltype (online_reps.reps)::value_type);\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"arrival\", count, sizeof_element }));\n\treturn composite;\n}\n}\n\nnamespace\n{\nboost::asio::ip::address_v6 mapped_from_v4_bytes (unsigned long address_a)\n{\n\treturn boost::asio::ip::address_v6::v4_mapped (boost::asio::ip::address_v4 (address_a));\n}\n}\n\nbool nano::reserved_address (nano::endpoint const & endpoint_a, bool blacklist_loopback)\n{\n\tassert (endpoint_a.address ().is_v6 ());\n\tauto bytes (endpoint_a.address ().to_v6 ());\n\tauto result (false);\n\tstatic auto const rfc1700_min (mapped_from_v4_bytes (0x00000000ul));\n\tstatic auto const rfc1700_max (mapped_from_v4_bytes (0x00fffffful));\n\tstatic auto const ipv4_loopback_min (mapped_from_v4_bytes (0x7f000000ul));\n\tstatic auto const ipv4_loopback_max (mapped_from_v4_bytes (0x7ffffffful));\n\tstatic auto const rfc1918_1_min (mapped_from_v4_bytes (0x0a000000ul));\n\tstatic auto const rfc1918_1_max (mapped_from_v4_bytes (0x0afffffful));\n\tstatic auto const rfc1918_2_min (mapped_from_v4_bytes (0xac100000ul));\n\tstatic auto const rfc1918_2_max (mapped_from_v4_bytes (0xac1ffffful));\n\tstatic auto const rfc1918_3_min (mapped_from_v4_bytes (0xc0a80000ul));\n\tstatic auto const rfc1918_3_max (mapped_from_v4_bytes (0xc0a8fffful));\n\tstatic auto const rfc6598_min (mapped_from_v4_bytes (0x64400000ul));\n\tstatic auto const rfc6598_max (mapped_from_v4_bytes (0x647ffffful));\n\tstatic auto const rfc5737_1_min (mapped_from_v4_bytes (0xc0000200ul));\n\tstatic auto const rfc5737_1_max (mapped_from_v4_bytes (0xc00002fful));\n\tstatic auto const rfc5737_2_min (mapped_from_v4_bytes (0xc6336400ul));\n\tstatic auto const rfc5737_2_max (mapped_from_v4_bytes (0xc63364fful));\n\tstatic auto const rfc5737_3_min (mapped_from_v4_bytes (0xcb007100ul));\n\tstatic auto const rfc5737_3_max (mapped_from_v4_bytes (0xcb0071fful));\n\tstatic auto const ipv4_multicast_min (mapped_from_v4_bytes (0xe0000000ul));\n\tstatic auto const ipv4_multicast_max (mapped_from_v4_bytes (0xeffffffful));\n\tstatic auto const rfc6890_min (mapped_from_v4_bytes (0xf0000000ul));\n\tstatic auto const rfc6890_max (mapped_from_v4_bytes (0xfffffffful));\n\tstatic auto const rfc6666_min (boost::asio::ip::address_v6::from_string (\"100::\"));\n\tstatic auto const rfc6666_max (boost::asio::ip::address_v6::from_string (\"100::ffff:ffff:ffff:ffff\"));\n\tstatic auto const rfc3849_min (boost::asio::ip::address_v6::from_string (\"2001:db8::\"));\n\tstatic auto const rfc3849_max (boost::asio::ip::address_v6::from_string (\"2001:db8:ffff:ffff:ffff:ffff:ffff:ffff\"));\n\tstatic auto const rfc4193_min (boost::asio::ip::address_v6::from_string (\"fc00::\"));\n\tstatic auto const rfc4193_max (boost::asio::ip::address_v6::from_string (\"fd00:ffff:ffff:ffff:ffff:ffff:ffff:ffff\"));\n\tstatic auto const ipv6_multicast_min (boost::asio::ip::address_v6::from_string (\"ff00::\"));\n\tstatic auto const ipv6_multicast_max (boost::asio::ip::address_v6::from_string (\"ff00:ffff:ffff:ffff:ffff:ffff:ffff:ffff\"));\n\tif (bytes >= rfc1700_min && bytes <= rfc1700_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc5737_1_min && bytes <= rfc5737_1_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc5737_2_min && bytes <= rfc5737_2_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc5737_3_min && bytes <= rfc5737_3_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= ipv4_multicast_min && bytes <= ipv4_multicast_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc6890_min && bytes <= rfc6890_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc6666_min && bytes <= rfc6666_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= rfc3849_min && bytes <= rfc3849_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (bytes >= ipv6_multicast_min && bytes <= ipv6_multicast_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (blacklist_loopback && bytes.is_loopback ())\n\t{\n\t\tresult = true;\n\t}\n\telse if (blacklist_loopback && bytes >= ipv4_loopback_min && bytes <= ipv4_loopback_max)\n\t{\n\t\tresult = true;\n\t}\n\telse if (nano::is_live_network)\n\t{\n\t\tif (bytes >= rfc1918_1_min && bytes <= rfc1918_1_max)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t\telse if (bytes >= rfc1918_2_min && bytes <= rfc1918_2_max)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t\telse if (bytes >= rfc1918_3_min && bytes <= rfc1918_3_max)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t\telse if (bytes >= rfc6598_min && bytes <= rfc6598_max)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t\telse if (bytes >= rfc4193_min && bytes <= rfc4193_max)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t}\n\treturn result;\n}\n\nvoid nano::network::send_buffer (uint8_t const * data_a, size_t size_a, nano::endpoint const & endpoint_a, std::function<void(boost::system::error_code const &, size_t)> callback_a)\n{\n\tstd::unique_lock<std::mutex> lock (socket_mutex);\n\tif (node.config.logging.network_packet_logging ())\n\t{\n\t\tBOOST_LOG (node.log) << \"Sending packet\";\n\t}\n\tsocket.async_send_to (boost::asio::buffer (data_a, size_a), endpoint_a, [this, callback_a](boost::system::error_code const & ec, size_t size_a) {\n\t\tcallback_a (ec, size_a);\n\t\tthis->node.stats.add (nano::stat::type::traffic, nano::stat::dir::out, size_a);\n\t\tif (ec == boost::system::errc::host_unreachable)\n\t\t{\n\t\t\tthis->node.stats.inc (nano::stat::type::error, nano::stat::detail::unreachable_host, nano::stat::dir::out);\n\t\t}\n\t\tif (this->node.config.logging.network_packet_logging ())\n\t\t{\n\t\t\tBOOST_LOG (this->node.log) << \"Packet send complete\";\n\t\t}\n\t});\n}\n\nstd::shared_ptr<nano::node> nano::node::shared ()\n{\n\treturn shared_from_this ();\n}\n\nnano::election_vote_result::election_vote_result () :\nreplay (false),\nprocessed (false)\n{\n}\n\nnano::election_vote_result::election_vote_result (bool replay_a, bool processed_a)\n{\n\treplay = replay_a;\n\tprocessed = processed_a;\n}\n\nnano::election::election (nano::node & node_a, std::shared_ptr<nano::block> block_a, std::function<void(std::shared_ptr<nano::block>)> const & confirmation_action_a) :\nconfirmation_action (confirmation_action_a),\nnode (node_a),\nelection_start (std::chrono::steady_clock::now ()),\nstatus ({ block_a, 0 }),\nconfirmed (false),\nstopped (false),\nannouncements (0)\n{\n\tlast_votes.insert (std::make_pair (nano::not_an_account, nano::vote_info{ std::chrono::steady_clock::now (), 0, block_a->hash () }));\n\tblocks.insert (std::make_pair (block_a->hash (), block_a));\n}\n\nvoid nano::election::compute_rep_votes (nano::transaction const & transaction_a)\n{\n\tif (node.config.enable_voting)\n\t{\n\t\tnode.wallets.foreach_representative (transaction_a, [this, &transaction_a](nano::public_key const & pub_a, nano::raw_key const & prv_a) {\n\t\t\tauto vote (this->node.store.vote_generate (transaction_a, pub_a, prv_a, status.winner));\n\t\t\tthis->node.vote_processor.vote (vote, this->node.network.endpoint ());\n\t\t});\n\t}\n}\n\nvoid nano::election::confirm_once (nano::transaction const & transaction_a, uint8_t & depth_a)\n{\n\tdepth_a++;\n\tif (!confirmed.exchange (true))\n\t{\n\t\tstatus.election_end = std::chrono::duration_cast<std::chrono::milliseconds> (std::chrono::system_clock::now ().time_since_epoch ());\n\t\tstatus.election_duration = std::chrono::duration_cast<std::chrono::milliseconds> (std::chrono::steady_clock::now () - election_start);\n\t\tauto winner_l (status.winner);\n\t\tauto node_l (node.shared ());\n\t\tauto confirmation_action_l (confirmation_action);\n\t\tnode.background ([node_l, winner_l, confirmation_action_l]() {\n\t\t\tnode_l->process_confirmed (winner_l);\n\t\t\tconfirmation_action_l (winner_l);\n\t\t});\n\t\tconfirm_back (transaction_a, depth_a);\n\t}\n}\n\nvoid nano::election::confirm_back (nano::transaction const & transaction_a, uint8_t & depth_a)\n{\n\tstd::vector<nano::block_hash> hashes = { status.winner->previous (), status.winner->source (), status.winner->link () };\n\tfor (auto & hash : hashes)\n\t{\n\t\t// Depth is limited to 200\n\t\tif (!hash.is_zero () && !node.ledger.is_epoch_link (hash) && depth_a < 200)\n\t\t{\n\t\t\tauto existing (node.active.blocks.find (hash));\n\t\t\tif (existing != node.active.blocks.end () && !existing->second->confirmed && !existing->second->stopped && existing->second->blocks.size () == 1)\n\t\t\t{\n\t\t\t\texisting->second->confirm_once (transaction_a, depth_a);\n\t\t\t}\n\t\t}\n\t}\n}\n\nvoid nano::election::stop ()\n{\n\tstopped = true;\n}\n\nbool nano::election::have_quorum (nano::tally_t const & tally_a, nano::uint128_t tally_sum)\n{\n\tbool result = false;\n\tif (tally_sum >= node.config.online_weight_minimum.number ())\n\t{\n\t\tauto i (tally_a.begin ());\n\t\tauto first (i->first);\n\t\t++i;\n\t\tauto second (i != tally_a.end () ? i->first : 0);\n\t\tauto delta_l (node.delta ());\n\t\tresult = tally_a.begin ()->first > (second + delta_l);\n\t}\n\treturn result;\n}\n\nnano::tally_t nano::election::tally (nano::transaction const & transaction_a)\n{\n\tstd::unordered_map<nano::block_hash, nano::uint128_t> block_weights;\n\tfor (auto vote_info : last_votes)\n\t{\n\t\tblock_weights[vote_info.second.hash] += node.ledger.weight (transaction_a, vote_info.first);\n\t}\n\tlast_tally = block_weights;\n\tnano::tally_t result;\n\tfor (auto item : block_weights)\n\t{\n\t\tauto block (blocks.find (item.first));\n\t\tif (block != blocks.end ())\n\t\t{\n\t\t\tresult.insert (std::make_pair (item.second, block->second));\n\t\t}\n\t}\n\treturn result;\n}\n\nvoid nano::election::confirm_if_quorum (nano::transaction const & transaction_a)\n{\n\tauto tally_l (tally (transaction_a));\n\tassert (tally_l.size () > 0);\n\tauto winner (tally_l.begin ());\n\tauto block_l (winner->second);\n\tstatus.tally = winner->first;\n\tnano::uint128_t sum (0);\n\tfor (auto & i : tally_l)\n\t{\n\t\tsum += i.first;\n\t}\n\tif (sum >= node.config.online_weight_minimum.number () && block_l->hash () != status.winner->hash ())\n\t{\n\t\tauto node_l (node.shared ());\n\t\tnode_l->block_processor.force (block_l);\n\t\tstatus.winner = block_l;\n\t}\n\tif (have_quorum (tally_l, sum))\n\t{\n\t\tif (node.config.logging.vote_logging () || blocks.size () > 1)\n\t\t{\n\t\t\tlog_votes (tally_l);\n\t\t}\n\t\tuint8_t depth (0);\n\t\tconfirm_once (transaction_a, depth);\n\t}\n}\n\nvoid nano::election::log_votes (nano::tally_t const & tally_a)\n{\n\tstd::stringstream tally;\n\ttally << boost::str (boost::format (\"\\nVote tally for root %1%\") % status.winner->root ().to_string ());\n\tfor (auto i (tally_a.begin ()), n (tally_a.end ()); i != n; ++i)\n\t{\n\t\ttally << boost::str (boost::format (\"\\nBlock %1% weight %2%\") % i->second->hash ().to_string () % i->first.convert_to<std::string> ());\n\t}\n\tfor (auto i (last_votes.begin ()), n (last_votes.end ()); i != n; ++i)\n\t{\n\t\ttally << boost::str (boost::format (\"\\n%1% %2%\") % i->first.to_account () % i->second.hash.to_string ());\n\t}\n\tBOOST_LOG (node.log) << tally.str ();\n}\n\nnano::election_vote_result nano::election::vote (nano::account rep, uint64_t sequence, nano::block_hash block_hash)\n{\n\t// see republish_vote documentation for an explanation of these rules\n\tauto transaction (node.store.tx_begin_read ());\n\tauto replay (false);\n\tauto supply (node.online_reps.online_stake ());\n\tauto weight (node.ledger.weight (transaction, rep));\n\tauto should_process (false);\n\tif (nano::is_test_network || weight > supply / 1000) // 0.1% or above\n\t{\n\t\tunsigned int cooldown;\n\t\tif (weight < supply / 100) // 0.1% to 1%\n\t\t{\n\t\t\tcooldown = 15;\n\t\t}\n\t\telse if (weight < supply / 20) // 1% to 5%\n\t\t{\n\t\t\tcooldown = 5;\n\t\t}\n\t\telse // 5% or above\n\t\t{\n\t\t\tcooldown = 1;\n\t\t}\n\t\tauto last_vote_it (last_votes.find (rep));\n\t\tif (last_vote_it == last_votes.end ())\n\t\t{\n\t\t\tshould_process = true;\n\t\t}\n\t\telse\n\t\t{\n\t\t\tauto last_vote (last_vote_it->second);\n\t\t\tif (last_vote.sequence < sequence || (last_vote.sequence == sequence && last_vote.hash < block_hash))\n\t\t\t{\n\t\t\t\tif (last_vote.time <= std::chrono::steady_clock::now () - std::chrono::seconds (cooldown))\n\t\t\t\t{\n\t\t\t\t\tshould_process = true;\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\treplay = true;\n\t\t\t}\n\t\t}\n\t\tif (should_process)\n\t\t{\n\t\t\tlast_votes[rep] = { std::chrono::steady_clock::now (), sequence, block_hash };\n\t\t\tif (!confirmed)\n\t\t\t{\n\t\t\t\tconfirm_if_quorum (transaction);\n\t\t\t}\n\t\t}\n\t}\n\treturn nano::election_vote_result (replay, should_process);\n}\n\nbool nano::node::validate_block_by_previous (nano::transaction const & transaction, std::shared_ptr<nano::block> block_a)\n{\n\tbool result (false);\n\tnano::account account;\n\tif (!block_a->previous ().is_zero ())\n\t{\n\t\tif (store.block_exists (transaction, block_a->previous ()))\n\t\t{\n\t\t\taccount = ledger.account (transaction, block_a->previous ());\n\t\t}\n\t\telse\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t}\n\telse\n\t{\n\t\taccount = block_a->root ();\n\t}\n\tif (!result && block_a->type () == nano::block_type::state)\n\t{\n\t\tstd::shared_ptr<nano::state_block> block_l (std::static_pointer_cast<nano::state_block> (block_a));\n\t\tnano::amount prev_balance (0);\n\t\tif (!block_l->hashables.previous.is_zero ())\n\t\t{\n\t\t\tif (store.block_exists (transaction, block_l->hashables.previous))\n\t\t\t{\n\t\t\t\tprev_balance = ledger.balance (transaction, block_l->hashables.previous);\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tresult = true;\n\t\t\t}\n\t\t}\n\t\tif (!result)\n\t\t{\n\t\t\tif (block_l->hashables.balance == prev_balance && !ledger.epoch_link.is_zero () && ledger.is_epoch_link (block_l->hashables.link))\n\t\t\t{\n\t\t\t\taccount = ledger.epoch_signer;\n\t\t\t}\n\t\t}\n\t}\n\tif (!result && (account.is_zero () || nano::validate_message (account, block_a->hash (), block_a->block_signature ())))\n\t{\n\t\tresult = true;\n\t}\n\treturn result;\n}\n\nbool nano::election::publish (std::shared_ptr<nano::block> block_a)\n{\n\tauto result (false);\n\tif (blocks.size () >= 10)\n\t{\n\t\tif (last_tally[block_a->hash ()] < node.online_reps.online_stake () / 10)\n\t\t{\n\t\t\tresult = true;\n\t\t}\n\t}\n\tif (!result)\n\t{\n\t\tauto transaction (node.store.tx_begin_read ());\n\t\tresult = node.validate_block_by_previous (transaction, block_a);\n\t\tif (!result)\n\t\t{\n\t\t\tif (blocks.find (block_a->hash ()) == blocks.end ())\n\t\t\t{\n\t\t\t\tblocks.insert (std::make_pair (block_a->hash (), block_a));\n\t\t\t\tconfirm_if_quorum (transaction);\n\t\t\t\tnode.network.republish_block (block_a);\n\t\t\t}\n\t\t}\n\t}\n\treturn result;\n}\n\nvoid nano::active_transactions::request_confirm (std::unique_lock<std::mutex> & lock_a)\n{\n\tstd::unordered_set<nano::uint512_union> inactive;\n\tauto transaction (node.store.tx_begin_read ());\n\tunsigned unconfirmed_count (0);\n\tunsigned unconfirmed_announcements (0);\n\tstd::unordered_map<nano::endpoint, std::vector<std::pair<nano::block_hash, nano::block_hash>>> requests_bundle;\n\tstd::deque<std::shared_ptr<nano::block>> rebroadcast_bundle;\n\tstd::deque<std::pair<std::shared_ptr<nano::block>, std::shared_ptr<std::vector<nano::peer_information>>>> confirm_req_bundle;\n\n\tauto roots_size (roots.size ());\n\tfor (auto i (roots.get<1> ().begin ()), n (roots.get<1> ().end ()); i != n; ++i)\n\t{\n\t\tauto root (i->root);\n\t\tlock_a.unlock ();\n\t\tauto election_l (i->election);\n\t\tif ((election_l->confirmed || election_l->stopped) && i->election->announcements >= announcement_min - 1)\n\t\t{\n\t\t\tif (election_l->confirmed)\n\t\t\t{\n\t\t\t\tconfirmed.push_back (i->election->status);\n\t\t\t\tif (confirmed.size () > election_history_size)\n\t\t\t\t{\n\t\t\t\t\tconfirmed.pop_front ();\n\t\t\t\t}\n\t\t\t}\n\t\t\tinactive.insert (root);\n\t\t}\n\t\telse\n\t\t{\n\t\t\tif (i->election->announcements > announcement_long)\n\t\t\t{\n\t\t\t\t++unconfirmed_count;\n\t\t\t\tunconfirmed_announcements += i->election->announcements;\n\t\t\t\t// Log votes for very long unconfirmed elections\n\t\t\t\tif (i->election->announcements % 50 == 1)\n\t\t\t\t{\n\t\t\t\t\tauto tally_l (election_l->tally (transaction));\n\t\t\t\t\telection_l->log_votes (tally_l);\n\t\t\t\t}\n\t\t\t\t/* Escalation for long unconfirmed elections\n\t\t\t\tStart new elections for previous block & source\n\t\t\t\tif there are less than 100 active elections */\n\t\t\t\tif (i->election->announcements % announcement_long == 1 && roots_size < 100 && !nano::is_test_network)\n\t\t\t\t{\n\t\t\t\t\tstd::shared_ptr<nano::block> previous;\n\t\t\t\t\tauto previous_hash (election_l->status.winner->previous ());\n\t\t\t\t\tif (!previous_hash.is_zero ())\n\t\t\t\t\t{\n\t\t\t\t\t\tprevious = node.store.block_get (transaction, previous_hash);\n\t\t\t\t\t\tif (previous != nullptr)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tadd (std::move (previous));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t/* If previous block not existing/not commited yet, block_source can cause segfault for state blocks\n\t\t\t\t\tSo source check can be done only if previous != nullptr or previous is 0 (open account) */\n\t\t\t\t\tif (previous_hash.is_zero () || previous != nullptr)\n\t\t\t\t\t{\n\t\t\t\t\t\tauto source_hash (node.ledger.block_source (transaction, *election_l->status.winner));\n\t\t\t\t\t\tif (!source_hash.is_zero ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto source (node.store.block_get (transaction, source_hash));\n\t\t\t\t\t\t\tif (source != nullptr)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tadd (std::move (source));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i->election->announcements < announcement_long || i->election->announcements % announcement_long == 1)\n\t\t\t{\n\t\t\t\tif (node.ledger.could_fit (transaction, *election_l->status.winner))\n\t\t\t\t{\n\t\t\t\t\t// Broadcast winner\n\t\t\t\t\tif (rebroadcast_bundle.size () < max_broadcast_queue)\n\t\t\t\t\t{\n\t\t\t\t\t\trebroadcast_bundle.push_back (election_l->status.winner);\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tif (i->election->announcements != 0)\n\t\t\t\t\t{\n\t\t\t\t\t\telection_l->stop ();\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tif (i->election->announcements % 4 == 1)\n\t\t\t{\n\t\t\t\tauto reps (std::make_shared<std::vector<nano::peer_information>> (node.peers.representatives (std::numeric_limits<size_t>::max ())));\n\t\t\t\tstd::unordered_set<nano::account> probable_reps;\n\t\t\t\tnano::uint128_t total_weight (0);\n\t\t\t\tfor (auto j (reps->begin ()), m (reps->end ()); j != m;)\n\t\t\t\t{\n\t\t\t\t\tauto & rep_votes (i->election->last_votes);\n\t\t\t\t\tauto rep_acct (j->probable_rep_account);\n\t\t\t\t\t// Calculate if representative isn't recorded for several IP addresses\n\t\t\t\t\tif (probable_reps.find (rep_acct) == probable_reps.end ())\n\t\t\t\t\t{\n\t\t\t\t\t\ttotal_weight = total_weight + j->rep_weight.number ();\n\t\t\t\t\t\tprobable_reps.insert (rep_acct);\n\t\t\t\t\t}\n\t\t\t\t\tif (rep_votes.find (rep_acct) != rep_votes.end ())\n\t\t\t\t\t{\n\t\t\t\t\t\tif (j + 1 == reps->end ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\treps->pop_back ();\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\n\t\t\t\t\t\tstd::swap (*j, reps->back ());\n\t\t\t\t\t\treps->pop_back ();\n\t\t\t\t\t\tm = reps->end ();\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\t++j;\n\t\t\t\t\t\tif (node.config.logging.vote_logging ())\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tBOOST_LOG (node.log) << \"Representative did not respond to confirm_req, retrying: \" << rep_acct.to_account ();\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif ((!reps->empty () && total_weight > node.config.online_weight_minimum.number ()) || roots_size > 5)\n\t\t\t\t{\n\t\t\t\t\t// broadcast_confirm_req_base modifies reps, so we clone it once to avoid aliasing\n\t\t\t\t\tif (!nano::is_test_network)\n\t\t\t\t\t{\n\t\t\t\t\t\tif (confirm_req_bundle.size () < max_broadcast_queue)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tconfirm_req_bundle.push_back (std::make_pair (i->election->status.winner, reps));\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tfor (auto & rep : *reps)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto rep_request (requests_bundle.find (rep.endpoint));\n\t\t\t\t\t\t\tauto block (i->election->status.winner);\n\t\t\t\t\t\t\tauto root_hash (std::make_pair (block->hash (), block->root ()));\n\t\t\t\t\t\t\tif (rep_request == requests_bundle.end ())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tif (requests_bundle.size () < max_broadcast_queue)\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tstd::vector<std::pair<nano::block_hash, nano::block_hash>> insert_vector = { root_hash };\n\t\t\t\t\t\t\t\t\trequests_bundle.insert (std::make_pair (rep.endpoint, insert_vector));\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse if (rep_request->second.size () < max_broadcast_queue * nano::network::confirm_req_hashes_max)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\trep_request->second.push_back (root_hash);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\telse\n\t\t\t\t{\n\t\t\t\t\tif (!nano::is_test_network)\n\t\t\t\t\t{\n\t\t\t\t\t\tconfirm_req_bundle.push_back (std::make_pair (i->election->status.winner, std::make_shared<std::vector<nano::peer_information>> (node.peers.list_vector (100))));\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tfor (auto & rep : *reps)\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tauto rep_request (requests_bundle.find (rep.endpoint));\n\t\t\t\t\t\t\tauto block (i->election->status.winner);\n\t\t\t\t\t\t\tauto root_hash (std::make_pair (block->hash (), block->root ()));\n\t\t\t\t\t\t\tif (rep_request == requests_bundle.end ())\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tstd::vector<std::pair<nano::block_hash, nano::block_hash>> insert_vector = { root_hash };\n\t\t\t\t\t\t\t\trequests_bundle.insert (std::make_pair (rep.endpoint, insert_vector));\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\trep_request->second.push_back (root_hash);\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t++election_l->announcements;\n\t\tlock_a.lock ();\n\t}\n\t// Rebroadcast unconfirmed blocks\n\tif (!rebroadcast_bundle.empty ())\n\t{\n\t\tnode.network.republish_block_batch (rebroadcast_bundle);\n\t}\n\t// Batch confirmation request\n\tif (!nano::is_live_network && !requests_bundle.empty ())\n\t{\n\t\tnode.network.broadcast_confirm_req_batch (requests_bundle, 50);\n\t}\n\t//confirm_req broadcast\n\tif (!confirm_req_bundle.empty ())\n\t{\n\t\tnode.network.broadcast_confirm_req_batch (confirm_req_bundle);\n\t}\n\tfor (auto i (inactive.begin ()), n (inactive.end ()); i != n; ++i)\n\t{\n\t\tauto root_it (roots.find (*i));\n\t\tassert (root_it != roots.end ());\n\t\tfor (auto & block : root_it->election->blocks)\n\t\t{\n\t\t\tauto erased (blocks.erase (block.first));\n\t\t\t(void)erased;\n\t\t\tassert (erased == 1);\n\t\t}\n\t\troots.erase (*i);\n\t}\n\tif (unconfirmed_count > 0)\n\t{\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"%1% blocks have been unconfirmed averaging %2% announcements\") % unconfirmed_count % (unconfirmed_announcements / unconfirmed_count));\n\t}\n}\n\nvoid nano::active_transactions::request_loop ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\tstarted = true;\n\n\tlock.unlock ();\n\tcondition.notify_all ();\n\tlock.lock ();\n\n\twhile (!stopped)\n\t{\n\t\trequest_confirm (lock);\n\t\tconst auto extra_delay (std::min (roots.size (), max_broadcast_queue) * node.network.broadcast_interval_ms * 2);\n\t\tcondition.wait_for (lock, std::chrono::milliseconds (request_interval_ms + extra_delay));\n\t}\n}\n\nvoid nano::active_transactions::stop ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (!started)\n\t{\n\t\tcondition.wait (lock);\n\t}\n\tstopped = true;\n\tlock.unlock ();\n\tcondition.notify_all ();\n\tif (thread.joinable ())\n\t{\n\t\tthread.join ();\n\t}\n\tlock.lock ();\n\troots.clear ();\n}\n\nbool nano::active_transactions::start (std::shared_ptr<nano::block> block_a, std::function<void(std::shared_ptr<nano::block>)> const & confirmation_action_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\treturn add (block_a, confirmation_action_a);\n}\n\nbool nano::active_transactions::add (std::shared_ptr<nano::block> block_a, std::function<void(std::shared_ptr<nano::block>)> const & confirmation_action_a)\n{\n\tauto error (true);\n\tif (!stopped)\n\t{\n\t\tauto root (nano::uint512_union (block_a->previous (), block_a->root ()));\n\t\tauto existing (roots.find (root));\n\t\tif (existing == roots.end ())\n\t\t{\n\t\t\tauto election (std::make_shared<nano::election> (node, block_a, confirmation_action_a));\n\t\t\tuint64_t difficulty (0);\n\t\t\tauto error (nano::work_validate (*block_a, &difficulty));\n\t\t\trelease_assert (!error);\n\t\t\troots.insert (nano::conflict_info{ root, difficulty, election });\n\t\t\tblocks.insert (std::make_pair (block_a->hash (), election));\n\t\t}\n\t\terror = existing != roots.end ();\n\t}\n\treturn error;\n}\n\n// Validate a vote and apply it to the current election if one exists\nbool nano::active_transactions::vote (std::shared_ptr<nano::vote> vote_a, bool single_lock)\n{\n\tstd::shared_ptr<nano::election> election;\n\tbool replay (false);\n\tbool processed (false);\n\t{\n\t\tstd::unique_lock<std::mutex> lock;\n\t\tif (!single_lock)\n\t\t{\n\t\t\tlock = std::unique_lock<std::mutex> (mutex);\n\t\t}\n\t\tfor (auto vote_block : vote_a->blocks)\n\t\t{\n\t\t\tnano::election_vote_result result;\n\t\t\tif (vote_block.which ())\n\t\t\t{\n\t\t\t\tauto block_hash (boost::get<nano::block_hash> (vote_block));\n\t\t\t\tauto existing (blocks.find (block_hash));\n\t\t\t\tif (existing != blocks.end ())\n\t\t\t\t{\n\t\t\t\t\tresult = existing->second->vote (vote_a->account, vote_a->sequence, block_hash);\n\t\t\t\t}\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tauto block (boost::get<std::shared_ptr<nano::block>> (vote_block));\n\t\t\t\tauto existing (roots.find (nano::uint512_union (block->previous (), block->root ())));\n\t\t\t\tif (existing != roots.end ())\n\t\t\t\t{\n\t\t\t\t\tresult = existing->election->vote (vote_a->account, vote_a->sequence, block->hash ());\n\t\t\t\t}\n\t\t\t}\n\t\t\treplay = replay || result.replay;\n\t\t\tprocessed = processed || result.processed;\n\t\t}\n\t}\n\tif (processed)\n\t{\n\t\tnode.network.republish_vote (vote_a);\n\t}\n\treturn replay;\n}\n\nbool nano::active_transactions::active (nano::block const & block_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\treturn roots.find (nano::uint512_union (block_a.previous (), block_a.root ())) != roots.end ();\n}\n\nvoid nano::active_transactions::update_difficulty (nano::block const & block_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto existing (roots.find (nano::uint512_union (block_a.previous (), block_a.root ())));\n\tif (existing != roots.end ())\n\t{\n\t\tuint64_t difficulty;\n\t\tauto error (nano::work_validate (block_a, &difficulty));\n\t\tassert (!error);\n\t\troots.modify (existing, [difficulty](nano::conflict_info & info_a) {\n\t\t\tinfo_a.difficulty = difficulty;\n\t\t});\n\t}\n}\n\n// List of active blocks in elections\nstd::deque<std::shared_ptr<nano::block>> nano::active_transactions::list_blocks (bool single_lock)\n{\n\tstd::deque<std::shared_ptr<nano::block>> result;\n\tstd::unique_lock<std::mutex> lock;\n\tif (!single_lock)\n\t{\n\t\tlock = std::unique_lock<std::mutex> (mutex);\n\t}\n\tfor (auto i (roots.begin ()), n (roots.end ()); i != n; ++i)\n\t{\n\t\tresult.push_back (i->election->status.winner);\n\t}\n\treturn result;\n}\n\nvoid nano::active_transactions::erase (nano::block const & block_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tif (roots.find (nano::uint512_union (block_a.previous (), block_a.root ())) != roots.end ())\n\t{\n\t\troots.erase (nano::uint512_union (block_a.previous (), block_a.root ()));\n\t\tBOOST_LOG (node.log) << boost::str (boost::format (\"Election erased for block block %1% root %2%\") % block_a.hash ().to_string () % block_a.root ().to_string ());\n\t}\n}\n\nnano::active_transactions::active_transactions (nano::node & node_a) :\nnode (node_a),\nstarted (false),\nstopped (false),\nthread ([this]() {\n\tnano::thread_role::set (nano::thread_role::name::request_loop);\n\trequest_loop ();\n})\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (!started)\n\t{\n\t\tcondition.wait (lock);\n\t}\n}\n\nnano::active_transactions::~active_transactions ()\n{\n\tstop ();\n}\n\nbool nano::active_transactions::publish (std::shared_ptr<nano::block> block_a)\n{\n\tstd::lock_guard<std::mutex> lock (mutex);\n\tauto existing (roots.find (nano::uint512_union (block_a->previous (), block_a->root ())));\n\tauto result (true);\n\tif (existing != roots.end ())\n\t{\n\t\tresult = existing->election->publish (block_a);\n\t\tif (!result)\n\t\t{\n\t\t\tblocks.insert (std::make_pair (block_a->hash (), existing->election));\n\t\t}\n\t}\n\treturn result;\n}\n\nnamespace nano\n{\nstd::unique_ptr<seq_con_info_component> collect_seq_con_info (active_transactions & active_transactions, const std::string & name)\n{\n\tsize_t roots_count = 0;\n\tsize_t blocks_count = 0;\n\tsize_t confirmed_count = 0;\n\n\t{\n\t\tstd::lock_guard<std::mutex> guard (active_transactions.mutex);\n\t\troots_count = active_transactions.roots.size ();\n\t\tblocks_count = active_transactions.blocks.size ();\n\t\tconfirmed_count = active_transactions.confirmed.size ();\n\t}\n\n\tauto composite = std::make_unique<seq_con_info_composite> (name);\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"roots\", roots_count, sizeof (decltype (active_transactions.roots)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"blocks\", blocks_count, sizeof (decltype (active_transactions.blocks)::value_type) }));\n\tcomposite->add_component (std::make_unique<seq_con_info_leaf> (seq_con_info{ \"confirmed\", confirmed_count, sizeof (decltype (active_transactions.confirmed)::value_type) }));\n\treturn composite;\n}\n}\nint nano::node::store_version ()\n{\n\tauto transaction (store.tx_begin_read ());\n\treturn store.version_get (transaction);\n}\n\nnano::thread_runner::thread_runner (boost::asio::io_context & io_ctx_a, unsigned service_threads_a)\n{\n\tboost::thread::attributes attrs;\n\tnano::thread_attributes::set (attrs);\n\tfor (auto i (0u); i < service_threads_a; ++i)\n\t{\n\t\tthreads.push_back (boost::thread (attrs, [&io_ctx_a]() {\n\t\t\tnano::thread_role::set (nano::thread_role::name::io);\n\t\t\ttry\n\t\t\t{\n\t\t\t\tio_ctx_a.run ();\n\t\t\t}\n\t\t\tcatch (...)\n\t\t\t{\n#ifndef NDEBUG\n\t\t\t\t/*\n\t\t\t\t * In a release build, catch and swallow the\n\t\t\t\t * io_context exception, in debug mode pass it\n\t\t\t\t * on\n\t\t\t\t */\n\t\t\t\tthrow;\n#endif\n\t\t\t}\n\t\t}));\n\t}\n}\n\nnano::thread_runner::~thread_runner ()\n{\n\tjoin ();\n}\n\nvoid nano::thread_runner::join ()\n{\n\tfor (auto & i : threads)\n\t{\n\t\tif (i.joinable ())\n\t\t{\n\t\t\ti.join ();\n\t\t}\n\t}\n}\n\nnano::inactive_node::inactive_node (boost::filesystem::path const & path, uint16_t peering_port_a) :\npath (path),\nio_context (std::make_shared<boost::asio::io_context> ()),\nalarm (*io_context),\nwork (1, nullptr),\npeering_port (peering_port_a)\n{\n\tboost::system::error_code error_chmod;\n\n\t/*\n\t * @warning May throw a filesystem exception\n\t */\n\tboost::filesystem::create_directories (path);\n\tnano::set_secure_perm_directory (path, error_chmod);\n\tlogging.max_size = std::numeric_limits<std::uintmax_t>::max ();\n\tlogging.init (path);\n\tnode = std::make_shared<nano::node> (init, *io_context, peering_port, path, alarm, logging, work);\n}\n\nnano::inactive_node::~inactive_node ()\n{\n\tnode->stop ();\n}\n\nnano::udp_buffer::udp_buffer (nano::stat & stats, size_t size, size_t count) :\nstats (stats),\nfree (count),\nfull (count),\nslab (size * count),\nentries (count),\nstopped (false)\n{\n\tassert (count > 0);\n\tassert (size > 0);\n\tauto slab_data (slab.data ());\n\tauto entry_data (entries.data ());\n\tfor (auto i (0); i < count; ++i, ++entry_data)\n\t{\n\t\t*entry_data = { slab_data + i * size, 0, nano::endpoint () };\n\t\tfree.push_back (entry_data);\n\t}\n}\nnano::udp_data * nano::udp_buffer::allocate ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (!stopped && free.empty () && full.empty ())\n\t{\n\t\tstats.inc (nano::stat::type::udp, nano::stat::detail::blocking, nano::stat::dir::in);\n\t\tcondition.wait (lock);\n\t}\n\tnano::udp_data * result (nullptr);\n\tif (!free.empty ())\n\t{\n\t\tresult = free.front ();\n\t\tfree.pop_front ();\n\t}\n\tif (result == nullptr)\n\t{\n\t\tresult = full.front ();\n\t\tfull.pop_front ();\n\t\tstats.inc (nano::stat::type::udp, nano::stat::detail::overflow, nano::stat::dir::in);\n\t}\n\treturn result;\n}\nvoid nano::udp_buffer::enqueue (nano::udp_data * data_a)\n{\n\tassert (data_a != nullptr);\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\tfull.push_back (data_a);\n\t}\n\tcondition.notify_all ();\n}\nnano::udp_data * nano::udp_buffer::dequeue ()\n{\n\tstd::unique_lock<std::mutex> lock (mutex);\n\twhile (!stopped && full.empty ())\n\t{\n\t\tcondition.wait (lock);\n\t}\n\tnano::udp_data * result (nullptr);\n\tif (!full.empty ())\n\t{\n\t\tresult = full.front ();\n\t\tfull.pop_front ();\n\t}\n\treturn result;\n}\nvoid nano::udp_buffer::release (nano::udp_data * data_a)\n{\n\tassert (data_a != nullptr);\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\tfree.push_back (data_a);\n\t}\n\tcondition.notify_all ();\n}\nvoid nano::udp_buffer::stop ()\n{\n\t{\n\t\tstd::lock_guard<std::mutex> lock (mutex);\n\t\tstopped = true;\n\t}\n\tcondition.notify_all ();\n}\n", "idx": 28, "id": 15004, "msg": "", "proj": "nanocurrency-nano-node", "lang": "cpp"}
{"patch": "@@ -535,7 +535,7 @@ func (jt journalTracker) getQuotaInfo() (usedQuotaBytes, quotaBytes int64) {\n }\n \n func (jt journalTracker) getDiskLimitInfo() (\n-\tusedBytes, limitBytes, usedFiles, limitFiles int64) {\n+\tusedBytes int64, limitBytes float64, usedFiles int64, limitFiles float64) {\n \tusedBytes, limitBytes = jt.byte.getLimitInfo()\n \tusedFiles, limitFiles = jt.file.getLimitInfo()\n \treturn usedBytes, limitBytes, usedFiles, limitFiles", "y": 0, "oldf": "// Copyright 2017 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"fmt\"\n\t\"math\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/logger\"\n\t\"github.com/keybase/kbfs/kbfssync\"\n\t\"github.com/pkg/errors\"\n\t\"golang.org/x/net/context\"\n)\n\n// backpressureTracker keeps track of the variables used to calculate\n// backpressure. It keeps track of a generic resource (which can be\n// either bytes or files).\n//\n// Let U be the (approximate) resource usage of the journal and F be\n// the free resources. Then we want to enforce\n//\n//   U <= min(k(U+F), L),\n//\n// where 0 < k <= 1 is some fraction, and L > 0 is the absolute\n// resource usage limit. But in addition to that, we want to set\n// thresholds 0 <= m <= M <= 1 such that we apply proportional\n// backpressure (with a given maximum delay) when\n//\n//   m <= max(U/(k(U+F)), U/L) <= M,\n//\n// which is equivalent to\n//\n//   m <= U/min(k(U+F), L) <= M.\n//\n// Note that this type doesn't do any locking, so it's the caller's\n// responsibility to do so.\ntype backpressureTracker struct {\n\t// minThreshold is m in the above.\n\tminThreshold float64\n\t// maxThreshold is M in the above.\n\tmaxThreshold float64\n\t// limitFrac is k in the above.\n\tlimitFrac float64\n\t// limit is L in the above.\n\tlimit int64\n\n\t// used is U in the above.\n\tused int64\n\t// free is F in the above.\n\tfree int64\n\n\t// semaphoreMax is the last calculated value of currLimit(),\n\t// which is min(k(U+F), L).\n\tsemaphoreMax int64\n\t// The count of the semaphore is semaphoreMax - U - I, where I\n\t// is the resource count that is currently \"in-flight\",\n\t// i.e. between beforeBlockPut() and afterBlockPut() calls.\n\tsemaphore *kbfssync.Semaphore\n}\n\nfunc newBackpressureTracker(minThreshold, maxThreshold, limitFrac float64,\n\tlimit, initialFree int64) (*backpressureTracker, error) {\n\tif minThreshold < 0.0 {\n\t\treturn nil, errors.Errorf(\"minThreshold=%f < 0.0\",\n\t\t\tminThreshold)\n\t}\n\tif maxThreshold < minThreshold {\n\t\treturn nil, errors.Errorf(\n\t\t\t\"maxThreshold=%f < minThreshold=%f\",\n\t\t\tmaxThreshold, minThreshold)\n\t}\n\tif 1.0 < maxThreshold {\n\t\treturn nil, errors.Errorf(\"1.0 < maxThreshold=%f\",\n\t\t\tmaxThreshold)\n\t}\n\tif limitFrac < 0.01 {\n\t\treturn nil, errors.Errorf(\"limitFrac=%f < 0.01\", limitFrac)\n\t}\n\tif limitFrac > 1.0 {\n\t\treturn nil, errors.Errorf(\"limitFrac=%f > 1.0\", limitFrac)\n\t}\n\tif limit < 0 {\n\t\treturn nil, errors.Errorf(\"limit=%d < 0\", limit)\n\t}\n\tif initialFree < 0 {\n\t\treturn nil, errors.Errorf(\"initialFree=%d < 0\", initialFree)\n\t}\n\tbt := &backpressureTracker{\n\t\tminThreshold, maxThreshold, limitFrac, limit,\n\t\t0, initialFree, 0, kbfssync.NewSemaphore(),\n\t}\n\tbt.updateSemaphoreMax()\n\treturn bt, nil\n}\n\n// currLimit returns the resource limit, taking into account the\n// amount of free resources left. This is min(k(U+F), L).\nfunc (bt backpressureTracker) currLimit() float64 {\n\t// Calculate k(U+F), converting to float64 first to avoid\n\t// overflow, although losing some precision in the process.\n\tusedFloat := float64(bt.used)\n\tfreeFloat := float64(bt.free)\n\tlimit := bt.limitFrac * (usedFloat + freeFloat)\n\treturn math.Min(limit, float64(bt.limit))\n}\n\nfunc (bt backpressureTracker) usedFrac() float64 {\n\treturn float64(bt.used) / bt.currLimit()\n}\n\n// delayScale returns a number between 0 and 1, which should be\n// multiplied with the maximum delay to get the backpressure delay to\n// apply.\nfunc (bt backpressureTracker) delayScale() float64 {\n\tusedFrac := bt.usedFrac()\n\n\t// We want the delay to be 0 if usedFrac <= m and the max\n\t// delay if usedFrac >= M, so linearly interpolate the delay\n\t// scale.\n\tm := bt.minThreshold\n\tM := bt.maxThreshold\n\treturn math.Min(1.0, math.Max(0.0, (usedFrac-m)/(M-m)))\n}\n\n// updateSemaphoreMax must be called whenever bt.used or bt.free\n// changes.\nfunc (bt *backpressureTracker) updateSemaphoreMax() {\n\tnewMax := int64(bt.currLimit())\n\tdelta := newMax - bt.semaphoreMax\n\t// These operations are adjusting the *maximum* value of\n\t// bt.semaphore.\n\tif delta > 0 {\n\t\tbt.semaphore.Release(delta)\n\t} else if delta < 0 {\n\t\tbt.semaphore.ForceAcquire(-delta)\n\t}\n\tbt.semaphoreMax = newMax\n}\n\nfunc (bt *backpressureTracker) onEnable(usedResources int64) (\n\tavailableResources int64) {\n\tbt.used += usedResources\n\tbt.updateSemaphoreMax()\n\tif usedResources == 0 {\n\t\treturn bt.semaphore.Count()\n\t}\n\treturn bt.semaphore.ForceAcquire(usedResources)\n}\n\nfunc (bt *backpressureTracker) onDisable(usedResources int64) {\n\tbt.used -= usedResources\n\tbt.updateSemaphoreMax()\n\tif usedResources > 0 {\n\t\tbt.semaphore.Release(usedResources)\n\t}\n}\n\nfunc (bt *backpressureTracker) updateFree(freeResources int64) {\n\tbt.free = freeResources\n\tbt.updateSemaphoreMax()\n}\n\nfunc (bt *backpressureTracker) beforeBlockPut(\n\tctx context.Context, blockResources int64) (\n\tavailableResources int64, err error) {\n\treturn bt.semaphore.Acquire(ctx, blockResources)\n}\n\nfunc (bt *backpressureTracker) afterBlockPut(\n\tblockResources int64, putData bool) {\n\tif putData {\n\t\tbt.used += blockResources\n\t\tbt.updateSemaphoreMax()\n\t} else {\n\t\tbt.semaphore.Release(blockResources)\n\t}\n}\n\nfunc (bt *backpressureTracker) onBlocksDelete(blockResources int64) {\n\tif blockResources == 0 {\n\t\treturn\n\t}\n\n\tbt.semaphore.Release(blockResources)\n\n\tbt.used -= blockResources\n\tbt.updateSemaphoreMax()\n}\n\nfunc (bt *backpressureTracker) beforeDiskBlockCachePut(blockResources int64) (\n\tavailableResources int64) {\n\t// TODO: Implement TryAcquire that automatically rolls back if it would go\n\t// negative.\n\tavailableResources = bt.semaphore.ForceAcquire(blockResources)\n\tif availableResources < 0 {\n\t\t// We must roll back the acquisition of resources. We should still\n\t\t// return the negative number, however, so the disk block cache\n\t\t// knows how much to evict.\n\t\tbt.afterBlockPut(blockResources, false)\n\t}\n\treturn availableResources\n}\n\nfunc (bt *backpressureTracker) getLimitInfo() (used int64, limit int64) {\n\treturn bt.used, bt.limit\n}\n\ntype backpressureTrackerStatus struct {\n\t// Derived numbers.\n\tUsedFrac   float64\n\tDelayScale float64\n\n\t// Constants.\n\tMinThreshold float64\n\tMaxThreshold float64\n\tLimitFrac    float64\n\tLimit        int64\n\n\t// Raw numbers.\n\tUsed  int64\n\tFree  int64\n\tMax   int64\n\tCount int64\n}\n\nfunc (bt *backpressureTracker) getStatus() backpressureTrackerStatus {\n\treturn backpressureTrackerStatus{\n\t\tUsedFrac:   bt.usedFrac(),\n\t\tDelayScale: bt.delayScale(),\n\n\t\tMinThreshold: bt.minThreshold,\n\t\tMaxThreshold: bt.maxThreshold,\n\t\tLimitFrac:    bt.limitFrac,\n\t\tLimit:        bt.limit,\n\n\t\tUsed:  bt.used,\n\t\tFree:  bt.free,\n\t\tMax:   bt.semaphoreMax,\n\t\tCount: bt.semaphore.Count(),\n\t}\n}\n\n// quotaBackpressureTracker keeps track of the variables used to\n// calculate quota-related backpressure.\n//\n// Let U be the (approximate) unflushed bytes in the journal, R be the\n// remote quota usage, and Q be the quota. Then we want to set\n// thresholds 0 <= m <= M such that we apply proportional backpressure\n// (with a given maximum delay) when\n//\n//   m <= (U+R)/Q <= M.\n//\n// Note that this type doesn't do any locking, so it's the caller's\n// responsibility to do so.\ntype quotaBackpressureTracker struct {\n\t// minThreshold is m in the above.\n\tminThreshold float64\n\t// maxThreshold is M in the above.\n\tmaxThreshold float64\n\n\t// unflushedBytes is U in the above.\n\tunflushedBytes int64\n\t// remoteUsedBytes is R in the above.\n\tremoteUsedBytes int64\n\t// quotaBytes is Q in the above.\n\tquotaBytes int64\n}\n\nfunc newQuotaBackpressureTracker(minThreshold, maxThreshold float64) (\n\t*quotaBackpressureTracker, error) {\n\tif minThreshold < 0.0 {\n\t\treturn nil, errors.Errorf(\"minThreshold=%f < 0.0\",\n\t\t\tminThreshold)\n\t}\n\tif maxThreshold < minThreshold {\n\t\treturn nil, errors.Errorf(\n\t\t\t\"maxThreshold=%f < minThreshold=%f\",\n\t\t\tmaxThreshold, minThreshold)\n\t}\n\tqbt := &quotaBackpressureTracker{\n\t\tminThreshold, maxThreshold, 0, 0, math.MaxInt64,\n\t}\n\treturn qbt, nil\n}\n\nfunc (qbt quotaBackpressureTracker) usedFrac() float64 {\n\treturn (float64(qbt.unflushedBytes) + float64(qbt.remoteUsedBytes)) /\n\t\tfloat64(qbt.quotaBytes)\n}\n\n// delayScale returns a number between 0 and 1, which should be\n// multiplied with the maximum delay to get the backpressure delay to\n// apply.\nfunc (qbt quotaBackpressureTracker) delayScale() float64 {\n\tusedFrac := qbt.usedFrac()\n\n\t// We want the delay to be 0 if usedFrac <= m and the max\n\t// delay if usedFrac >= M, so linearly interpolate the delay\n\t// scale.\n\tm := qbt.minThreshold\n\tM := qbt.maxThreshold\n\treturn math.Min(1.0, math.Max(0.0, (usedFrac-m)/(M-m)))\n}\n\nfunc (qbt quotaBackpressureTracker) getQuotaInfo() (\n\tusedQuotaBytes, quotaBytes int64) {\n\tusedQuotaBytes = qbt.unflushedBytes + qbt.remoteUsedBytes\n\tquotaBytes = qbt.quotaBytes\n\treturn usedQuotaBytes, quotaBytes\n}\n\nfunc (qbt *quotaBackpressureTracker) onJournalEnable(unflushedBytes int64) {\n\tqbt.unflushedBytes += unflushedBytes\n}\n\nfunc (qbt *quotaBackpressureTracker) onJournalDisable(unflushedBytes int64) {\n\tqbt.unflushedBytes -= unflushedBytes\n}\n\nfunc (qbt *quotaBackpressureTracker) updateRemote(\n\tremoteUsedBytes, quotaBytes int64) {\n\tqbt.remoteUsedBytes = remoteUsedBytes\n\tqbt.quotaBytes = quotaBytes\n}\n\nfunc (qbt *quotaBackpressureTracker) afterBlockPut(\n\tblockBytes int64, putData bool) {\n\tif putData {\n\t\tqbt.unflushedBytes += blockBytes\n\t}\n}\n\nfunc (qbt *quotaBackpressureTracker) onBlocksFlush(blockBytes int64) {\n\tqbt.unflushedBytes -= blockBytes\n}\n\ntype quotaBackpressureTrackerStatus struct {\n\t// Derived numbers.\n\tUsedFrac   float64\n\tDelayScale float64\n\n\t// Constants.\n\tMinThreshold float64\n\tMaxThreshold float64\n\n\t// Raw numbers.\n\tUnflushedBytes  int64\n\tRemoteUsedBytes int64\n\tQuotaBytes      int64\n}\n\nfunc (qbt *quotaBackpressureTracker) getStatus() quotaBackpressureTrackerStatus {\n\treturn quotaBackpressureTrackerStatus{\n\t\tUsedFrac:   qbt.usedFrac(),\n\t\tDelayScale: qbt.delayScale(),\n\n\t\tMinThreshold: qbt.minThreshold,\n\t\tMaxThreshold: qbt.maxThreshold,\n\n\t\tUnflushedBytes:  qbt.unflushedBytes,\n\t\tRemoteUsedBytes: qbt.remoteUsedBytes,\n\t\tQuotaBytes:      qbt.quotaBytes,\n\t}\n}\n\n// journalTracker aggregates all the journal trackers. This type also\n// doesn't do any locking, so it's the caller's responsibility to do\n// so.\ntype journalTracker struct {\n\tbyte, file *backpressureTracker\n\tquota      *quotaBackpressureTracker\n}\n\nfunc newJournalTracker(\n\tminThreshold, maxThreshold, quotaMinThreshold, quotaMaxThreshold, journalFrac float64,\n\tbyteLimit, fileLimit, freeBytes, freeFiles int64) (\n\tjournalTracker, error) {\n\t// byteLimit and fileLimit must be scaled by the proportion of\n\t// the limit that the journal should consume. Add 0.5 to round\n\t// up.\n\tjournalByteLimit := int64((float64(byteLimit) * journalFrac) + 0.5)\n\tbyteTracker, err := newBackpressureTracker(\n\t\tminThreshold, maxThreshold, journalFrac, journalByteLimit,\n\t\tfreeBytes)\n\tif err != nil {\n\t\treturn journalTracker{}, err\n\t}\n\t// the fileLimit is only used by the journal, so in theory we\n\t// don't have to scale it by journalFrac, but in the interest\n\t// of consistency with how we treat the byteLimit, we do so\n\t// anyway. Add 0.5 to round up.\n\tjournalFileLimit := int64((float64(fileLimit) * journalFrac) + 0.5)\n\tfileTracker, err := newBackpressureTracker(\n\t\tminThreshold, maxThreshold, journalFrac, journalFileLimit,\n\t\tfreeFiles)\n\tif err != nil {\n\t\treturn journalTracker{}, err\n\t}\n\n\tjournalQuotaTracker, err := newQuotaBackpressureTracker(\n\t\tquotaMinThreshold, quotaMaxThreshold)\n\tif err != nil {\n\t\treturn journalTracker{}, err\n\t}\n\n\treturn journalTracker{byteTracker, fileTracker, journalQuotaTracker}, nil\n}\n\ntype jtSnapshot struct {\n\tused  int64\n\tfree  int64\n\tmax   int64\n\tcount int64\n}\n\nfunc (jt journalTracker) getSnapshotsForTest() (\n\tbyteSnapshot, fileSnapshot, quotaSnapshot jtSnapshot) {\n\tbyteSnapshot = jtSnapshot{jt.byte.used, jt.byte.free,\n\t\tjt.byte.semaphoreMax, jt.byte.semaphore.Count()}\n\tfileSnapshot = jtSnapshot{jt.file.used, jt.file.free,\n\t\tjt.file.semaphoreMax, jt.file.semaphore.Count()}\n\tusedQuotaBytes, quotaBytes := jt.quota.getQuotaInfo()\n\tfree := quotaBytes - usedQuotaBytes\n\tquotaSnapshot = jtSnapshot{usedQuotaBytes, free, 0, 0}\n\treturn byteSnapshot, fileSnapshot, quotaSnapshot\n\n}\n\nfunc (jt journalTracker) onEnable(storedBytes, unflushedBytes, files int64) (\n\tavailableBytes, availableFiles int64) {\n\t// storedBytes should be >= unflushedBytes. But it's not too\n\t// bad to let it go through.\n\tavailableBytes = jt.byte.onEnable(storedBytes)\n\tavailableFiles = jt.file.onEnable(files)\n\tjt.quota.onJournalEnable(unflushedBytes)\n\treturn availableBytes, availableFiles\n}\n\nfunc (jt journalTracker) onDisable(storedBytes, unflushedBytes, files int64) {\n\t// As above, storedBytes should be >= unflushedBytes. Let it\n\t// go through here, too.\n\tjt.byte.onDisable(storedBytes)\n\tjt.file.onDisable(files)\n\tjt.quota.onJournalDisable(unflushedBytes)\n}\n\nfunc (jt journalTracker) getDelayScale() float64 {\n\tbyteDelayScale := jt.byte.delayScale()\n\tfileDelayScale := jt.file.delayScale()\n\tquotaDelayScale := jt.quota.delayScale()\n\tdelayScale := math.Max(\n\t\tmath.Max(byteDelayScale, fileDelayScale), quotaDelayScale)\n\treturn delayScale\n}\n\nfunc (jt journalTracker) updateFree(\n\tfreeBytes, otherUsedBytes, freeFiles int64) {\n\t// We calculate the total free bytes by adding up the reported\n\t// free bytes, the journal used bytes, *and* any other used\n\t// bytes (e.g., the disk cache). For now, we hack this by\n\t// lumping the other used bytes with the reported free bytes.\n\t//\n\t// TODO: Keep track of other used bytes separately.\n\tjt.byte.updateFree(freeBytes + otherUsedBytes)\n\tjt.file.updateFree(freeFiles)\n}\n\nfunc (jt journalTracker) updateRemote(remoteUsedBytes, quotaBytes int64) {\n\tjt.quota.updateRemote(remoteUsedBytes, quotaBytes)\n}\n\nfunc (jt journalTracker) getSemaphoreCounts() (byteCount, fileCount int64) {\n\treturn jt.byte.semaphore.Count(), jt.file.semaphore.Count()\n}\n\nfunc (jt journalTracker) beforeBlockPut(\n\tctx context.Context, blockBytes, blockFiles int64) (\n\tavailableBytes, availableFiles int64, err error) {\n\tavailableBytes, err = jt.byte.beforeBlockPut(ctx, blockBytes)\n\tif err != nil {\n\t\treturn availableBytes, jt.file.semaphore.Count(), err\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tjt.byte.afterBlockPut(blockBytes, false)\n\t\t\tavailableBytes = jt.byte.semaphore.Count()\n\t\t}\n\t}()\n\n\tavailableFiles, err = jt.file.beforeBlockPut(ctx, blockFiles)\n\tif err != nil {\n\t\treturn availableBytes, availableFiles, err\n\t}\n\n\treturn availableBytes, availableFiles, nil\n}\n\nfunc (jt journalTracker) afterBlockPut(\n\tblockBytes, blockFiles int64, putData bool) {\n\tjt.byte.afterBlockPut(blockBytes, putData)\n\tjt.file.afterBlockPut(blockFiles, putData)\n\tjt.quota.afterBlockPut(blockBytes, putData)\n}\n\nfunc (jt journalTracker) onBlocksFlush(blockBytes int64) {\n\tjt.quota.onBlocksFlush(blockBytes)\n}\n\nfunc (jt journalTracker) onBlocksDelete(blockBytes, blockFiles int64) {\n\tjt.byte.onBlocksDelete(blockBytes)\n\tjt.file.onBlocksDelete(blockFiles)\n}\n\nfunc (jt journalTracker) getUsedBytes() int64 {\n\treturn jt.byte.used\n}\n\nfunc (jt journalTracker) getStatusLine() string {\n\treturn fmt.Sprintf(\"journalBytes=%d, freeBytes=%d, \"+\n\t\t\"journalFiles=%d, freeFiles=%d, \"+\n\t\t\"quotaUnflushedBytes=%d, quotaRemoteUsedBytes=%d, \"+\n\t\t\"quotaBytes=%d\",\n\t\tjt.byte.used, jt.byte.free,\n\t\tjt.file.used, jt.file.free,\n\t\tjt.quota.unflushedBytes, jt.quota.remoteUsedBytes,\n\t\tjt.quota.quotaBytes)\n}\n\nfunc (jt journalTracker) getQuotaInfo() (usedQuotaBytes, quotaBytes int64) {\n\treturn jt.quota.getQuotaInfo()\n}\n\nfunc (jt journalTracker) getDiskLimitInfo() (\n\tusedBytes, limitBytes, usedFiles, limitFiles int64) {\n\tusedBytes, limitBytes = jt.byte.getLimitInfo()\n\tusedFiles, limitFiles = jt.file.getLimitInfo()\n\treturn usedBytes, limitBytes, usedFiles, limitFiles\n}\n\ntype journalTrackerStatus struct {\n\tByteStatus  backpressureTrackerStatus\n\tFileStatus  backpressureTrackerStatus\n\tQuotaStatus quotaBackpressureTrackerStatus\n}\n\nfunc (jt journalTracker) getStatus() journalTrackerStatus {\n\treturn journalTrackerStatus{\n\t\tByteStatus:  jt.byte.getStatus(),\n\t\tFileStatus:  jt.file.getStatus(),\n\t\tQuotaStatus: jt.quota.getStatus(),\n\t}\n}\n\n// backpressureDiskLimiter is an implementation of diskLimiter that\n// uses backpressure to slow down block puts before they hit the disk\n// limits.\ntype backpressureDiskLimiter struct {\n\tlog logger.Logger\n\n\tmaxDelay            time.Duration\n\tdelayFn             func(context.Context, time.Duration) error\n\tfreeBytesAndFilesFn func() (int64, int64, error)\n\tquotaFn             func(ctx context.Context) (int64, int64)\n\n\t// lock protects everything in journalTracker and\n\t// diskCacheByteTracker, including the (implicit) maximum\n\t// values of the semaphores, but not the actual semaphores\n\t// themselves.\n\tlock                 sync.RWMutex\n\tjournalTracker       journalTracker\n\tdiskCacheByteTracker *backpressureTracker\n}\n\nvar _ DiskLimiter = (*backpressureDiskLimiter)(nil)\n\ntype backpressureDiskLimiterParams struct {\n\t// minThreshold is the fraction of the free bytes/files at\n\t// which we start to apply backpressure.\n\tminThreshold float64\n\t// maxThreshold is the fraction of the free bytes/files at\n\t// which we max out on backpressure.\n\tmaxThreshold float64\n\t// quotaMinThreshold is the fraction of used quota at which we\n\t// start to apply backpressure.\n\tquotaMinThreshold float64\n\t// quotaMaxThreshold is the fraction of used quota at which we\n\t// max out on backpressure.\n\tquotaMaxThreshold float64\n\t// journalFrac is fraction of the free bytes/files that the\n\t// journal is allowed to use.\n\tjournalFrac float64\n\t// diskCacheFrac is the fraction of the free bytes that the\n\t// disk cache is allowed to use. The disk cache doesn't store\n\t// individual files.\n\tdiskCacheFrac float64\n\t// byteLimit is the total cap for free bytes. The journal will\n\t// be allowed to use at most journalFrac*byteLimit, and the\n\t// disk cache will be allowed to use at most\n\t// diskCacheFrac*byteLimit.\n\tbyteLimit int64\n\t// maxFreeFiles is the cap for free files. The journal will be\n\t// allowed to use at most journalFrac*fileLimit. This limit\n\t// doesn't apply to the disk cache, since it doesn't store\n\t// individual files.\n\tfileLimit int64\n\t// maxDelay is the maximum delay used for backpressure.\n\tmaxDelay time.Duration\n\t// delayFn is a function that takes a context and a duration\n\t// and returns after sleeping for that duration, or if the\n\t// context is cancelled. Overridable for testing.\n\tdelayFn func(context.Context, time.Duration) error\n\t// freeBytesAndFilesFn is a function that returns the current\n\t// free bytes and files on the disk containing the\n\t// journal/disk cache directory. Overridable for testing.\n\tfreeBytesAndFilesFn func() (int64, int64, error)\n\t// quotaFn is a function that returns the current used and\n\t// total quota bytes. Overridable for testing.\n\tquotaFn func(context.Context) (int64, int64)\n}\n\n// defaultDiskLimitMaxDelay is the maximum amount to delay a block\n// put. Exposed as a constant as it is used by\n// tlfJournalConfigAdapter.\nconst defaultDiskLimitMaxDelay = 10 * time.Second\n\nfunc makeDefaultBackpressureDiskLimiterParams(\n\tstorageRoot string,\n\tquotaUsage *EventuallyConsistentQuotaUsage) backpressureDiskLimiterParams {\n\treturn backpressureDiskLimiterParams{\n\t\t// Start backpressure when 50% of free bytes or files\n\t\t// are used...\n\t\tminThreshold: 0.5,\n\t\t// ...and max it out at 95% (slightly less than 100%\n\t\t// to allow for inaccuracies in estimates).\n\t\tmaxThreshold: 0.95,\n\t\t// Start backpressure when we've used 100% of our quota...\n\t\tquotaMinThreshold: 1.0,\n\t\t// ...and max it out at 120% of quota.\n\t\tquotaMaxThreshold: 1.2,\n\t\t// Cap journal usage to 85% of free bytes and files...\n\t\tjournalFrac: 0.85,\n\t\t// ...and cap disk cache usage to 10% of free\n\t\t// bytes. The disk cache doesn't store individual\n\t\t// files.\n\t\tdiskCacheFrac: 0.10,\n\t\t// Set the byte limit to 200 GiB, which translates to\n\t\t// having the journal take up at most 30 GiB, and the\n\t\t// disk cache to take up at most 20 GiB.\n\t\tbyteLimit: 200 * 1024 * 1024 * 1024,\n\t\t// Set the file limit to 6 million files, which\n\t\t// translates to having the journal take up at most\n\t\t// 900k files.\n\t\tfileLimit: 6000000,\n\t\tmaxDelay:  defaultDiskLimitMaxDelay,\n\t\tdelayFn:   defaultDoDelay,\n\t\tfreeBytesAndFilesFn: func() (int64, int64, error) {\n\t\t\treturn defaultGetFreeBytesAndFiles(storageRoot)\n\t\t},\n\t\tquotaFn: func(ctx context.Context) (int64, int64) {\n\t\t\ttimestamp, usageBytes, limitBytes, err :=\n\t\t\t\tquotaUsage.Get(ctx, 1*time.Minute, math.MaxInt64)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, math.MaxInt64\n\t\t\t}\n\n\t\t\tif timestamp.IsZero() {\n\t\t\t\treturn 0, math.MaxInt64\n\t\t\t}\n\n\t\t\treturn usageBytes, limitBytes\n\t\t},\n\t}\n}\n\n// newBackpressureDiskLimiter constructs a new backpressureDiskLimiter\n// with the given params.\nfunc newBackpressureDiskLimiter(\n\tlog logger.Logger, params backpressureDiskLimiterParams) (\n\t*backpressureDiskLimiter, error) {\n\tfreeBytes, freeFiles, err := params.freeBytesAndFilesFn()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tjournalTracker, err := newJournalTracker(\n\t\tparams.minThreshold, params.maxThreshold,\n\t\tparams.quotaMinThreshold, params.quotaMaxThreshold,\n\t\tparams.journalFrac, params.byteLimit, params.fileLimit,\n\t\tfreeBytes, freeFiles)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// byteLimit must be scaled by the proportion of the limit\n\t// that the disk journal should consume. Add 0.5 to round up.\n\tdiskCacheByteLimit := int64((float64(params.byteLimit) * params.diskCacheFrac) + 0.5)\n\tdiskCacheByteTracker, err := newBackpressureTracker(\n\t\t1.0, 1.0, params.diskCacheFrac, diskCacheByteLimit, freeBytes)\n\n\tbdl := &backpressureDiskLimiter{\n\t\tlog, params.maxDelay, params.delayFn,\n\t\tparams.freeBytesAndFilesFn, params.quotaFn, sync.RWMutex{},\n\t\tjournalTracker, diskCacheByteTracker,\n\t}\n\treturn bdl, nil\n}\n\n// defaultDoDelay uses a timer to delay by the given duration.\nfunc defaultDoDelay(ctx context.Context, delay time.Duration) error {\n\tif delay == 0 {\n\t\treturn nil\n\t}\n\n\ttimer := time.NewTimer(delay)\n\tselect {\n\tcase <-timer.C:\n\t\treturn nil\n\tcase <-ctx.Done():\n\t\ttimer.Stop()\n\t\treturn errors.WithStack(ctx.Err())\n\t}\n}\n\nfunc defaultGetFreeBytesAndFiles(path string) (int64, int64, error) {\n\t// getDiskLimits returns availableBytes and availableFiles,\n\t// but we want to avoid confusing that with availBytes and\n\t// availFiles in the sense of the semaphore value.\n\tfreeBytes, freeFiles, err := getDiskLimits(path)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\tif freeBytes > uint64(math.MaxInt64) {\n\t\tfreeBytes = math.MaxInt64\n\t}\n\tif freeFiles > uint64(math.MaxInt64) {\n\t\tfreeFiles = math.MaxInt64\n\t}\n\treturn int64(freeBytes), int64(freeFiles), nil\n}\n\nfunc (bdl *backpressureDiskLimiter) getJournalSnapshotsForTest() (\n\tbyteSnapshot, fileSnapshot, quotaSnapshot jtSnapshot) {\n\tbdl.lock.RLock()\n\tdefer bdl.lock.RUnlock()\n\treturn bdl.journalTracker.getSnapshotsForTest()\n}\n\nfunc (bdl *backpressureDiskLimiter) onJournalEnable(\n\tctx context.Context,\n\tjournalStoredBytes, journalUnflushedBytes, journalFiles int64) (\n\tavailableBytes, availableFiles int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\treturn bdl.journalTracker.onEnable(\n\t\tjournalStoredBytes, journalUnflushedBytes, journalFiles)\n}\n\nfunc (bdl *backpressureDiskLimiter) onJournalDisable(\n\tctx context.Context,\n\tjournalStoredBytes, journalUnflushedBytes, journalFiles int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.journalTracker.onDisable(\n\t\tjournalStoredBytes, journalUnflushedBytes, journalFiles)\n}\n\nfunc (bdl *backpressureDiskLimiter) onDiskBlockCacheEnable(ctx context.Context,\n\tdiskCacheBytes int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.diskCacheByteTracker.onEnable(diskCacheBytes)\n}\n\nfunc (bdl *backpressureDiskLimiter) onDiskBlockCacheDisable(ctx context.Context,\n\tdiskCacheBytes int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.diskCacheByteTracker.onDisable(diskCacheBytes)\n}\n\nfunc (bdl *backpressureDiskLimiter) getDelayLocked(\n\tctx context.Context, now time.Time) time.Duration {\n\tdelayScale := bdl.journalTracker.getDelayScale()\n\n\t// Set maxDelay to min(bdl.maxDelay, time until deadline - 1s).\n\tmaxDelay := bdl.maxDelay\n\tif deadline, ok := ctx.Deadline(); ok {\n\t\t// Subtract a second to allow for some slack.\n\t\tremainingTime := deadline.Sub(now) - time.Second\n\t\tif remainingTime < maxDelay {\n\t\t\tmaxDelay = remainingTime\n\t\t}\n\t}\n\n\treturn time.Duration(delayScale * float64(maxDelay))\n}\n\nfunc (bdl *backpressureDiskLimiter) onBeforeBlockPutError(err error) (\n\tavailableBytes, availableFiles int64, _ error) {\n\tavailableBytes, availableFiles =\n\t\tbdl.journalTracker.getSemaphoreCounts()\n\treturn availableBytes, availableFiles, err\n}\n\nfunc (bdl *backpressureDiskLimiter) beforeBlockPut(\n\tctx context.Context, blockBytes, blockFiles int64) (\n\tavailableBytes, availableFiles int64, err error) {\n\tif blockBytes == 0 {\n\t\t// Better to return an error than to panic in Acquire.\n\t\treturn bdl.onBeforeBlockPutError(errors.New(\n\t\t\t\"backpressureDiskLimiter.beforeBlockPut called with 0 blockBytes\"))\n\t}\n\tif blockFiles == 0 {\n\t\t// Better to return an error than to panic in Acquire.\n\t\treturn bdl.onBeforeBlockPutError(errors.New(\n\t\t\t\"backpressureDiskLimiter.beforeBlockPut called with 0 blockFiles\"))\n\t}\n\n\tdelay, err := func() (time.Duration, error) {\n\t\tbdl.lock.Lock()\n\t\tdefer bdl.lock.Unlock()\n\n\t\t// Call this under lock to avoid problems with its\n\t\t// return values going stale while blocking on\n\t\t// bdl.lock.\n\t\tfreeBytes, freeFiles, err := bdl.freeBytesAndFilesFn()\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\n\t\tbdl.journalTracker.updateFree(\n\t\t\tfreeBytes, bdl.diskCacheByteTracker.used, freeFiles)\n\n\t\tremoteUsedBytes, quotaBytes := bdl.quotaFn(ctx)\n\t\tbdl.journalTracker.updateRemote(remoteUsedBytes, quotaBytes)\n\n\t\tdelay := bdl.getDelayLocked(ctx, time.Now())\n\t\tif delay > 0 {\n\t\t\tbdl.log.CDebugf(ctx, \"Delaying block put of %d bytes and %d files by %f s (%s)\",\n\t\t\t\tblockBytes, blockFiles, delay.Seconds(),\n\t\t\t\tbdl.journalTracker.getStatusLine())\n\t\t}\n\n\t\treturn delay, nil\n\t}()\n\tif err != nil {\n\t\treturn bdl.onBeforeBlockPutError(err)\n\t}\n\n\t// TODO: Update delay if any variables change (i.e., we\n\t// suddenly free up a lot of space).\n\terr = bdl.delayFn(ctx, delay)\n\tif err != nil {\n\t\treturn bdl.onBeforeBlockPutError(err)\n\t}\n\n\treturn bdl.journalTracker.beforeBlockPut(ctx, blockBytes, blockFiles)\n}\n\nfunc (bdl *backpressureDiskLimiter) afterBlockPut(\n\tctx context.Context, blockBytes, blockFiles int64, putData bool) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.journalTracker.afterBlockPut(blockBytes, blockFiles, putData)\n}\n\nfunc (bdl *backpressureDiskLimiter) onBlocksFlush(\n\tctx context.Context, blockBytes int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.journalTracker.onBlocksFlush(blockBytes)\n}\n\nfunc (bdl *backpressureDiskLimiter) onBlocksDelete(\n\tctx context.Context, blockBytes, blockFiles int64) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.journalTracker.onBlocksDelete(blockBytes, blockFiles)\n}\n\nfunc (bdl *backpressureDiskLimiter) onDiskBlockCacheDelete(\n\tctx context.Context, blockBytes int64) {\n\tif blockBytes == 0 {\n\t\treturn\n\t}\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.diskCacheByteTracker.onBlocksDelete(blockBytes)\n}\n\nfunc (bdl *backpressureDiskLimiter) beforeDiskBlockCachePut(\n\tctx context.Context, blockBytes int64) (\n\tavailableBytes int64, err error) {\n\tif blockBytes == 0 {\n\t\t// Better to return an error than to panic in ForceAcquire.\n\t\treturn 0, errors.New(\"backpressureDiskLimiter.beforeDiskBlockCachePut\" +\n\t\t\t\" called with 0 blockBytes\")\n\t}\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\n\t// Call this under lock to avoid problems with its return\n\t// values going stale while blocking on bdl.lock.\n\tfreeBytes, _, err := bdl.freeBytesAndFilesFn()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\t// We calculate the total free bytes by adding up the reported\n\t// free bytes, the disk cache used bytes, *and* any other used\n\t// bytes (e.g., the journal cache). For now, we hack this by\n\t// lumping the other used bytes with the reported free bytes.\n\t//\n\t// TODO: Keep track of other used bytes separately.\n\tbdl.diskCacheByteTracker.updateFree(\n\t\tfreeBytes + bdl.journalTracker.getUsedBytes())\n\n\treturn bdl.diskCacheByteTracker.beforeDiskBlockCachePut(blockBytes), nil\n}\n\nfunc (bdl *backpressureDiskLimiter) afterDiskBlockCachePut(\n\tctx context.Context, blockBytes int64, putData bool) {\n\tbdl.lock.Lock()\n\tdefer bdl.lock.Unlock()\n\tbdl.diskCacheByteTracker.afterBlockPut(blockBytes, putData)\n}\n\nfunc (bdl *backpressureDiskLimiter) getQuotaInfo() (\n\tusedQuotaBytes, quotaBytes int64) {\n\tbdl.lock.RLock()\n\tdefer bdl.lock.RUnlock()\n\treturn bdl.journalTracker.getQuotaInfo()\n}\n\nfunc (bdl *backpressureDiskLimiter) getDiskLimitInfo() (\n\tusedBytes, limitBytes, usedFiles, limitFiles int64) {\n\tbdl.lock.RLock()\n\tdefer bdl.lock.RUnlock()\n\treturn bdl.journalTracker.getDiskLimitInfo()\n}\n\ntype backpressureDiskLimiterStatus struct {\n\tType string\n\n\t// Derived stats.\n\tCurrentDelaySec float64\n\n\tJournalTrackerStatus journalTrackerStatus\n\tDiskCacheByteStatus  backpressureTrackerStatus\n}\n\nfunc (bdl *backpressureDiskLimiter) getStatus() interface{} {\n\tbdl.lock.RLock()\n\tdefer bdl.lock.RUnlock()\n\n\tcurrentDelay := bdl.getDelayLocked(context.Background(), time.Now())\n\n\treturn backpressureDiskLimiterStatus{\n\t\tType: \"BackpressureDiskLimiter\",\n\n\t\tCurrentDelaySec: currentDelay.Seconds(),\n\n\t\tJournalTrackerStatus: bdl.journalTracker.getStatus(),\n\t\tDiskCacheByteStatus:  bdl.diskCacheByteTracker.getStatus(),\n\t}\n}\n", "idx": 2, "id": 16812, "msg": "", "proj": "keybase-kbfs", "lang": "go"}
{"patch": "@@ -1266,7 +1266,7 @@ const instr_info_t * const op_instr[] =\n     /* OP_vpbroadcastd  */   &third_byte_38[118],\n     /* OP_vpbroadcastq  */   &third_byte_38[119],\n \n-    /* added in Intel Skylake */\n+    /* Added in Intel Skylake */\n     /* OP_xsavec32      */   &rex_w_extensions[5][0],\n     /* OP_xsavec64      */   &rex_w_extensions[5][1],\n ", "y": 1, "oldf": "/* **********************************************************\n * Copyright (c) 2011-2019 Google, Inc.  All rights reserved.\n * Copyright (c) 2001-2010 VMware, Inc.  All rights reserved.\n * **********************************************************/\n\n/*\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * * Redistributions of source code must retain the above copyright notice,\n *   this list of conditions and the following disclaimer.\n *\n * * Redistributions in binary form must reproduce the above copyright notice,\n *   this list of conditions and the following disclaimer in the documentation\n *   and/or other materials provided with the distribution.\n *\n * * Neither the name of VMware, Inc. nor the names of its contributors may be\n *   used to endorse or promote products derived from this software without\n *   specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n * ARE DISCLAIMED. IN NO EVENT SHALL VMWARE, INC. OR CONTRIBUTORS BE LIABLE\n * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH\n * DAMAGE.\n */\n\n/* Copyright (c) 2003-2007 Determina Corp. */\n/* Copyright (c) 2001-2003 Massachusetts Institute of Technology */\n/* Copyright (c) 2001 Hewlett-Packard Company */\n\n/* decode_table.c -- tables for decoding x86 instructions\n */\n\n#include \"../globals.h\" /* need this to include decode.h (uint, etc.) */\n#include \"arch.h\"       /* need this to include decode.h (byte, etc. */\n#include \"instr.h\"      /* for REG_ constants */\n#include \"decode.h\"\n#include \"decode_private.h\"\n\n/****************************************************************************\n * All code below based on tables in the ``Intel Architecture Software\n * Developer's Manual,'' Volume 2: Instruction Set Reference, 2001.\n * Updated with information from later Intel manuals and AMD manuals.\n *\n * I added many new types not present in the Intel tables: see decode.h\n *\n * I don't list %eflags as a source or dest operand, but the particular\n * flags written are encoded.\n *\n * XXX: some day it may be worth adding flags indicating which instrs\n * are valid on which models of which processors (probably best to just add\n * which cpuid flag must be set for the instr to be supported): for\n * now though we do not rely on being able to predict which instrs are\n * invalid.\n */\n\n// We skip auto-formatting for the entire file to keep our aligned op_instr\n// entries and our single-line table entries:\n/* clang-format off */\n\n/****************************************************************************\n * Operand pointers into tables\n * When there are multiple encodings of an opcode, this points to the first\n * entry in a linked list.\n * This array corresponds with the enum in opcode.h\n * IF YOU CHANGE ONE YOU MUST CHANGE THE OTHER\n */\nconst instr_info_t * const op_instr[] =\n{\n    /* OP_INVALID */   NULL,\n    /* OP_UNDECODED */ NULL,\n    /* OP_CONTD   */   NULL,\n    /* OP_LABEL   */   NULL,\n\n    /* OP_add     */   &first_byte[0x05],\n    /* OP_or      */   &first_byte[0x0d],\n    /* OP_adc     */   &first_byte[0x15],\n    /* OP_sbb     */   &first_byte[0x1d],\n    /* OP_and     */   &first_byte[0x25],\n    /* OP_daa     */   &first_byte[0x27],\n    /* OP_sub     */   &first_byte[0x2d],\n    /* OP_das     */   &first_byte[0x2f],\n    /* OP_xor     */   &first_byte[0x35],\n    /* OP_aaa     */   &first_byte[0x37],\n    /* OP_cmp     */   &first_byte[0x3d],\n    /* OP_aas     */   &first_byte[0x3f],\n    /* OP_inc     */   &x64_extensions[0][0],\n    /* OP_dec     */   &x64_extensions[8][0],\n    /* OP_push    */   &first_byte[0x50],\n    /* OP_push_imm*/   &first_byte[0x68],\n    /* OP_pop     */   &first_byte[0x58],\n    /* OP_pusha   */   &first_byte[0x60],\n    /* OP_popa    */   &first_byte[0x61],\n    /* OP_bound   */   &evex_prefix_extensions[0][0],\n    /* OP_arpl    */   &x64_extensions[16][0],\n    /* OP_imul    */   &base_extensions[10][5],\n\n    /* OP_jo_short    */   &first_byte[0x70],\n    /* OP_jno_short   */   &first_byte[0x71],\n    /* OP_jb_short    */   &first_byte[0x72],\n    /* OP_jnb_short   */   &first_byte[0x73],\n    /* OP_jz_short    */   &first_byte[0x74],\n    /* OP_jnz_short   */   &first_byte[0x75],\n    /* OP_jbe_short   */   &first_byte[0x76],\n    /* OP_jnbe_short  */   &first_byte[0x77],\n    /* OP_js_short    */   &first_byte[0x78],\n    /* OP_jns_short   */   &first_byte[0x79],\n    /* OP_jp_short    */   &first_byte[0x7a],\n    /* OP_jnp_short   */   &first_byte[0x7b],\n    /* OP_jl_short    */   &first_byte[0x7c],\n    /* OP_jnl_short   */   &first_byte[0x7d],\n    /* OP_jle_short   */   &first_byte[0x7e],\n    /* OP_jnle_short  */   &first_byte[0x7f],\n\n    /* OP_call          */   &first_byte[0xe8],\n    /* OP_call_ind      */   &base_extensions[12][2],\n    /* OP_call_far      */   &first_byte[0x9a],\n    /* OP_call_far_ind  */   &base_extensions[12][3],\n    /* OP_jmp           */   &first_byte[0xe9],\n    /* OP_jmp_short     */   &first_byte[0xeb],\n    /* OP_jmp_ind       */   &base_extensions[12][4],\n    /* OP_jmp_far       */   &first_byte[0xea],\n    /* OP_jmp_far_ind   */   &base_extensions[12][5],\n\n    /* OP_loopne  */   &first_byte[0xe0],\n    /* OP_loope   */   &first_byte[0xe1],\n    /* OP_loop    */   &first_byte[0xe2],\n    /* OP_jecxz   */   &first_byte[0xe3],\n\n    /* point ld & st at eAX & al instrs, they save 1 byte (no modrm),\n     * hopefully time taken considering them doesn't offset that */\n    /* OP_mov_ld     */   &first_byte[0xa1],\n    /* OP_mov_st     */   &first_byte[0xa3],\n    /* PR 250397: store of immed is mov_st not mov_imm, even though can be immed->reg,\n     * which we address by sharing part of the mov_st template chain */\n    /* OP_mov_imm    */   &first_byte[0xb8],\n    /* OP_mov_seg    */   &first_byte[0x8e],\n    /* OP_mov_priv   */   &second_byte[0x20],\n\n    /* OP_test    */   &first_byte[0xa9],\n    /* OP_lea     */   &first_byte[0x8d],\n    /* OP_xchg    */   &first_byte[0x91],\n    /* OP_cwde    */   &first_byte[0x98],\n    /* OP_cdq     */   &first_byte[0x99],\n    /* OP_fwait   */   &first_byte[0x9b],\n    /* OP_pushf   */   &first_byte[0x9c],\n    /* OP_popf    */   &first_byte[0x9d],\n    /* OP_sahf    */   &first_byte[0x9e],\n    /* OP_lahf    */   &first_byte[0x9f],\n\n    /* OP_ret      */   &first_byte[0xc2],\n    /* OP_ret_far  */   &first_byte[0xca],\n\n    /* OP_les     */   &vex_prefix_extensions[0][0],\n    /* OP_lds     */   &vex_prefix_extensions[1][0],\n    /* OP_enter   */   &first_byte[0xc8],\n    /* OP_leave   */   &first_byte[0xc9],\n    /* OP_int3    */   &first_byte[0xcc],\n    /* OP_int     */   &first_byte[0xcd],\n    /* OP_into    */   &first_byte[0xce],\n    /* OP_iret    */   &first_byte[0xcf],\n    /* OP_aam     */   &first_byte[0xd4],\n    /* OP_aad     */   &first_byte[0xd5],\n    /* OP_xlat    */   &first_byte[0xd7],\n    /* OP_in      */   &first_byte[0xe5],\n    /* OP_out     */   &first_byte[0xe7],\n    /* OP_hlt     */   &first_byte[0xf4],\n    /* OP_cmc     */   &first_byte[0xf5],\n    /* OP_clc     */   &first_byte[0xf8],\n    /* OP_stc     */   &first_byte[0xf9],\n    /* OP_cli     */   &first_byte[0xfa],\n    /* OP_sti     */   &first_byte[0xfb],\n    /* OP_cld     */   &first_byte[0xfc],\n    /* OP_std     */   &first_byte[0xfd],\n\n\n    /* OP_lar         */   &second_byte[0x02],\n    /* OP_lsl         */   &second_byte[0x03],\n    /* OP_syscall     */   &second_byte[0x05],\n    /* OP_clts        */   &second_byte[0x06],\n    /* OP_sysret      */   &second_byte[0x07],\n    /* OP_invd        */   &second_byte[0x08],\n    /* OP_wbinvd      */   &second_byte[0x09],\n    /* OP_ud2a        */   &second_byte[0x0b],\n    /* OP_nop_modrm   */   &second_byte[0x1f],\n    /* OP_movntps     */   &prefix_extensions[11][0],\n    /* OP_movntpd     */   &prefix_extensions[11][2],\n    /* OP_wrmsr       */   &second_byte[0x30],\n    /* OP_rdtsc       */   &second_byte[0x31],\n    /* OP_rdmsr       */   &second_byte[0x32],\n    /* OP_rdpmc       */   &second_byte[0x33],\n    /* OP_sysenter    */   &second_byte[0x34],\n    /* OP_sysexit     */   &second_byte[0x35],\n\n    /* OP_cmovo       */   &second_byte[0x40],\n    /* OP_cmovno      */   &e_vex_extensions[83][0],\n    /* OP_cmovb       */   &e_vex_extensions[84][0],\n    /* OP_cmovnb      */   &second_byte[0x43],\n    /* OP_cmovz       */   &e_vex_extensions[86][0],\n    /* OP_cmovnz      */   &e_vex_extensions[87][0],\n    /* OP_cmovbe      */   &e_vex_extensions[88][0],\n    /* OP_cmovnbe     */   &e_vex_extensions[89][0],\n    /* OP_cmovs       */   &second_byte[0x48],\n    /* OP_cmovns      */   &second_byte[0x49],\n    /* OP_cmovp       */   &e_vex_extensions[90][0],\n    /* OP_cmovnp      */   &e_vex_extensions[85][0],\n    /* OP_cmovl       */   &second_byte[0x4c],\n    /* OP_cmovnl      */   &second_byte[0x4d],\n    /* OP_cmovle      */   &second_byte[0x4e],\n    /* OP_cmovnle     */   &second_byte[0x4f],\n\n    /* OP_punpcklbw   */   &prefix_extensions[32][0],\n    /* OP_punpcklwd   */   &prefix_extensions[33][0],\n    /* OP_punpckldq   */   &prefix_extensions[34][0],\n    /* OP_packsswb    */   &prefix_extensions[35][0],\n    /* OP_pcmpgtb     */   &prefix_extensions[36][0],\n    /* OP_pcmpgtw     */   &prefix_extensions[37][0],\n    /* OP_pcmpgtd     */   &prefix_extensions[38][0],\n    /* OP_packuswb    */   &prefix_extensions[39][0],\n    /* OP_punpckhbw   */   &prefix_extensions[40][0],\n    /* OP_punpckhwd   */   &prefix_extensions[41][0],\n    /* OP_punpckhdq   */   &prefix_extensions[42][0],\n    /* OP_packssdw    */   &prefix_extensions[43][0],\n    /* OP_punpcklqdq  */   &prefix_extensions[44][2],\n    /* OP_punpckhqdq  */   &prefix_extensions[45][2],\n    /* OP_movd        */   &prefix_extensions[46][0],\n    /* OP_movq        */   &prefix_extensions[112][0],\n    /* OP_movdqu      */   &prefix_extensions[112][1],\n    /* OP_movdqa      */   &prefix_extensions[112][2],\n    /* OP_pshufw      */   &prefix_extensions[47][0],\n    /* OP_pshufd      */   &prefix_extensions[47][2],\n    /* OP_pshufhw     */   &prefix_extensions[47][1],\n    /* OP_pshuflw     */   &prefix_extensions[47][3],\n    /* OP_pcmpeqb     */   &prefix_extensions[48][0],\n    /* OP_pcmpeqw     */   &prefix_extensions[49][0],\n    /* OP_pcmpeqd     */   &prefix_extensions[50][0],\n    /* OP_emms        */   &vex_L_extensions[0][0],\n\n    /* OP_jo      */   &second_byte[0x80],\n    /* OP_jno     */   &second_byte[0x81],\n    /* OP_jb      */   &second_byte[0x82],\n    /* OP_jnb     */   &second_byte[0x83],\n    /* OP_jz      */   &second_byte[0x84],\n    /* OP_jnz     */   &second_byte[0x85],\n    /* OP_jbe     */   &second_byte[0x86],\n    /* OP_jnbe    */   &second_byte[0x87],\n    /* OP_js      */   &second_byte[0x88],\n    /* OP_jns     */   &second_byte[0x89],\n    /* OP_jp      */   &second_byte[0x8a],\n    /* OP_jnp     */   &second_byte[0x8b],\n    /* OP_jl      */   &second_byte[0x8c],\n    /* OP_jnl     */   &second_byte[0x8d],\n    /* OP_jle     */   &second_byte[0x8e],\n    /* OP_jnle    */   &second_byte[0x8f],\n\n    /* OP_seto        */   &e_vex_extensions[79][0],\n    /* OP_setno       */   &e_vex_extensions[80][0],\n    /* OP_setb        */   &e_vex_extensions[81][0],\n    /* OP_setnb       */   &e_vex_extensions[82][0],\n    /* OP_setz        */   &second_byte[0x94],\n    /* OP_setnz       */   &second_byte[0x95],\n    /* OP_setbe       */   &second_byte[0x96],\n    /* OP_setnbe      */   &second_byte[0x97],\n    /* OP_sets        */   &e_vex_extensions[91][0],\n    /* OP_setns       */   &e_vex_extensions[92][0],\n    /* OP_setp        */   &second_byte[0x9a],\n    /* OP_setnp       */   &second_byte[0x9b],\n    /* OP_setl        */   &second_byte[0x9c],\n    /* OP_setnl       */   &second_byte[0x9d],\n    /* OP_setle       */   &second_byte[0x9e],\n    /* OP_setnle        */   &second_byte[0x9f],\n\n    /* OP_cpuid       */   &second_byte[0xa2],\n    /* OP_bt          */   &second_byte[0xa3],\n    /* OP_shld        */   &second_byte[0xa4],\n    /* OP_rsm         */   &second_byte[0xaa],\n    /* OP_bts         */   &second_byte[0xab],\n    /* OP_shrd        */   &second_byte[0xac],\n    /* OP_cmpxchg     */   &second_byte[0xb1],\n    /* OP_lss         */   &second_byte[0xb2],\n    /* OP_btr         */   &second_byte[0xb3],\n    /* OP_lfs         */   &second_byte[0xb4],\n    /* OP_lgs         */   &second_byte[0xb5],\n    /* OP_movzx       */   &second_byte[0xb7],\n    /* OP_ud2b        */   &second_byte[0xb9],\n    /* OP_btc         */   &second_byte[0xbb],\n    /* OP_bsf         */   &prefix_extensions[140][0],\n    /* OP_bsr         */   &prefix_extensions[136][0],\n    /* OP_movsx       */   &second_byte[0xbf],\n    /* OP_xadd        */   &second_byte[0xc1],\n    /* OP_movnti      */   &second_byte[0xc3],\n    /* OP_pinsrw      */   &prefix_extensions[53][0],\n    /* OP_pextrw      */   &prefix_extensions[54][0],\n    /* OP_bswap       */   &second_byte[0xc8],\n    /* OP_psrlw       */   &prefix_extensions[56][0],\n    /* OP_psrld       */   &prefix_extensions[57][0],\n    /* OP_psrlq       */   &prefix_extensions[58][0],\n    /* OP_paddq       */   &prefix_extensions[59][0],\n    /* OP_pmullw      */   &prefix_extensions[60][0],\n    /* OP_pmovmskb    */   &prefix_extensions[62][0],\n    /* OP_psubusb     */   &prefix_extensions[63][0],\n    /* OP_psubusw     */   &prefix_extensions[64][0],\n    /* OP_pminub      */   &prefix_extensions[65][0],\n    /* OP_pand        */   &prefix_extensions[66][0],\n    /* OP_paddusb     */   &prefix_extensions[67][0],\n    /* OP_paddusw     */   &prefix_extensions[68][0],\n    /* OP_pmaxub      */   &prefix_extensions[69][0],\n    /* OP_pandn       */   &prefix_extensions[70][0],\n    /* OP_pavgb       */   &prefix_extensions[71][0],\n    /* OP_psraw       */   &prefix_extensions[72][0],\n    /* OP_psrad       */   &prefix_extensions[73][0],\n    /* OP_pavgw       */   &prefix_extensions[74][0],\n    /* OP_pmulhuw     */   &prefix_extensions[75][0],\n    /* OP_pmulhw      */   &prefix_extensions[76][0],\n    /* OP_movntq      */   &prefix_extensions[78][0],\n    /* OP_movntdq     */   &prefix_extensions[78][2],\n    /* OP_psubsb      */   &prefix_extensions[79][0],\n    /* OP_psubsw      */   &prefix_extensions[80][0],\n    /* OP_pminsw      */   &prefix_extensions[81][0],\n    /* OP_por         */   &prefix_extensions[82][0],\n    /* OP_paddsb      */   &prefix_extensions[83][0],\n    /* OP_paddsw      */   &prefix_extensions[84][0],\n    /* OP_pmaxsw      */   &prefix_extensions[85][0],\n    /* OP_pxor        */   &prefix_extensions[86][0],\n    /* OP_psllw       */   &prefix_extensions[87][0],\n    /* OP_pslld       */   &prefix_extensions[88][0],\n    /* OP_psllq       */   &prefix_extensions[89][0],\n    /* OP_pmuludq     */   &prefix_extensions[90][0],\n    /* OP_pmaddwd     */   &prefix_extensions[91][0],\n    /* OP_psadbw      */   &prefix_extensions[92][0],\n    /* OP_maskmovq    */   &prefix_extensions[93][0],\n    /* OP_maskmovdqu  */   &prefix_extensions[93][2],\n    /* OP_psubb       */   &prefix_extensions[94][0],\n    /* OP_psubw       */   &prefix_extensions[95][0],\n    /* OP_psubd       */   &prefix_extensions[96][0],\n    /* OP_psubq       */   &prefix_extensions[97][0],\n    /* OP_paddb       */   &prefix_extensions[98][0],\n    /* OP_paddw       */   &prefix_extensions[99][0],\n    /* OP_paddd       */   &prefix_extensions[100][0],\n    /* OP_psrldq      */   &prefix_extensions[101][2],\n    /* OP_pslldq      */   &prefix_extensions[102][2],\n\n\n    /* OP_rol          */   &base_extensions[ 4][0],\n    /* OP_ror          */   &base_extensions[ 4][1],\n    /* OP_rcl          */   &base_extensions[ 4][2],\n    /* OP_rcr          */   &base_extensions[ 4][3],\n    /* OP_shl          */   &base_extensions[ 4][4],\n    /* OP_shr          */   &base_extensions[ 4][5],\n    /* OP_sar          */   &base_extensions[ 4][7],\n    /* OP_not          */   &base_extensions[10][2],\n    /* OP_neg          */   &base_extensions[10][3],\n    /* OP_mul          */   &base_extensions[10][4],\n    /* OP_div          */   &base_extensions[10][6],\n    /* OP_idiv         */   &base_extensions[10][7],\n    /* OP_sldt         */   &base_extensions[13][0],\n    /* OP_str          */   &base_extensions[13][1],\n    /* OP_lldt         */   &base_extensions[13][2],\n    /* OP_ltr          */   &base_extensions[13][3],\n    /* OP_verr         */   &base_extensions[13][4],\n    /* OP_verw         */   &base_extensions[13][5],\n    /* OP_sgdt         */   &mod_extensions[0][0],\n    /* OP_sidt         */   &mod_extensions[1][0],\n    /* OP_lgdt         */   &mod_extensions[5][0],\n    /* OP_lidt         */   &mod_extensions[4][0],\n    /* OP_smsw         */   &base_extensions[14][4],\n    /* OP_lmsw         */   &base_extensions[14][6],\n    /* OP_invlpg       */   &mod_extensions[2][0],\n    /* OP_cmpxchg8b    */   &base_extensions[16][1],\n    /* OP_fxsave32     */   &rex_w_extensions[0][0],\n    /* OP_fxrstor32    */   &rex_w_extensions[1][0],\n    /* OP_ldmxcsr      */   &e_vex_extensions[61][0],\n    /* OP_stmxcsr      */   &e_vex_extensions[62][0],\n    /* OP_lfence       */   &mod_extensions[6][1],\n    /* OP_mfence       */   &mod_extensions[7][1],\n    /* OP_clflush      */   &mod_extensions[3][0],\n    /* OP_sfence       */   &mod_extensions[3][1],\n    /* OP_prefetchnta  */   &base_extensions[23][0],\n    /* OP_prefetcht0   */   &base_extensions[23][1],\n    /* OP_prefetcht1   */   &base_extensions[23][2],\n    /* OP_prefetcht2   */   &base_extensions[23][3],\n    /* OP_prefetch     */   &base_extensions[24][0],\n    /* OP_prefetchw    */   &base_extensions[24][1],\n\n\n    /* OP_movups     */   &prefix_extensions[ 0][0],\n    /* OP_movss      */   &mod_extensions[18][0],\n    /* OP_movupd     */   &prefix_extensions[ 0][2],\n    /* OP_movsd      */   &mod_extensions[19][0],\n    /* OP_movlps     */   &prefix_extensions[ 2][0],\n    /* OP_movlpd     */   &prefix_extensions[ 2][2],\n    /* OP_unpcklps   */   &prefix_extensions[ 4][0],\n    /* OP_unpcklpd   */   &prefix_extensions[ 4][2],\n    /* OP_unpckhps   */   &prefix_extensions[ 5][0],\n    /* OP_unpckhpd   */   &prefix_extensions[ 5][2],\n    /* OP_movhps     */   &prefix_extensions[ 6][0],\n    /* OP_movhpd     */   &prefix_extensions[ 6][2],\n    /* OP_movaps     */   &prefix_extensions[ 8][0],\n    /* OP_movapd     */   &prefix_extensions[ 8][2],\n    /* OP_cvtpi2ps   */   &prefix_extensions[10][0],\n    /* OP_cvtsi2ss   */   &prefix_extensions[10][1],\n    /* OP_cvtpi2pd   */   &prefix_extensions[10][2],\n    /* OP_cvtsi2sd   */   &prefix_extensions[10][3],\n    /* OP_cvttps2pi  */   &prefix_extensions[12][0],\n    /* OP_cvttss2si  */   &prefix_extensions[12][1],\n    /* OP_cvttpd2pi  */   &prefix_extensions[12][2],\n    /* OP_cvttsd2si  */   &prefix_extensions[12][3],\n    /* OP_cvtps2pi   */   &prefix_extensions[13][0],\n    /* OP_cvtss2si   */   &prefix_extensions[13][1],\n    /* OP_cvtpd2pi   */   &prefix_extensions[13][2],\n    /* OP_cvtsd2si   */   &prefix_extensions[13][3],\n    /* OP_ucomiss    */   &prefix_extensions[14][0],\n    /* OP_ucomisd    */   &prefix_extensions[14][2],\n    /* OP_comiss     */   &prefix_extensions[15][0],\n    /* OP_comisd     */   &prefix_extensions[15][2],\n    /* OP_movmskps   */   &prefix_extensions[16][0],\n    /* OP_movmskpd   */   &prefix_extensions[16][2],\n    /* OP_sqrtps     */   &prefix_extensions[17][0],\n    /* OP_sqrtss     */   &prefix_extensions[17][1],\n    /* OP_sqrtpd     */   &prefix_extensions[17][2],\n    /* OP_sqrtsd     */   &prefix_extensions[17][3],\n    /* OP_rsqrtps    */   &prefix_extensions[18][0],\n    /* OP_rsqrtss    */   &prefix_extensions[18][1],\n    /* OP_rcpps      */   &prefix_extensions[19][0],\n    /* OP_rcpss      */   &prefix_extensions[19][1],\n    /* OP_andps      */   &prefix_extensions[20][0],\n    /* OP_andpd      */   &prefix_extensions[20][2],\n    /* OP_andnps     */   &prefix_extensions[21][0],\n    /* OP_andnpd     */   &prefix_extensions[21][2],\n    /* OP_orps       */   &prefix_extensions[22][0],\n    /* OP_orpd       */   &prefix_extensions[22][2],\n    /* OP_xorps      */   &prefix_extensions[23][0],\n    /* OP_xorpd      */   &prefix_extensions[23][2],\n    /* OP_addps      */   &prefix_extensions[24][0],\n    /* OP_addss      */   &prefix_extensions[24][1],\n    /* OP_addpd      */   &prefix_extensions[24][2],\n    /* OP_addsd      */   &prefix_extensions[24][3],\n    /* OP_mulps      */   &prefix_extensions[25][0],\n    /* OP_mulss      */   &prefix_extensions[25][1],\n    /* OP_mulpd      */   &prefix_extensions[25][2],\n    /* OP_mulsd      */   &prefix_extensions[25][3],\n    /* OP_cvtps2pd   */   &prefix_extensions[26][0],\n    /* OP_cvtss2sd   */   &prefix_extensions[26][1],\n    /* OP_cvtpd2ps   */   &prefix_extensions[26][2],\n    /* OP_cvtsd2ss   */   &prefix_extensions[26][3],\n    /* OP_cvtdq2ps   */   &prefix_extensions[27][0],\n    /* OP_cvttps2dq  */   &prefix_extensions[27][1],\n    /* OP_cvtps2dq   */   &prefix_extensions[27][2],\n    /* OP_subps      */   &prefix_extensions[28][0],\n    /* OP_subss      */   &prefix_extensions[28][1],\n    /* OP_subpd      */   &prefix_extensions[28][2],\n    /* OP_subsd      */   &prefix_extensions[28][3],\n    /* OP_minps      */   &prefix_extensions[29][0],\n    /* OP_minss      */   &prefix_extensions[29][1],\n    /* OP_minpd      */   &prefix_extensions[29][2],\n    /* OP_minsd      */   &prefix_extensions[29][3],\n    /* OP_divps      */   &prefix_extensions[30][0],\n    /* OP_divss      */   &prefix_extensions[30][1],\n    /* OP_divpd      */   &prefix_extensions[30][2],\n    /* OP_divsd      */   &prefix_extensions[30][3],\n    /* OP_maxps      */   &prefix_extensions[31][0],\n    /* OP_maxss      */   &prefix_extensions[31][1],\n    /* OP_maxpd      */   &prefix_extensions[31][2],\n    /* OP_maxsd      */   &prefix_extensions[31][3],\n    /* OP_cmpps      */   &prefix_extensions[52][0],\n    /* OP_cmpss      */   &prefix_extensions[52][1],\n    /* OP_cmppd      */   &prefix_extensions[52][2],\n    /* OP_cmpsd      */   &prefix_extensions[52][3],\n    /* OP_shufps     */   &prefix_extensions[55][0],\n    /* OP_shufpd     */   &prefix_extensions[55][2],\n    /* OP_cvtdq2pd   */   &prefix_extensions[77][1],\n    /* OP_cvttpd2dq  */   &prefix_extensions[77][2],\n    /* OP_cvtpd2dq   */   &prefix_extensions[77][3],\n    /* OP_nop        */   &rex_b_extensions[0][0],\n    /* OP_pause      */   &prefix_extensions[103][1],\n\n    /* OP_ins         */   &rep_extensions[1][0],\n    /* OP_rep_ins     */   &rep_extensions[1][2],\n    /* OP_outs        */   &rep_extensions[3][0],\n    /* OP_rep_outs    */   &rep_extensions[3][2],\n    /* OP_movs        */   &rep_extensions[5][0],\n    /* OP_rep_movs    */   &rep_extensions[5][2],\n    /* OP_stos        */   &rep_extensions[7][0],\n    /* OP_rep_stos    */   &rep_extensions[7][2],\n    /* OP_lods        */   &rep_extensions[9][0],\n    /* OP_rep_lods    */   &rep_extensions[9][2],\n    /* OP_cmps        */   &repne_extensions[1][0],\n    /* OP_rep_cmps    */   &repne_extensions[1][2],\n    /* OP_repne_cmps  */   &repne_extensions[1][4],\n    /* OP_scas        */   &repne_extensions[3][0],\n    /* OP_rep_scas    */   &repne_extensions[3][2],\n    /* OP_repne_scas  */   &repne_extensions[3][4],\n\n\n    /* OP_fadd     */   &float_low_modrm[0x00],\n    /* OP_fmul     */   &float_low_modrm[0x01],\n    /* OP_fcom     */   &float_low_modrm[0x02],\n    /* OP_fcomp    */   &float_low_modrm[0x03],\n    /* OP_fsub     */   &float_low_modrm[0x04],\n    /* OP_fsubr    */   &float_low_modrm[0x05],\n    /* OP_fdiv     */   &float_low_modrm[0x06],\n    /* OP_fdivr    */   &float_low_modrm[0x07],\n    /* OP_fld      */   &float_low_modrm[0x08],\n    /* OP_fst      */   &float_low_modrm[0x0a],\n    /* OP_fstp     */   &float_low_modrm[0x0b],\n    /* OP_fldenv   */   &float_low_modrm[0x0c],\n    /* OP_fldcw    */   &float_low_modrm[0x0d],\n    /* OP_fnstenv  */   &float_low_modrm[0x0e],\n    /* OP_fnstcw   */   &float_low_modrm[0x0f],\n    /* OP_fiadd    */   &float_low_modrm[0x10],\n    /* OP_fimul    */   &float_low_modrm[0x11],\n    /* OP_ficom    */   &float_low_modrm[0x12],\n    /* OP_ficomp   */   &float_low_modrm[0x13],\n    /* OP_fisub    */   &float_low_modrm[0x14],\n    /* OP_fisubr   */   &float_low_modrm[0x15],\n    /* OP_fidiv    */   &float_low_modrm[0x16],\n    /* OP_fidivr   */   &float_low_modrm[0x17],\n    /* OP_fild     */   &float_low_modrm[0x18],\n    /* OP_fist     */   &float_low_modrm[0x1a],\n    /* OP_fistp    */   &float_low_modrm[0x1b],\n    /* OP_frstor   */   &float_low_modrm[0x2c],\n    /* OP_fnsave   */   &float_low_modrm[0x2e],\n    /* OP_fnstsw   */   &float_low_modrm[0x2f],\n\n    /* OP_fbld     */   &float_low_modrm[0x3c],\n    /* OP_fbstp    */   &float_low_modrm[0x3e],\n\n\n    /* OP_fxch      */   &float_high_modrm[1][0x08],\n    /* OP_fnop      */   &float_high_modrm[1][0x10],\n    /* OP_fchs      */   &float_high_modrm[1][0x20],\n    /* OP_fabs      */   &float_high_modrm[1][0x21],\n    /* OP_ftst      */   &float_high_modrm[1][0x24],\n    /* OP_fxam      */   &float_high_modrm[1][0x25],\n    /* OP_fld1      */   &float_high_modrm[1][0x28],\n    /* OP_fldl2t    */   &float_high_modrm[1][0x29],\n    /* OP_fldl2e    */   &float_high_modrm[1][0x2a],\n    /* OP_fldpi     */   &float_high_modrm[1][0x2b],\n    /* OP_fldlg2    */   &float_high_modrm[1][0x2c],\n    /* OP_fldln2    */   &float_high_modrm[1][0x2d],\n    /* OP_fldz      */   &float_high_modrm[1][0x2e],\n    /* OP_f2xm1     */   &float_high_modrm[1][0x30],\n    /* OP_fyl2x     */   &float_high_modrm[1][0x31],\n    /* OP_fptan     */   &float_high_modrm[1][0x32],\n    /* OP_fpatan    */   &float_high_modrm[1][0x33],\n    /* OP_fxtract   */   &float_high_modrm[1][0x34],\n    /* OP_fprem1    */   &float_high_modrm[1][0x35],\n    /* OP_fdecstp   */   &float_high_modrm[1][0x36],\n    /* OP_fincstp   */   &float_high_modrm[1][0x37],\n    /* OP_fprem     */   &float_high_modrm[1][0x38],\n    /* OP_fyl2xp1   */   &float_high_modrm[1][0x39],\n    /* OP_fsqrt     */   &float_high_modrm[1][0x3a],\n    /* OP_fsincos   */   &float_high_modrm[1][0x3b],\n    /* OP_frndint   */   &float_high_modrm[1][0x3c],\n    /* OP_fscale    */   &float_high_modrm[1][0x3d],\n    /* OP_fsin      */   &float_high_modrm[1][0x3e],\n    /* OP_fcos      */   &float_high_modrm[1][0x3f],\n    /* OP_fcmovb    */   &float_high_modrm[2][0x00],\n    /* OP_fcmove    */   &float_high_modrm[2][0x08],\n    /* OP_fcmovbe   */   &float_high_modrm[2][0x10],\n    /* OP_fcmovu    */   &float_high_modrm[2][0x18],\n    /* OP_fucompp   */   &float_high_modrm[2][0x29],\n    /* OP_fcmovnb   */   &float_high_modrm[3][0x00],\n    /* OP_fcmovne   */   &float_high_modrm[3][0x08],\n    /* OP_fcmovnbe  */   &float_high_modrm[3][0x10],\n    /* OP_fcmovnu   */   &float_high_modrm[3][0x18],\n    /* OP_fnclex    */   &float_high_modrm[3][0x22],\n    /* OP_fninit    */   &float_high_modrm[3][0x23],\n    /* OP_fucomi    */   &float_high_modrm[3][0x28],\n    /* OP_fcomi     */   &float_high_modrm[3][0x30],\n    /* OP_ffree     */   &float_high_modrm[5][0x00],\n    /* OP_fucom     */   &float_high_modrm[5][0x20],\n    /* OP_fucomp    */   &float_high_modrm[5][0x28],\n    /* OP_faddp     */   &float_high_modrm[6][0x00],\n    /* OP_fmulp     */   &float_high_modrm[6][0x08],\n    /* OP_fcompp    */   &float_high_modrm[6][0x19],\n    /* OP_fsubrp    */   &float_high_modrm[6][0x20],\n    /* OP_fsubp     */   &float_high_modrm[6][0x28],\n    /* OP_fdivrp    */   &float_high_modrm[6][0x30],\n    /* OP_fdivp     */   &float_high_modrm[6][0x38],\n    /* OP_fucomip   */   &float_high_modrm[7][0x28],\n    /* OP_fcomip    */   &float_high_modrm[7][0x30],\n\n    /* SSE3 instructions */\n    /* OP_fisttp      */   &float_low_modrm[0x29],\n    /* OP_haddpd      */   &prefix_extensions[114][2],\n    /* OP_haddps      */   &prefix_extensions[114][3],\n    /* OP_hsubpd      */   &prefix_extensions[115][2],\n    /* OP_hsubps      */   &prefix_extensions[115][3],\n    /* OP_addsubpd    */   &prefix_extensions[116][2],\n    /* OP_addsubps    */   &prefix_extensions[116][3],\n    /* OP_lddqu       */   &prefix_extensions[117][3],\n    /* OP_monitor     */    &rm_extensions[1][0],\n    /* OP_mwait       */    &rm_extensions[1][1],\n    /* OP_movsldup    */   &prefix_extensions[ 2][1],\n    /* OP_movshdup    */   &prefix_extensions[ 6][1],\n    /* OP_movddup     */   &prefix_extensions[ 2][3],\n\n    /* 3D-Now! instructions */\n    /* OP_femms         */   &second_byte[0x0e],\n    /* OP_unknown_3dnow */   &suffix_extensions[0],\n    /* OP_pavgusb       */   &suffix_extensions[1],\n    /* OP_pfadd         */   &suffix_extensions[2],\n    /* OP_pfacc         */   &suffix_extensions[3],\n    /* OP_pfcmpge       */   &suffix_extensions[4],\n    /* OP_pfcmpgt       */   &suffix_extensions[5],\n    /* OP_pfcmpeq       */   &suffix_extensions[6],\n    /* OP_pfmin         */   &suffix_extensions[7],\n    /* OP_pfmax         */   &suffix_extensions[8],\n    /* OP_pfmul         */   &suffix_extensions[9],\n    /* OP_pfrcp         */   &suffix_extensions[10],\n    /* OP_pfrcpit1      */   &suffix_extensions[11],\n    /* OP_pfrcpit2      */   &suffix_extensions[12],\n    /* OP_pfrsqrt       */   &suffix_extensions[13],\n    /* OP_pfrsqit1      */   &suffix_extensions[14],\n    /* OP_pmulhrw       */   &suffix_extensions[15],\n    /* OP_pfsub         */   &suffix_extensions[16],\n    /* OP_pfsubr        */   &suffix_extensions[17],\n    /* OP_pi2fd         */   &suffix_extensions[18],\n    /* OP_pf2id         */   &suffix_extensions[19],\n    /* OP_pi2fw         */   &suffix_extensions[20],\n    /* OP_pf2iw         */   &suffix_extensions[21],\n    /* OP_pfnacc        */   &suffix_extensions[22],\n    /* OP_pfpnacc       */   &suffix_extensions[23],\n    /* OP_pswapd        */   &suffix_extensions[24],\n\n    /* SSSE3 */\n    /* OP_pshufb        */   &prefix_extensions[118][0],\n    /* OP_phaddw        */   &prefix_extensions[119][0],\n    /* OP_phaddd        */   &prefix_extensions[120][0],\n    /* OP_phaddsw       */   &prefix_extensions[121][0],\n    /* OP_pmaddubsw     */   &prefix_extensions[122][0],\n    /* OP_phsubw        */   &prefix_extensions[123][0],\n    /* OP_phsubd        */   &prefix_extensions[124][0],\n    /* OP_phsubsw       */   &prefix_extensions[125][0],\n    /* OP_psignb        */   &prefix_extensions[126][0],\n    /* OP_psignw        */   &prefix_extensions[127][0],\n    /* OP_psignd        */   &prefix_extensions[128][0],\n    /* OP_pmulhrsw      */   &prefix_extensions[129][0],\n    /* OP_pabsb         */   &prefix_extensions[130][0],\n    /* OP_pabsw         */   &prefix_extensions[131][0],\n    /* OP_pabsd         */   &prefix_extensions[132][0],\n    /* OP_palignr       */   &prefix_extensions[133][0],\n\n    /* SSE4 (incl AMD (SSE4A) and Intel-specific (SSE4.1, SSE4.2) extensions */\n    /* OP_popcnt        */   &second_byte[0xb8],\n    /* OP_movntss       */   &prefix_extensions[11][1],\n    /* OP_movntsd       */   &prefix_extensions[11][3],\n    /* OP_extrq         */   &prefix_extensions[134][2],\n    /* OP_insertq       */   &prefix_extensions[134][3],\n    /* OP_lzcnt         */   &prefix_extensions[136][1],\n    /* OP_pblendvb      */   &third_byte_38[16],\n    /* OP_blendvps      */   &third_byte_38[17],\n    /* OP_blendvpd      */   &third_byte_38[18],\n    /* OP_ptest         */   &e_vex_extensions[3][0],\n    /* OP_pmovsxbw      */   &e_vex_extensions[4][0],\n    /* OP_pmovsxbd      */   &e_vex_extensions[5][0],\n    /* OP_pmovsxbq      */   &e_vex_extensions[6][0],\n    /* OP_pmovsxwd      */   &e_vex_extensions[7][0],\n    /* OP_pmovsxwq      */   &e_vex_extensions[8][0],\n    /* OP_pmovsxdq      */   &e_vex_extensions[9][0],\n    /* OP_pmuldq        */   &e_vex_extensions[10][0],\n    /* OP_pcmpeqq       */   &e_vex_extensions[11][0],\n    /* OP_movntdqa      */   &e_vex_extensions[12][0],\n    /* OP_packusdw      */   &e_vex_extensions[13][0],\n    /* OP_pmovzxbw      */   &e_vex_extensions[14][0],\n    /* OP_pmovzxbd      */   &e_vex_extensions[15][0],\n    /* OP_pmovzxbq      */   &e_vex_extensions[16][0],\n    /* OP_pmovzxwd      */   &e_vex_extensions[17][0],\n    /* OP_pmovzxwq      */   &e_vex_extensions[18][0],\n    /* OP_pmovzxdq      */   &e_vex_extensions[19][0],\n    /* OP_pcmpgtq       */   &e_vex_extensions[20][0],\n    /* OP_pminsb        */   &e_vex_extensions[21][0],\n    /* OP_pminsd        */   &e_vex_extensions[22][0],\n    /* OP_pminuw        */   &e_vex_extensions[23][0],\n    /* OP_pminud        */   &e_vex_extensions[24][0],\n    /* OP_pmaxsb        */   &e_vex_extensions[25][0],\n    /* OP_pmaxsd        */   &e_vex_extensions[26][0],\n    /* OP_pmaxuw        */   &e_vex_extensions[27][0],\n    /* OP_pmaxud        */   &e_vex_extensions[28][0],\n    /* OP_pmulld        */   &e_vex_extensions[29][0],\n    /* OP_phminposuw    */   &e_vex_extensions[30][0],\n    /* OP_crc32         */   &prefix_extensions[139][3],\n    /* OP_pextrb        */   &e_vex_extensions[36][0],\n    /* OP_pextrd        */   &e_vex_extensions[38][0],\n    /* OP_extractps     */   &e_vex_extensions[39][0],\n    /* OP_roundps       */   &e_vex_extensions[40][0],\n    /* OP_roundpd       */   &e_vex_extensions[41][0],\n    /* OP_roundss       */   &e_vex_extensions[42][0],\n    /* OP_roundsd       */   &e_vex_extensions[43][0],\n    /* OP_blendps       */   &e_vex_extensions[44][0],\n    /* OP_blendpd       */   &e_vex_extensions[45][0],\n    /* OP_pblendw       */   &e_vex_extensions[46][0],\n    /* OP_pinsrb        */   &e_vex_extensions[47][0],\n    /* OP_insertps      */   &e_vex_extensions[48][0],\n    /* OP_pinsrd        */   &e_vex_extensions[49][0],\n    /* OP_dpps          */   &e_vex_extensions[50][0],\n    /* OP_dppd          */   &e_vex_extensions[51][0],\n    /* OP_mpsadbw       */   &e_vex_extensions[52][0],\n    /* OP_pcmpestrm     */   &e_vex_extensions[53][0],\n    /* OP_pcmpestri     */   &e_vex_extensions[54][0],\n    /* OP_pcmpistrm     */   &e_vex_extensions[55][0],\n    /* OP_pcmpistri     */   &e_vex_extensions[56][0],\n\n    /* x64 */\n    /* OP_movsxd        */   &x64_extensions[16][1],\n    /* OP_swapgs        */   &rm_extensions[2][0],\n\n    /* VMX */\n    /* OP_vmcall        */   &rm_extensions[0][1],\n    /* OP_vmlaunch      */   &rm_extensions[0][2],\n    /* OP_vmresume      */   &rm_extensions[0][3],\n    /* OP_vmxoff        */   &rm_extensions[0][4],\n    /* OP_vmptrst       */   &mod_extensions[13][0],\n    /* OP_vmptrld       */   &prefix_extensions[137][0],\n    /* OP_vmxon         */   &prefix_extensions[137][1],\n    /* OP_vmclear       */   &prefix_extensions[137][2],\n    /* OP_vmread        */   &prefix_extensions[134][0],\n    /* OP_vmwrite       */   &prefix_extensions[135][0],\n\n    /* undocumented */\n    /* OP_int1          */   &first_byte[0xf1],\n    /* OP_salc          */   &first_byte[0xd6],\n    /* OP_ffreep        */   &float_high_modrm[7][0x00],\n\n    /* AMD SVM */\n    /* OP_vmrun         */   &rm_extensions[3][0],\n    /* OP_vmmcall       */   &rm_extensions[3][1],\n    /* OP_vmload        */   &rm_extensions[3][2],\n    /* OP_vmsave        */   &rm_extensions[3][3],\n    /* OP_stgi          */   &rm_extensions[3][4],\n    /* OP_clgi          */   &rm_extensions[3][5],\n    /* OP_skinit        */   &rm_extensions[3][6],\n    /* OP_invlpga       */   &rm_extensions[3][7],\n    /* AMD though not part of SVM */\n    /* OP_rdtscp        */   &rm_extensions[2][1],\n\n    /* Intel VMX additions */\n    /* OP_invept        */   &third_byte_38[49],\n    /* OP_invvpid       */   &third_byte_38[50],\n\n    /* added in Intel Westmere */\n    /* OP_pclmulqdq     */   &e_vex_extensions[57][0],\n    /* OP_aesimc        */   &e_vex_extensions[31][0],\n    /* OP_aesenc        */   &e_vex_extensions[32][0],\n    /* OP_aesenclast    */   &e_vex_extensions[33][0],\n    /* OP_aesdec        */   &e_vex_extensions[34][0],\n    /* OP_aesdeclast    */   &e_vex_extensions[35][0],\n    /* OP_aeskeygenassist*/  &e_vex_extensions[58][0],\n\n    /* added in Intel Atom */\n    /* OP_movbe         */   &prefix_extensions[138][0],\n\n    /* added in Intel Sandy Bridge */\n    /* OP_xgetbv        */   &rm_extensions[4][0],\n    /* OP_xsetbv        */   &rm_extensions[4][1],\n    /* OP_xsave32       */   &rex_w_extensions[2][0],\n    /* OP_xrstor32      */   &rex_w_extensions[3][0],\n    /* OP_xsaveopt32    */   &rex_w_extensions[4][0],\n\n    /* AVX */\n    /* OP_vmovss        */  &mod_extensions[ 8][0],\n    /* OP_vmovsd        */  &mod_extensions[ 9][0],\n    /* OP_vmovups       */  &prefix_extensions[ 0][4],\n    /* OP_vmovupd       */  &prefix_extensions[ 0][6],\n    /* OP_vmovlps       */  &prefix_extensions[ 2][4],\n    /* OP_vmovsldup     */  &prefix_extensions[ 2][5],\n    /* OP_vmovlpd       */  &prefix_extensions[ 2][6],\n    /* OP_vmovddup      */  &prefix_extensions[ 2][7],\n    /* OP_vunpcklps     */  &prefix_extensions[ 4][4],\n    /* OP_vunpcklpd     */  &prefix_extensions[ 4][6],\n    /* OP_vunpckhps     */  &prefix_extensions[ 5][4],\n    /* OP_vunpckhpd     */  &prefix_extensions[ 5][6],\n    /* OP_vmovhps       */  &prefix_extensions[ 6][4],\n    /* OP_vmovshdup     */  &prefix_extensions[ 6][5],\n    /* OP_vmovhpd       */  &prefix_extensions[ 6][6],\n    /* OP_vmovaps       */  &prefix_extensions[ 8][4],\n    /* OP_vmovapd       */  &prefix_extensions[ 8][6],\n    /* OP_vcvtsi2ss     */  &prefix_extensions[10][5],\n    /* OP_vcvtsi2sd     */  &prefix_extensions[10][7],\n    /* OP_vmovntps      */  &prefix_extensions[11][4],\n    /* OP_vmovntpd      */  &prefix_extensions[11][6],\n    /* OP_vcvttss2si    */  &prefix_extensions[12][5],\n    /* OP_vcvttsd2si    */  &prefix_extensions[12][7],\n    /* OP_vcvtss2si     */  &prefix_extensions[13][5],\n    /* OP_vcvtsd2si     */  &prefix_extensions[13][7],\n    /* OP_vucomiss      */  &prefix_extensions[14][4],\n    /* OP_vucomisd      */  &prefix_extensions[14][6],\n    /* OP_vcomiss       */  &prefix_extensions[15][4],\n    /* OP_vcomisd       */  &prefix_extensions[15][6],\n    /* OP_vmovmskps     */  &prefix_extensions[16][4],\n    /* OP_vmovmskpd     */  &prefix_extensions[16][6],\n    /* OP_vsqrtps       */  &prefix_extensions[17][4],\n    /* OP_vsqrtss       */  &prefix_extensions[17][5],\n    /* OP_vsqrtpd       */  &prefix_extensions[17][6],\n    /* OP_vsqrtsd       */  &prefix_extensions[17][7],\n    /* OP_vrsqrtps      */  &prefix_extensions[18][4],\n    /* OP_vrsqrtss      */  &prefix_extensions[18][5],\n    /* OP_vrcpps        */  &prefix_extensions[19][4],\n    /* OP_vrcpss        */  &prefix_extensions[19][5],\n    /* OP_vandps        */  &prefix_extensions[20][4],\n    /* OP_vandpd        */  &prefix_extensions[20][6],\n    /* OP_vandnps       */  &prefix_extensions[21][4],\n    /* OP_vandnpd       */  &prefix_extensions[21][6],\n    /* OP_vorps         */  &prefix_extensions[22][4],\n    /* OP_vorpd         */  &prefix_extensions[22][6],\n    /* OP_vxorps        */  &prefix_extensions[23][4],\n    /* OP_vxorpd        */  &prefix_extensions[23][6],\n    /* OP_vaddps        */  &prefix_extensions[24][4],\n    /* OP_vaddss        */  &prefix_extensions[24][5],\n    /* OP_vaddpd        */  &prefix_extensions[24][6],\n    /* OP_vaddsd        */  &prefix_extensions[24][7],\n    /* OP_vmulps        */  &prefix_extensions[25][4],\n    /* OP_vmulss        */  &prefix_extensions[25][5],\n    /* OP_vmulpd        */  &prefix_extensions[25][6],\n    /* OP_vmulsd        */  &prefix_extensions[25][7],\n    /* OP_vcvtps2pd     */  &prefix_extensions[26][4],\n    /* OP_vcvtss2sd     */  &prefix_extensions[26][5],\n    /* OP_vcvtpd2ps     */  &prefix_extensions[26][6],\n    /* OP_vcvtsd2ss     */  &prefix_extensions[26][7],\n    /* OP_vcvtdq2ps     */  &prefix_extensions[27][4],\n    /* OP_vcvttps2dq    */  &prefix_extensions[27][5],\n    /* OP_vcvtps2dq     */  &prefix_extensions[27][6],\n    /* OP_vsubps        */  &prefix_extensions[28][4],\n    /* OP_vsubss        */  &prefix_extensions[28][5],\n    /* OP_vsubpd        */  &prefix_extensions[28][6],\n    /* OP_vsubsd        */  &prefix_extensions[28][7],\n    /* OP_vminps        */  &prefix_extensions[29][4],\n    /* OP_vminss        */  &prefix_extensions[29][5],\n    /* OP_vminpd        */  &prefix_extensions[29][6],\n    /* OP_vminsd        */  &prefix_extensions[29][7],\n    /* OP_vdivps        */  &prefix_extensions[30][4],\n    /* OP_vdivss        */  &prefix_extensions[30][5],\n    /* OP_vdivpd        */  &prefix_extensions[30][6],\n    /* OP_vdivsd        */  &prefix_extensions[30][7],\n    /* OP_vmaxps        */  &prefix_extensions[31][4],\n    /* OP_vmaxss        */  &prefix_extensions[31][5],\n    /* OP_vmaxpd        */  &prefix_extensions[31][6],\n    /* OP_vmaxsd        */  &prefix_extensions[31][7],\n    /* OP_vpunpcklbw    */  &prefix_extensions[32][6],\n    /* OP_vpunpcklwd    */  &prefix_extensions[33][6],\n    /* OP_vpunpckldq    */  &prefix_extensions[34][6],\n    /* OP_vpacksswb     */  &prefix_extensions[35][6],\n    /* OP_vpcmpgtb      */  &prefix_extensions[36][6],\n    /* OP_vpcmpgtw      */  &prefix_extensions[37][6],\n    /* OP_vpcmpgtd      */  &prefix_extensions[38][6],\n    /* OP_vpackuswb     */  &prefix_extensions[39][6],\n    /* OP_vpunpckhbw    */  &prefix_extensions[40][6],\n    /* OP_vpunpckhwd    */  &prefix_extensions[41][6],\n    /* OP_vpunpckhdq    */  &prefix_extensions[42][6],\n    /* OP_vpackssdw     */  &prefix_extensions[43][6],\n    /* OP_vpunpcklqdq   */  &prefix_extensions[44][6],\n    /* OP_vpunpckhqdq   */  &prefix_extensions[45][6],\n    /* OP_vmovd         */  &prefix_extensions[46][6],\n    /* OP_vpshufhw      */  &prefix_extensions[47][5],\n    /* OP_vpshufd       */  &prefix_extensions[47][6],\n    /* OP_vpshuflw      */  &prefix_extensions[47][7],\n    /* OP_vpcmpeqb      */  &prefix_extensions[48][6],\n    /* OP_vpcmpeqw      */  &prefix_extensions[49][6],\n    /* OP_vpcmpeqd      */  &prefix_extensions[50][6],\n    /* OP_vmovq         */  &prefix_extensions[51][5],\n    /* OP_vcmpps        */  &prefix_extensions[52][4],\n    /* OP_vcmpss        */  &prefix_extensions[52][5],\n    /* OP_vcmppd        */  &prefix_extensions[52][6],\n    /* OP_vcmpsd        */  &prefix_extensions[52][7],\n    /* OP_vpinsrw       */  &prefix_extensions[53][6],\n    /* OP_vpextrw       */  &prefix_extensions[54][6],\n    /* OP_vshufps       */  &prefix_extensions[55][4],\n    /* OP_vshufpd       */  &prefix_extensions[55][6],\n    /* OP_vpsrlw        */  &prefix_extensions[56][6],\n    /* OP_vpsrld        */  &prefix_extensions[57][6],\n    /* OP_vpsrlq        */  &prefix_extensions[58][6],\n    /* OP_vpaddq        */  &prefix_extensions[59][6],\n    /* OP_vpmullw       */  &prefix_extensions[60][6],\n    /* OP_vpmovmskb     */  &prefix_extensions[62][6],\n    /* OP_vpsubusb      */  &prefix_extensions[63][6],\n    /* OP_vpsubusw      */  &prefix_extensions[64][6],\n    /* OP_vpminub       */  &prefix_extensions[65][6],\n    /* OP_vpand         */  &prefix_extensions[66][6],\n    /* OP_vpaddusb      */  &prefix_extensions[67][6],\n    /* OP_vpaddusw      */  &prefix_extensions[68][6],\n    /* OP_vpmaxub       */  &prefix_extensions[69][6],\n    /* OP_vpandn        */  &prefix_extensions[70][6],\n    /* OP_vpavgb        */  &prefix_extensions[71][6],\n    /* OP_vpsraw        */  &prefix_extensions[72][6],\n    /* OP_vpsrad        */  &prefix_extensions[73][6],\n    /* OP_vpavgw        */  &prefix_extensions[74][6],\n    /* OP_vpmulhuw      */  &prefix_extensions[75][6],\n    /* OP_vpmulhw       */  &prefix_extensions[76][6],\n    /* OP_vcvtdq2pd     */  &prefix_extensions[77][5],\n    /* OP_vcvttpd2dq    */  &prefix_extensions[77][6],\n    /* OP_vcvtpd2dq     */  &prefix_extensions[77][7],\n    /* OP_vmovntdq      */  &prefix_extensions[78][6],\n    /* OP_vpsubsb       */  &prefix_extensions[79][6],\n    /* OP_vpsubsw       */  &prefix_extensions[80][6],\n    /* OP_vpminsw       */  &prefix_extensions[81][6],\n    /* OP_vpor          */  &prefix_extensions[82][6],\n    /* OP_vpaddsb       */  &prefix_extensions[83][6],\n    /* OP_vpaddsw       */  &prefix_extensions[84][6],\n    /* OP_vpmaxsw       */  &prefix_extensions[85][6],\n    /* OP_vpxor         */  &prefix_extensions[86][6],\n    /* OP_vpsllw        */  &prefix_extensions[87][6],\n    /* OP_vpslld        */  &prefix_extensions[88][6],\n    /* OP_vpsllq        */  &prefix_extensions[89][6],\n    /* OP_vpmuludq      */  &prefix_extensions[90][6],\n    /* OP_vpmaddwd      */  &prefix_extensions[91][6],\n    /* OP_vpsadbw       */  &prefix_extensions[92][6],\n    /* OP_vmaskmovdqu   */  &prefix_extensions[93][6],\n    /* OP_vpsubb        */  &prefix_extensions[94][6],\n    /* OP_vpsubw        */  &prefix_extensions[95][6],\n    /* OP_vpsubd        */  &prefix_extensions[96][6],\n    /* OP_vpsubq        */  &prefix_extensions[97][6],\n    /* OP_vpaddb        */  &prefix_extensions[98][6],\n    /* OP_vpaddw        */  &prefix_extensions[99][6],\n    /* OP_vpaddd        */  &prefix_extensions[100][6],\n    /* OP_vpsrldq       */  &prefix_extensions[101][6],\n    /* OP_vpslldq       */  &prefix_extensions[102][6],\n    /* OP_vmovdqu       */  &prefix_extensions[112][5],\n    /* OP_vmovdqa       */  &prefix_extensions[112][6],\n    /* OP_vhaddpd       */  &prefix_extensions[114][6],\n    /* OP_vhaddps       */  &prefix_extensions[114][7],\n    /* OP_vhsubpd       */  &prefix_extensions[115][6],\n    /* OP_vhsubps       */  &prefix_extensions[115][7],\n    /* OP_vaddsubpd     */  &prefix_extensions[116][6],\n    /* OP_vaddsubps     */  &prefix_extensions[116][7],\n    /* OP_vlddqu        */  &prefix_extensions[117][7],\n    /* OP_vpshufb       */  &prefix_extensions[118][6],\n    /* OP_vphaddw       */  &prefix_extensions[119][6],\n    /* OP_vphaddd       */  &prefix_extensions[120][6],\n    /* OP_vphaddsw      */  &prefix_extensions[121][6],\n    /* OP_vpmaddubsw    */  &prefix_extensions[122][6],\n    /* OP_vphsubw       */  &prefix_extensions[123][6],\n    /* OP_vphsubd       */  &prefix_extensions[124][6],\n    /* OP_vphsubsw      */  &prefix_extensions[125][6],\n    /* OP_vpsignb       */  &prefix_extensions[126][6],\n    /* OP_vpsignw       */  &prefix_extensions[127][6],\n    /* OP_vpsignd       */  &prefix_extensions[128][6],\n    /* OP_vpmulhrsw     */  &prefix_extensions[129][6],\n    /* OP_vpabsb        */  &prefix_extensions[130][6],\n    /* OP_vpabsw        */  &prefix_extensions[131][6],\n    /* OP_vpabsd        */  &prefix_extensions[132][6],\n    /* OP_vpalignr      */  &prefix_extensions[133][6],\n    /* OP_vpblendvb     */  &e_vex_extensions[ 2][1],\n    /* OP_vblendvps     */  &e_vex_extensions[ 0][1],\n    /* OP_vblendvpd     */  &e_vex_extensions[ 1][1],\n    /* OP_vptest        */  &e_vex_extensions[ 3][1],\n    /* OP_vpmovsxbw     */  &e_vex_extensions[ 4][1],\n    /* OP_vpmovsxbd     */  &e_vex_extensions[ 5][1],\n    /* OP_vpmovsxbq     */  &e_vex_extensions[ 6][1],\n    /* OP_vpmovsxwd     */  &e_vex_extensions[ 7][1],\n    /* OP_vpmovsxwq     */  &e_vex_extensions[ 8][1],\n    /* OP_vpmovsxdq     */  &e_vex_extensions[ 9][1],\n    /* OP_vpmuldq       */  &e_vex_extensions[10][1],\n    /* OP_vpcmpeqq      */  &e_vex_extensions[11][1],\n    /* OP_vmovntdqa     */  &e_vex_extensions[12][1],\n    /* OP_vpackusdw     */  &e_vex_extensions[13][1],\n    /* OP_vpmovzxbw     */  &e_vex_extensions[14][1],\n    /* OP_vpmovzxbd     */  &e_vex_extensions[15][1],\n    /* OP_vpmovzxbq     */  &e_vex_extensions[16][1],\n    /* OP_vpmovzxwd     */  &e_vex_extensions[17][1],\n    /* OP_vpmovzxwq     */  &e_vex_extensions[18][1],\n    /* OP_vpmovzxdq     */  &e_vex_extensions[19][1],\n    /* OP_vpcmpgtq      */  &e_vex_extensions[20][1],\n    /* OP_vpminsb       */  &e_vex_extensions[21][1],\n    /* OP_vpminsd       */  &e_vex_extensions[22][1],\n    /* OP_vpminuw       */  &e_vex_extensions[23][1],\n    /* OP_vpminud       */  &e_vex_extensions[24][1],\n    /* OP_vpmaxsb       */  &e_vex_extensions[25][1],\n    /* OP_vpmaxsd       */  &e_vex_extensions[26][1],\n    /* OP_vpmaxuw       */  &e_vex_extensions[27][1],\n    /* OP_vpmaxud       */  &e_vex_extensions[28][1],\n    /* OP_vpmulld       */  &e_vex_extensions[29][1],\n    /* OP_vphminposuw   */  &e_vex_extensions[30][1],\n    /* OP_vaesimc       */  &e_vex_extensions[31][1],\n    /* OP_vaesenc       */  &e_vex_extensions[32][1],\n    /* OP_vaesenclast   */  &e_vex_extensions[33][1],\n    /* OP_vaesdec       */  &e_vex_extensions[34][1],\n    /* OP_vaesdeclast   */  &e_vex_extensions[35][1],\n    /* OP_vpextrb       */  &e_vex_extensions[36][1],\n    /* OP_vpextrd       */  &e_vex_extensions[38][1],\n    /* OP_vextractps    */  &e_vex_extensions[39][1],\n    /* OP_vroundps      */  &e_vex_extensions[40][1],\n    /* OP_vroundpd      */  &e_vex_extensions[41][1],\n    /* OP_vroundss      */  &e_vex_extensions[42][1],\n    /* OP_vroundsd      */  &e_vex_extensions[43][1],\n    /* OP_vblendps      */  &e_vex_extensions[44][1],\n    /* OP_vblendpd      */  &e_vex_extensions[45][1],\n    /* OP_vpblendw      */  &e_vex_extensions[46][1],\n    /* OP_vpinsrb       */  &e_vex_extensions[47][1],\n    /* OP_vinsertps     */  &e_vex_extensions[48][1],\n    /* OP_vpinsrd       */  &e_vex_extensions[49][1],\n    /* OP_vdpps         */  &e_vex_extensions[50][1],\n    /* OP_vdppd         */  &e_vex_extensions[51][1],\n    /* OP_vmpsadbw      */  &e_vex_extensions[52][1],\n    /* OP_vpcmpestrm    */  &e_vex_extensions[53][1],\n    /* OP_vpcmpestri    */  &e_vex_extensions[54][1],\n    /* OP_vpcmpistrm    */  &e_vex_extensions[55][1],\n    /* OP_vpcmpistri    */  &e_vex_extensions[56][1],\n    /* OP_vpclmulqdq    */  &e_vex_extensions[57][1],\n    /* OP_vaeskeygenassist*/ &e_vex_extensions[58][1],\n    /* OP_vtestps       */  &e_vex_extensions[59][1],\n    /* OP_vtestpd       */  &e_vex_extensions[60][1],\n    /* OP_vzeroupper    */  &vex_L_extensions[0][1],\n    /* OP_vzeroall      */  &vex_L_extensions[0][2],\n    /* OP_vldmxcsr      */  &e_vex_extensions[61][1],\n    /* OP_vstmxcsr      */  &e_vex_extensions[62][1],\n    /* OP_vbroadcastss  */  &e_vex_extensions[64][1],\n    /* OP_vbroadcastsd  */  &e_vex_extensions[65][1],\n    /* OP_vbroadcastf128*/  &e_vex_extensions[66][1],\n    /* OP_vmaskmovps    */  &e_vex_extensions[67][1],\n    /* OP_vmaskmovpd    */  &e_vex_extensions[68][1],\n    /* OP_vpermilps     */  &e_vex_extensions[71][1],\n    /* OP_vpermilpd     */  &e_vex_extensions[72][1],\n    /* OP_vperm2f128    */  &e_vex_extensions[73][1],\n    /* OP_vinsertf128   */  &e_vex_extensions[74][1],\n    /* OP_vextractf128  */  &e_vex_extensions[75][1],\n\n    /* added in Ivy Bridge I believe, and covered by F16C cpuid flag */\n    /* OP_vcvtph2ps     */  &e_vex_extensions[63][1],\n    /* OP_vcvtps2ph     */  &e_vex_extensions[76][1],\n\n    /* FMA */\n    /* OP_vfmadd132ps   */  &vex_W_extensions[ 0][0],\n    /* OP_vfmadd132pd   */  &vex_W_extensions[ 0][1],\n    /* OP_vfmadd213ps   */  &vex_W_extensions[ 1][0],\n    /* OP_vfmadd213pd   */  &vex_W_extensions[ 1][1],\n    /* OP_vfmadd231ps   */  &vex_W_extensions[ 2][0],\n    /* OP_vfmadd231pd   */  &vex_W_extensions[ 2][1],\n    /* OP_vfmadd132ss   */  &vex_W_extensions[ 3][0],\n    /* OP_vfmadd132sd   */  &vex_W_extensions[ 3][1],\n    /* OP_vfmadd213ss   */  &vex_W_extensions[ 4][0],\n    /* OP_vfmadd213sd   */  &vex_W_extensions[ 4][1],\n    /* OP_vfmadd231ss   */  &vex_W_extensions[ 5][0],\n    /* OP_vfmadd231sd   */  &vex_W_extensions[ 5][1],\n    /* OP_vfmaddsub132ps*/  &vex_W_extensions[ 6][0],\n    /* OP_vfmaddsub132pd*/  &vex_W_extensions[ 6][1],\n    /* OP_vfmaddsub213ps*/  &vex_W_extensions[ 7][0],\n    /* OP_vfmaddsub213pd*/  &vex_W_extensions[ 7][1],\n    /* OP_vfmaddsub231ps*/  &vex_W_extensions[ 8][0],\n    /* OP_vfmaddsub231pd*/  &vex_W_extensions[ 8][1],\n    /* OP_vfmsubadd132ps*/  &vex_W_extensions[ 9][0],\n    /* OP_vfmsubadd132pd*/  &vex_W_extensions[ 9][1],\n    /* OP_vfmsubadd213ps*/  &vex_W_extensions[10][0],\n    /* OP_vfmsubadd213pd*/  &vex_W_extensions[10][1],\n    /* OP_vfmsubadd231ps*/  &vex_W_extensions[11][0],\n    /* OP_vfmsubadd231pd*/  &vex_W_extensions[11][1],\n    /* OP_vfmsub132ps   */  &vex_W_extensions[12][0],\n    /* OP_vfmsub132pd   */  &vex_W_extensions[12][1],\n    /* OP_vfmsub213ps   */  &vex_W_extensions[13][0],\n    /* OP_vfmsub213pd   */  &vex_W_extensions[13][1],\n    /* OP_vfmsub231ps   */  &vex_W_extensions[14][0],\n    /* OP_vfmsub231pd   */  &vex_W_extensions[14][1],\n    /* OP_vfmsub132ss   */  &vex_W_extensions[15][0],\n    /* OP_vfmsub132sd   */  &vex_W_extensions[15][1],\n    /* OP_vfmsub213ss   */  &vex_W_extensions[16][0],\n    /* OP_vfmsub213sd   */  &vex_W_extensions[16][1],\n    /* OP_vfmsub231ss   */  &vex_W_extensions[17][0],\n    /* OP_vfmsub231sd   */  &vex_W_extensions[17][1],\n    /* OP_vfnmadd132ps  */  &vex_W_extensions[18][0],\n    /* OP_vfnmadd132pd  */  &vex_W_extensions[18][1],\n    /* OP_vfnmadd213ps  */  &vex_W_extensions[19][0],\n    /* OP_vfnmadd213pd  */  &vex_W_extensions[19][1],\n    /* OP_vfnmadd231ps  */  &vex_W_extensions[20][0],\n    /* OP_vfnmadd231pd  */  &vex_W_extensions[20][1],\n    /* OP_vfnmadd132ss  */  &vex_W_extensions[21][0],\n    /* OP_vfnmadd132sd  */  &vex_W_extensions[21][1],\n    /* OP_vfnmadd213ss  */  &vex_W_extensions[22][0],\n    /* OP_vfnmadd213sd  */  &vex_W_extensions[22][1],\n    /* OP_vfnmadd231ss  */  &vex_W_extensions[23][0],\n    /* OP_vfnmadd231sd  */  &vex_W_extensions[23][1],\n    /* OP_vfnmsub132ps  */  &vex_W_extensions[24][0],\n    /* OP_vfnmsub132pd  */  &vex_W_extensions[24][1],\n    /* OP_vfnmsub213ps  */  &vex_W_extensions[25][0],\n    /* OP_vfnmsub213pd  */  &vex_W_extensions[25][1],\n    /* OP_vfnmsub231ps  */  &vex_W_extensions[26][0],\n    /* OP_vfnmsub231pd  */  &vex_W_extensions[26][1],\n    /* OP_vfnmsub132ss  */  &vex_W_extensions[27][0],\n    /* OP_vfnmsub132sd  */  &vex_W_extensions[27][1],\n    /* OP_vfnmsub213ss  */  &vex_W_extensions[28][0],\n    /* OP_vfnmsub213sd  */  &vex_W_extensions[28][1],\n    /* OP_vfnmsub231ss  */  &vex_W_extensions[29][0],\n    /* OP_vfnmsub231sd  */  &vex_W_extensions[29][1],\n\n    /* SSE2 that were omitted before */\n    /* OP_movq2dq       */  &prefix_extensions[61][1],\n    /* OP_movdq2q       */  &prefix_extensions[61][3],\n\n    /* OP_fxsave64      */   &rex_w_extensions[0][1],\n    /* OP_fxrstor64     */   &rex_w_extensions[1][1],\n    /* OP_xsave64       */   &rex_w_extensions[2][1],\n    /* OP_xrstor64      */   &rex_w_extensions[3][1],\n    /* OP_xsaveopt64    */   &rex_w_extensions[4][1],\n\n    /* added in Intel Ivy Bridge: RDRAND and FSGSBASE cpuid flags */\n    /* OP_rdrand        */   &mod_extensions[12][1],\n    /* OP_rdfsbase      */   &mod_extensions[14][1],\n    /* OP_rdgsbase      */   &mod_extensions[15][1],\n    /* OP_wrfsbase      */   &mod_extensions[16][1],\n    /* OP_wrgsbase      */   &mod_extensions[17][1],\n\n    /* coming in the future but adding now since enough details are known */\n    /* OP_rdseed        */   &mod_extensions[13][1],\n\n    /* AMD FMA4 */\n    /* OP_vfmaddsubps   */   &vex_W_extensions[30][0],\n    /* OP_vfmaddsubpd   */   &vex_W_extensions[31][0],\n    /* OP_vfmsubaddps   */   &vex_W_extensions[32][0],\n    /* OP_vfmsubaddpd   */   &vex_W_extensions[33][0],\n    /* OP_vfmaddps      */   &vex_W_extensions[34][0],\n    /* OP_vfmaddpd      */   &vex_W_extensions[35][0],\n    /* OP_vfmaddss      */   &vex_W_extensions[36][0],\n    /* OP_vfmaddsd      */   &vex_W_extensions[37][0],\n    /* OP_vfmsubps      */   &vex_W_extensions[38][0],\n    /* OP_vfmsubpd      */   &vex_W_extensions[39][0],\n    /* OP_vfmsubss      */   &vex_W_extensions[40][0],\n    /* OP_vfmsubsd      */   &vex_W_extensions[41][0],\n    /* OP_vfnmaddps     */   &vex_W_extensions[42][0],\n    /* OP_vfnmaddpd     */   &vex_W_extensions[43][0],\n    /* OP_vfnmaddss     */   &vex_W_extensions[44][0],\n    /* OP_vfnmaddsd     */   &vex_W_extensions[45][0],\n    /* OP_vfnmsubps     */   &vex_W_extensions[46][0],\n    /* OP_vfnmsubpd     */   &vex_W_extensions[47][0],\n    /* OP_vfnmsubss     */   &vex_W_extensions[48][0],\n    /* OP_vfnmsubsd     */   &vex_W_extensions[49][0],\n\n    /* AMD XOP */\n    /* OP_vfrczps       */   &xop_extensions[27],\n    /* OP_vfrczpd       */   &xop_extensions[28],\n    /* OP_vfrczss       */   &xop_extensions[29],\n    /* OP_vfrczsd       */   &xop_extensions[30],\n    /* OP_vpcmov        */   &vex_W_extensions[50][0],\n    /* OP_vpcomb        */   &xop_extensions[19],\n    /* OP_vpcomw        */   &xop_extensions[20],\n    /* OP_vpcomd        */   &xop_extensions[21],\n    /* OP_vpcomq        */   &xop_extensions[22],\n    /* OP_vpcomub       */   &xop_extensions[23],\n    /* OP_vpcomuw       */   &xop_extensions[24],\n    /* OP_vpcomud       */   &xop_extensions[25],\n    /* OP_vpcomuq       */   &xop_extensions[26],\n    /* OP_vpermil2pd    */   &vex_W_extensions[65][0],\n    /* OP_vpermil2ps    */   &vex_W_extensions[64][0],\n    /* OP_vphaddbw      */   &xop_extensions[43],\n    /* OP_vphaddbd      */   &xop_extensions[44],\n    /* OP_vphaddbq      */   &xop_extensions[45],\n    /* OP_vphaddwd      */   &xop_extensions[46],\n    /* OP_vphaddwq      */   &xop_extensions[47],\n    /* OP_vphadddq      */   &xop_extensions[48],\n    /* OP_vphaddubw     */   &xop_extensions[49],\n    /* OP_vphaddubd     */   &xop_extensions[50],\n    /* OP_vphaddubq     */   &xop_extensions[51],\n    /* OP_vphadduwd     */   &xop_extensions[52],\n    /* OP_vphadduwq     */   &xop_extensions[53],\n    /* OP_vphaddudq     */   &xop_extensions[54],\n    /* OP_vphsubbw      */   &xop_extensions[55],\n    /* OP_vphsubwd      */   &xop_extensions[56],\n    /* OP_vphsubdq      */   &xop_extensions[57],\n    /* OP_vpmacssww     */   &xop_extensions[ 1],\n    /* OP_vpmacsswd     */   &xop_extensions[ 2],\n    /* OP_vpmacssdql    */   &xop_extensions[ 3],\n    /* OP_vpmacssdd     */   &xop_extensions[ 4],\n    /* OP_vpmacssdqh    */   &xop_extensions[ 5],\n    /* OP_vpmacsww      */   &xop_extensions[ 6],\n    /* OP_vpmacswd      */   &xop_extensions[ 7],\n    /* OP_vpmacsdql     */   &xop_extensions[ 8],\n    /* OP_vpmacsdd      */   &xop_extensions[ 9],\n    /* OP_vpmacsdqh     */   &xop_extensions[10],\n    /* OP_vpmadcsswd    */   &xop_extensions[13],\n    /* OP_vpmadcswd     */   &xop_extensions[14],\n    /* OP_vpperm        */   &vex_W_extensions[51][0],\n    /* OP_vprotb        */   &xop_extensions[15],\n    /* OP_vprotw        */   &xop_extensions[16],\n    /* OP_vprotd        */   &xop_extensions[17],\n    /* OP_vprotq        */   &xop_extensions[18],\n    /* OP_vpshlb        */   &vex_W_extensions[56][0],\n    /* OP_vpshlw        */   &vex_W_extensions[57][0],\n    /* OP_vpshld        */   &vex_W_extensions[58][0],\n    /* OP_vpshlq        */   &vex_W_extensions[59][0],\n    /* OP_vpshab        */   &vex_W_extensions[60][0],\n    /* OP_vpshaw        */   &vex_W_extensions[61][0],\n    /* OP_vpshad        */   &vex_W_extensions[62][0],\n    /* OP_vpshaq        */   &vex_W_extensions[63][0],\n\n    /* AMD TBM */\n    /* OP_bextr         */   &prefix_extensions[141][4],\n    /* OP_blcfill       */   &base_extensions[27][1],\n    /* OP_blci          */   &base_extensions[28][6],\n    /* OP_blcic         */   &base_extensions[27][5],\n    /* OP_blcmsk        */   &base_extensions[28][1],\n    /* OP_blcs          */   &base_extensions[27][3],\n    /* OP_blsfill       */   &base_extensions[27][2],\n    /* OP_blsic         */   &base_extensions[27][6],\n    /* OP_t1mskc        */   &base_extensions[27][7],\n    /* OP_tzmsk         */   &base_extensions[27][4],\n\n    /* AMD LWP */\n    /* OP_llwpcb        */   &base_extensions[29][0],\n    /* OP_slwpcb        */   &base_extensions[29][1],\n    /* OP_lwpins        */   &base_extensions[30][0],\n    /* OP_lwpval        */   &base_extensions[30][1],\n\n    /* Intel BMI1 */\n    /* (includes non-immed form of OP_bextr) */\n    /* OP_andn          */   &third_byte_38[100],\n    /* OP_blsr          */   &base_extensions[31][1],\n    /* OP_blsmsk        */   &base_extensions[31][2],\n    /* OP_blsi          */   &base_extensions[31][3],\n    /* OP_tzcnt         */   &prefix_extensions[140][1],\n\n    /* Intel BMI2 */\n    /* OP_bzhi          */   &prefix_extensions[142][4],\n    /* OP_pext          */   &prefix_extensions[142][6],\n    /* OP_pdep          */   &prefix_extensions[142][7],\n    /* OP_sarx          */   &prefix_extensions[141][5],\n    /* OP_shlx          */   &prefix_extensions[141][6],\n    /* OP_shrx          */   &prefix_extensions[141][7],\n    /* OP_rorx          */   &third_byte_3a[56],\n    /* OP_mulx          */   &prefix_extensions[143][7],\n\n    /* Intel Safer Mode Extensions */\n    /* OP_getsec        */   &second_byte[0x37],\n\n    /* Misc Intel additions */\n    /* OP_vmfunc        */   &rm_extensions[4][4],\n    /* OP_invpcid       */   &third_byte_38[103],\n\n    /* Intel TSX */\n    /* OP_xabort        */   &base_extensions[17][7],\n    /* OP_xbegin        */   &base_extensions[18][7],\n    /* OP_xend          */   &rm_extensions[4][5],\n    /* OP_xtest         */   &rm_extensions[4][6],\n\n    /* AVX2 */\n    /* OP_vpgatherdd    */   &vex_W_extensions[66][0],\n    /* OP_vpgatherdq    */   &vex_W_extensions[66][1],\n    /* OP_vpgatherqd    */   &vex_W_extensions[67][0],\n    /* OP_vpgatherqq    */   &vex_W_extensions[67][1],\n    /* OP_vgatherdps    */   &vex_W_extensions[68][0],\n    /* OP_vgatherdpd    */   &vex_W_extensions[68][1],\n    /* OP_vgatherqps    */   &vex_W_extensions[69][0],\n    /* OP_vgatherqpd    */   &vex_W_extensions[69][1],\n    /* OP_vbroadcasti128 */  &third_byte_38[108],\n    /* OP_vinserti128   */   &third_byte_3a[57],\n    /* OP_vextracti128  */   &third_byte_3a[58],\n    /* OP_vpmaskmovd    */   &vex_W_extensions[70][0],\n    /* OP_vpmaskmovq    */   &vex_W_extensions[70][1],\n    /* OP_vperm2i128    */   &third_byte_3a[62],\n    /* OP_vpermd        */   &third_byte_38[112],\n    /* OP_vpermps       */   &third_byte_38[111],\n    /* OP_vpermq        */   &third_byte_3a[59],\n    /* OP_vpermpd       */   &third_byte_3a[60],\n    /* OP_vpblendd      */   &third_byte_3a[61],\n    /* OP_vpsllvd       */   &vex_W_extensions[73][0],\n    /* OP_vpsllvq       */   &vex_W_extensions[73][1],\n    /* OP_vpsravd       */   &third_byte_38[114],\n    /* OP_vpsrlvd       */   &vex_W_extensions[72][0],\n    /* OP_vpsrlvq       */   &vex_W_extensions[72][1],\n    /* OP_vpbroadcastb  */   &third_byte_38[116],\n    /* OP_vpbroadcastw  */   &third_byte_38[117],\n    /* OP_vpbroadcastd  */   &third_byte_38[118],\n    /* OP_vpbroadcastq  */   &third_byte_38[119],\n\n    /* added in Intel Skylake */\n    /* OP_xsavec32      */   &rex_w_extensions[5][0],\n    /* OP_xsavec64      */   &rex_w_extensions[5][1],\n\n    /* Intel ADX */\n    /* OP_adox          */   &prefix_extensions[143][1],\n    /* OP_adcx          */   &prefix_extensions[143][2],\n\n    /* AVX-512 VEX encoded (scalar opmask instructions) */\n    /* OP_kmovw         */  &vex_W_extensions[74][0],\n    /* OP_kmovb         */  &vex_W_extensions[75][0],\n    /* OP_kmovq         */  &vex_W_extensions[74][1],\n    /* OP_kmovd         */  &vex_W_extensions[75][1],\n    /* OP_kandw         */  &vex_W_extensions[82][0],\n    /* OP_kandb         */  &vex_W_extensions[83][0],\n    /* OP_kandq         */  &vex_W_extensions[82][1],\n    /* OP_kandd         */  &vex_W_extensions[83][1],\n    /* OP_kandnw        */  &vex_W_extensions[84][0],\n    /* OP_kandnb        */  &vex_W_extensions[85][0],\n    /* OP_kandnq        */  &vex_W_extensions[84][1],\n    /* OP_kandnd        */  &vex_W_extensions[85][1],\n    /* OP_kunpckbw      */  &vex_W_extensions[87][0],\n    /* OP_kunpckwd      */  &vex_W_extensions[86][0],\n    /* OP_kunpckdq      */  &vex_W_extensions[86][1],\n    /* OP_knotw         */  &vex_W_extensions[88][0],\n    /* OP_knotb         */  &vex_W_extensions[89][0],\n    /* OP_knotq         */  &vex_W_extensions[88][1],\n    /* OP_knotd         */  &vex_W_extensions[89][1],\n    /* OP_korw          */  &vex_W_extensions[90][0],\n    /* OP_korb          */  &vex_W_extensions[91][0],\n    /* OP_korq          */  &vex_W_extensions[90][1],\n    /* OP_kord          */  &vex_W_extensions[91][1],\n    /* OP_kxnorw        */  &vex_W_extensions[92][0],\n    /* OP_kxnorb        */  &vex_W_extensions[93][0],\n    /* OP_kxnorq        */  &vex_W_extensions[92][1],\n    /* OP_kxnord        */  &vex_W_extensions[93][1],\n    /* OP_kxorw         */  &vex_W_extensions[94][0],\n    /* OP_kxorb         */  &vex_W_extensions[95][0],\n    /* OP_kxorq         */  &vex_W_extensions[94][1],\n    /* OP_kxord         */  &vex_W_extensions[95][1],\n    /* OP_kaddw         */  &vex_W_extensions[96][0],\n    /* OP_kaddb         */  &vex_W_extensions[97][0],\n    /* OP_kaddq         */  &vex_W_extensions[96][1],\n    /* OP_kaddd         */  &vex_W_extensions[97][1],\n    /* OP_kortestw      */  &vex_W_extensions[98][0],\n    /* OP_kortestb      */  &vex_W_extensions[99][0],\n    /* OP_kortestq      */  &vex_W_extensions[98][1],\n    /* OP_kortestd      */  &vex_W_extensions[99][1],\n    /* OP_kshiftlw      */  &vex_W_extensions[100][1],\n    /* OP_kshiftlb      */  &vex_W_extensions[100][0],\n    /* OP_kshiftlq      */  &vex_W_extensions[101][1],\n    /* OP_kshiftld      */  &vex_W_extensions[101][0],\n    /* OP_kshiftrw      */  &vex_W_extensions[102][1],\n    /* OP_kshiftrb      */  &vex_W_extensions[102][0],\n    /* OP_kshiftrq      */  &vex_W_extensions[103][1],\n    /* OP_kshiftrd      */  &vex_W_extensions[103][0],\n    /* OP_ktestw        */  &vex_W_extensions[104][0],\n    /* OP_ktestb        */  &vex_W_extensions[105][0],\n    /* OP_ktestq        */  &vex_W_extensions[104][1],\n    /* OP_ktestd        */  &vex_W_extensions[105][1],\n\n    /* AVX-512 EVEX encoded */\n    /* TODO i#1312. */\n};\n\n\n/****************************************************************************\n * Macros to make tables legible\n */\n\n/* Jb is defined in dynamo.h, undefine it for this file */\n#undef Jb\n\n#define xx  TYPE_NONE, OPSZ_NA\n\n/* from Intel tables, using our corresponding OPSZ constants */\n#define Ap  TYPE_A, OPSZ_6_irex10_short4 /* NOTE - not legal for 64-bit instructions */\n#define By  TYPE_B, OPSZ_4_rex8\n#define Cr  TYPE_C, OPSZ_4x8\n#define Dr  TYPE_D, OPSZ_4x8\n#define Eb  TYPE_E, OPSZ_1\n#define Ew  TYPE_E, OPSZ_2\n#define Ev  TYPE_E, OPSZ_4_rex8_short2\n#define Esv TYPE_E, OPSZ_4x8_short2 /* \"stack v\", or \"d64\" in Intel tables */\n#define Ed  TYPE_E, OPSZ_4\n#define Ep  TYPE_E, OPSZ_6_irex10_short4\n#define Ed_q TYPE_E, OPSZ_4_rex8\n#define Ey  TYPE_E, OPSZ_4_rex8\n#define Rd_Mb TYPE_E, OPSZ_1_reg4\n#define Rd_Mw TYPE_E, OPSZ_2_reg4\n#define Gb  TYPE_G, OPSZ_1\n#define Gw  TYPE_G, OPSZ_2\n#define Gv  TYPE_G, OPSZ_4_rex8_short2\n#define Gz  TYPE_G, OPSZ_4_short2\n#define Gd  TYPE_G, OPSZ_4\n#define Gd_q TYPE_G, OPSZ_4_rex8\n#define Gr  TYPE_G, OPSZ_4x8\n#define Gy  TYPE_G, OPSZ_4_rex8\n#define Ib  TYPE_I, OPSZ_1\n#define Iw  TYPE_I, OPSZ_2\n#define Id  TYPE_I, OPSZ_4\n#define Iv  TYPE_I, OPSZ_4_rex8_short2\n#define Iz  TYPE_I, OPSZ_4_short2\n#define Jb  TYPE_J, OPSZ_1\n#define Jz  TYPE_J, OPSZ_4_short2xi4\n#define Ma  TYPE_M, OPSZ_8_short4\n#define Mp  TYPE_M, OPSZ_6_irex10_short4\n#define Ms  TYPE_M, OPSZ_6x10\n#define Ob  TYPE_O, OPSZ_1\n#define Ov  TYPE_O, OPSZ_4_rex8_short2\n#define Pd  TYPE_P, OPSZ_4\n#define Pq  TYPE_P, OPSZ_8\n#define Pw_q TYPE_P, OPSZ_2_of_8\n#define Pd_q TYPE_P, OPSZ_4_of_8\n#define Ppi TYPE_P, OPSZ_8\n#define Nw_q  TYPE_P_MODRM, OPSZ_2_of_8\n#define Nq  TYPE_P_MODRM, OPSZ_8\n#define Qd  TYPE_Q, OPSZ_4\n#define Qq  TYPE_Q, OPSZ_8\n#define Qpi TYPE_Q, OPSZ_8\n#define Rr  TYPE_R, OPSZ_4x8\n#define Rv  TYPE_R, OPSZ_4_rex8_short2\n#define Ry  TYPE_R, OPSZ_4_rex8\n#define Sw  TYPE_S, OPSZ_2\n#define Vq  TYPE_V, OPSZ_8\n#define Vdq TYPE_V, OPSZ_16\n#define Vb_dq TYPE_V, OPSZ_1_of_16\n#define Vw_dq TYPE_V, OPSZ_2_of_16\n#define Vd_dq TYPE_V, OPSZ_4_of_16\n#define Vd_q_dq TYPE_V, OPSZ_4_rex8_of_16\n#define Vq_dq TYPE_V, OPSZ_8_of_16\n#define Vps TYPE_V, OPSZ_16\n#define Vpd TYPE_V, OPSZ_16\n#define Vss TYPE_V, OPSZ_4_of_16\n#define Vsd TYPE_V, OPSZ_8_of_16\n#define Ups TYPE_V_MODRM, OPSZ_16\n#define Upd TYPE_V_MODRM, OPSZ_16\n#define Udq TYPE_V_MODRM, OPSZ_16\n#define Uw_dq TYPE_V_MODRM, OPSZ_2_of_16\n#define Uq_dq TYPE_V_MODRM, OPSZ_8_of_16\n#define Wq  TYPE_W, OPSZ_8\n#define Wdq TYPE_W, OPSZ_16\n#define Wb_dq TYPE_W, OPSZ_1_of_16\n#define Ww_dq TYPE_W, OPSZ_2_of_16\n#define Wd_dq TYPE_W, OPSZ_4_of_16\n#define Wq_dq TYPE_W, OPSZ_8_of_16\n#define Wps TYPE_W, OPSZ_16\n#define Wpd TYPE_W, OPSZ_16\n#define Wss TYPE_W, OPSZ_4_of_16\n#define Wsd TYPE_W, OPSZ_8_of_16\n#define Udq_Md TYPE_W, OPSZ_4_reg16\n#define Xb  TYPE_X, OPSZ_1\n#define Xv  TYPE_X, OPSZ_4_rex8_short2\n#define Xz  TYPE_X, OPSZ_4_short2\n#define Yb  TYPE_Y, OPSZ_1\n#define Yv  TYPE_Y, OPSZ_4_rex8_short2\n#define Yz  TYPE_Y, OPSZ_4_short2\n\n/* AVX additions */\n#define Vvs TYPE_V, OPSZ_16_vex32\n#define Vvd TYPE_V, OPSZ_16_vex32\n#define Vx TYPE_V, OPSZ_16_vex32\n#define Vqq TYPE_V, OPSZ_32\n#define Vdq_qq TYPE_V, OPSZ_16_of_32\n#define Wvs TYPE_W, OPSZ_16_vex32\n#define Wvd TYPE_W, OPSZ_16_vex32\n#define Wx TYPE_W, OPSZ_16_vex32\n#define Uvs TYPE_V_MODRM, OPSZ_16_vex32\n#define Uvd TYPE_V_MODRM, OPSZ_16_vex32\n#define Uss TYPE_V_MODRM, OPSZ_4_of_16\n#define Usd TYPE_V_MODRM, OPSZ_8_of_16\n#define Ux TYPE_V_MODRM, OPSZ_16_vex32\n#define Udq TYPE_V_MODRM, OPSZ_16\n#define Hvs TYPE_H, OPSZ_16_vex32\n#define Hvd TYPE_H, OPSZ_16_vex32\n#define Hss TYPE_H, OPSZ_4_of_16\n#define Hsd TYPE_H, OPSZ_8_of_16\n#define Hq_dq TYPE_H, OPSZ_8_of_16\n#define Hdq TYPE_H, OPSZ_16\n#define H12_dq TYPE_H, OPSZ_12_of_16\n#define H12_8_dq TYPE_H, OPSZ_12_rex8_of_16\n#define H14_dq TYPE_H, OPSZ_14_of_16\n#define H15_dq TYPE_H, OPSZ_15_of_16\n#define Hqq TYPE_H, OPSZ_32\n#define Hx TYPE_H, OPSZ_16_vex32\n#define Wvq_dq TYPE_W, OPSZ_8_of_16_vex32\n#define Wqq TYPE_W, OPSZ_32\n#define Mvs TYPE_M, OPSZ_16_vex32\n#define Mvd TYPE_M, OPSZ_16_vex32\n#define Mx TYPE_M, OPSZ_16_vex32\n#define Ldq TYPE_L, OPSZ_16 /* immed is 1 byte but reg is xmm */\n#define Lx TYPE_L, OPSZ_16_vex32 /* immed is 1 byte but reg is xmm/ymm */\n#define Lvs TYPE_L, OPSZ_16_vex32 /* immed is 1 byte but reg is xmm/ymm */\n#define Lss TYPE_L, OPSZ_4_of_16 /* immed is 1 byte but reg is xmm/ymm */\n#define Lsd TYPE_L, OPSZ_8_of_16 /* immed is 1 byte but reg is xmm/ymm */\n\n/* AVX-512 additions */\n#define KPb TYPE_K_REG, OPSZ_1\n#define KPw TYPE_K_REG, OPSZ_2\n#define KPd TYPE_K_REG, OPSZ_4\n#define KPq TYPE_K_REG, OPSZ_8\n#define KRb TYPE_K_MODRM_R, OPSZ_1\n#define KRw TYPE_K_MODRM_R, OPSZ_2\n#define KRd TYPE_K_MODRM_R, OPSZ_4\n#define KRq TYPE_K_MODRM_R, OPSZ_8\n#define KQb TYPE_K_MODRM, OPSZ_1\n#define KQw TYPE_K_MODRM, OPSZ_2\n#define KQd TYPE_K_MODRM, OPSZ_4\n#define KQq TYPE_K_MODRM, OPSZ_8\n#define KVb TYPE_K_VEX, OPSZ_1\n#define KVw TYPE_K_VEX, OPSZ_2\n#define KVd TYPE_K_VEX, OPSZ_4\n#define KVq TYPE_K_VEX, OPSZ_8\n#define KEb TYPE_K_EVEX, OPSZ_1\n#define KEw TYPE_K_EVEX, OPSZ_2\n#define KEd TYPE_K_EVEX, OPSZ_4\n#define KEq TYPE_K_EVEX, OPSZ_8\n#define Ves TYPE_V, OPSZ_16_vex32_evex64\n#define Ved TYPE_V, OPSZ_16_vex32_evex64\n#define Wes TYPE_W, OPSZ_16_vex32_evex64\n#define Wed TYPE_W, OPSZ_16_vex32_evex64\n\n/* my own codes\n * size m = 32 or 16 bit depending on addr size attribute\n * B=ds:eDI, Z=xlat's mem, K=float in mem, i_==indirect\n */\n#define Mb  TYPE_M, OPSZ_1\n#define Md  TYPE_M, OPSZ_4\n#define Md_q  TYPE_M, OPSZ_4_rex8\n#define Mw  TYPE_M, OPSZ_2\n#define Mm  TYPE_M, OPSZ_lea\n#define Me  TYPE_M, OPSZ_512\n#define Mxsave TYPE_M, OPSZ_xsave\n#define Mps  TYPE_M, OPSZ_16\n#define Mpd  TYPE_M, OPSZ_16\n#define Mss  TYPE_M, OPSZ_4\n#define Msd  TYPE_M, OPSZ_8\n#define Mq  TYPE_M, OPSZ_8\n#define Mdq  TYPE_M, OPSZ_16\n#define Mq_dq TYPE_M, OPSZ_8_rex16\n#define Mv  TYPE_M, OPSZ_4_rex8_short2\n#define MVd TYPE_VSIB, OPSZ_4\n#define MVq TYPE_VSIB, OPSZ_8\n#define Zb  TYPE_XLAT, OPSZ_1\n#define Bq  TYPE_MASKMOVQ, OPSZ_8\n#define Bdq  TYPE_MASKMOVQ, OPSZ_16\n#define Fw  TYPE_FLOATMEM, OPSZ_2\n#define Fd  TYPE_FLOATMEM, OPSZ_4\n#define Fq  TYPE_FLOATMEM, OPSZ_8\n#define Fx  TYPE_FLOATMEM, OPSZ_10\n#define Fy  TYPE_FLOATMEM, OPSZ_28_short14 /* _14_ if data16 */\n#define Fz  TYPE_FLOATMEM, OPSZ_108_short94 /* _98_ if data16 */\n#define i_dx  TYPE_INDIR_REG, REG_DX\n#define i_Ev  TYPE_INDIR_E, OPSZ_4_rex8_short2\n#define i_Exi  TYPE_INDIR_E, OPSZ_4x8_short2xi8\n#define i_Ep  TYPE_INDIR_E, OPSZ_6_irex10_short4\n#define i_xSP TYPE_INDIR_VAR_XREG, REG_ESP\n#define i_iSP TYPE_INDIR_VAR_XIREG, REG_ESP\n#define i_xBP TYPE_INDIR_VAR_XREG, REG_EBP\n/* negative offset from (%xsp) for pushes */\n#define i_iSPo1 TYPE_INDIR_VAR_XIREG_OFFS_1, REG_ESP\n#define i_vSPo2 TYPE_INDIR_VAR_REG_OFFS_2, REG_ESP\n#define i_xSPo1 TYPE_INDIR_VAR_XREG_OFFS_1, REG_ESP\n#define i_xSPo8 TYPE_INDIR_VAR_XREG_OFFS_8, REG_ESP\n#define i_xSPs8 TYPE_INDIR_VAR_XREG_SIZEx8, REG_ESP\n#define i_vSPs2 TYPE_INDIR_VAR_REG_SIZEx2, REG_ESP\n#define i_vSPs3 TYPE_INDIR_VAR_REG_SIZEx3x5, REG_ESP\n/* pop but unusual size */\n#define i_xSPoN TYPE_INDIR_VAR_XREG_OFFS_N, REG_ESP\n#define c1  TYPE_1, OPSZ_0\n/* we pick the right constant based on the opcode */\n#define cF  TYPE_FLOATCONST, OPSZ_0\n\n/* registers that are base 32 but vary down or up */\n#define eAX TYPE_VAR_REG, REG_EAX\n#define eCX TYPE_VAR_REG, REG_ECX\n#define eDX TYPE_VAR_REG, REG_EDX\n#define eBX TYPE_VAR_REG, REG_EBX\n#define eSP TYPE_VAR_REG, REG_ESP\n#define eBP TYPE_VAR_REG, REG_EBP\n#define eSI TYPE_VAR_REG, REG_ESI\n#define eDI TYPE_VAR_REG, REG_EDI\n\n/* registers that are base 32 and can vary down but not up */\n#define zAX TYPE_VARZ_REG, REG_EAX\n#define zCX TYPE_VARZ_REG, REG_ECX\n#define zDX TYPE_VARZ_REG, REG_EDX\n#define zBX TYPE_VARZ_REG, REG_EBX\n#define zSP TYPE_VARZ_REG, REG_ESP\n#define zBP TYPE_VARZ_REG, REG_EBP\n#define zSI TYPE_VARZ_REG, REG_ESI\n#define zDI TYPE_VARZ_REG, REG_EDI\n\n/* registers whose base matches the mode, and can vary down but not up.\n * we use the 32-bit versions but expand in resolve_var_reg()\n */\n#define xAX TYPE_VAR_XREG, REG_EAX\n#define xCX TYPE_VAR_XREG, REG_ECX\n#define xDX TYPE_VAR_XREG, REG_EDX\n#define xBX TYPE_VAR_XREG, REG_EBX\n#define xSP TYPE_VAR_XREG, REG_ESP\n#define xBP TYPE_VAR_XREG, REG_EBP\n#define xSI TYPE_VAR_XREG, REG_ESI\n#define xDI TYPE_VAR_XREG, REG_EDI\n\n/* jecxz and loop* vary by addr16 */\n#define axCX TYPE_VAR_ADDR_XREG, REG_ECX\n/* string ops also use addr16 */\n#define axSI TYPE_VAR_ADDR_XREG, REG_ESI\n#define axDI TYPE_VAR_ADDR_XREG, REG_EDI\n#define axAX TYPE_VAR_ADDR_XREG, REG_EAX\n\n/* 8-bit implicit registers (not from modrm) that can be exteded via rex.r */\n#define al_x TYPE_REG_EX, REG_AL\n#define cl_x TYPE_REG_EX, REG_CL\n#define dl_x TYPE_REG_EX, REG_DL\n#define bl_x TYPE_REG_EX, REG_BL\n#define ah_x TYPE_REG_EX, REG_AH\n#define ch_x TYPE_REG_EX, REG_CH\n#define dh_x TYPE_REG_EX, REG_DH\n#define bh_x TYPE_REG_EX, REG_BH\n\n/* 4_rex8_short2 implicit registers (not from modrm) that can be exteded via rex.r */\n#define eAX_x TYPE_VAR_REG_EX, REG_EAX\n#define eCX_x TYPE_VAR_REG_EX, REG_ECX\n#define eDX_x TYPE_VAR_REG_EX, REG_EDX\n#define eBX_x TYPE_VAR_REG_EX, REG_EBX\n#define eSP_x TYPE_VAR_REG_EX, REG_ESP\n#define eBP_x TYPE_VAR_REG_EX, REG_EBP\n#define eSI_x TYPE_VAR_REG_EX, REG_ESI\n#define eDI_x TYPE_VAR_REG_EX, REG_EDI\n\n/* 4x8_short2 implicit registers (not from modrm) that can be exteded via rex.r */\n#define xAX_x TYPE_VAR_XREG_EX, REG_EAX\n#define xCX_x TYPE_VAR_XREG_EX, REG_ECX\n#define xDX_x TYPE_VAR_XREG_EX, REG_EDX\n#define xBX_x TYPE_VAR_XREG_EX, REG_EBX\n#define xSP_x TYPE_VAR_XREG_EX, REG_ESP\n#define xBP_x TYPE_VAR_XREG_EX, REG_EBP\n#define xSI_x TYPE_VAR_XREG_EX, REG_ESI\n#define xDI_x TYPE_VAR_XREG_EX, REG_EDI\n\n/* 4_rex8 implicit registers (not from modrm) that can be exteded via rex.r */\n#define uAX_x TYPE_VAR_REGX_EX, REG_EAX\n#define uCX_x TYPE_VAR_REGX_EX, REG_ECX\n#define uDX_x TYPE_VAR_REGX_EX, REG_EDX\n#define uBX_x TYPE_VAR_REGX_EX, REG_EBX\n#define uSP_x TYPE_VAR_REGX_EX, REG_ESP\n#define uBP_x TYPE_VAR_REGX_EX, REG_EBP\n#define uSI_x TYPE_VAR_REGX_EX, REG_ESI\n#define uDI_x TYPE_VAR_REGX_EX, REG_EDI\n\n/* 4_rex8 implicit registers (not from modrm) */\n#define uDX TYPE_VAR_REGX, REG_EDX\n\n#define ax TYPE_REG, REG_AX\n#define cx TYPE_REG, REG_CX\n#define dx TYPE_REG, REG_DX\n#define bx TYPE_REG, REG_BX\n#define sp TYPE_REG, REG_SP\n#define bp TYPE_REG, REG_BP\n#define si TYPE_REG, REG_SI\n#define di TYPE_REG, REG_DI\n\n#define al TYPE_REG, REG_AL\n#define cl TYPE_REG, REG_CL\n#define dl TYPE_REG, REG_DL\n#define bl TYPE_REG, REG_BL\n#define ah TYPE_REG, REG_AH\n#define ch TYPE_REG, REG_CH\n#define dh TYPE_REG, REG_DH\n#define bh TYPE_REG, REG_BH\n\n#define eax TYPE_REG, REG_EAX\n#define ecx TYPE_REG, REG_ECX\n#define edx TYPE_REG, REG_EDX\n#define ebx TYPE_REG, REG_EBX\n#define esp TYPE_REG, REG_ESP\n#define ebp TYPE_REG, REG_EBP\n#define esi TYPE_REG, REG_ESI\n#define edi TYPE_REG, REG_EDI\n\n#define xsp TYPE_XREG, REG_ESP\n#define xbp TYPE_XREG, REG_EBP\n#define xcx TYPE_XREG, REG_ECX\n\n#define cs  TYPE_REG, SEG_CS\n#define ss  TYPE_REG, SEG_SS\n#define ds  TYPE_REG, SEG_DS\n#define es  TYPE_REG, SEG_ES\n#define fs  TYPE_REG, SEG_FS\n#define gs  TYPE_REG, SEG_GS\n\n#define st0 TYPE_REG, REG_ST0\n#define st1 TYPE_REG, REG_ST1\n#define st2 TYPE_REG, REG_ST2\n#define st3 TYPE_REG, REG_ST3\n#define st4 TYPE_REG, REG_ST4\n#define st5 TYPE_REG, REG_ST5\n#define st6 TYPE_REG, REG_ST6\n#define st7 TYPE_REG, REG_ST7\n\n#define xmm0 TYPE_REG, REG_XMM0\n\n/* flags */\n#define no       0\n#define mrm      HAS_MODRM\n#define xop      (HAS_EXTRA_OPERANDS|EXTRAS_IN_CODE_FIELD)\n#define mrm_xop  (HAS_MODRM|HAS_EXTRA_OPERANDS|EXTRAS_IN_CODE_FIELD)\n#define xop_next (HAS_EXTRA_OPERANDS)\n#define i64      X64_INVALID\n#define o64      X86_INVALID\n#define reqp     REQUIRES_PREFIX\n#define vex      REQUIRES_VEX\n#define rex      REQUIRES_REX\n#define reqL0    REQUIRES_VEX_L_0\n#define reqL1    REQUIRES_VEX_L_1\n#define predcc   HAS_PRED_CC\n#define predcx   HAS_PRED_COMPLEX\n#define evex     REQUIRES_EVEX\n#define reqLL0   REQUIRES_EVEX_LL_0\n#define reqLL1   REQUIRES_EVEX_LL_1\n\n/* eflags */\n#define x     0\n#define fRC   EFLAGS_READ_CF\n#define fRP   EFLAGS_READ_PF\n#define fRA   EFLAGS_READ_AF\n#define fRZ   EFLAGS_READ_ZF\n#define fRS   EFLAGS_READ_SF\n#define fRT   EFLAGS_READ_TF\n#define fRI   EFLAGS_READ_IF\n#define fRD   EFLAGS_READ_DF\n#define fRO   EFLAGS_READ_OF\n#define fRN   EFLAGS_READ_NT\n#define fRR   EFLAGS_READ_RF\n#define fRX   EFLAGS_READ_ALL\n#define fR6   EFLAGS_READ_6\n#define fWC   EFLAGS_WRITE_CF\n#define fWP   EFLAGS_WRITE_PF\n#define fWA   EFLAGS_WRITE_AF\n#define fWZ   EFLAGS_WRITE_ZF\n#define fWS   EFLAGS_WRITE_SF\n#define fWT   EFLAGS_WRITE_TF\n#define fWI   EFLAGS_WRITE_IF\n#define fWD   EFLAGS_WRITE_DF\n#define fWO   EFLAGS_WRITE_OF\n#define fWN   EFLAGS_WRITE_NT\n#define fWR   EFLAGS_WRITE_RF\n#define fWX   EFLAGS_WRITE_ALL\n#define fW6   EFLAGS_WRITE_6\n/* flags affected by OP_int*\n * FIXME: should we add AC and VM flags?\n */\n#define fINT  (fRX|fWT|fWN|fWI|fWR)\n\n/* for constructing linked lists of table entries */\n#define NA 0\n#define END_LIST  0\n#define tfb (ptr_int_t)&first_byte\n#define tsb (ptr_int_t)&second_byte\n#define tex (ptr_int_t)&base_extensions\n#define t38 (ptr_int_t)&third_byte_38\n#define t3a (ptr_int_t)&third_byte_3a\n#define tpe (ptr_int_t)&prefix_extensions\n#define tvex (ptr_int_t)&e_vex_extensions\n#define modx (ptr_int_t)&mod_extensions\n#define tre (ptr_int_t)&rep_extensions\n#define tne (ptr_int_t)&repne_extensions\n#define tfl (ptr_int_t)&float_low_modrm\n#define tfh (ptr_int_t)&float_high_modrm\n#define exop (ptr_int_t)&extra_operands\n#define t64e (ptr_int_t)&x64_extensions\n#define trexb (ptr_int_t)&rex_b_extensions\n#define trexw (ptr_int_t)&rex_w_extensions\n#define tvex (ptr_int_t)&e_vex_extensions\n#define tvexw (ptr_int_t)&vex_W_extensions\n#define txop (ptr_int_t)&xop_extensions\n#define tevexw (ptr_int_t)&evex_W_extensions\n\n/****************************************************************************\n * One-byte opcodes\n * This is from Tables A-2 & A-3\n */\nconst instr_info_t first_byte[] = {\n    /* {op/type, op encoding, name, dst1, dst2, src1, src2, src3, modrm?, eflags, code} */\n    /* 00 */\n    {OP_add,  0x000000, \"add\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][0]},\n    {OP_add,  0x010000, \"add\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x00]},\n    {OP_add,  0x020000, \"add\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x01]},\n    {OP_add,  0x030000, \"add\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x02]},\n    {OP_add,  0x040000, \"add\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x03]},\n    {OP_add,  0x050000, \"add\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x04]},\n    {OP_push, 0x060000, \"push\", xsp, i_xSPo1, es, xsp, xx, i64, x, tfb[0x0e]},\n    {OP_pop,  0x070000, \"pop\", es, xsp, xsp, i_xSP, xx, i64, x, tsb[0xa1]},\n    /* 08 */\n    {OP_or,  0x080000, \"or\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][1]},\n    {OP_or,  0x090000, \"or\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x08]},\n    {OP_or,  0x0a0000, \"or\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x09]},\n    {OP_or,  0x0b0000, \"or\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x0a]},\n    {OP_or,  0x0c0000, \"or\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x0b]},\n    {OP_or,  0x0d0000, \"or\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x0c]},\n    {OP_push,0x0e0000, \"push\", xsp, i_xSPo1, cs, xsp, xx, i64, x, tfb[0x16]},\n    {ESCAPE, 0x0f0000, \"(escape)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* 10 */\n    {OP_adc,  0x100000, \"adc\",  Eb, xx, Gb, Eb, xx, mrm, (fW6|fRC), tex[1][2]},\n    {OP_adc,  0x110000, \"adc\",  Ev, xx, Gv, Ev, xx, mrm, (fW6|fRC), tfb[0x10]},\n    {OP_adc,  0x120000, \"adc\",  Gb, xx, Eb, Gb, xx, mrm, (fW6|fRC), tfb[0x11]},\n    {OP_adc,  0x130000, \"adc\",  Gv, xx, Ev, Gv, xx, mrm, (fW6|fRC), tfb[0x12]},\n    {OP_adc,  0x140000, \"adc\",  al, xx, Ib, al, xx, no,  (fW6|fRC), tfb[0x13]},\n    {OP_adc,  0x150000, \"adc\", eAX, xx, Iz, eAX, xx, no,  (fW6|fRC), tfb[0x14]},\n    {OP_push, 0x160000, \"push\", xsp, i_xSPo1, ss, xsp, xx, i64, x, tfb[0x1e]},\n    {OP_pop,  0x170000, \"pop\", ss, xsp, xsp, i_xSP, xx, i64, x, tfb[0x1f]},\n    /* 18 */\n    {OP_sbb,  0x180000, \"sbb\",  Eb, xx, Gb, Eb, xx, mrm, (fW6|fRC), tex[1][3]},\n    {OP_sbb,  0x190000, \"sbb\",  Ev, xx, Gv, Ev, xx, mrm, (fW6|fRC), tfb[0x18]},\n    {OP_sbb,  0x1a0000, \"sbb\",  Gb, xx, Eb, Gb, xx, mrm, (fW6|fRC), tfb[0x19]},\n    {OP_sbb,  0x1b0000, \"sbb\",  Gv, xx, Ev, Gv, xx, mrm, (fW6|fRC), tfb[0x1a]},\n    {OP_sbb,  0x1c0000, \"sbb\",  al, xx, Ib, al, xx, no,  (fW6|fRC), tfb[0x1b]},\n    {OP_sbb,  0x1d0000, \"sbb\", eAX, xx, Iz, eAX, xx, no,  (fW6|fRC), tfb[0x1c]},\n    {OP_push, 0x1e0000, \"push\", xsp, i_xSPo1, ds, xsp, xx, i64, x, tsb[0xa0]},\n    {OP_pop,  0x1f0000, \"pop\", ds, xsp, xsp, i_xSP, xx, i64, x, tfb[0x07]},\n    /* 20 */\n    {OP_and,  0x200000, \"and\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][4]},\n    {OP_and,  0x210000, \"and\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x20]},\n    {OP_and,  0x220000, \"and\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x21]},\n    {OP_and,  0x230000, \"and\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x22]},\n    {OP_and,  0x240000, \"and\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x23]},\n    {OP_and,  0x250000, \"and\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x24]},\n    {PREFIX,  0x260000, \"es\", xx, xx, xx, xx, xx, no, x, SEG_ES},\n    {OP_daa,  0x270000, \"daa\", al, xx, al, xx, xx, i64, (fW6|fRC|fRA), END_LIST},\n    /* 28 */\n    {OP_sub,  0x280000, \"sub\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][5]},\n    {OP_sub,  0x290000, \"sub\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x28]},\n    {OP_sub,  0x2a0000, \"sub\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x29]},\n    {OP_sub,  0x2b0000, \"sub\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x2a]},\n    {OP_sub,  0x2c0000, \"sub\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x2b]},\n    {OP_sub,  0x2d0000, \"sub\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x2c]},\n    {PREFIX,  0x2e0000, \"cs\", xx, xx, xx, xx, xx, no, x, SEG_CS},\n    {OP_das,  0x2f0000, \"das\", al, xx, al, xx, xx, i64, (fW6|fRC|fRA), END_LIST},\n    /* 30 */\n    {OP_xor,  0x300000, \"xor\",  Eb, xx, Gb, Eb, xx, mrm, fW6, tex[1][6]},\n    {OP_xor,  0x310000, \"xor\",  Ev, xx, Gv, Ev, xx, mrm, fW6, tfb[0x30]},\n    {OP_xor,  0x320000, \"xor\",  Gb, xx, Eb, Gb, xx, mrm, fW6, tfb[0x31]},\n    {OP_xor,  0x330000, \"xor\",  Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x32]},\n    {OP_xor,  0x340000, \"xor\",  al, xx, Ib, al, xx, no,  fW6, tfb[0x33]},\n    {OP_xor,  0x350000, \"xor\", eAX, xx, Iz, eAX, xx, no,  fW6, tfb[0x34]},\n    {PREFIX,  0x360000, \"ss\", xx, xx, xx, xx, xx, no, x, SEG_SS},\n    {OP_aaa,  0x370000, \"aaa\", ax, xx, ax, xx, xx, i64, (fW6|fRA), END_LIST},\n    /* 38 */\n    {OP_cmp,  0x380000, \"cmp\", xx, xx,  Eb, Gb, xx, mrm, fW6, tex[1][7]},\n    {OP_cmp,  0x390000, \"cmp\", xx, xx,  Ev, Gv, xx, mrm, fW6, tfb[0x38]},\n    {OP_cmp,  0x3a0000, \"cmp\", xx, xx,  Gb, Eb, xx, mrm, fW6, tfb[0x39]},\n    {OP_cmp,  0x3b0000, \"cmp\", xx, xx,  Gv, Ev, xx, mrm, fW6, tfb[0x3a]},\n    {OP_cmp,  0x3c0000, \"cmp\", xx, xx,  al, Ib, xx, no,  fW6, tfb[0x3b]},\n    {OP_cmp,  0x3d0000, \"cmp\", xx, xx, eAX, Iz, xx, no,  fW6, tfb[0x3c]},\n    {PREFIX,  0x3e0000, \"ds\", xx, xx, xx, xx, xx, no, x, SEG_DS},\n    {OP_aas,  0x3f0000, \"aas\", ax, xx, ax, xx, xx, i64, (fW6|fRA), END_LIST},\n    /* 40 */\n    {X64_EXT, 0x400000, \"(x64_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {X64_EXT, 0x410000, \"(x64_ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {X64_EXT, 0x420000, \"(x64_ext 2)\", xx, xx, xx, xx, xx, no, x, 2},\n    {X64_EXT, 0x430000, \"(x64_ext 3)\", xx, xx, xx, xx, xx, no, x, 3},\n    {X64_EXT, 0x440000, \"(x64_ext 4)\", xx, xx, xx, xx, xx, no, x, 4},\n    {X64_EXT, 0x450000, \"(x64_ext 5)\", xx, xx, xx, xx, xx, no, x, 5},\n    {X64_EXT, 0x460000, \"(x64_ext 6)\", xx, xx, xx, xx, xx, no, x, 6},\n    {X64_EXT, 0x470000, \"(x64_ext 7)\", xx, xx, xx, xx, xx, no, x, 7},\n    /* 48 */\n    {X64_EXT, 0x480000, \"(x64_ext 8)\", xx, xx, xx, xx, xx, no, x, 8},\n    {X64_EXT, 0x490000, \"(x64_ext 9)\", xx, xx, xx, xx, xx, no, x, 9},\n    {X64_EXT, 0x4a0000, \"(x64_ext 10)\", xx, xx, xx, xx, xx, no, x, 10},\n    {X64_EXT, 0x4b0000, \"(x64_ext 11)\", xx, xx, xx, xx, xx, no, x, 11},\n    {X64_EXT, 0x4c0000, \"(x64_ext 12)\", xx, xx, xx, xx, xx, no, x, 12},\n    {X64_EXT, 0x4d0000, \"(x64_ext 13)\", xx, xx, xx, xx, xx, no, x, 13},\n    {X64_EXT, 0x4e0000, \"(x64_ext 14)\", xx, xx, xx, xx, xx, no, x, 14},\n    {X64_EXT, 0x4f0000, \"(x64_ext 15)\", xx, xx, xx, xx, xx, no, x, 15},\n    /* 50 */\n    {OP_push,  0x500000, \"push\", xsp, i_xSPo1, xAX_x, xsp, xx, no, x, tfb[0x51]},\n    {OP_push,  0x510000, \"push\", xsp, i_xSPo1, xCX_x, xsp, xx, no, x, tfb[0x52]},\n    {OP_push,  0x520000, \"push\", xsp, i_xSPo1, xDX_x, xsp, xx, no, x, tfb[0x53]},\n    {OP_push,  0x530000, \"push\", xsp, i_xSPo1, xBX_x, xsp, xx, no, x, tfb[0x54]},\n    {OP_push,  0x540000, \"push\", xsp, i_xSPo1, xSP_x, xsp, xx, no, x, tfb[0x55]},\n    {OP_push,  0x550000, \"push\", xsp, i_xSPo1, xBP_x, xsp, xx, no, x, tfb[0x56]},\n    {OP_push,  0x560000, \"push\", xsp, i_xSPo1, xSI_x, xsp, xx, no, x, tfb[0x57]},\n    {OP_push,  0x570000, \"push\", xsp, i_xSPo1, xDI_x, xsp, xx, no, x, tex[12][6]},\n    /* 58 */\n    {OP_pop,  0x580000, \"pop\", xAX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x59]},\n    {OP_pop,  0x590000, \"pop\", xCX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5a]},\n    {OP_pop,  0x5a0000, \"pop\", xDX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5b]},\n    {OP_pop,  0x5b0000, \"pop\", xBX_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5c]},\n    {OP_pop,  0x5c0000, \"pop\", xSP_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5d]},\n    {OP_pop,  0x5d0000, \"pop\", xBP_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5e]},\n    {OP_pop,  0x5e0000, \"pop\", xSI_x, xsp, xsp, i_xSP, xx, no, x, tfb[0x5f]},\n    {OP_pop,  0x5f0000, \"pop\", xDI_x, xsp, xsp, i_xSP, xx, no, x, tex[26][0]},\n    /* 60 */\n    {OP_pusha, 0x600000, \"pusha\", xsp, i_xSPo8, xsp, eAX, eBX, xop|i64, x, exop[0x00]},\n    {OP_popa,  0x610000, \"popa\", xsp, eAX, xsp, i_xSPs8, xx, xop|i64, x, exop[0x02]},\n    {EVEX_PREFIX_EXT, 0x620000, \"(evex_prefix_ext)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {X64_EXT,  0x630000, \"(x64_ext 16)\", xx, xx, xx, xx, xx, no, x, 16},\n    {PREFIX, 0x640000, \"fs\", xx, xx, xx, xx, xx, no, x, SEG_FS},\n    {PREFIX, 0x650000, \"gs\", xx, xx, xx, xx, xx, no, x, SEG_GS},\n    {PREFIX, 0x660000, \"data size\", xx, xx, xx, xx, xx, no, x, PREFIX_DATA},\n    {PREFIX, 0x670000, \"addr size\", xx, xx, xx, xx, xx, no, x, PREFIX_ADDR},\n    /* 68 */\n    {OP_push_imm, 0x680000, \"push\", xsp, i_xSPo1, Iz, xsp, xx, no, x, tfb[0x6a]},\n    {OP_imul,  0x690000, \"imul\", Gv, xx, Ev, Iz, xx, mrm, fW6, tfb[0x6b]},\n    {OP_push_imm, 0x6a0000, \"push\", xsp, i_xSPo1, Ib, xsp, xx, no, x, END_LIST},/* sign-extend to push 2/4/8 bytes */\n    {OP_imul,  0x6b0000, \"imul\", Gv, xx, Ev, Ib, xx, mrm, fW6, END_LIST},\n    {REP_EXT,  0x6c0000, \"((rep) ins)\", Yb, xx, i_dx, xx, xx, no, fRD, 0},\n    {REP_EXT,  0x6d0000, \"((rep) ins)\", Yz, xx, i_dx, xx, xx, no, fRD, 1},\n    {REP_EXT,  0x6e0000, \"((rep) outs)\", i_dx, xx, Xb, xx, xx, no, fRD, 2},\n    {REP_EXT,  0x6f0000, \"((rep) outs)\", i_dx, xx, Xz, xx, xx, no, fRD, 3},\n    /* 70 */\n    {OP_jo_short,  0x700000, \"jo\",  xx, xx, Jb, xx, xx, no, fRO, END_LIST},\n    {OP_jno_short, 0x710000, \"jno\", xx, xx, Jb, xx, xx, no, fRO, END_LIST},\n    {OP_jb_short,  0x720000, \"jb\",  xx, xx, Jb, xx, xx, no, fRC, END_LIST},\n    {OP_jnb_short, 0x730000, \"jnb\", xx, xx, Jb, xx, xx, no, fRC, END_LIST},\n    {OP_jz_short,  0x740000, \"jz\",  xx, xx, Jb, xx, xx, no, fRZ, END_LIST},\n    {OP_jnz_short, 0x750000, \"jnz\", xx, xx, Jb, xx, xx, no, fRZ, END_LIST},\n    {OP_jbe_short, 0x760000, \"jbe\", xx, xx, Jb, xx, xx, no, (fRC|fRZ), END_LIST},\n    {OP_jnbe_short,0x770000, \"jnbe\",xx, xx, Jb, xx, xx, no, (fRC|fRZ), END_LIST},\n    /* 78 */\n    {OP_js_short,  0x780000, \"js\",  xx, xx, Jb, xx, xx, no, fRS, END_LIST},\n    {OP_jns_short, 0x790000, \"jns\", xx, xx, Jb, xx, xx, no, fRS, END_LIST},\n    {OP_jp_short,  0x7a0000, \"jp\",  xx, xx, Jb, xx, xx, no, fRP, END_LIST},\n    {OP_jnp_short, 0x7b0000, \"jnp\", xx, xx, Jb, xx, xx, no, fRP, END_LIST},\n    {OP_jl_short,  0x7c0000, \"jl\",  xx, xx, Jb, xx, xx, no, (fRS|fRO), END_LIST},\n    {OP_jnl_short, 0x7d0000, \"jnl\", xx, xx, Jb, xx, xx, no, (fRS|fRO), END_LIST},\n    {OP_jle_short, 0x7e0000, \"jle\", xx, xx, Jb, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n    {OP_jnle_short,0x7f0000, \"jnle\",xx, xx, Jb, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n    /* 80 */\n    {EXTENSION, 0x800000, \"(group 1a)\", Eb, xx, Ib, xx, xx, mrm, x, 0},\n    {EXTENSION, 0x810000, \"(group 1b)\", Ev, xx, Iz, xx, xx, mrm, x, 1},\n    {EXTENSION, 0x820000, \"(group 1c*)\", Ev, xx, Ib, xx, xx, mrm|i64, x, 25}, /* PR 235092: gnu tools (gdb, objdump) think this is a bad opcode but windbg and the hardware disagree */\n    {EXTENSION, 0x830000, \"(group 1c)\", Ev, xx, Ib, xx, xx, mrm, x, 2},\n    {OP_test,  0x840000, \"test\", xx, xx, Eb, Gb, xx, mrm, fW6, tex[10][0]},\n    {OP_test,  0x850000, \"test\", xx, xx, Ev, Gv, xx, mrm, fW6, tfb[0x84]},\n    {OP_xchg,  0x860000, \"xchg\", Eb, Gb, Eb, Gb, xx, mrm, x, END_LIST},\n    {OP_xchg,  0x870000, \"xchg\", Ev, Gv, Ev, Gv, xx, mrm, x, tfb[0x86]},\n    /* 88 */\n    {OP_mov_st,  0x880000, \"mov\", Eb, xx, Gb, xx, xx, mrm, x, tex[18][0]},\n    {OP_mov_st,  0x890000, \"mov\", Ev, xx, Gv, xx, xx, mrm, x, tfb[0x88]},\n    {OP_mov_ld,  0x8a0000, \"mov\", Gb, xx, Eb, xx, xx, mrm, x, END_LIST},\n    {OP_mov_ld,  0x8b0000, \"mov\", Gv, xx, Ev, xx, xx, mrm, x, tfb[0x8a]},\n    {OP_mov_seg, 0x8c0000, \"mov\", Ev, xx, Sw, xx, xx, mrm, x, END_LIST},\n    {OP_lea,  0x8d0000, \"lea\", Gv, xx, Mm, xx, xx, mrm, x, END_LIST}, /* Intel has just M */\n    {OP_mov_seg, 0x8e0000, \"mov\", Sw, xx, Ev, xx, xx, mrm, x, tfb[0x8c]},\n    {XOP_PREFIX_EXT, 0x8f0000, \"(xop_prefix_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    /* 90 */\n    {PREFIX_EXT, 0x900000, \"(prefix ext 103)\", xx, xx, xx, xx, xx, no, x, 103},\n    {OP_xchg, 0x910000, \"xchg\", eCX_x, eAX, eCX_x, eAX, xx, no, x, tfb[0x92]},\n    {OP_xchg, 0x920000, \"xchg\", eDX_x, eAX, eDX_x, eAX, xx, no, x, tfb[0x93]},\n    {OP_xchg, 0x930000, \"xchg\", eBX_x, eAX, eBX_x, eAX, xx, no, x, tfb[0x94]},\n    {OP_xchg, 0x940000, \"xchg\", eSP_x, eAX, eSP_x, eAX, xx, no, x, tfb[0x95]},\n    {OP_xchg, 0x950000, \"xchg\", eBP_x, eAX, eBP_x, eAX, xx, no, x, tfb[0x96]},\n    {OP_xchg, 0x960000, \"xchg\", eSI_x, eAX, eSI_x, eAX, xx, no, x, tfb[0x97]},\n    {OP_xchg, 0x970000, \"xchg\", eDI_x, eAX, eDI_x, eAX, xx, no, x, tfb[0x87]},\n    /* 98 */\n    {OP_cwde, 0x980000, \"cwde\", eAX, xx, ax, xx, xx, no, x, END_LIST},/*16-bit==\"cbw\", src is al not ax; FIXME: newer gdb calls it \"cwtl\"?!?*/\n    /* PR 354096: does not write to ax/eax/rax: sign-extends into dx/edx/rdx */\n    {OP_cdq,  0x990000, \"cdq\", eDX, xx, eAX, xx, xx, no, x, END_LIST},/*16-bit==\"cwd\";64-bit==\"cqo\"*/\n    {OP_call_far, 0x9a0000, \"lcall\",  xsp, i_vSPo2, Ap, xsp, xx, i64, x, END_LIST},\n    {OP_fwait, 0x9b0000, \"fwait\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pushf, 0x9c0000, \"pushf\", xsp, i_xSPo1, xsp, xx, xx, no, fRX, END_LIST},\n    {OP_popf,  0x9d0000, \"popf\", xsp, xx, xsp, i_xSP, xx, no, fWX, END_LIST},\n    {OP_sahf,  0x9e0000, \"sahf\", xx, xx, ah, xx, xx, no, (fW6&(~fWO)), END_LIST},\n    {OP_lahf,  0x9f0000, \"lahf\", ah, xx, xx, xx, xx, no, (fR6&(~fRO)), END_LIST},\n    /* a0 */\n    {OP_mov_ld,  0xa00000, \"mov\", al, xx, Ob, xx, xx, no, x, tfb[0x8b]},\n    {OP_mov_ld,  0xa10000, \"mov\", eAX, xx, Ov, xx, xx, no, x, tfb[0xa0]},\n    {OP_mov_st,  0xa20000, \"mov\", Ob, xx, al, xx, xx, no, x, tfb[0x89]},\n    {OP_mov_st,  0xa30000, \"mov\", Ov, xx, eAX, xx, xx, no, x, tfb[0xa2]},\n    {REP_EXT, 0xa40000, \"((rep) movs)\", Yb, xx, Xb, xx, xx, no, fRD, 4},\n    {REP_EXT, 0xa50000, \"((rep) movs)\", Yv, xx, Xv, xx, xx, no, fRD, 5},\n    {REPNE_EXT, 0xa60000, \"((rep/ne) cmps)\", Xb, xx, Yb, xx, xx, no, (fW6|fRD|fRZ), 0},\n    {REPNE_EXT, 0xa70000, \"((rep/ne) cmps)\", Xv, xx, Yv, xx, xx, no, (fW6|fRD|fRZ), 1},\n    /* a8 */\n    {OP_test,  0xa80000, \"test\", xx, xx,  al, Ib, xx, no, fW6, tfb[0x85]},\n    {OP_test,  0xa90000, \"test\", xx, xx, eAX, Iz, xx, no, fW6, tfb[0xa8]},\n    {REP_EXT, 0xaa0000, \"((rep) stos)\", Yb, xx, al, xx, xx, no, fRD, 6},\n    {REP_EXT, 0xab0000, \"((rep) stos)\", Yv, xx, eAX, xx, xx, no, fRD, 7},\n    {REP_EXT, 0xac0000, \"((rep) lods)\", al, xx, Xb, xx, xx, no, fRD, 8},\n    {REP_EXT, 0xad0000, \"((rep) lods)\", eAX, xx, Xv, xx, xx, no, fRD, 9},\n    {REPNE_EXT, 0xae0000, \"((rep/ne) scas)\", al, xx, Yb, xx, xx, no, (fW6|fRD|fRZ), 2},\n    {REPNE_EXT, 0xaf0000, \"((rep/ne) scas)\", eAX, xx, Yv, xx, xx, no, (fW6|fRD|fRZ), 3},\n    /* b0 */\n    {OP_mov_imm, 0xb00000, \"mov\", al_x, xx, Ib, xx, xx, no, x, tfb[0xb1]},\n    {OP_mov_imm, 0xb10000, \"mov\", cl_x, xx, Ib, xx, xx, no, x, tfb[0xb2]},\n    {OP_mov_imm, 0xb20000, \"mov\", dl_x, xx, Ib, xx, xx, no, x, tfb[0xb3]},\n    {OP_mov_imm, 0xb30000, \"mov\", bl_x, xx, Ib, xx, xx, no, x, tfb[0xb4]},\n    {OP_mov_imm, 0xb40000, \"mov\", ah_x, xx, Ib, xx, xx, no, x, tfb[0xb5]},\n    {OP_mov_imm, 0xb50000, \"mov\", ch_x, xx, Ib, xx, xx, no, x, tfb[0xb6]},\n    {OP_mov_imm, 0xb60000, \"mov\", dh_x, xx, Ib, xx, xx, no, x, tfb[0xb7]},\n    /* PR 250397: we point at the tail end of the mov_st templates */\n    {OP_mov_imm, 0xb70000, \"mov\", bh_x, xx, Ib, xx, xx, no, x, tex[18][0]},\n    /* b8 */\n    {OP_mov_imm, 0xb80000, \"mov\", eAX_x, xx, Iv, xx, xx, no, x, tfb[0xb9]},\n    {OP_mov_imm, 0xb90000, \"mov\", eCX_x, xx, Iv, xx, xx, no, x, tfb[0xba]},\n    {OP_mov_imm, 0xba0000, \"mov\", eDX_x, xx, Iv, xx, xx, no, x, tfb[0xbb]},\n    {OP_mov_imm, 0xbb0000, \"mov\", eBX_x, xx, Iv, xx, xx, no, x, tfb[0xbc]},\n    {OP_mov_imm, 0xbc0000, \"mov\", eSP_x, xx, Iv, xx, xx, no, x, tfb[0xbd]},\n    {OP_mov_imm, 0xbd0000, \"mov\", eBP_x, xx, Iv, xx, xx, no, x, tfb[0xbe]},\n    {OP_mov_imm, 0xbe0000, \"mov\", eSI_x, xx, Iv, xx, xx, no, x, tfb[0xbf]},\n    {OP_mov_imm, 0xbf0000, \"mov\", eDI_x, xx, Iv, xx, xx, no, x, tfb[0xb0]},\n    /* c0 */\n    {EXTENSION, 0xc00000, \"(group 2a)\", Eb, xx, Ib, xx, xx, mrm, x, 3},\n    {EXTENSION, 0xc10000, \"(group 2b)\", Ev, xx, Ib, xx, xx, mrm, x, 4},\n    {OP_ret,  0xc20000, \"ret\", xsp, xx, Iw, xsp, i_iSP, no, x, tfb[0xc3]},\n    {OP_ret,  0xc30000, \"ret\", xsp, xx, xsp, i_iSP, xx, no, x, END_LIST},\n    {VEX_PREFIX_EXT, 0xc40000, \"(vex_prefix_ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {VEX_PREFIX_EXT, 0xc50000, \"(vex_prefix_ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {EXTENSION, 0xc60000, \"(group 11a)\", Eb, xx, Ib, xx, xx, mrm, x, 17},\n    {EXTENSION, 0xc70000, \"(group 11b)\", Ev, xx, Iz, xx, xx, mrm, x, 18},\n    /* c8 */\n    {OP_enter,  0xc80000, \"enter\", xsp, i_xSPoN, Iw, Ib, xsp, xop, x, exop[0x05]},\n    {OP_leave,  0xc90000, \"leave\", xsp, xbp, xbp, xsp, i_xBP, no, x, END_LIST},\n    {OP_ret_far,  0xca0000, \"lret\", xsp, xx, Iw, xsp, i_vSPs2, no, x, tfb[0xcb]},\n    {OP_ret_far,  0xcb0000, \"lret\", xsp, xx, xsp, i_vSPs2, xx, no, x, END_LIST},\n    /* we ignore the operations on the kernel stack */\n    {OP_int3, 0xcc0000, \"int3\", xx, xx, xx, xx, xx, no, fINT, END_LIST},\n    {OP_int,  0xcd0000, \"int\",  xx, xx, Ib, xx, xx, no, fINT, END_LIST},\n    {OP_into, 0xce0000, \"into\", xx, xx, xx, xx, xx, i64, fINT, END_LIST},\n    {OP_iret, 0xcf0000, \"iret\", xsp, xx, xsp, i_vSPs3, xx, no, fWX, END_LIST},\n    /* d0 */\n    {EXTENSION, 0xd00000, \"(group 2c)\", Eb, xx, c1,  xx, xx, mrm, x, 5},\n    {EXTENSION, 0xd10000, \"(group 2d)\", Ev, xx, c1,  xx, xx, mrm, x, 6},\n    {EXTENSION, 0xd20000, \"(group 2e)\", Eb, xx, cl, xx, xx, mrm, x, 7},\n    {EXTENSION, 0xd30000, \"(group 2f)\", Ev, xx, cl, xx, xx, mrm, x, 8},\n    {OP_aam,  0xd40000, \"aam\", ax, xx, Ib, ax, xx, i64, fW6, END_LIST},\n    {OP_aad,  0xd50000, \"aad\", ax, xx, Ib, ax, xx, i64, fW6, END_LIST},\n    {OP_salc,  0xd60000, \"salc\", al, xx, xx, xx, xx, i64, fRC, END_LIST},/*undocumented*/\n    {OP_xlat,  0xd70000, \"xlat\", al, xx, Zb, xx, xx, no, x, END_LIST},\n    /* d8 */\n    {FLOAT_EXT, 0xd80000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},/* all floats need modrm */\n    {FLOAT_EXT, 0xd90000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xda0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdb0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdc0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdd0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xde0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    {FLOAT_EXT, 0xdf0000, \"(float)\", xx, xx, xx, xx, xx, mrm, x, NA},\n    /* e0 */\n    {OP_loopne,0xe00000, \"loopne\", axCX, xx, Jb, axCX, xx, no, fRZ, END_LIST},\n    {OP_loope, 0xe10000, \"loope\",  axCX, xx, Jb, axCX, xx, no, fRZ, END_LIST},\n    {OP_loop,  0xe20000, \"loop\",   axCX, xx, Jb, axCX, xx, no, x, END_LIST},\n    {OP_jecxz, 0xe30000, \"jecxz\",  xx, xx, Jb, axCX, xx, no, x, END_LIST},/*16-bit==\"jcxz\",64-bit=\"jrcxz\"*/\n    /* FIXME: in & out access \"I/O ports\", are these memory addresses?\n     * if so, change Ib to Ob and change dx to i_dx (move to dest for out)\n     */\n    {OP_in,  0xe40000, \"in\", al, xx, Ib, xx, xx, no, x, tfb[0xed]},\n    {OP_in,  0xe50000, \"in\", zAX, xx, Ib, xx, xx, no, x, tfb[0xe4]},\n    {OP_out,  0xe60000, \"out\", xx, xx, Ib, al, xx, no, x, tfb[0xef]},\n    {OP_out,  0xe70000, \"out\", xx, xx, Ib, zAX, xx, no, x, tfb[0xe6]},\n    /* e8 */\n    {OP_call,     0xe80000, \"call\",  xsp, i_iSPo1, Jz, xsp, xx, no, x, END_LIST},\n    {OP_jmp,       0xe90000, \"jmp\", xx, xx, Jz, xx, xx, no, x, END_LIST},\n    {OP_jmp_far,   0xea0000, \"ljmp\", xx, xx, Ap, xx, xx, i64, x, END_LIST},\n    {OP_jmp_short, 0xeb0000, \"jmp\", xx, xx, Jb, xx, xx, no, x, END_LIST},\n    {OP_in,  0xec0000, \"in\", al, xx, dx, xx, xx, no, x, END_LIST},\n    {OP_in,  0xed0000, \"in\", zAX, xx, dx, xx, xx, no, x, tfb[0xec]},\n    {OP_out,  0xee0000, \"out\", xx, xx, al, dx, xx, no, x, END_LIST},\n    {OP_out,  0xef0000, \"out\", xx, xx, zAX, dx, xx, no, x, tfb[0xee]},\n    /* f0 */\n    {PREFIX, 0xf00000, \"lock\", xx, xx, xx, xx, xx, no, x, PREFIX_LOCK},\n    /* Also called OP_icebp.  Undocumented.  I'm assuming looks like OP_int* */\n    {OP_int1, 0xf10000, \"int1\", xx, xx, xx, xx, xx, no, fINT, END_LIST},\n    {PREFIX, 0xf20000, \"repne\", xx, xx, xx, xx, xx, no, x, PREFIX_REPNE},\n    {PREFIX, 0xf30000, \"rep\", xx, xx, xx, xx, xx, no, x, PREFIX_REP},\n    {OP_hlt,  0xf40000, \"hlt\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_cmc,  0xf50000, \"cmc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {EXTENSION, 0xf60000, \"(group 3a)\", Eb, xx, xx, xx, xx, mrm, x, 9},\n    {EXTENSION, 0xf70000, \"(group 3b)\", Ev, xx, xx, xx, xx, mrm, x, 10},\n    /* f8 */\n    {OP_clc,  0xf80000, \"clc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {OP_stc,  0xf90000, \"stc\", xx, xx, xx, xx, xx, no, fWC, END_LIST},\n    {OP_cli,  0xfa0000, \"cli\", xx, xx, xx, xx, xx, no, fWI, END_LIST},\n    {OP_sti,  0xfb0000, \"sti\", xx, xx, xx, xx, xx, no, fWI, END_LIST},\n    {OP_cld,  0xfc0000, \"cld\", xx, xx, xx, xx, xx, no, fWD, END_LIST},\n    {OP_std,  0xfd0000, \"std\", xx, xx, xx, xx, xx, no, fWD, END_LIST},\n    {EXTENSION, 0xfe0000, \"(group 4)\", xx, xx, xx, xx, xx, mrm, x, 11},\n    {EXTENSION, 0xff0000, \"(group 5)\", xx, xx, xx, xx, xx, mrm, x, 12},\n};\n/****************************************************************************\n * Two-byte opcodes\n * This is from Tables A-4 & A-5\n */\nconst instr_info_t second_byte[] = {\n  /* 00 */\n  {EXTENSION, 0x0f0010, \"(group 6)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  {EXTENSION, 0x0f0110, \"(group 7)\", xx, xx, xx, xx, xx, mrm, x, 14},\n  {OP_lar, 0x0f0210, \"lar\", Gv, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n  {OP_lsl, 0x0f0310, \"lsl\", Gv, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n  {INVALID, 0x0f0410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* XXX: writes ss and cs */\n  {OP_syscall, 0x0f0510, \"syscall\", xcx, xx, xx, xx, xx, no, x, NA}, /* AMD/x64 only */\n  {OP_clts, 0x0f0610, \"clts\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  /* XXX: writes ss and cs */\n  {OP_sysret, 0x0f0710, \"sysret\", xx, xx, xx, xx, xx, no, x, NA}, /* AMD/x64 only */\n  /* 08 */\n  {OP_invd, 0x0f0810, \"invd\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {OP_wbinvd, 0x0f0910, \"wbinvd\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {INVALID, 0x0f0a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {OP_ud2a, 0x0f0b10, \"ud2a\", xx, xx, xx, xx, xx, no, x, END_LIST}, /* \"undefined instr\" instr */\n  {INVALID, 0x0f0c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {EXTENSION, 0x0f0d10, \"(group amd)\", xx, xx, xx, xx, xx, mrm, x, 24}, /* AMD only */\n  {OP_femms, 0x0f0e10, \"femms\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {SUFFIX_EXT, 0x0f0f10, \"(group 3DNow!)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  /* 10 */\n  {PREFIX_EXT, 0x0f1010, \"(prefix ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  {PREFIX_EXT, 0x0f1110, \"(prefix ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n  {PREFIX_EXT, 0x0f1210, \"(prefix ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n  {PREFIX_EXT, 0x0f1310, \"(prefix ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n  {PREFIX_EXT, 0x0f1410, \"(prefix ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n  {PREFIX_EXT, 0x0f1510, \"(prefix ext 5)\", xx, xx, xx, xx, xx, mrm, x, 5},\n  {PREFIX_EXT, 0x0f1610, \"(prefix ext 6)\", xx, xx, xx, xx, xx, mrm, x, 6},\n  {PREFIX_EXT, 0x0f1710, \"(prefix ext 7)\", xx, xx, xx, xx, xx, mrm, x, 7},\n  /* 18 */\n  {EXTENSION, 0x0f1810, \"(group 16)\", xx, xx, xx, xx, xx, mrm, x, 23},\n  /* xref case 9862/PR 214297 : 0f19-0f1e are \"HINT_NOP\": valid on P6+.\n   * we treat them the same as 0f1f but do not put on encoding chain.\n   * The operand is ignored but to support encoding it we must list it.\n   * i453: analysis routines now special case nop_modrm to ignore src opnd */\n  {OP_nop_modrm, 0x0f1910, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1a10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1b10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1c10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1d10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1e10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  {OP_nop_modrm, 0x0f1f10, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n  /* 20 */\n  {OP_mov_priv, 0x0f2010, \"mov\", Rr, xx, Cr, xx, xx, mrm, fW6, tsb[0x21]},\n  {OP_mov_priv, 0x0f2110, \"mov\", Rr, xx, Dr, xx, xx, mrm, fW6, tsb[0x22]},\n  {OP_mov_priv, 0x0f2210, \"mov\", Cr, xx, Rr, xx, xx, mrm, fW6, tsb[0x23]},\n  {OP_mov_priv, 0x0f2310, \"mov\", Dr, xx, Rr, xx, xx, mrm, fW6, END_LIST},\n  {INVALID, 0x0f2410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* FIXME: gdb thinks ok! */\n  {INVALID, 0x0f2510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f2610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* FIXME: gdb thinks ok! */\n  {INVALID, 0x0f2710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* 28 */\n  {PREFIX_EXT, 0x0f2810, \"(prefix ext 8)\", xx, xx, xx, xx, xx, mrm, x, 8},\n  {PREFIX_EXT, 0x0f2910, \"(prefix ext 9)\", xx, xx, xx, xx, xx, mrm, x, 9},\n  {PREFIX_EXT, 0x0f2a10, \"(prefix ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},\n  {PREFIX_EXT, 0x0f2b10, \"(prefix ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},\n  {PREFIX_EXT, 0x0f2c10, \"(prefix ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},\n  {PREFIX_EXT, 0x0f2d10, \"(prefix ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  {PREFIX_EXT, 0x0f2e10, \"(prefix ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},\n  {PREFIX_EXT, 0x0f2f10, \"(prefix ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},\n  /* 30 */\n  {OP_wrmsr, 0x0f3010, \"wrmsr\", xx, xx, edx, eax, ecx, no, x, END_LIST},\n  {OP_rdtsc, 0x0f3110, \"rdtsc\", edx, eax, xx, xx, xx, no, x, END_LIST},\n  {OP_rdmsr, 0x0f3210, \"rdmsr\", edx, eax, ecx, xx, xx, no, x, END_LIST},\n  {OP_rdpmc, 0x0f3310, \"rdpmc\", edx, eax, ecx, xx, xx, no, x, END_LIST},\n  /* XXX: sysenter writes cs and ss */\n  {OP_sysenter, 0x0f3410, \"sysenter\", xsp, xx, xx, xx, xx, no, x, END_LIST},\n  /* XXX: sysexit writes cs and ss */\n  {OP_sysexit, 0x0f3510, \"sysexit\", xsp, xx, xcx, xx, xx, no, x, END_LIST},\n  {INVALID, 0x0f3610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* XXX i#1313: various getsec leaf funcs at CPL 0 write to all kinds of\n   * processor state including eflags and eip.  Leaf funcs are indicated by eax\n   * value, though.  Here we only model the CPL > 0 effects, which conditionally\n   * write to ebx + ecx.\n   */\n  {OP_getsec, 0x0f3710, \"getsec\", eax, ebx, eax, ebx, xx, xop|predcx, x, exop[13]},\n  /* 38 */\n  {ESCAPE_3BYTE_38, 0x0f3810, \"(3byte 38)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {ESCAPE_3BYTE_3a, 0x0f3a10, \"(3byte 3a)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f3f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* 40 */\n  {OP_cmovo,   0x0f4010, \"cmovo\",  Gv, xx, Ev, xx, xx, mrm|predcc, fRO, END_LIST},\n  {E_VEX_EXT, 0x0f4110, \"(e_vex ext 83)\", xx, xx, xx, xx, xx, mrm, x, 83},\n  {E_VEX_EXT, 0x0f4210, \"(e_vex ext 84)\", xx, xx, xx, xx, xx, mrm, x, 84},\n  {OP_cmovnb,  0x0f4310, \"cmovnb\", Gv, xx, Ev, xx, xx, mrm|predcc, fRC, END_LIST},\n  {E_VEX_EXT, 0x0f4410, \"(e_vex ext 86)\", xx, xx, xx, xx, xx, mrm, x, 86},\n  {E_VEX_EXT, 0x0f4510, \"(e_vex ext 87)\", xx, xx, xx, xx, xx, mrm, x, 87},\n  {E_VEX_EXT, 0x0f4610, \"(e_vex ext 88)\", xx, xx, xx, xx, xx, mrm, x, 88},\n  {E_VEX_EXT, 0x0f4710, \"(e_vex ext 89)\", xx, xx, xx, xx, xx, mrm, x, 89},\n  /* 48 */\n  {OP_cmovs,  0x0f4810, \"cmovs\",  Gv, xx, Ev, xx, xx, mrm|predcc, fRS, END_LIST},\n  {OP_cmovns, 0x0f4910, \"cmovns\", Gv, xx, Ev, xx, xx, mrm|predcc, fRS, END_LIST},\n  {E_VEX_EXT, 0x0f4a10, \"(e_vex ext 90)\", xx, xx, xx, xx, xx, mrm, x, 90},\n  {E_VEX_EXT, 0x0f4b10, \"(e_vex ext 85)\", xx, xx, xx, xx, xx, mrm, x, 85},\n  {OP_cmovl,  0x0f4c10, \"cmovl\",  Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO), END_LIST},\n  {OP_cmovnl, 0x0f4d10, \"cmovnl\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO), END_LIST},\n  {OP_cmovle, 0x0f4e10, \"cmovle\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO|fRZ), END_LIST},\n  {OP_cmovnle,0x0f4f10, \"cmovnle\",Gv, xx, Ev, xx, xx, mrm|predcc, (fRS|fRO|fRZ), END_LIST},\n  /* 50 */\n  {PREFIX_EXT, 0x0f5010, \"(prefix ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},\n  {PREFIX_EXT, 0x0f5110, \"(prefix ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},\n  {PREFIX_EXT, 0x0f5210, \"(prefix ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},\n  {PREFIX_EXT, 0x0f5310, \"(prefix ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},\n  {PREFIX_EXT, 0x0f5410, \"(prefix ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},\n  {PREFIX_EXT, 0x0f5510, \"(prefix ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},\n  {PREFIX_EXT, 0x0f5610, \"(prefix ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},\n  {PREFIX_EXT, 0x0f5710, \"(prefix ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},\n  /* 58 */\n  {PREFIX_EXT, 0x0f5810, \"(prefix ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},\n  {PREFIX_EXT, 0x0f5910, \"(prefix ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},\n  {PREFIX_EXT, 0x0f5a10, \"(prefix ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},\n  {PREFIX_EXT, 0x0f5b10, \"(prefix ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},\n  {PREFIX_EXT, 0x0f5c10, \"(prefix ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},\n  {PREFIX_EXT, 0x0f5d10, \"(prefix ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},\n  {PREFIX_EXT, 0x0f5e10, \"(prefix ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},\n  {PREFIX_EXT, 0x0f5f10, \"(prefix ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},\n  /* 60 */\n  {PREFIX_EXT, 0x0f6010, \"(prefix ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},\n  {PREFIX_EXT, 0x0f6110, \"(prefix ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},\n  {PREFIX_EXT, 0x0f6210, \"(prefix ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},\n  {PREFIX_EXT, 0x0f6310, \"(prefix ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},\n  {PREFIX_EXT, 0x0f6410, \"(prefix ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},\n  {PREFIX_EXT, 0x0f6510, \"(prefix ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},\n  {PREFIX_EXT, 0x0f6610, \"(prefix ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},\n  {PREFIX_EXT, 0x0f6710, \"(prefix ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},\n  /* 68 */\n  {PREFIX_EXT, 0x0f6810, \"(prefix ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},\n  {PREFIX_EXT, 0x0f6910, \"(prefix ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},\n  {PREFIX_EXT, 0x0f6a10, \"(prefix ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},\n  {PREFIX_EXT, 0x0f6b10, \"(prefix ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},\n  {PREFIX_EXT, 0x0f6c10, \"(prefix ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},\n  {PREFIX_EXT, 0x0f6d10, \"(prefix ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},\n  {PREFIX_EXT, 0x0f6e10, \"(prefix ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},\n  {PREFIX_EXT, 0x0f6f10, \"(prefix ext 112)\", xx, xx, xx, xx, xx, mrm, x, 112},\n  /* 70 */\n  {PREFIX_EXT, 0x0f7010, \"(prefix ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},\n  {EXTENSION, 0x0f7110, \"(group 12)\", xx, xx, xx, xx, xx, mrm, x, 19},\n  {EXTENSION, 0x0f7210, \"(group 13)\", xx, xx, xx, xx, xx, mrm, x, 20},\n  {EXTENSION, 0x0f7310, \"(group 14)\", xx, xx, xx, xx, xx, mrm, x, 21},\n  {PREFIX_EXT, 0x0f7410, \"(prefix ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},\n  {PREFIX_EXT, 0x0f7510, \"(prefix ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},\n  {PREFIX_EXT, 0x0f7610, \"(prefix ext 50)\", xx, xx, xx, xx, xx, mrm, x, 50},\n  {VEX_L_EXT,  0x0f7710, \"(vex L ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n  /* 78 */\n  {PREFIX_EXT, 0x0f7810, \"(prefix ext 134)\", xx, xx, xx, xx, xx, mrm, x, 134},\n  {PREFIX_EXT, 0x0f7910, \"(prefix ext 135)\", xx, xx, xx, xx, xx, mrm, x, 135},\n  {INVALID, 0x0f7a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0f7b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {PREFIX_EXT, 0x0f7c10, \"(prefix ext 114)\", xx, xx, xx, xx, xx, mrm, x, 114},\n  {PREFIX_EXT, 0x0f7d10, \"(prefix ext 115)\", xx, xx, xx, xx, xx, mrm, x, 115},\n  {PREFIX_EXT, 0x0f7e10, \"(prefix ext 51)\", xx, xx, xx, xx, xx, mrm, x, 51},\n  {PREFIX_EXT, 0x0f7f10, \"(prefix ext 113)\", xx, xx, xx, xx, xx, mrm, x, 113},\n  /* 80 */\n  {OP_jo,  0x0f8010, \"jo\",  xx, xx, Jz, xx, xx, no, fRO, END_LIST},\n  {OP_jno, 0x0f8110, \"jno\", xx, xx, Jz, xx, xx, no, fRO, END_LIST},\n  {OP_jb,  0x0f8210, \"jb\",  xx, xx, Jz, xx, xx, no, fRC, END_LIST},\n  {OP_jnb, 0x0f8310, \"jnb\", xx, xx, Jz, xx, xx, no, fRC, END_LIST},\n  {OP_jz,  0x0f8410, \"jz\",  xx, xx, Jz, xx, xx, no, fRZ, END_LIST},\n  {OP_jnz, 0x0f8510, \"jnz\", xx, xx, Jz, xx, xx, no, fRZ, END_LIST},\n  {OP_jbe, 0x0f8610, \"jbe\", xx, xx, Jz, xx, xx, no, (fRC|fRZ), END_LIST},\n  {OP_jnbe,0x0f8710, \"jnbe\",xx, xx, Jz, xx, xx, no, (fRC|fRZ), END_LIST},\n  /* 88 */\n  {OP_js,  0x0f8810, \"js\",  xx, xx, Jz, xx, xx, no, fRS, END_LIST},\n  {OP_jns, 0x0f8910, \"jns\", xx, xx, Jz, xx, xx, no, fRS, END_LIST},\n  {OP_jp,  0x0f8a10, \"jp\",  xx, xx, Jz, xx, xx, no, fRP, END_LIST},\n  {OP_jnp, 0x0f8b10, \"jnp\", xx, xx, Jz, xx, xx, no, fRP, END_LIST},\n  {OP_jl,  0x0f8c10, \"jl\",  xx, xx, Jz, xx, xx, no, (fRS|fRO), END_LIST},\n  {OP_jnl, 0x0f8d10, \"jnl\", xx, xx, Jz, xx, xx, no, (fRS|fRO), END_LIST},\n  {OP_jle, 0x0f8e10, \"jle\", xx, xx, Jz, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n  {OP_jnle,0x0f8f10, \"jnle\",xx, xx, Jz, xx, xx, no, (fRS|fRO|fRZ), END_LIST},\n  /* 90 */\n  {E_VEX_EXT, 0x0f9010, \"(e_vex ext 79)\", xx, xx, xx, xx, xx, mrm, x, 79},\n  {E_VEX_EXT, 0x0f9110, \"(e_vex ext 80)\", xx, xx, xx, xx, xx, mrm, x, 80},\n  {E_VEX_EXT, 0x0f9210, \"(e_vex ext 81)\", xx, xx, xx, xx, xx, mrm, x, 81},\n  {E_VEX_EXT, 0x0f9310, \"(e_vex ext 82)\", xx, xx, xx, xx, xx, mrm, x, 82},\n  {OP_setz,  0x0f9410, \"setz\",  Eb, xx, xx, xx, xx, mrm, fRZ, END_LIST},\n  {OP_setnz, 0x0f9510, \"setnz\", Eb, xx, xx, xx, xx, mrm, fRZ, END_LIST},\n  {OP_setbe, 0x0f9610, \"setbe\", Eb, xx, xx, xx, xx, mrm, (fRC|fRZ), END_LIST},\n  {OP_setnbe,0x0f9710, \"setnbe\",Eb, xx, xx, xx, xx, mrm, (fRC|fRZ), END_LIST},\n  /* 98 */\n  {E_VEX_EXT, 0x0f9810, \"(e_vex ext 91)\", xx, xx, xx, xx, xx, mrm, x, 91},\n  {E_VEX_EXT, 0x0f9910, \"(e_vex ext 92)\", xx, xx, xx, xx, xx, mrm, x, 92},\n  {OP_setp,  0x0f9a10, \"setp\",  Eb, xx, xx, xx, xx, mrm, fRP, END_LIST},\n  {OP_setnp, 0x0f9b10, \"setnp\", Eb, xx, xx, xx, xx, mrm, fRP, END_LIST},\n  {OP_setl,  0x0f9c10, \"setl\",  Eb, xx, xx, xx, xx, mrm, (fRS|fRO), END_LIST},\n  {OP_setnl, 0x0f9d10, \"setnl\", Eb, xx, xx, xx, xx, mrm, (fRS|fRO), END_LIST},\n  {OP_setle, 0x0f9e10, \"setle\", Eb, xx, xx, xx, xx, mrm, (fRS|fRO|fRZ), END_LIST},\n  {OP_setnle,0x0f9f10, \"setnle\",Eb, xx, xx, xx, xx, mrm, (fRS|fRO|fRZ), END_LIST},\n  /* a0 */\n  {OP_push, 0x0fa010, \"push\", xsp, i_xSPo1, fs, xsp, xx, no, x, tsb[0xa8]},\n  {OP_pop,  0x0fa110, \"pop\", fs, xsp, xsp, i_xSP, xx, no, x, tsb[0xa9]},\n  {OP_cpuid, 0x0fa210, \"cpuid\", eax, ebx, eax, ecx, xx, xop, x, exop[0x06]},\n  {OP_bt,   0x0fa310, \"bt\",   xx, xx, Ev, Gv, xx, mrm, fW6, tex[15][4]},\n  {OP_shld, 0x0fa410, \"shld\", Ev, xx, Gv, Ib, Ev, mrm, fW6, tsb[0xa5]},\n  {OP_shld, 0x0fa510, \"shld\", Ev, xx, Gv, cl, Ev, mrm, fW6, END_LIST},\n  {INVALID, 0x0fa610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  {INVALID, 0x0fa710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  /* a8 */\n  {OP_push, 0x0fa810, \"push\", xsp, i_xSPo1, gs, xsp, xx, no, x, END_LIST},\n  {OP_pop,  0x0fa910, \"pop\", gs, xsp, xsp, i_xSP, xx, no, x, END_LIST},\n  {OP_rsm,  0x0faa10, \"rsm\", xx, xx, xx, xx, xx, no, fWX, END_LIST},\n  {OP_bts,  0x0fab10, \"bts\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][5]},\n  {OP_shrd, 0x0fac10, \"shrd\", Ev, xx, Gv, Ib, Ev, mrm, fW6, tsb[0xad]},\n  {OP_shrd, 0x0fad10, \"shrd\", Ev, xx, Gv, cl, Ev, mrm, fW6, END_LIST},\n  {EXTENSION, 0x0fae10, \"(group 15)\", xx, xx, xx, xx, xx, mrm, x, 22},\n  {OP_imul, 0x0faf10, \"imul\", Gv, xx, Ev, Gv, xx, mrm, fW6, tfb[0x69]},\n  /* b0 */\n  {OP_cmpxchg, 0x0fb010, \"cmpxchg\", Eb, al, Gb, Eb, al, mrm, fW6, END_LIST},\n  {OP_cmpxchg, 0x0fb110, \"cmpxchg\", Ev, eAX, Gv, Ev, eAX, mrm, fW6, tsb[0xb0]},\n  {OP_lss, 0x0fb210, \"lss\", Gv, ss, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_btr, 0x0fb310, \"btr\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][6]},\n  {OP_lfs, 0x0fb410, \"lfs\", Gv, fs, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_lgs, 0x0fb510, \"lgs\", Gv, gs, Mp, xx, xx, mrm, x, END_LIST},\n  {OP_movzx, 0x0fb610, \"movzx\", Gv, xx, Eb, xx, xx, mrm, x, END_LIST},\n  {OP_movzx, 0x0fb710, \"movzx\", Gv, xx, Ew, xx, xx, mrm, x, tsb[0xb6]},\n  /* b8 */\n  {OP_popcnt, 0xf30fb810, \"popcnt\", Gv, xx, Ev, xx, xx, mrm|reqp, fW6, END_LIST},\n  /* This is Group 10, but all identical (ud2b) so no reason to split opcode by /reg */\n  {OP_ud2b, 0x0fb910, \"ud2b\", xx, xx, xx, xx, xx, no, x, END_LIST},\n  {EXTENSION, 0x0fba10, \"(group 8)\", xx, xx, xx, xx, xx, mrm, x, 15},\n  {OP_btc, 0x0fbb10, \"btc\", Ev, xx, Gv, Ev, xx, mrm, fW6, tex[15][7]},\n  {PREFIX_EXT, 0x0fbc10, \"(prefix ext 140)\", xx, xx, xx, xx, xx, mrm, x, 140},\n  {PREFIX_EXT, 0x0fbd10, \"(prefix ext 136)\", xx, xx, xx, xx, xx, mrm, x, 136},\n  {OP_movsx, 0x0fbe10, \"movsx\", Gv, xx, Eb, xx, xx, mrm, x, END_LIST},\n  {OP_movsx, 0x0fbf10, \"movsx\", Gv, xx, Ew, xx, xx, mrm, x, tsb[0xbe]},\n  /* c0 */\n  {OP_xadd, 0x0fc010, \"xadd\", Eb, Gb, Eb, Gb, xx, mrm, fW6, END_LIST},\n  {OP_xadd, 0x0fc110, \"xadd\", Ev, Gv, Ev, Gv, xx, mrm, fW6, tsb[0xc0]},\n  {PREFIX_EXT, 0x0fc210, \"(prefix ext 52)\", xx, xx, xx, xx, xx, mrm, x, 52},\n  {OP_movnti, 0x0fc310, \"movnti\", Md_q, xx, Gd_q, xx, xx, mrm, x, END_LIST},\n  {PREFIX_EXT, 0x0fc410, \"(prefix ext 53)\", xx, xx, xx, xx, xx, mrm, x, 53},\n  {PREFIX_EXT, 0x0fc510, \"(prefix ext 54)\", xx, xx, xx, xx, xx, mrm, x, 54},\n  {PREFIX_EXT, 0x0fc610, \"(prefix ext 55)\", xx, xx, xx, xx, xx, mrm, x, 55},\n  {EXTENSION, 0x0fc710, \"(group 9)\", xx, xx, xx, xx, xx, mrm, x, 16},\n  /* c8 */\n  {OP_bswap, 0x0fc810, \"bswap\", uAX_x, xx, uAX_x, xx, xx, no, x, tsb[0xc9]},\n  {OP_bswap, 0x0fc910, \"bswap\", uCX_x, xx, uCX_x, xx, xx, no, x, tsb[0xca]},\n  {OP_bswap, 0x0fca10, \"bswap\", uDX_x, xx, uDX_x, xx, xx, no, x, tsb[0xcb]},\n  {OP_bswap, 0x0fcb10, \"bswap\", uBX_x, xx, uBX_x, xx, xx, no, x, tsb[0xcc]},\n  {OP_bswap, 0x0fcc10, \"bswap\", uSP_x, xx, uSP_x, xx, xx, no, x, tsb[0xcd]},\n  {OP_bswap, 0x0fcd10, \"bswap\", uBP_x, xx, uBP_x, xx, xx, no, x, tsb[0xce]},\n  {OP_bswap, 0x0fce10, \"bswap\", uSI_x, xx, uSI_x, xx, xx, no, x, tsb[0xcf]},\n  {OP_bswap, 0x0fcf10, \"bswap\", uDI_x, xx, uDI_x, xx, xx, no, x, END_LIST},\n  /* d0 */\n  {PREFIX_EXT, 0x0fd010, \"(prefix ext 116)\", xx, xx, xx, xx, xx, mrm, x, 116},\n  {PREFIX_EXT, 0x0fd110, \"(prefix ext 56)\", xx, xx, xx, xx, xx, mrm, x, 56},\n  {PREFIX_EXT, 0x0fd210, \"(prefix ext 57)\", xx, xx, xx, xx, xx, mrm, x, 57},\n  {PREFIX_EXT, 0x0fd310, \"(prefix ext 58)\", xx, xx, xx, xx, xx, mrm, x, 58},\n  {PREFIX_EXT, 0x0fd410, \"(prefix ext 59)\", xx, xx, xx, xx, xx, mrm, x, 59},\n  {PREFIX_EXT, 0x0fd510, \"(prefix ext 60)\", xx, xx, xx, xx, xx, mrm, x, 60},\n  {PREFIX_EXT, 0x0fd610, \"(prefix ext 61)\", xx, xx, xx, xx, xx, mrm, x, 61},\n  {PREFIX_EXT, 0x0fd710, \"(prefix ext 62)\", xx, xx, xx, xx, xx, mrm, x, 62},\n  /* d8 */\n  {PREFIX_EXT, 0x0fd810, \"(prefix ext 63)\", xx, xx, xx, xx, xx, mrm, x, 63},\n  {PREFIX_EXT, 0x0fd910, \"(prefix ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},\n  {PREFIX_EXT, 0x0fda10, \"(prefix ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},\n  {PREFIX_EXT, 0x0fdb10, \"(prefix ext 66)\", xx, xx, xx, xx, xx, mrm, x, 66},\n  {PREFIX_EXT, 0x0fdc10, \"(prefix ext 67)\", xx, xx, xx, xx, xx, mrm, x, 67},\n  {PREFIX_EXT, 0x0fdd10, \"(prefix ext 68)\", xx, xx, xx, xx, xx, mrm, x, 68},\n  {PREFIX_EXT, 0x0fde10, \"(prefix ext 69)\", xx, xx, xx, xx, xx, mrm, x, 69},\n  {PREFIX_EXT, 0x0fdf10, \"(prefix ext 70)\", xx, xx, xx, xx, xx, mrm, x, 70},\n  /* e0 */\n  {PREFIX_EXT, 0x0fe010, \"(prefix ext 71)\", xx, xx, xx, xx, xx, mrm, x, 71},\n  {PREFIX_EXT, 0x0fe110, \"(prefix ext 72)\", xx, xx, xx, xx, xx, mrm, x, 72},\n  {PREFIX_EXT, 0x0fe210, \"(prefix ext 73)\", xx, xx, xx, xx, xx, mrm, x, 73},\n  {PREFIX_EXT, 0x0fe310, \"(prefix ext 74)\", xx, xx, xx, xx, xx, mrm, x, 74},\n  {PREFIX_EXT, 0x0fe410, \"(prefix ext 75)\", xx, xx, xx, xx, xx, mrm, x, 75},\n  {PREFIX_EXT, 0x0fe510, \"(prefix ext 76)\", xx, xx, xx, xx, xx, mrm, x, 76},\n  {PREFIX_EXT, 0x0fe610, \"(prefix ext 77)\", xx, xx, xx, xx, xx, mrm, x, 77},\n  {PREFIX_EXT, 0x0fe710, \"(prefix ext 78)\", xx, xx, xx, xx, xx, mrm, x, 78},\n  /* e8 */\n  {PREFIX_EXT, 0x0fe810, \"(prefix ext 79)\", xx, xx, xx, xx, xx, mrm, x, 79},\n  {PREFIX_EXT, 0x0fe910, \"(prefix ext 80)\", xx, xx, xx, xx, xx, mrm, x, 80},\n  {PREFIX_EXT, 0x0fea10, \"(prefix ext 81)\", xx, xx, xx, xx, xx, mrm, x, 81},\n  {PREFIX_EXT, 0x0feb10, \"(prefix ext 82)\", xx, xx, xx, xx, xx, mrm, x, 82},\n  {PREFIX_EXT, 0x0fec10, \"(prefix ext 83)\", xx, xx, xx, xx, xx, mrm, x, 83},\n  {PREFIX_EXT, 0x0fed10, \"(prefix ext 84)\", xx, xx, xx, xx, xx, mrm, x, 84},\n  {PREFIX_EXT, 0x0fee10, \"(prefix ext 85)\", xx, xx, xx, xx, xx, mrm, x, 85},\n  {PREFIX_EXT, 0x0fef10, \"(prefix ext 86)\", xx, xx, xx, xx, xx, mrm, x, 86},\n  /* f0 */\n  {PREFIX_EXT, 0x0ff010, \"(prefix ext 117)\", xx, xx, xx, xx, xx, mrm, x, 117},\n  {PREFIX_EXT, 0x0ff110, \"(prefix ext 87)\", xx, xx, xx, xx, xx, mrm, x, 87},\n  {PREFIX_EXT, 0x0ff210, \"(prefix ext 88)\", xx, xx, xx, xx, xx, mrm, x, 88},\n  {PREFIX_EXT, 0x0ff310, \"(prefix ext 89)\", xx, xx, xx, xx, xx, mrm, x, 89},\n  {PREFIX_EXT, 0x0ff410, \"(prefix ext 90)\", xx, xx, xx, xx, xx, mrm, x, 90},\n  {PREFIX_EXT, 0x0ff510, \"(prefix ext 91)\", xx, xx, xx, xx, xx, mrm, x, 91},\n  {PREFIX_EXT, 0x0ff610, \"(prefix ext 92)\", xx, xx, xx, xx, xx, mrm, x, 92},\n  {PREFIX_EXT, 0x0ff710, \"(prefix ext 93)\", xx, xx, xx, xx, xx, mrm, x, 93},\n  /* f8 */\n  {PREFIX_EXT, 0x0ff810, \"(prefix ext 94)\", xx, xx, xx, xx, xx, mrm, x, 94},\n  {PREFIX_EXT, 0x0ff910, \"(prefix ext 95)\", xx, xx, xx, xx, xx, mrm, x, 95},\n  {PREFIX_EXT, 0x0ffa10, \"(prefix ext 96)\", xx, xx, xx, xx, xx, mrm, x, 96},\n  {PREFIX_EXT, 0x0ffb10, \"(prefix ext 97)\", xx, xx, xx, xx, xx, mrm, x, 97},\n  {PREFIX_EXT, 0x0ffc10, \"(prefix ext 98)\", xx, xx, xx, xx, xx, mrm, x, 98},\n  {PREFIX_EXT, 0x0ffd10, \"(prefix ext 99)\", xx, xx, xx, xx, xx, mrm, x, 99},\n  {PREFIX_EXT, 0x0ffe10, \"(prefix ext 100)\", xx, xx, xx, xx, xx, mrm, x, 100},\n  {INVALID, 0x0fff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n};\n\n/****************************************************************************\n * Opcode extensions\n * This is from Table A-6\n */\nconst instr_info_t base_extensions[][8] = {\n  /* group 1a -- first opcode byte 80: all assumed to have Ib */\n  { /* extensions[0] */\n    {OP_add, 0x800020, \"add\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][0]},\n    {OP_or,  0x800021, \"or\",  Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][1]},\n    {OP_adc, 0x800022, \"adc\", Eb, xx, Ib, Eb, xx, mrm, (fW6|fRC), tex[25][2]},\n    {OP_sbb, 0x800023, \"sbb\", Eb, xx, Ib, Eb, xx, mrm, (fW6|fRC), tex[25][3]},\n    {OP_and, 0x800024, \"and\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][4]},\n    {OP_sub, 0x800025, \"sub\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][5]},\n    {OP_xor, 0x800026, \"xor\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[25][6]},\n    {OP_cmp, 0x800027, \"cmp\", xx, xx, Eb, Ib, xx, mrm, fW6,  tex[25][7]},\n },\n  /* group 1b -- first opcode byte 81: all assumed to have Iz */\n  { /* extensions[1] */\n    {OP_add, 0x810020, \"add\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][0]},\n    {OP_or,  0x810021, \"or\",  Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][1]},\n    {OP_adc, 0x810022, \"adc\", Ev, xx, Iz, Ev, xx, mrm, (fW6|fRC), tex[2][2]},\n    {OP_sbb, 0x810023, \"sbb\", Ev, xx, Iz, Ev, xx, mrm, (fW6|fRC), tex[2][3]},\n    {OP_and, 0x810024, \"and\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][4]},\n    {OP_sub, 0x810025, \"sub\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][5]},\n    {OP_xor, 0x810026, \"xor\", Ev, xx, Iz, Ev, xx, mrm, fW6,  tex[2][6]},\n    {OP_cmp, 0x810027, \"cmp\", xx, xx, Ev, Iz, xx, mrm, fW6,  tex[2][7]},\n },\n  /* group 1c -- first opcode byte 83 (for 82, see below \"group 1c*\"):\n   * all assumed to have Ib */\n  { /* extensions[2] */\n    {OP_add, 0x830020, \"add\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][0]},\n    {OP_or,  0x830021, \"or\",  Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][1]},\n    {OP_adc, 0x830022, \"adc\", Ev, xx, Ib, Ev, xx, mrm, (fW6|fRC), tex[0][2]},\n    {OP_sbb, 0x830023, \"sbb\", Ev, xx, Ib, Ev, xx, mrm, (fW6|fRC), tex[0][3]},\n    {OP_and, 0x830024, \"and\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][4]},\n    {OP_sub, 0x830025, \"sub\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][5]},\n    {OP_xor, 0x830026, \"xor\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[0][6]},\n    {OP_cmp, 0x830027, \"cmp\", xx, xx, Ev, Ib, xx, mrm, fW6,  tex[0][7]},\n },\n  /* group 2a -- first opcode byte c0: all assumed to have Ib */\n  { /* extensions[3] */\n    {OP_rol, 0xc00020, \"rol\", Eb, xx, Ib, Eb, xx, mrm, (fWC|fWO),  tex[5][0]},\n    {OP_ror, 0xc00021, \"ror\", Eb, xx, Ib, Eb, xx, mrm, (fWC|fWO),  tex[5][1]},\n    {OP_rcl, 0xc00022, \"rcl\", Eb, xx, Ib, Eb, xx, mrm, (fRC|fWC|fWO), tex[5][2]},\n    {OP_rcr, 0xc00023, \"rcr\", Eb, xx, Ib, Eb, xx, mrm, (fRC|fWC|fWO), tex[5][3]},\n    {OP_shl, 0xc00024, \"shl\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][4]},\n    {OP_shr, 0xc00025, \"shr\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xc00026, \"shl\", Eb, xx, Ib, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xc00027, \"sar\", Eb, xx, Ib, Eb, xx, mrm, fW6,  tex[5][7]},\n },\n  /* group 2b -- first opcode byte c1: all assumed to have Ib */\n  { /* extensions[4] */\n    {OP_rol, 0xc10020, \"rol\", Ev, xx, Ib, Ev, xx, mrm, (fWC|fWO),  tex[6][0]},\n    {OP_ror, 0xc10021, \"ror\", Ev, xx, Ib, Ev, xx, mrm, (fWC|fWO),  tex[6][1]},\n    {OP_rcl, 0xc10022, \"rcl\", Ev, xx, Ib, Ev, xx, mrm, (fRC|fWC|fWO), tex[6][2]},\n    {OP_rcr, 0xc10023, \"rcr\", Ev, xx, Ib, Ev, xx, mrm, (fRC|fWC|fWO), tex[6][3]},\n    {OP_shl, 0xc10024, \"shl\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][4]},\n    {OP_shr, 0xc10025, \"shr\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xc10026, \"shl\", Ev, xx, Ib, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xc10027, \"sar\", Ev, xx, Ib, Ev, xx, mrm, fW6,  tex[6][7]},\n },\n  /* group 2c -- first opcode byte d0 */\n  { /* extensions[5] */\n    {OP_rol, 0xd00020, \"rol\", Eb, xx, c1, Eb, xx, mrm, (fWC|fWO),  tex[8][0]},\n    {OP_ror, 0xd00021, \"ror\", Eb, xx, c1, Eb, xx, mrm, (fWC|fWO),  tex[8][1]},\n    {OP_rcl, 0xd00022, \"rcl\", Eb, xx, c1, Eb, xx, mrm, (fRC|fWC|fWO), tex[8][2]},\n    {OP_rcr, 0xd00023, \"rcr\", Eb, xx, c1, Eb, xx, mrm, (fRC|fWC|fWO), tex[8][3]},\n    {OP_shl, 0xd00024, \"shl\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][4]},\n    {OP_shr, 0xd00025, \"shr\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd00026, \"shl\", Eb, xx, c1, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd00027, \"sar\", Eb, xx, c1, Eb, xx, mrm, fW6,  tex[8][7]},\n },\n  /* group 2d -- first opcode byte d1 */\n  { /* extensions[6] */\n    {OP_rol, 0xd10020, \"rol\", Ev, xx, c1, Ev, xx, mrm, (fWC|fWO),  tex[3][0]},\n    {OP_ror, 0xd10021, \"ror\", Ev, xx, c1, Ev, xx, mrm, (fWC|fWO),  tex[3][1]},\n    {OP_rcl, 0xd10022, \"rcl\", Ev, xx, c1, Ev, xx, mrm, (fRC|fWC|fWO), tex[3][2]},\n    {OP_rcr, 0xd10023, \"rcr\", Ev, xx, c1, Ev, xx, mrm, (fRC|fWC|fWO), tex[3][3]},\n    {OP_shl, 0xd10024, \"shl\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][4]},\n    {OP_shr, 0xd10025, \"shr\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd10026, \"shl\", Ev, xx, c1, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd10027, \"sar\", Ev, xx, c1, Ev, xx, mrm, fW6,  tex[3][7]},\n },\n  /* group 2e -- first opcode byte d2 */\n  { /* extensions[7] */\n    {OP_rol, 0xd20020, \"rol\", Eb, xx, cl, Eb, xx, mrm, (fWC|fWO),  END_LIST},\n    {OP_ror, 0xd20021, \"ror\", Eb, xx, cl, Eb, xx, mrm, (fWC|fWO),  END_LIST},\n    {OP_rcl, 0xd20022, \"rcl\", Eb, xx, cl, Eb, xx, mrm, (fRC|fWC|fWO), END_LIST},\n    {OP_rcr, 0xd20023, \"rcr\", Eb, xx, cl, Eb, xx, mrm, (fRC|fWC|fWO), END_LIST},\n    {OP_shl, 0xd20024, \"shl\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_shr, 0xd20025, \"shr\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd20026, \"shl\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd20027, \"sar\", Eb, xx, cl, Eb, xx, mrm, fW6,  END_LIST},\n },\n  /* group 2f -- first opcode byte d3 */\n  { /* extensions[8] */\n    {OP_rol, 0xd30020, \"rol\", Ev, xx, cl, Ev, xx, mrm, (fWC|fWO),  tex[7][0]},\n    {OP_ror, 0xd30021, \"ror\", Ev, xx, cl, Ev, xx, mrm, (fWC|fWO),  tex[7][1]},\n    {OP_rcl, 0xd30022, \"rcl\", Ev, xx, cl, Ev, xx, mrm, (fRC|fWC|fWO), tex[7][2]},\n    {OP_rcr, 0xd30023, \"rcr\", Ev, xx, cl, Ev, xx, mrm, (fRC|fWC|fWO), tex[7][3]},\n    {OP_shl, 0xd30024, \"shl\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][4]},\n    {OP_shr, 0xd30025, \"shr\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][5]},\n    /* PR 332254: /6 is an alias for /4; we do not add to encoding chain though */\n    {OP_shl, 0xd30026, \"shl\", Ev, xx, cl, Ev, xx, mrm, fW6,  END_LIST},\n    {OP_sar, 0xd30027, \"sar\", Ev, xx, cl, Ev, xx, mrm, fW6,  tex[7][7]},\n },\n  /* group 3a -- first opcode byte f6 */\n  { /* extensions[9] */\n    {OP_test, 0xf60020, \"test\", xx, xx, Eb, Ib, xx, mrm, fW6, END_LIST},\n    /* PR 332254: /1 is an alias for /0; we do not add to encoding chain though */\n    {OP_test, 0xf60021, \"test\", xx, xx, Eb, Ib, xx, mrm, fW6, END_LIST},\n    {OP_not,  0xf60022, \"not\", Eb, xx, Eb, xx, xx, mrm, x, END_LIST},\n    {OP_neg,  0xf60023, \"neg\", Eb, xx, Eb, xx, xx, mrm, fW6, END_LIST},\n    {OP_mul,  0xf60024, \"mul\", ax, xx, Eb, al, xx, mrm, fW6, END_LIST},\n    {OP_imul, 0xf60025, \"imul\", ax, xx, Eb, al, xx, mrm, fW6, tsb[0xaf]},\n    {OP_div,  0xf60026, \"div\", ah, al, Eb, ax, xx, mrm, fW6, END_LIST},\n    {OP_idiv, 0xf60027, \"idiv\", ah, al, Eb, ax, xx, mrm, fW6, END_LIST},\n },\n  /* group 3b -- first opcode byte f7 */\n  { /* extensions[10] */\n    {OP_test, 0xf70020, \"test\", xx,  xx, Ev, Iz, xx, mrm, fW6, tex[9][0]},\n    /* PR 332254: /1 is an alias for /0; we do not add to encoding chain though */\n    {OP_test, 0xf70021, \"test\", xx,  xx, Ev, Iz, xx, mrm, fW6, END_LIST},\n    {OP_not,  0xf70022, \"not\", Ev,  xx, Ev, xx, xx, mrm, x, tex[9][2]},\n    {OP_neg,  0xf70023, \"neg\", Ev,  xx, Ev, xx, xx, mrm, fW6, tex[9][3]},\n    {OP_mul,  0xf70024, \"mul\",   eDX, eAX, Ev, eAX, xx, mrm, fW6, tex[9][4]},\n    {OP_imul, 0xf70025, \"imul\",  eDX, eAX, Ev, eAX, xx, mrm, fW6, tex[9][5]},\n    {OP_div,  0xf70026, \"div\",   eDX, eAX, Ev, eDX, eAX, mrm, fW6, tex[9][6]},\n    {OP_idiv, 0xf70027, \"idiv\",  eDX, eAX, Ev, eDX, eAX, mrm, fW6, tex[9][7]},\n },\n  /* group 4 (first byte fe) */\n  { /* extensions[11] */\n    {OP_inc, 0xfe0020, \"inc\", Eb, xx, Eb, xx, xx, mrm, (fW6&(~fWC)), END_LIST},\n    {OP_dec, 0xfe0021, \"dec\", Eb, xx, Eb, xx, xx, mrm, (fW6&(~fWC)), END_LIST},\n    {INVALID, 0xfe0022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xfe0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 5 (first byte ff) */\n  { /* extensions[12] */\n    {OP_inc, 0xff0020, \"inc\", Ev, xx, Ev, xx, xx, mrm, (fW6&(~fWC)), tex[11][0]},\n    {OP_dec, 0xff0021, \"dec\", Ev, xx, Ev, xx, xx, mrm, (fW6&(~fWC)), tex[11][1]},\n    {OP_call_ind,     0xff0022, \"call\",  xsp, i_iSPo1, i_Exi, xsp, xx, mrm, x, END_LIST},\n    /* Note how a far call's stack operand size matches far ret rather than call */\n    {OP_call_far_ind, 0xff0023, \"lcall\",  xsp, i_vSPo2, i_Ep, xsp, xx, mrm, x, END_LIST},\n    {OP_jmp_ind,      0xff0024, \"jmp\",  xx, xx, i_Exi, xx, xx, mrm, x, END_LIST},\n    {OP_jmp_far_ind,  0xff0025, \"ljmp\",  xx, xx, i_Ep, xx, xx, mrm, x, END_LIST},\n    {OP_push, 0xff0026, \"push\", xsp, i_xSPo1, Esv, xsp, xx, mrm, x, tfb[0x06]},\n    {INVALID, 0xff0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 6 (first bytes 0f 00) */\n  { /* extensions[13] */\n    {OP_sldt, 0x0f0030, \"sldt\", Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_str,  0x0f0031, \"str\", Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_lldt, 0x0f0032, \"lldt\", xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {OP_ltr,  0x0f0033, \"ltr\", xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {OP_verr, 0x0f0034, \"verr\", xx, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n    {OP_verw, 0x0f0035, \"verw\", xx, xx, Ew, xx, xx, mrm, fWZ, END_LIST},\n    {INVALID, 0x0f0036, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0037, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 7 (first bytes 0f 01) */\n  { /* extensions[14] */\n    {MOD_EXT, 0x0f0130, \"(group 7 mod ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {MOD_EXT, 0x0f0131, \"(group 7 mod ext 1)\", xx, xx, xx, xx, xx, no, x, 1},\n    {MOD_EXT, 0x0f0132, \"(group 7 mod ext 5)\", xx, xx, xx, xx, xx, no, x, 5},\n    {MOD_EXT, 0x0f0133, \"(group 7 mod ext 4)\", xx, xx, xx, xx, xx, no, x, 4},\n    {OP_smsw, 0x0f0134, \"smsw\",  Ew, xx, xx, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f0135, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_lmsw, 0x0f0136, \"lmsw\",  xx, xx, Ew, xx, xx, mrm, x, END_LIST},\n    {MOD_EXT, 0x0f0137, \"(group 7 mod ext 2)\", xx, xx, xx, xx, xx, no, x, 2},\n  },\n  /* group 8 (first bytes 0f ba): all assumed to have Ib */\n  { /* extensions[15] */\n    {INVALID, 0x0fba30, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba31, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba32, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fba33, \"(bad)\",xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bt,  0x0fba34, \"bt\",    xx, xx, Ev, Ib, xx, mrm, fW6, END_LIST},\n    {OP_bts, 0x0fba35, \"bts\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n    {OP_btr, 0x0fba36, \"btr\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n    {OP_btc, 0x0fba37, \"btc\", Ev, xx, Ib, Ev, xx, mrm, fW6, END_LIST},\n  },\n  /* group 9 (first bytes 0f c7) */\n  { /* extensions[16] */\n    {INVALID, 0x0fc730, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_cmpxchg8b, 0x0fc731, \"cmpxchg8b\", Mq_dq, eAX, Mq_dq, eAX, eDX, mrm_xop, fWZ, exop[0x07]},/*\"cmpxchg16b\" w/ rex.w*/\n    {INVALID, 0x0fc732, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0fc733, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {REX_W_EXT, 0x0fc734, \"(rex.w ext 5)\", xx, xx, xx, xx, xx, mrm, x, 5},\n    {INVALID, 0x0fc735, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {MOD_EXT, 0x0fc736, \"(group 9 mod ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},\n    {MOD_EXT, 0x0fc737, \"(mod ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},\n  },\n  /* group 10 is all ud2b and is not used by us since identical */\n  /* group 11a (first byte c6) */\n  { /* extensions[17] */\n    {OP_mov_st, 0xc60020, \"mov\", Eb, xx, Ib, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xc60021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc60026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* XXX i#1314: this also sets eip */\n    {OP_xabort, 0xf8c60067, \"xabort\", eax, xx, Ib, xx, xx, mrm, x, END_LIST},\n  },\n  /* group 11b (first byte c7) */\n  { /* extensions[18] */\n    /* PR 250397: be aware that mov_imm shares this tail end of mov_st templates */\n    {OP_mov_st, 0xc70020, \"mov\", Ev, xx, Iz, xx, xx, mrm, x, tex[17][0]},\n    {INVALID, 0xc70021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xc70026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_xbegin, 0xf8c70067, \"xbegin\", xx, xx, Jz, xx, xx, mrm, x, END_LIST},\n  },\n  /* group 12 (first bytes 0f 71): all assumed to have Ib */\n  { /* extensions[19] */\n    {INVALID, 0x0f7130, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7132, \"(prefix ext 104)\", xx, xx, xx, xx, xx, no, x, 104},\n    {INVALID, 0x0f7133, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7134, \"(prefix ext 105)\", xx, xx, xx, xx, xx, no, x, 105},\n    {INVALID, 0x0f7135, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7136, \"(prefix ext 106)\", xx, xx, xx, xx, xx, no, x, 106},\n    {INVALID, 0x0f7137, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 13 (first bytes 0f 72): all assumed to have Ib */\n  { /* extensions[20] */\n    {INVALID, 0x0f7230, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7231, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7232, \"(prefix ext 107)\", xx, xx, xx, xx, xx, no, x, 107},\n    {INVALID, 0x0f7233, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7234, \"(prefix ext 108)\", xx, xx, xx, xx, xx, no, x, 108},\n    {INVALID, 0x0f7235, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7236, \"(prefix ext 109)\", xx, xx, xx, xx, xx, no, x, 109},\n    {INVALID, 0x0f7237, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n },\n  /* group 14 (first bytes 0f 73): all assumed to have Ib */\n  { /* extensions[21] */\n    {INVALID, 0x0f7330, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7331, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7332, \"(prefix ext 110)\", xx, xx, xx, xx, xx, no, x, 110},\n    {PREFIX_EXT, 0x0f7333, \"(prefix ext 101)\", xx, xx, xx, xx, xx, no, x, 101},\n    {INVALID, 0x0f7334, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f7335, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {PREFIX_EXT, 0x0f7336, \"(prefix ext 111)\", xx, xx, xx, xx, xx, no, x, 111},\n    {PREFIX_EXT, 0x0f7337, \"(prefix ext 102)\", xx, xx, xx, xx, xx, no, x, 102},\n  },\n  /* group 15 (first bytes 0f ae) */\n  { /* extensions[22] */\n    /* Intel tables imply they may add opcodes in the mod=3 (non-mem) space in future */\n    {MOD_EXT,    0x0fae30, \"(group 15 mod ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},\n    {MOD_EXT,    0x0fae31, \"(group 15 mod ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},\n    {MOD_EXT,    0x0fae32, \"(group 15 mod ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},\n    {MOD_EXT,    0x0fae33, \"(group 15 mod ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},\n    {REX_W_EXT,  0x0fae34, \"(rex.w ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n    {MOD_EXT,    0x0fae35, \"(group 15 mod ext 6)\", xx, xx, xx, xx, xx, no, x, 6},\n    {MOD_EXT,    0x0fae36, \"(group 15 mod ext 7)\", xx, xx, xx, xx, xx, no, x, 7},\n    {MOD_EXT,    0x0fae37, \"(group 15 mod ext 3)\", xx, xx, xx, xx, xx, no, x, 3},\n },\n  /* group 16 (first bytes 0f 18) */\n  { /* extensions[23] */\n    /* Intel tables imply they may add opcodes in the mod=3 (non-mem) space in future */\n    {OP_prefetchnta, 0x0f1830, \"prefetchnta\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht0,  0x0f1831, \"prefetcht0\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht1,  0x0f1832, \"prefetcht1\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetcht2,  0x0f1833, \"prefetcht2\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1834, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1835, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1836, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n    {OP_nop_modrm, 0x0f1837, \"nop\", xx, xx, Ed, xx, xx, mrm, x, END_LIST},\n },\n  /* group AMD (first bytes 0f 0d) */\n  { /* extensions[24] */\n    {OP_prefetch,  0x0f0d30, \"prefetch\",  xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_prefetchw, 0x0f0d31, \"prefetchw\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f0d32, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d33, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d34, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d35, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d36, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x0f0d37, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* group 1c* -- first opcode byte 82\n   * see PR 235092 for the discrepancies in what 0x82 should be: empirically\n   * and according to recent Intel manuals it matches 0x80, not 0x83 (as old\n   * Intel manuals implied) or invalid (as gnu tools claim).\n   * not linked into any encode chain.\n   */\n  { /* extensions[25]: all assumed to have Ib */\n    {OP_add, 0x820020, \"add\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_or,  0x820021, \"or\",  Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_adc, 0x820022, \"adc\", Eb, xx, Ib, Eb, xx, mrm|i64, (fW6|fRC), END_LIST},\n    {OP_sbb, 0x820023, \"sbb\", Eb, xx, Ib, Eb, xx, mrm|i64, (fW6|fRC), END_LIST},\n    {OP_and, 0x820024, \"and\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_sub, 0x820025, \"sub\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_xor, 0x820026, \"xor\", Eb, xx, Ib, Eb, xx, mrm|i64, fW6,  END_LIST},\n    {OP_cmp, 0x820027, \"cmp\", xx, xx, Eb, Ib, xx, mrm|i64, fW6,  END_LIST},\n  },\n  /* group 1d (Intel now calling Group 1A) -- first opcode byte 8f */\n  { /* extensions[26] */\n    {OP_pop,  0x8f0020, \"pop\", Esv, xsp, xsp, i_xSP, xx, mrm, x, tfb[0x17]},\n    /* we shouldn't ever get here for these, as this becomes an XOP prefix */\n    {INVALID, 0x8f0021, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0022, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0023, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0024, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0025, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0026, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x8f0027, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 1 */\n  { /* extensions[27] */\n    {INVALID,     0x090138, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blcfill,  0x090139, \"blcfill\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsfill,  0x09013a, \"blsfill\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blcs,     0x09013b, \"blcs\",    By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_tzmsk,    0x09013c, \"tzmsk\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blcic,    0x09013d, \"blcic\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsic,    0x09013e, \"blsic\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_t1mskc,   0x09013f, \"t1mskc\",  By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n  },\n  /* XOP group 2 */\n  { /* extensions[28] */\n    {INVALID,     0x090238, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blcmsk,   0x090239, \"blcmsk\",By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x09023a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09023d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blci,     0x09023e, \"blci\",  By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x09023f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 3 */\n  { /* extensions[29] */\n    /* XXX i#1311: these instrs implicitly write to memory which we should\n     * find a way to encode into the IR.\n     */\n    {OP_llwpcb,   0x091238, \"llwpcb\", xx, xx, Ry, xx, xx, mrm|vex, x, END_LIST},\n    {OP_slwpcb,   0x091239, \"slwpcb\", Ry, xx, xx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0x09123a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123e, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x09123f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* XOP group 4: all assumed to have a 4-byte immediate by xop_a_extra[] */\n  { /* extensions[30] */\n    /* XXX i#1311: these instrs implicitly write to memory which we should\n     * find a way to encode into the IR.\n     */\n    {OP_lwpins,   0x0a1238, \"lwpins\", xx, xx, By, Ed, Id, mrm|vex, fWC, END_LIST},\n    {OP_lwpval,   0x0a1239, \"lwpval\", xx, xx, By, Ed, Id, mrm|vex, x, END_LIST},\n    {INVALID,     0x0a123a, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123b, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123c, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123d, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123e, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0a123f, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* group 17 */\n  { /* extensions[31] */\n    {INVALID,     0x38f338, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_blsr,     0x38f339, \"blsr\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsmsk,   0x38f33a, \"blsmsk\", By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {OP_blsi,     0x38f33b, \"blsi\",   By, xx, Ey, xx, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,     0x38f33c, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33d, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33e, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x38f33f, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Two-byte instructions that differ depending on presence of\n * prefixes, indexed in this order:\n *   none, 0xf3, 0x66, 0xf2\n * A second set is used for vex-encoded instructions, indexed in the\n * same order by prefix.\n * A third set is used for evex-encoded instructions, indexed in the\n * same order by prefix.\n *\n * N.B.: to avoid having a full entry here when there is only one\n * valid opcode prefix, use |reqp in the original entry instead of\n * pointing to this table.\n */\nconst instr_info_t prefix_extensions[][12] = {\n  /* prefix extension 0 */\n  {\n    {OP_movups, 0x0f1010, \"movups\", Vps, xx, Wps, xx, xx, mrm, x, tpe[1][0]},\n    {MOD_EXT,   0xf30f1010, \"(mod ext 18)\",  xx, xx, xx, xx, xx, mrm, x, 18},\n    {OP_movupd, 0x660f1010, \"movupd\", Vpd, xx, Wpd, xx, xx, mrm, x, tpe[1][2]},\n    {MOD_EXT,   0xf20f1010, \"(mod ext 19)\",  xx, xx, xx, xx, xx, mrm, x, 19},\n    {OP_vmovups,   0x0f1010, \"vmovups\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tpe[1][4]},\n    {MOD_EXT,    0xf30f1010, \"(mod ext 8)\", xx, xx, xx, xx, xx, mrm|vex, x, 8},\n    {OP_vmovupd, 0x660f1010, \"vmovupd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, tpe[1][6]},\n    {MOD_EXT,    0xf20f1010, \"(mod ext 9)\", xx, xx, xx, xx, xx, mrm|vex, x, 9},\n    {EVEX_W_EXT, 0x0f1010, \"(evex_W ext 0)\", xx, xx, xx, xx, xx, mrm|evex, x, 0},\n    {INVALID, 0xf30f1010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1010, \"(evex_W ext 2)\", xx, xx, xx, xx, xx, mrm|evex, x, 2},\n    {INVALID, 0xf20f1010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 1 */\n  {\n    {OP_movups, 0x0f1110, \"movups\", Wps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {OP_movss,  0xf30f1110, \"movss\",  Wss, xx, Vss, xx, xx, mrm, x, END_LIST},\n    {OP_movupd, 0x660f1110, \"movupd\", Wpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {OP_movsd,  0xf20f1110, \"movsd\",  Wsd, xx, Vsd, xx, xx, mrm, x, END_LIST},\n    {OP_vmovups,   0x0f1110, \"vmovups\", Wvs, xx, Vvs, xx, xx, mrm|vex, x, tevexw[0][0]},\n    {MOD_EXT,    0xf30f1110, \"(mod ext 10)\", xx, xx, xx, xx, xx, mrm|vex, x, 10},\n    {OP_vmovupd, 0x660f1110, \"vmovupd\", Wvd, xx, Vvd, xx, xx, mrm|vex, x, tevexw[2][1]},\n    {MOD_EXT,    0xf20f1110, \"(mod ext 11)\", xx, xx, xx, xx, xx, mrm|vex, x, 11},\n    {EVEX_W_EXT, 0x0f1110, \"(evex_W ext 1)\", xx, xx, xx, xx, xx, mrm|evex, x, 1},\n    {INVALID, 0xf30f1110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f1110, \"(evex_W ext 3)\", xx, xx, xx, xx, xx, mrm|evex, x, 3},\n    {INVALID, 0xf20f1110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 2 */\n  {\n    /* i#319: note that the reg-reg form of the load version (0f12) is legal\n     * and has a separate pneumonic (\"movhlps\"), yet the reg-reg form of\n     * the store version (0f13) is illegal\n     */\n    {OP_movlps, 0x0f1210, \"movlps\", Vq_dq, xx, Wq_dq, xx, xx, mrm, x, tpe[3][0]}, /*\"movhlps\" if reg-reg */\n    {OP_movsldup, 0xf30f1210, \"movsldup\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_movlpd, 0x660f1210, \"movlpd\", Vq_dq, xx, Mq, xx, xx, mrm, x, tpe[3][2]},\n    {OP_movddup, 0xf20f1210, \"movddup\", Vpd, xx, Wq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_vmovlps,    0x0f1210, \"vmovlps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|vex|reqL0, x, tpe[3][4]}, /*\"vmovhlps\" if reg-reg */\n    {OP_vmovsldup,0xf30f1210, \"vmovsldup\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vmovlpd,  0x660f1210, \"vmovlpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|vex, x, tpe[3][6]},\n    {OP_vmovddup, 0xf20f1210, \"vmovddup\", Vvd, xx, Wvq_dq, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 3 */\n  {\n    {OP_movlps, 0x0f1310, \"movlps\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movlpd, 0x660f1310, \"movlpd\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovlps, 0x0f1310, \"vmovlps\", Mq, xx, Vq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovlpd, 0x660f1310, \"vmovlpd\", Mq, xx, Vq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 4 */\n  {\n    {OP_unpcklps, 0x0f1410, \"unpcklps\", Vps, xx, Wq_dq, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_unpcklpd, 0x660f1410, \"unpcklpd\", Vpd, xx, Wq_dq, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpcklps, 0x0f1410, \"vunpcklps\", Vvs, xx, Hvs, Wvq_dq, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpcklpd, 0x660f1410, \"vunpcklpd\", Vvd, xx, Hvd, Wvq_dq, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 5 */\n  {\n    {OP_unpckhps, 0x0f1510, \"unpckhps\", Vps, xx, Wq_dq, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_unpckhpd, 0x660f1510, \"unpckhpd\", Vpd, xx, Wq_dq, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpckhps, 0x0f1510, \"vunpckhps\", Vvs, xx, Hvs, Wvq_dq, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vunpckhpd, 0x660f1510, \"vunpckhpd\", Vvd, xx, Hvd, Wvq_dq, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 6 */\n  {\n    /* i#319: note that the reg-reg form of the load version (0f16) is legal\n     * and has a separate pneumonic (\"movhlps\"), yet the reg-reg form of\n     * the store version (0f17) is illegal\n     */\n    {OP_movhps, 0x0f1610, \"movhps\", Vq_dq, xx, Wq_dq, xx, xx, mrm, x, tpe[7][0]}, /*\"movlhps\" if reg-reg */\n    {OP_movshdup, 0xf30f1610, \"movshdup\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_movhpd, 0x660f1610, \"movhpd\", Vq_dq, xx, Mq, xx, xx, mrm, x, tpe[7][2]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhps, 0x0f1610, \"vmovhps\", Vq_dq, xx, Hq_dq, Wq_dq, xx, mrm|vex|reqL0, x, tpe[7][4]}, /*\"vmovlhps\" if reg-reg */\n    {OP_vmovshdup, 0xf30f1610, \"vmovshdup\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vmovhpd, 0x660f1610, \"vmovhpd\", Vq_dq, xx, Hq_dq, Mq, xx, mrm|vex|reqL0, x, tpe[7][6]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 7 */\n  {\n    {OP_movhps, 0x0f1710, \"movhps\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movhpd, 0x660f1710, \"movhpd\", Mq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhps, 0x0f1710, \"vmovhps\", Mq, xx, Vq_dq, xx, xx, mrm|vex|reqL0, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovhpd, 0x660f1710, \"vmovhpd\", Mq, xx, Vq_dq, xx, xx, mrm|vex|reqL0, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f1710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 8 */\n  {\n    {OP_movaps, 0x0f2810, \"movaps\", Vps, xx, Wps, xx, xx, mrm, x, tpe[9][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movapd, 0x660f2810, \"movapd\", Vpd, xx, Wpd, xx, xx, mrm, x, tpe[9][2]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovaps, 0x0f2810, \"vmovaps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, tpe[9][4]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovapd, 0x660f2810, \"vmovapd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, tpe[9][6]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT,   0x0f2810, \"(evex_W ext 4)\", xx, xx, xx, xx, xx, mrm|evex, x, 4},\n    {INVALID,    0xf30f2810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2810, \"(evex_W ext 6)\", xx, xx, xx, xx, xx, mrm|evex, x, 6},\n    {INVALID,    0xf20f2810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 9 */\n  {\n    {OP_movaps, 0x0f2910, \"movaps\", Wps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movapd, 0x660f2910, \"movapd\", Wpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovaps, 0x0f2910, \"vmovaps\", Wvs, xx, Vvs, xx, xx, mrm|vex, x, tevexw[4][0]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovapd, 0x660f2910, \"vmovapd\", Wvd, xx, Vvd, xx, xx, mrm|vex, x, tevexw[6][1]},\n    {INVALID, 0x00000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT,   0x0f2910, \"(evex_W ext 5)\", xx, xx, xx, xx, xx, mrm|evex, x, 5},\n    {INVALID,    0xf30f2910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {EVEX_W_EXT, 0x660f2910, \"(evex_W ext 7)\", xx, xx, xx, xx, xx, mrm|evex, x, 7},\n    {INVALID,    0xf20f2910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 10 */\n  {\n    {OP_cvtpi2ps,  0x0f2a10, \"cvtpi2ps\", Vq_dq, xx, Qq, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsi2ss, 0xf30f2a10, \"cvtsi2ss\", Vss, xx, Ed_q, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpi2pd, 0x660f2a10, \"cvtpi2pd\", Vpd, xx, Qq, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsi2sd, 0xf20f2a10, \"cvtsi2sd\", Vsd, xx, Ed_q, xx, xx, mrm, x, END_LIST},\n    {INVALID,  0x0f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsi2ss, 0xf30f2a10, \"vcvtsi2ss\", Vss, xx, H12_dq, Ed_q, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsi2sd, 0xf20f2a10, \"vcvtsi2sd\", Vsd, xx, Hsd, Ed_q, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 11 */\n  {\n    {OP_movntps,   0x0f2b10, \"movntps\", Mps, xx, Vps, xx, xx, mrm, x, END_LIST},\n    {OP_movntss, 0xf30f2b10, \"movntss\", Mss, xx, Vss, xx, xx, mrm, x, END_LIST},\n    {OP_movntpd, 0x660f2b10, \"movntpd\", Mpd, xx, Vpd, xx, xx, mrm, x, END_LIST},\n    {OP_movntsd, 0xf20f2b10, \"movntsd\", Msd, xx, Vsd, xx, xx, mrm, x, END_LIST},\n    {OP_vmovntps,   0x0f2b10, \"vmovntps\", Mvs, xx, Vvs, xx, xx, mrm|vex, x, END_LIST},\n    /* XXX: AMD doesn't list movntss in their new manual => assuming no vex version */\n    {INVALID, 0xf30f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovntpd, 0x660f2b10, \"vmovntpd\", Mvd, xx, Vvd, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 12 */\n  {\n    {OP_cvttps2pi, 0x0f2c10, \"cvttps2pi\", Pq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvttss2si, 0xf30f2c10, \"cvttss2si\", Gd_q, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvttpd2pi, 0x660f2c10, \"cvttpd2pi\", Pq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvttsd2si, 0xf20f2c10, \"cvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvttss2si, 0xf30f2c10, \"vcvttss2si\", Gd_q, xx, Wss, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvttsd2si, 0xf20f2c10, \"vcvttsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 13 */\n  {\n    {OP_cvtps2pi, 0x0f2d10, \"cvtps2pi\", Pq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtss2si, 0xf30f2d10, \"cvtss2si\", Gd_q, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2pi, 0x660f2d10, \"cvtpd2pi\", Pq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsd2si, 0xf20f2d10, \"cvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x0f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtss2si, 0xf30f2d10, \"vcvtss2si\", Gd_q, xx, Wss, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtsd2si, 0xf20f2d10, \"vcvtsd2si\", Gd_q, xx, Wsd, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 14 */\n  {\n    {OP_ucomiss, 0x0f2e10, \"ucomiss\", xx, xx, Vss, Wss, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_ucomisd, 0x660f2e10, \"ucomisd\", xx, xx, Vsd, Wsd, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vucomiss, 0x0f2e10, \"vucomiss\", xx, xx, Vss, Wss, xx, mrm|vex, fW6, END_LIST},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vucomisd, 0x660f2e10, \"vucomisd\", xx, xx, Vsd, Wsd, xx, mrm|vex, fW6, END_LIST},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 15 */\n  {\n    {OP_comiss,  0x0f2f10, \"comiss\",  xx, xx, Vss, Wss, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_comisd,  0x660f2f10, \"comisd\",  xx, xx, Vsd, Wsd, xx, mrm, fW6, END_LIST},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcomiss,  0x0f2f10, \"vcomiss\",  xx, xx, Vss, Wss, xx, mrm|vex, fW6, END_LIST},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vcomisd,  0x660f2f10, \"vcomisd\",  xx, xx, Vsd, Wsd, xx, mrm|vex, fW6, END_LIST},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f2f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 16 */\n  {\n    {OP_movmskps, 0x0f5010, \"movmskps\", Gr, xx, Ups, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_movmskpd, 0x660f5010, \"movmskpd\", Gr, xx, Upd, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovmskps, 0x0f5010, \"vmovmskps\", Gr, xx, Uvs, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovmskpd, 0x660f5010, \"vmovmskpd\", Gr, xx, Uvd, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 17 */\n  {\n    {OP_sqrtps, 0x0f5110, \"sqrtps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtss, 0xf30f5110, \"sqrtss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtpd, 0x660f5110, \"sqrtpd\", Vpd, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_sqrtsd, 0xf20f5110, \"sqrtsd\", Vsd, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {OP_vsqrtps, 0x0f5110, \"vsqrtps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtss, 0xf30f5110, \"vsqrtss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtpd, 0x660f5110, \"vsqrtpd\", Vvd, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vsqrtsd, 0xf20f5110, \"vsqrtsd\", Vdq, xx, Hsd, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 18 */\n  {\n    {OP_rsqrtps, 0x0f5210, \"rsqrtps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_rsqrtss, 0xf30f5210, \"rsqrtss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vrsqrtps, 0x0f5210, \"vrsqrtps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vrsqrtss, 0xf30f5210, \"vrsqrtss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 19 */\n  {\n    {OP_rcpps, 0x0f5310, \"rcpps\", Vps, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_rcpss, 0xf30f5310, \"rcpss\", Vss, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vrcpps, 0x0f5310, \"vrcpps\", Vvs, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vrcpss, 0xf30f5310, \"vrcpss\", Vdq, xx, H12_dq, Wss, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 20 */\n  {\n    {OP_andps,  0x0f5410, \"andps\",  Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_andpd,  0x660f5410, \"andpd\",  Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandps,  0x0f5410, \"vandps\",  Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vandpd,  0x660f5410, \"vandpd\",  Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 21 */\n  {\n    {OP_andnps, 0x0f5510, \"andnps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_andnpd, 0x660f5510, \"andnpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vandnps, 0x0f5510, \"vandnps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vandnpd, 0x660f5510, \"vandnpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 22 */\n  {\n    {OP_orps,   0x0f5610, \"orps\",   Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_orpd,   0x660f5610, \"orpd\",   Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vorps,   0x0f5610, \"vorps\",   Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vorpd,   0x660f5610, \"vorpd\",   Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 23 */\n  {\n    {OP_xorps,  0x0f5710, \"xorps\",  Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_xorpd,  0x660f5710, \"xorpd\",  Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vxorps,  0x0f5710, \"vxorps\",  Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vxorpd,  0x660f5710, \"vxorpd\",  Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 24 */\n  {\n    {OP_addps, 0x0f5810, \"addps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_addss, 0xf30f5810, \"addss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_addpd, 0x660f5810, \"addpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_addsd, 0xf20f5810, \"addsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vaddps, 0x0f5810, \"vaddps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vaddss, 0xf30f5810, \"vaddss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vaddpd, 0x660f5810, \"vaddpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vaddsd, 0xf20f5810, \"vaddsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 25 */\n  {\n    {OP_mulps, 0x0f5910, \"mulps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_mulss, 0xf30f5910, \"mulss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_mulpd, 0x660f5910, \"mulpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_mulsd, 0xf20f5910, \"mulsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vmulps, 0x0f5910, \"vmulps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vmulss, 0xf30f5910, \"vmulss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vmulpd, 0x660f5910, \"vmulpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vmulsd, 0xf20f5910, \"vmulsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n  },\n  /* prefix extension 26 */\n  {\n    {OP_cvtps2pd, 0x0f5a10, \"cvtps2pd\", Vpd, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtss2sd, 0xf30f5a10, \"cvtss2sd\", Vsd, xx, Wss, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2ps, 0x660f5a10, \"cvtpd2ps\", Vps, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtsd2ss, 0xf20f5a10, \"cvtsd2ss\", Vss, xx, Wsd, xx, xx, mrm, x, END_LIST},\n    {OP_vcvtps2pd, 0x0f5a10, \"vcvtps2pd\", Vvd, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtss2sd, 0xf30f5a10, \"vcvtss2sd\", Vsd, xx, Hsd, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtpd2ps, 0x660f5a10, \"vcvtpd2ps\", Vvs, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtsd2ss, 0xf20f5a10, \"vcvtsd2ss\", Vss, xx, H12_dq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 27 */\n  {\n    {OP_cvtdq2ps, 0x0f5b10, \"cvtdq2ps\", Vps, xx, Wdq, xx, xx, mrm, x, END_LIST},\n    {OP_cvttps2dq, 0xf30f5b10, \"cvttps2dq\", Vdq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {OP_cvtps2dq, 0x660f5b10, \"cvtps2dq\", Vdq, xx, Wps, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtdq2ps, 0x0f5b10, \"vcvtdq2ps\", Vvs, xx, Wx, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvttps2dq, 0xf30f5b10, \"vcvttps2dq\", Vx, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtps2dq, 0x660f5b10, \"vcvtps2dq\", Vx, xx, Wvs, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 28 */\n  {\n    {OP_subps, 0x0f5c10, \"subps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_subss, 0xf30f5c10, \"subss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_subpd, 0x660f5c10, \"subpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_subsd, 0xf20f5c10, \"subsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vsubps, 0x0f5c10, \"vsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vsubss, 0xf30f5c10, \"vsubss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vsubpd, 0x660f5c10, \"vsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vsubsd, 0xf20f5c10, \"vsubsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 29 */\n  {\n    {OP_minps, 0x0f5d10, \"minps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_minss, 0xf30f5d10, \"minss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_minpd, 0x660f5d10, \"minpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_minsd, 0xf20f5d10, \"minsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vminps, 0x0f5d10, \"vminps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vminss, 0xf30f5d10, \"vminss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vminpd, 0x660f5d10, \"vminpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vminsd, 0xf20f5d10, \"vminsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 30 */\n  {\n    {OP_divps, 0x0f5e10, \"divps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_divss, 0xf30f5e10, \"divss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_divpd, 0x660f5e10, \"divpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_divsd, 0xf20f5e10, \"divsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vdivps, 0x0f5e10, \"vdivps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vdivss, 0xf30f5e10, \"vdivss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vdivpd, 0x660f5e10, \"vdivpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vdivsd, 0xf20f5e10, \"vdivsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 31 */\n  {\n    {OP_maxps, 0x0f5f10, \"maxps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {OP_maxss, 0xf30f5f10, \"maxss\", Vss, xx, Wss, Vss, xx, mrm, x, END_LIST},\n    {OP_maxpd, 0x660f5f10, \"maxpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_maxsd, 0xf20f5f10, \"maxsd\", Vsd, xx, Wsd, Vsd, xx, mrm, x, END_LIST},\n    {OP_vmaxps, 0x0f5f10, \"vmaxps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxss, 0xf30f5f10, \"vmaxss\", Vdq, xx, Hdq, Wss, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxpd, 0x660f5f10, \"vmaxpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vmaxsd, 0xf20f5f10, \"vmaxsd\", Vdq, xx, Hdq, Wsd, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f5f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 32 */\n  {\n    {OP_punpcklbw,   0x0f6010, \"punpcklbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[32][2]},\n    {INVALID,      0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklbw, 0x660f6010, \"punpcklbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6010,   \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklbw, 0x660f6010, \"vpunpcklbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 33 */\n  {\n    {OP_punpcklwd,   0x0f6110, \"punpcklwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[33][2]},\n    {INVALID,      0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklwd, 0x660f6110, \"punpcklwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklwd, 0x660f6110, \"vpunpcklwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 34 */\n  {\n    {OP_punpckldq,   0x0f6210, \"punpckldq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[34][2]},\n    {INVALID,      0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckldq, 0x660f6210, \"punpckldq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckldq, 0x660f6210, \"vpunpckldq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 35 */\n  {\n    {OP_packsswb,   0x0f6310, \"packsswb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[35][2]},\n    {INVALID,     0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packsswb, 0x660f6310, \"packsswb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpacksswb, 0x660f6310, \"vpacksswb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 36 */\n  {\n    {OP_pcmpgtb,   0x0f6410, \"pcmpgtb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[36][2]},\n    {INVALID,    0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtb, 0x660f6410, \"pcmpgtb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtb, 0x660f6410, \"vpcmpgtb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 37 */\n  {\n    {OP_pcmpgtw,   0x0f6510, \"pcmpgtw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[37][2]},\n    {INVALID,    0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtw, 0x660f6510, \"pcmpgtw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtw, 0x660f6510, \"vpcmpgtw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 38 */\n  {\n    {OP_pcmpgtd,   0x0f6610, \"pcmpgtd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[38][2]},\n    {INVALID,    0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpgtd, 0x660f6610, \"pcmpgtd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpgtd, 0x660f6610, \"vpcmpgtd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 39 */\n  {\n    {OP_packuswb,   0x0f6710, \"packuswb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[39][2]},\n    {INVALID,     0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packuswb, 0x660f6710, \"packuswb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpackuswb, 0x660f6710, \"vpackuswb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 40 */\n  {\n    {OP_punpckhbw,   0x0f6810, \"punpckhbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[40][2]},\n    {INVALID,      0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhbw, 0x660f6810, \"punpckhbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhbw, 0x660f6810, \"vpunpckhbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 41 */\n  {\n    {OP_punpckhwd,   0x0f6910, \"punpckhwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[41][2]},\n    {INVALID,      0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhwd, 0x660f6910, \"punpckhwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhwd, 0x660f6910, \"vpunpckhwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 42 */\n  {\n    {OP_punpckhdq,   0x0f6a10, \"punpckhdq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[42][2]},\n    {INVALID,      0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhdq, 0x660f6a10, \"punpckhdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,        0x0f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhdq, 0x660f6a10, \"vpunpckhdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 43 */\n  {\n    {OP_packssdw,   0x0f6b10, \"packssdw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[43][2]},\n    {INVALID,     0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_packssdw, 0x660f6b10, \"packssdw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0x0f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpackssdw, 0x660f6b10, \"vpackssdw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 44 */\n  {\n    {INVALID,         0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpcklqdq, 0x660f6c10, \"punpcklqdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,       0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpcklqdq, 0x660f6c10, \"vpunpcklqdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,       0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 45 */\n  {\n    {INVALID,         0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_punpckhqdq, 0x660f6d10, \"punpckhqdq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,       0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpunpckhqdq, 0x660f6d10, \"vpunpckhqdq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,       0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 46 */\n  {\n    /* movd zeroes the top bits when the destination is an mmx or xmm reg */\n    {OP_movd,   0x0f6e10, \"movd\", Pq, xx, Ed_q, xx, xx, mrm, x, tpe[46][2]},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movd, 0x660f6e10, \"movd\", Vdq, xx, Ed_q, xx, xx, mrm, x, tpe[51][0]},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovd, 0x660f6e10, \"vmovd\", Vdq, xx, Ed_q, xx, xx, mrm|vex, x, tpe[51][6]},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 47: all assumed to have Ib */\n  {\n    {OP_pshufw,   0x0f7010, \"pshufw\",   Pq, xx, Qq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshufhw, 0xf30f7010, \"pshufhw\", Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshufd,  0x660f7010, \"pshufd\",  Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {OP_pshuflw, 0xf20f7010, \"pshuflw\", Vdq, xx, Wdq, Ib, xx, mrm, x, END_LIST},\n    {INVALID,       0x0f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpshufhw, 0xf30f7010, \"vpshufhw\", Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    {OP_vpshufd,  0x660f7010, \"vpshufd\",  Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    {OP_vpshuflw, 0xf20f7010, \"vpshuflw\", Vx, xx, Wx, Ib, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 48 */\n  {\n    {OP_pcmpeqb,   0x0f7410, \"pcmpeqb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[48][2]},\n    {INVALID,    0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqb, 0x660f7410, \"pcmpeqb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqb, 0x660f7410, \"vpcmpeqb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 49 */\n  {\n    {OP_pcmpeqw,   0x0f7510, \"pcmpeqw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[49][2]},\n    {INVALID,    0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqw, 0x660f7510, \"pcmpeqw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqw, 0x660f7510, \"vpcmpeqw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 50 */\n  {\n    {OP_pcmpeqd,   0x0f7610, \"pcmpeqd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[50][2]},\n    {INVALID,    0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pcmpeqd, 0x660f7610, \"pcmpeqd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpcmpeqd, 0x660f7610, \"vpcmpeqd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 51 */\n  {\n    {OP_movd,   0x0f7e10, \"movd\", Ed_q, xx, Pd_q, xx, xx, mrm, x, tpe[51][2]},\n    /* movq zeroes the top bits when the destination is an mmx or xmm reg */\n    {OP_movq, 0xf30f7e10, \"movq\", Vdq, xx, Wq_dq, xx, xx, mrm, x, tpe[61][2]},\n    {OP_movd, 0x660f7e10, \"movd\", Ed_q, xx, Vd_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovq, 0xf30f7e10, \"vmovq\", Vdq, xx, Wq_dq, xx, xx, mrm|vex, x, tpe[61][6]},\n    {OP_vmovd, 0x660f7e10, \"vmovd\", Ed_q, xx, Vd_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7e10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 52: all assumed to have Ib */\n  {\n    {OP_cmpps, 0x0fc210, \"cmpps\", Vps, xx, Wps, Ib, Vps, mrm, x, END_LIST},\n    {OP_cmpss, 0xf30fc210, \"cmpss\", Vss, xx, Wss, Ib, Vss, mrm, x, END_LIST},\n    {OP_cmppd, 0x660fc210, \"cmppd\", Vpd, xx, Wpd, Ib, Vpd, mrm, x, END_LIST},\n    {OP_cmpsd, 0xf20fc210, \"cmpsd\", Vsd, xx, Wsd, Ib, Vsd, mrm, x, END_LIST},\n    {OP_vcmpps, 0x0fc210, \"vcmpps\", Vvs, xx, Hvs, Wvs, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmpss, 0xf30fc210, \"vcmpss\", Vdq, xx, Hdq, Wss, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmppd, 0x660fc210, \"vcmppd\", Vvd, xx, Hvd, Wvd, Ib, mrm|vex, x, END_LIST},\n    {OP_vcmpsd, 0xf20fc210, \"vcmpsd\", Vdq, xx, Hdq, Wsd, Ib, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 53: all assumed to have Ib */\n  { /* note that gnu tools print immed first: pinsrw $0x0,(%esp),%xmm0 */\n    /* FIXME i#1388: pinsrw actually reads only bottom word of reg */\n    {OP_pinsrw,   0x0fc410, \"pinsrw\", Pw_q, xx, Rd_Mw, Ib, xx, mrm, x, tpe[53][2]},\n    {INVALID,   0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pinsrw, 0x660fc410, \"pinsrw\", Vw_dq, xx, Rd_Mw, Ib, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x0fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpinsrw, 0x660fc410, \"vpinsrw\", Vdq, xx, H14_dq, Rd_Mw, Ib, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 54: all assumed to have Ib */\n  { /* note that gnu tools print immed first: pextrw $0x7,%xmm7,%edx */\n    {OP_pextrw,   0x0fc510, \"pextrw\", Gd, xx, Nw_q, Ib, xx, mrm, x, tpe[54][2]},\n    {INVALID,   0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pextrw, 0x660fc510, \"pextrw\", Gd, xx, Uw_dq, Ib, xx, mrm, x, tvex[37][0]},\n    {INVALID,   0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x0fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpextrw, 0x660fc510, \"vpextrw\", Gd, xx, Uw_dq, Ib, xx, mrm|vex, x, tvex[37][1]},\n    {INVALID,   0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 55: all assumed to have Ib */\n  {\n    {OP_shufps, 0x0fc610, \"shufps\", Vps, xx, Wps, Ib, Vps, mrm, x, END_LIST},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_shufpd, 0x660fc610, \"shufpd\", Vpd, xx, Wpd, Ib, Vpd, mrm, x, END_LIST},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vshufps, 0x0fc610, \"vshufps\", Vvs, xx, Hvs, Wvs, Ib, mrm|vex, x, END_LIST},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vshufpd, 0x660fc610, \"vshufpd\", Vvd, xx, Hvd, Wvd, Ib, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 56 */\n  {\n    {OP_psrlw,   0x0fd110, \"psrlw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[56][2]},\n    {INVALID,  0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrlw, 0x660fd110, \"psrlw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[104][0]},\n    {INVALID,  0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrlw,  0x660fd110, \"vpsrlw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[104][6]},\n    {INVALID,  0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 57 */\n  {\n    {OP_psrld,   0x0fd210, \"psrld\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[57][2]},\n    {INVALID,  0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrld, 0x660fd210, \"psrld\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[107][0]},\n    {INVALID,  0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrld, 0x660fd210, \"vpsrld\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[107][6]},\n    {INVALID,  0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 58 */\n  {\n    {OP_psrlq,   0x0fd310, \"psrlq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[58][2]},\n    {INVALID,  0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrlq, 0x660fd310, \"psrlq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[110][0]},\n    {INVALID,  0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrlq, 0x660fd310, \"vpsrlq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[110][6]},\n    {INVALID,  0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 59 */\n  {\n    {OP_paddq,   0x0fd410, \"paddq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[59][2]},\n    {INVALID,  0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_paddq, 0x660fd410, \"paddq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,  0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpaddq, 0x660fd410, \"vpaddq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,  0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 60 */\n  {\n    {OP_pmullw,   0x0fd510, \"pmullw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[60][2]},\n    {INVALID,   0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmullw, 0x660fd510, \"pmullw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmullw, 0x660fd510, \"vpmullw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 61 */\n  {\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_movq2dq, 0xf30fd610, \"movq2dq\", Vdq, xx, Nq, xx, xx, mrm, x, END_LIST},\n    {OP_movq, 0x660fd610, \"movq\", Wq_dq, xx, Vq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_movdq2q, 0xf20fd610, \"movdq2q\", Pq, xx, Uq_dq, xx, xx, mrm, x, END_LIST},\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmovq, 0x660fd610, \"vmovq\", Wq_dq, xx, Vq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0xf20fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 62 */\n  {\n    {OP_pmovmskb,   0x0fd710, \"pmovmskb\", Gd, xx, Nq, xx, xx, mrm, x, tpe[62][2]},\n    {INVALID,     0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmovmskb, 0x660fd710, \"pmovmskb\", Gd, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {INVALID,     0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,       0x0fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmovmskb, 0x660fd710, \"vpmovmskb\", Gd, xx, Ux, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 63 */\n  {\n    {OP_psubusb,   0x0fd810, \"psubusb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[63][2]},\n    {INVALID,    0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubusb, 0x660fd810, \"psubusb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubusb, 0x660fd810, \"vpsubusb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 64 */\n  {\n    {OP_psubusw,   0x0fd910, \"psubusw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[64][2]},\n    {INVALID,    0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubusw, 0x660fd910, \"psubusw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubusw, 0x660fd910, \"vpsubusw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 65 */\n  {\n    {OP_pminub,   0x0fda10, \"pminub\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[65][2]},\n    {INVALID,    0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pminub, 0x660fda10, \"pminub\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpminub, 0x660fda10, \"vpminub\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 66 */\n  {\n    {OP_pand,   0x0fdb10, \"pand\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[66][2]},\n    {INVALID,    0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pand, 0x660fdb10, \"pand\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpand, 0x660fdb10, \"vpand\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fdb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 67 */\n  {\n    {OP_paddusb,   0x0fdc10, \"paddusb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[67][2]},\n    {INVALID,    0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddusb, 0x660fdc10, \"paddusb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddusb, 0x660fdc10, \"vpaddusb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fdc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 68 */\n  {\n    {OP_paddusw,   0x0fdd10, \"paddusw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[68][2]},\n    {INVALID,    0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddusw, 0x660fdd10, \"paddusw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddusw, 0x660fdd10, \"vpaddusw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fdd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 69 */\n  {\n    {OP_pmaxub,   0x0fde10, \"pmaxub\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[69][2]},\n    {INVALID,    0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaxub, 0x660fde10, \"pmaxub\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaxub, 0x660fde10, \"vpmaxub\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fde10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 70 */\n  {\n    {OP_pandn,   0x0fdf10, \"pandn\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[70][2]},\n    {INVALID,    0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pandn, 0x660fdf10, \"pandn\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpandn, 0x660fdf10, \"vpandn\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fdf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 71 */\n  {\n    {OP_pavgb,   0x0fe010, \"pavgb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[71][2]},\n    {INVALID,    0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pavgb, 0x660fe010, \"pavgb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpavgb, 0x660fe010, \"vpavgb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 72 */\n  {\n    {OP_psraw,   0x0fe110, \"psraw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[72][2]},\n    {INVALID,    0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psraw, 0x660fe110, \"psraw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[105][0]},\n    {INVALID,    0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsraw, 0x660fe110, \"vpsraw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[105][6]},\n    {INVALID,    0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 73 */\n  {\n    {OP_psrad,   0x0fe210, \"psrad\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[73][2]},\n    {INVALID,    0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psrad, 0x660fe210, \"psrad\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[108][0]},\n    {INVALID,    0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsrad, 0x660fe210, \"vpsrad\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[108][6]},\n    {INVALID,    0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 74 */\n  {\n    {OP_pavgw,   0x0fe310, \"pavgw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[74][2]},\n    {INVALID,    0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pavgw, 0x660fe310, \"pavgw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpavgw, 0x660fe310, \"vpavgw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 75 */\n  {\n    {OP_pmulhuw,   0x0fe410, \"pmulhuw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[75][2]},\n    {INVALID,    0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmulhuw, 0x660fe410, \"pmulhuw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmulhuw, 0x660fe410, \"vpmulhuw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 76 */\n  {\n    {OP_pmulhw,   0x0fe510, \"pmulhw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[76][2]},\n    {INVALID,    0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmulhw, 0x660fe510, \"pmulhw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmulhw, 0x660fe510, \"vpmulhw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 77 */\n  {\n    {INVALID, 0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_cvtdq2pd, 0xf30fe610, \"cvtdq2pd\",  Vpd, xx, Wq_dq, xx, xx, mrm, x, END_LIST},\n    {OP_cvttpd2dq,0x660fe610, \"cvttpd2dq\", Vdq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {OP_cvtpd2dq, 0xf20fe610, \"cvtpd2dq\",  Vdq, xx, Wpd, xx, xx, mrm, x, END_LIST},\n    {INVALID,        0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vcvtdq2pd, 0xf30fe610, \"vcvtdq2pd\",  Vvd, xx, Wvq_dq, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvttpd2dq,0x660fe610, \"vcvttpd2dq\", Vx, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vcvtpd2dq, 0xf20fe610, \"vcvtpd2dq\",  Vx, xx, Wvd, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 78 */\n  {\n    {OP_movntq,    0x0fe710, \"movntq\",  Mq, xx, Pq, xx, xx, mrm, x, END_LIST},\n    {INVALID,    0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_movntdq, 0x660fe710, \"movntdq\", Mdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovntdq, 0x660fe710, \"vmovntdq\", Mx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 79 */\n  {\n    {OP_psubsb,   0x0fe810, \"psubsb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[79][2]},\n    {INVALID,    0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubsb, 0x660fe810, \"psubsb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubsb, 0x660fe810, \"vpsubsb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 80 */\n  {\n    {OP_psubsw,   0x0fe910, \"psubsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[80][2]},\n    {INVALID,    0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubsw, 0x660fe910, \"psubsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubsw, 0x660fe910, \"vpsubsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fe910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 81 */\n  {\n    {OP_pminsw,   0x0fea10, \"pminsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[81][2]},\n    {INVALID,    0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pminsw, 0x660fea10, \"pminsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpminsw, 0x660fea10, \"vpminsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 82 */\n  {\n    {OP_por,   0x0feb10, \"por\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[82][2]},\n    {INVALID,    0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_por, 0x660feb10, \"por\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpor, 0x660feb10, \"vpor\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20feb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 83 */\n  {\n    {OP_paddsb,   0x0fec10, \"paddsb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[83][2]},\n    {INVALID,    0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddsb, 0x660fec10, \"paddsb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,   0x0fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddsb, 0x660fec10, \"vpaddsb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 84 */\n  {\n    {OP_paddsw,   0x0fed10, \"paddsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[84][2]},\n    {INVALID,    0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddsw, 0x660fed10, \"paddsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddsw, 0x660fed10, \"vpaddsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 85 */\n  {\n    {OP_pmaxsw,   0x0fee10, \"pmaxsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[85][2]},\n    {INVALID,    0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaxsw, 0x660fee10, \"pmaxsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaxsw, 0x660fee10, \"vpmaxsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 86 */\n  {\n    {OP_pxor,   0x0fef10, \"pxor\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[86][2]},\n    {INVALID,    0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pxor, 0x660fef10, \"pxor\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpxor, 0x660fef10, \"vpxor\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 87 */\n  {\n    {OP_psllw,   0x0ff110, \"psllw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[87][2]},\n    {INVALID,    0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psllw, 0x660ff110, \"psllw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[106][0]},\n    {INVALID,    0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsllw,  0x660ff110, \"vpsllw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[106][6]},\n    {INVALID,    0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 88 */\n  {\n    {OP_pslld,   0x0ff210, \"pslld\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[88][2]},\n    {INVALID,    0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pslld, 0x660ff210, \"pslld\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[109][0]},\n    {INVALID,    0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpslld, 0x660ff210, \"vpslld\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[109][6]},\n    {INVALID,    0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 89 */\n  {\n    {OP_psllq,   0x0ff310, \"psllq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[89][2]},\n    {INVALID,    0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psllq, 0x660ff310, \"psllq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, tpe[111][0]},\n    {INVALID,    0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsllq, 0x660ff310, \"vpsllq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, tpe[111][6]},\n    {INVALID,    0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 90 */\n  {\n    {OP_pmuludq,   0x0ff410, \"pmuludq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[90][2]},\n    {INVALID,    0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmuludq, 0x660ff410, \"pmuludq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmuludq, 0x660ff410, \"vpmuludq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 91 */\n  {\n    {OP_pmaddwd,   0x0ff510, \"pmaddwd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[91][2]},\n    {INVALID,    0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_pmaddwd, 0x660ff510, \"pmaddwd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpmaddwd, 0x660ff510, \"vpmaddwd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 92 */\n  {\n    {OP_psadbw,   0x0ff610, \"psadbw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[92][2]},\n    {INVALID,    0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psadbw, 0x660ff610, \"psadbw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsadbw, 0x660ff610, \"vpsadbw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 93 */\n  {\n    {OP_maskmovq,     0x0ff710, \"maskmovq\", Bq, xx, Pq, Nq, xx, mrm|predcx, x, END_LIST}, /* Intel table says \"Ppi, Qpi\" */\n    {INVALID,       0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_maskmovdqu, 0x660ff710, \"maskmovdqu\", Bdq, xx, Vdq, Udq, xx, mrm|predcx, x, END_LIST},\n    {INVALID,       0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,         0x0ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,       0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmaskmovdqu, 0x660ff710, \"vmaskmovdqu\", Bdq, xx, Vdq, Udq, xx, mrm|vex|reqL0|predcx, x, END_LIST},\n    {INVALID,       0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 94 */\n  {\n    {OP_psubb,   0x0ff810, \"psubb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[94][2]},\n    {INVALID,    0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubb, 0x660ff810, \"psubb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubb, 0x660ff810, \"vpsubb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 95 */\n  {\n    {OP_psubw,   0x0ff910, \"psubw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[95][2]},\n    {INVALID,    0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubw, 0x660ff910, \"psubw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubw, 0x660ff910, \"vpsubw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 96 */\n  {\n    {OP_psubd,   0x0ffa10, \"psubd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[96][2]},\n    {INVALID,    0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_psubd, 0x660ffa10, \"psubd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpsubd, 0x660ffa10, \"vpsubd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 97 */\n  {\n    {OP_psubq,   0x0ffb10, \"psubq\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[97][2]},\n    {INVALID,  0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psubq, 0x660ffb10, \"psubq\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,  0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,    0x0ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,  0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsubq, 0x660ffb10, \"vpsubq\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,  0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 98 */\n  {\n    {OP_paddb,   0x0ffc10, \"paddb\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[98][2]},\n    {INVALID,    0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddb, 0x660ffc10, \"paddb\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddb, 0x660ffc10, \"vpaddb\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 99 */\n  {\n    {OP_paddw,   0x0ffd10, \"paddw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[99][2]},\n    {INVALID,    0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddw, 0x660ffd10, \"paddw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddw, 0x660ffd10, \"vpaddw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 100 */\n  {\n    {OP_paddd,   0x0ffe10, \"paddd\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[100][2]},\n    {INVALID,    0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_paddd, 0x660ffe10, \"paddd\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,    0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0x0ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,    0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vpaddd, 0x660ffe10, \"vpaddd\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,    0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 101: all assumed to have Ib */\n  {\n    {INVALID,     0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrldq, 0x660f7333, \"psrldq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrldq, 0x660f7333, \"vpsrldq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7333, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 102: all assumed to have Ib */\n  {\n    {INVALID,     0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pslldq, 0x660f7337, \"pslldq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpslldq, 0x660f7337, \"vpslldq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7337, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 103 */\n  {\n    {REX_B_EXT,  0x900000, \"(rex.b ext 0)\", xx, xx, xx, xx, xx, no, x, 0},\n    {OP_pause,0xf3900000, \"pause\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    /* we chain these even though encoding won't find them */\n    {OP_nop, 0x66900000, \"nop\", xx, xx, xx, xx, xx, no, x, tpe[103][3]},\n    /* windbg displays as \"repne nop\" */\n    {OP_nop, 0xf2900000, \"nop\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0x900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf3900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x66900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf2900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2900000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 104: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrlw,    0x0f7132, \"psrlw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[104][2]},\n    {INVALID,   0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrlw,  0x660f7132, \"psrlw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrlw,  0x660f7132, \"vpsrlw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7132, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 105: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psraw,    0x0f7134, \"psraw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[105][2]},\n    {INVALID,   0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psraw,  0x660f7134, \"psraw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsraw,  0x660f7134, \"vpsraw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7134, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 106: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psllw,    0x0f7136, \"psllw\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[106][2]},\n    {INVALID,   0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psllw,  0x660f7136, \"psllw\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsllw,  0x660f7136, \"vpsllw\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7136, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 107: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrld,    0x0f7232, \"psrld\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[107][2]},\n    {INVALID,   0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrld,  0x660f7232, \"psrld\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrld,  0x660f7232, \"vpsrld\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7232, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 108: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrad,    0x0f7234, \"psrad\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[108][2]},\n    {INVALID,   0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrad,  0x660f7234, \"psrad\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrad,  0x660f7234, \"vpsrad\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7234, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 109: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_pslld,    0x0f7236, \"pslld\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[109][2]},\n    {INVALID,   0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pslld,  0x660f7236, \"pslld\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpslld,  0x660f7236, \"vpslld\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7236, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 110: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psrlq,    0x0f7332, \"psrlq\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[110][2]},\n    {INVALID,   0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psrlq,  0x660f7332, \"psrlq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsrlq,  0x660f7332, \"vpsrlq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7332, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 111: all assumed to have Ib */\n  {\n    /* Intel tables imply they may add opcodes in the mod<3 (mem) space in future */\n    {OP_psllq,    0x0f7336, \"psllq\", Nq, xx, Ib, Nq, xx, mrm, x, tpe[111][2]},\n    {INVALID,   0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psllq,  0x660f7336, \"psllq\", Udq, xx, Ib, Udq, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsllq,  0x660f7336, \"vpsllq\", Hx, xx, Ib, Ux, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7336, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 112 */\n  {\n    {OP_movq,     0x0f6f10, \"movq\", Pq, xx, Qq, xx, xx, mrm, x, tpe[113][0]},\n    {OP_movdqu, 0xf30f6f10, \"movdqu\", Vdq, xx, Wdq, xx, xx, mrm, x, tpe[113][1]},\n    {OP_movdqa, 0x660f6f10, \"movdqa\", Vdq, xx, Wdq, xx, xx, mrm, x, tpe[113][2]},\n    {INVALID,   0xf20f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovdqu, 0xf30f6f10, \"vmovdqu\", Vx, xx, Wx, xx, xx, mrm|vex, x, tpe[113][5]},\n    {OP_vmovdqa, 0x660f6f10, \"vmovdqa\", Vx, xx, Wx, xx, xx, mrm|vex, x, tpe[113][6]},\n    {INVALID,   0xf20f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f6f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 113 */\n  {\n    {OP_movq,     0x0f7f10, \"movq\", Qq, xx, Pq, xx, xx, mrm, x, tpe[51][1]},\n    {OP_movdqu, 0xf30f7f10, \"movdqu\", Wdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {OP_movdqa, 0x660f7f10, \"movdqa\", Wdq, xx, Vdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,   0xf20f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x0f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vmovdqu, 0xf30f7f10, \"vmovdqu\", Wx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {OP_vmovdqa, 0x660f7f10, \"vmovdqa\", Wx, xx, Vx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID,   0xf20f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7f10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 114 */\n  {\n    {INVALID,     0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_haddpd, 0x660f7c10, \"haddpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_haddps, 0xf20f7c10, \"haddps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vhaddpd, 0x660f7c10, \"vhaddpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vhaddps, 0xf20f7c10, \"vhaddps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7c10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 115 */\n  {\n    {INVALID,     0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_hsubpd, 0x660f7d10, \"hsubpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_hsubps, 0xf20f7d10, \"hsubps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vhsubpd, 0x660f7d10, \"vhsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vhsubps, 0xf20f7d10, \"vhsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7d10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 116 */\n  {\n    {INVALID,     0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_addsubpd, 0x660fd010, \"addsubpd\", Vpd, xx, Wpd, Vpd, xx, mrm, x, END_LIST},\n    {OP_addsubps, 0xf20fd010, \"addsubps\", Vps, xx, Wps, Vps, xx, mrm, x, END_LIST},\n    {INVALID,     0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vaddsubpd, 0x660fd010, \"vaddsubpd\", Vvd, xx, Hvd, Wvd, xx, mrm|vex, x, END_LIST},\n    {OP_vaddsubps, 0xf20fd010, \"vaddsubps\", Vvs, xx, Hvs, Wvs, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fd010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /* prefix extension 117 */\n  {\n    {INVALID,     0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_lddqu,  0xf20ff010, \"lddqu\", Vdq, xx, Mdq, xx, xx, mrm, x, END_LIST},\n    {INVALID,     0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vlddqu,  0xf20ff010, \"vlddqu\", Vx, xx, Mx, xx, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20ff010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  /***************************************************\n   * SSSE3\n   */\n  { /* prefix extension 118 */\n    {OP_pshufb,     0x380018, \"pshufb\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[118][2]},\n    {INVALID,     0xf3380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pshufb,   0x66380018, \"pshufb\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,     0xf2380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,     0x380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,     0xf3380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpshufb,   0x66380018, \"vpshufb\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,     0xf2380018, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 119 */\n    {OP_phaddw,      0x380118, \"phaddw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[119][2]},\n    {INVALID,      0xf3380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddw,    0x66380118, \"phaddw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddw,    0x66380118, \"vphaddw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 120 */\n    {OP_phaddd,      0x380218, \"phaddd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[120][2]},\n    {INVALID,      0xf3380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddd,    0x66380218, \"phaddd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddd,    0x66380218, \"vphaddd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380218, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 121 */\n    {OP_phaddsw,     0x380318, \"phaddsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[121][2]},\n    {INVALID,      0xf3380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phaddsw,   0x66380318, \"phaddsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphaddsw,   0x66380318, \"vphaddsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380318, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 122 */\n    {OP_pmaddubsw,   0x380418, \"pmaddubsw\",Pq, xx, Qq, Pq, xx, mrm, x, tpe[122][2]},\n    {INVALID,      0xf3380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmaddubsw, 0x66380418, \"pmaddubsw\",Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmaddubsw, 0x66380418, \"vpmaddubsw\",Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380418, \"(bad)\",    xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 123 */\n    {OP_phsubw,      0x380518, \"phsubw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[123][2]},\n    {INVALID,      0xf3380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubw,    0x66380518, \"phsubw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubw,    0x66380518, \"vphsubw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 124 */\n    {OP_phsubd,      0x380618, \"phsubd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[124][2]},\n    {INVALID,      0xf3380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubd,    0x66380618, \"phsubd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubd,    0x66380618, \"vphsubd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 125 */\n    {OP_phsubsw,     0x380718, \"phsubsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[125][2]},\n    {INVALID,      0xf3380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_phsubsw,   0x66380718, \"phsubsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vphsubsw,   0x66380718, \"vphsubsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 126 */\n    {OP_psignb,      0x380818, \"psignb\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[126][2]},\n    {INVALID,      0xf3380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignb,    0x66380818, \"psignb\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignb,    0x66380818, \"vpsignb\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380818, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 127 */\n    {OP_psignw,      0x380918, \"psignw\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[127][2]},\n    {INVALID,      0xf3380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignw,    0x66380918, \"psignw\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignw,    0x66380918, \"vpsignw\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380918, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 128 */\n    {OP_psignd,      0x380a18, \"psignd\",  Pq, xx, Qq, Pq, xx, mrm, x, tpe[128][2]},\n    {INVALID,      0xf3380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_psignd,    0x66380a18, \"psignd\",  Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpsignd,    0x66380a18, \"vpsignd\",  Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380a18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 129 */\n    {OP_pmulhrsw,    0x380b18, \"pmulhrsw\", Pq, xx, Qq, Pq, xx, mrm, x, tpe[129][2]},\n    {INVALID,      0xf3380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pmulhrsw,  0x66380b18, \"pmulhrsw\", Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpmulhrsw,  0x66380b18, \"vpmulhrsw\", Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2380b18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2380b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 130 */\n    {OP_pabsb,       0x381c18, \"pabsb\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[130][2]},\n    {INVALID,      0xf3381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsb,     0x66381c18, \"pabsb\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsb,     0x66381c18, \"vpabsb\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381c18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 131 */\n    {OP_pabsw,       0x381d18, \"pabsw\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[131][2]},\n    {INVALID,      0xf3381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsw,     0x66381d18, \"pabsw\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsw,     0x66381d18, \"vpabsw\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381d18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 132 */\n    {OP_pabsd,       0x381e18, \"pabsd\",   Pq, xx, Qq, Pq, xx, mrm, x, tpe[132][2]},\n    {INVALID,      0xf3381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pabsd,     0x66381e18, \"pabsd\",   Vdq, xx, Wdq, Vdq, xx, mrm, x, END_LIST},\n    {INVALID,      0xf2381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf3381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpabsd,     0x66381e18, \"vpabsd\",   Vx, xx, Hx, Wx, xx, mrm|vex, x, END_LIST},\n    {INVALID,      0xf2381e18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf3381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x66381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf2381e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 133: all assumed to have Ib */\n    {OP_palignr,     0x3a0f18, \"palignr\", Pq, xx, Qq, Ib, Pq, mrm, x, tpe[133][2]},\n    {INVALID,      0xf33a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_palignr,   0x663a0f18, \"palignr\", Vdq, xx, Wdq, Ib, Vdq, mrm, x, END_LIST},\n    {INVALID,      0xf23a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x3a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {INVALID,      0xf33a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpalignr,   0x663a0f18, \"vpalignr\", Vx, xx, Hx, Wx, Ib, mrm|vex, x, END_LIST},\n    {INVALID,      0xf23a0f18, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x3a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf33a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x663a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf23a0f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 134 */\n    {OP_vmread,      0x0f7810, \"vmread\",  Ed_q, xx, Gd_q, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf30f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* FIXME PR 338279: this is listed as /0 but I'm not going to chain it into\n     * the reg extensions table until I can verify, since gdb thinks it\n     * does NOT need /0.  Waiting for a processor that actually supports it.\n     * It's ok for DR proper to think a non-cti instr is valid when really it's not,\n     * though for our decoding library use we should get it right.\n     */\n    {OP_extrq,     0x660f7810, \"extrq\",   Udq, xx, Ib, Ib, xx, mrm, x, tpe[135][2]},\n    /* FIXME: is src or dst Udq? */\n    {OP_insertq,   0xf20f7810, \"insertq\", Vdq, xx, Udq, Ib, Ib, mrm, x, tpe[135][3]},\n    {INVALID,        0x0f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f7810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 135 */\n    {OP_vmwrite,     0x0f7910, \"vmwrite\", Gd_q, xx, Ed_q, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf30f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* FIXME: is src or dst Udq? */\n    {OP_extrq,     0x660f7910, \"extrq\",   Vdq, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {OP_insertq,   0xf20f7910, \"insertq\", Vdq, xx, Udq, xx, xx, mrm, x, END_LIST},\n    {INVALID,        0x0f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f7910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f7910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 136 */\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, END_LIST},\n    /* XXX: if cpuid doesn't show lzcnt support, this is treated as bsr */\n    {OP_lzcnt,     0xf30fbd10, \"lzcnt\",   Gv, xx, Ev, xx, xx, mrm, fW6, END_LIST},\n    /* This is bsr w/ DATA_PREFIX, which we indicate by omitting 0x66 (i#1118).\n     * It's not in the encoding chain.  Ditto for 0xf2.  If we keep the \"all\n     * prefix ext marked invalid are really treated valid\" we don't need these,\n     * but better to be explicit where we have to so we can easily remove that.\n     */\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {OP_bsr,         0x0fbd10, \"bsr\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {INVALID,        0x0fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fbd10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fbd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 137 */\n    {OP_vmptrld,     0x0fc736, \"vmptrld\", xx, xx, Mq, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmxon,     0xf30fc736, \"vmxon\",   xx, xx, Mq, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmclear,   0x660fc736, \"vmclear\", Mq, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,      0xf20fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x0fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fc736, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fc736, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 138 */\n    {OP_movbe,   0x38f018, \"movbe\", Gv, xx, Mv, xx, xx, mrm, x, tpe[139][0]},\n    {INVALID,  0xf338f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* really this is regular data-size prefix */\n    {OP_movbe, 0x6638f018, \"movbe\", Gw, xx, Mw, xx, xx, mrm, x, tpe[139][2]},\n    {OP_crc32, 0xf238f018, \"crc32\", Gv, xx, Eb, Gv, xx, mrm, x, END_LIST},\n    {INVALID,    0x38f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf338f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0x6638f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf238f018, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 139 */\n    {OP_movbe,   0x38f118, \"movbe\", Mv, xx, Gv, xx, xx, mrm, x, tpe[138][2]},\n    {INVALID,  0xf338f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* really this is regular data-size prefix */\n    {OP_movbe, 0x6638f118, \"movbe\", Mw, xx, Gw, xx, xx, mrm, x, END_LIST},\n    {OP_crc32, 0xf238f118, \"crc32\", Gv, xx, Ev, Gv, xx, mrm, x, tpe[138][3]},\n    {INVALID,    0x38f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf338f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0x6638f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,  0xf238f118, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    /* XXX: Intel Vol2B Sep2010 decode table claims crc32 has Gd\n     * instead of Gv, and that f2 f1 has Ey instead of Ev, and that\n     * there is a separate instruction with both 66 and f2 prefixes!\n     * But detail page doesn't corroborate that...\n     */\n  },\n  { /* prefix extension 140 */\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, END_LIST},\n    /* XXX: if cpuid doesn't show tzcnt support, this is treated as bsf */\n    {OP_tzcnt,     0xf30fbc10, \"tzcnt\",   Gv, xx, Ev, xx, xx, mrm, fW6, END_LIST},\n    /* see OP_bsr comments above -- this is the same but for bsf: */\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {OP_bsf,         0x0fbc10, \"bsf\",     Gv, xx, Ev, xx, xx, mrm|predcx, fW6, NA},\n    {INVALID,        0x0fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20fbc10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20fbc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 141 */\n    {INVALID,        0x38f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf238f718, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bextr,       0x38f718, \"bextr\",   Gy, xx, Ey, By, xx, mrm|vex, fW6, txop[60]},\n    {OP_sarx,      0xf338f718, \"sarx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_shlx,      0x6638f718, \"shlx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_shrx,      0xf238f718, \"shrx\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 142 */\n    {INVALID,        0x38f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf238f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_bzhi,        0x38f518, \"bzhi\",    Gy, xx, Ey, By, xx, mrm|vex, fW6, END_LIST},\n    {INVALID,      0xf338f518, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_pext,      0x6638f518, \"pext\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    {OP_pdep,      0xf238f518, \"pdep\",    Gy, xx, Ey, By, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 143 */\n    {INVALID,        0x38f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_adox,      0xf338f618, \"adox\",    Gy, xx, Ey, Gy, xx, mrm, (fWO|fRO), END_LIST},\n    {OP_adcx,      0x6638f618, \"adcx\",    Gy, xx, Ey, Gy, xx, mrm, (fWC|fRC), END_LIST},\n    {INVALID,      0xf238f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,        0x38f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf338f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x6638f618, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {OP_mulx,      0xf238f618, \"mulx\",    By, Gy, Ey, uDX, xx, mrm|vex, x, END_LIST},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x38f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf338f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x6638f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf238f618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 144 */\n    {INVALID,        0x0f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9010, \"(vex_W ext 74)\", xx, xx, xx, xx, xx, mrm|vex, x, 74},\n    {INVALID,      0xf30f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9010, \"(vex_W ext 75)\", xx, xx, xx, xx, xx, mrm|vex, x, 75},\n    {INVALID,      0xf20f9010, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 145 */\n    {INVALID,        0x0f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9110, \"(vex_W ext 76)\", xx, xx, xx, xx, xx, mrm|vex, x, 76},\n    {INVALID,      0xf30f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9110, \"(vex_W ext 77)\", xx, xx, xx, xx, xx, mrm|vex, x, 77},\n    {INVALID,      0xf20f9110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 146 */\n    {INVALID,        0x0f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9210, \"(vex_W ext 78)\", xx, xx, xx, xx, xx, mrm|vex, x, 78},\n    {INVALID,      0xf30f9210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9210, \"(vex_W ext 79)\", xx, xx, xx, xx, xx, mrm|vex, x, 79},\n    {VEX_W_EXT,    0xf20f9210, \"(vex_W ext 106)\",xx, xx, xx, xx, xx, mrm|vex, x, 106},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 147 */\n    {INVALID,        0x0f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9310, \"(vex_W ext 80)\", xx, xx, xx, xx, xx, mrm|vex, x, 80},\n    {INVALID,      0xf30f9310, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9310, \"(vex_W ext 81)\", xx, xx, xx, xx, xx, mrm|vex, x, 81},\n    {VEX_W_EXT,    0xf20f9310, \"(vex_W ext 107)\",xx, xx, xx, xx, xx, mrm|vex, x, 107},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 148 */\n    {INVALID,        0x0f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4110, \"(vex_W ext 82)\", xx, xx, xx, xx, xx, mrm|vex, x, 82},\n    {INVALID,      0xf30f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4110, \"(vex_W ext 83)\", xx, xx, xx, xx, xx, mrm|vex, x, 83},\n    {INVALID,      0xf20f4110, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 149 */\n    {INVALID,        0x0f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4210, \"(vex_W ext 84)\", xx, xx, xx, xx, xx, mrm|vex, x, 84},\n    {INVALID,      0xf30f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4210, \"(vex_W ext 85)\", xx, xx, xx, xx, xx, mrm|vex, x, 85},\n    {INVALID,      0xf20f4210, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 150 */\n    {INVALID,        0x0f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4b10, \"(vex_W ext 86)\", xx, xx, xx, xx, xx, mrm|vex, x, 86},\n    {INVALID,      0xf30f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4b10, \"(vex_W ext 87)\", xx, xx, xx, xx, xx, mrm|vex, x, 87},\n    {INVALID,      0xf20f4b10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 151 */\n    {INVALID,        0x0f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4410, \"(vex_W ext 88)\", xx, xx, xx, xx, xx, mrm|vex, x, 88},\n    {INVALID,      0xf30f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4410, \"(vex_W ext 89)\", xx, xx, xx, xx, xx, mrm|vex, x, 89},\n    {INVALID,      0xf20f4410, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 152 */\n    {INVALID,        0x0f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4510, \"(vex_W ext 90)\", xx, xx, xx, xx, xx, mrm|vex, x, 90},\n    {INVALID,      0xf30f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4510, \"(vex_W ext 91)\", xx, xx, xx, xx, xx, mrm|vex, x, 91},\n    {INVALID,      0xf20f4510, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 153 */\n    {INVALID,        0x0f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4610, \"(vex_W ext 92)\", xx, xx, xx, xx, xx, mrm|vex, x, 92},\n    {INVALID,      0xf30f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4610, \"(vex_W ext 93)\", xx, xx, xx, xx, xx, mrm|vex, x, 93},\n    {INVALID,      0xf20f4610, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 154 */\n    {INVALID,        0x0f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4710, \"(vex_W ext 94)\", xx, xx, xx, xx, xx, mrm|vex, x, 94},\n    {INVALID,      0xf30f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4710, \"(vex_W ext 95)\", xx, xx, xx, xx, xx, mrm|vex, x, 95},\n    {INVALID,      0xf20f4710, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 155 */\n    {INVALID,        0x0f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f4a10, \"(vex_W ext 96)\", xx, xx, xx, xx, xx, mrm|vex, x, 96},\n    {INVALID,      0xf30f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f4a10, \"(vex_W ext 97)\", xx, xx, xx, xx, xx, mrm|vex, x, 97},\n    {INVALID,      0xf20f4a10, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 156 */\n    {INVALID,        0x0f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9810, \"(vex_W ext 98)\", xx, xx, xx, xx, xx, mrm|vex, x, 98},\n    {INVALID,      0xf30f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9810, \"(vex_W ext 99)\", xx, xx, xx, xx, xx, mrm|vex, x, 99},\n    {INVALID,      0xf20f9810, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* prefix extension 157 */\n    {INVALID,        0x0f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf30f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0x660f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,      0xf20f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,      0x0f9910, \"(vex_W ext 104)\", xx, xx, xx, xx, xx, mrm|vex, x, 104},\n    {INVALID,      0xf30f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    {VEX_W_EXT,    0x660f9910, \"(vex_W ext 105)\", xx, xx, xx, xx, xx, mrm|vex, x, 105},\n    {INVALID,      0xf20f9910, \"(bad)\",   xx, xx, xx, xx, xx, no, x, NA},\n    /* TODO i#1312: Support AVX-512. */\n    {INVALID,   0x0f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf30f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0x660f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID, 0xf20f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n/****************************************************************************\n * Instructions that differ based on whether vex-encoded or not.\n * Most of these require an 0x66 prefix but we use reqp for that\n * so there's nothing inherent here about prefixes.\n * TODO i#1312: A third row has been added for AVX-512 w/ EVEX prefix. Most or all\n * EVEX instructions seem to resemble their corresponding VEX version. If we add\n * a decode_table entry here, we currently can't test them throgh instr_create macros,\n * unless we force the creation of EVEX versions.\n */\nconst instr_info_t e_vex_extensions[][3] = {\n  {    /* e_vex ext  0 */\n    {INVALID, 0x663a4a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vblendvps, 0x663a4a18, \"vblendvps\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  1 */\n    {INVALID, 0x663a4b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vblendvpd, 0x663a4b18, \"vblendvpd\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  2 */\n    {INVALID, 0x663a4c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpblendvb, 0x663a4c18, \"vpblendvb\", Vx, xx, Hx,Wx,Lx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  3 */\n    {OP_ptest,  0x66381718, \"ptest\",    xx, xx,  Vdq,Wdq, xx, mrm|reqp, fW6, END_LIST},\n    {OP_vptest, 0x66381718, \"vptest\",   xx, xx,    Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66381718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  4 */\n    {OP_pmovsxbw,  0x66382018, \"pmovsxbw\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbw, 0x66382018, \"vpmovsxbw\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  5 */\n    {OP_pmovsxbd,  0x66382118, \"pmovsxbd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbd, 0x66382118, \"vpmovsxbd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  6 */\n    {OP_pmovsxbq,  0x66382218, \"pmovsxbq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxbq, 0x66382218, \"vpmovsxbq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  7 */\n    {OP_pmovsxwd,  0x66382318, \"pmovsxwd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxwd, 0x66382318, \"vpmovsxwd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  8 */\n    {OP_pmovsxwq,  0x66382418, \"pmovsxwq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxwq, 0x66382418, \"vpmovsxwq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext  9 */\n    {OP_pmovsxdq, 0x66382518, \"pmovsxdq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovsxdq,0x66382518, \"vpmovsxdq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 10 */\n    {OP_pmuldq,   0x66382818, \"pmuldq\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmuldq,  0x66382818, \"vpmuldq\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 11 */\n    {OP_pcmpeqq,  0x66382918, \"pcmpeqq\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpcmpeqq, 0x66382918, \"vpcmpeqq\",  Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 12 */\n    {OP_movntdqa,  0x66382a18, \"movntdqa\", Mdq, xx, Vdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vmovntdqa, 0x66382a18, \"vmovntdqa\", Mx, xx, Vx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 13 */\n    {OP_packusdw,  0x66382b18, \"packusdw\", Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpackusdw, 0x66382b18, \"vpackusdw\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66382b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 14 */\n    {OP_pmovzxbw,  0x66383018, \"pmovzxbw\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbw, 0x66383018, \"vpmovzxbw\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 15 */\n    {OP_pmovzxbd,  0x66383118, \"pmovzxbd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbd, 0x66383118, \"vpmovzxbd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 16 */\n    {OP_pmovzxbq,  0x66383218, \"pmovzxbq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxbq, 0x66383218, \"vpmovzxbq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 17 */\n    {OP_pmovzxwd,  0x66383318, \"pmovzxwd\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxwd, 0x66383318, \"vpmovzxwd\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 18 */\n    {OP_pmovzxwq,  0x66383418, \"pmovzxwq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxwq, 0x66383418, \"vpmovzxwq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 19 */\n    {OP_pmovzxdq,  0x66383518, \"pmovzxdq\", Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmovzxdq, 0x66383518, \"vpmovzxdq\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 20 */\n    {OP_pcmpgtq,  0x66383718, \"pcmpgtq\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpcmpgtq, 0x66383718, \"vpcmpgtq\",  Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 21 */\n    {OP_pminsb,   0x66383818, \"pminsb\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminsb,  0x66383818, \"vpminsb\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 22 */\n    {OP_pminsd,   0x66383918, \"pminsd\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminsd,  0x66383918, \"vpminsd\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 23 */\n    {OP_pminuw,   0x66383a18, \"pminuw\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminuw,  0x66383a18, \"vpminuw\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 24 */\n    {OP_pminud,   0x66383b18, \"pminud\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpminud,  0x66383b18, \"vpminud\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 25 */\n    {OP_pmaxsb,   0x66383c18, \"pmaxsb\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxsb,  0x66383c18, \"vpmaxsb\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 26 */\n    {OP_pmaxsd,   0x66383d18, \"pmaxsd\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxsd,  0x66383d18, \"vpmaxsd\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 27 */\n    {OP_pmaxuw,   0x66383e18, \"pmaxuw\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxuw,  0x66383e18, \"vpmaxuw\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 28 */\n    {OP_pmaxud,   0x66383f18, \"pmaxud\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmaxud,  0x66383f18, \"vpmaxud\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66383f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 29 */\n    {OP_pmulld,   0x66384018, \"pmulld\",   Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vpmulld,  0x66384018, \"vpmulld\",   Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66384018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 30 */\n    {OP_phminposuw,  0x66384118,\"phminposuw\",Vdq,xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vphminposuw, 0x66384118,\"vphminposuw\",Vdq,xx, Wdq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66384118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 31 */\n    {OP_aesimc,  0x6638db18, \"aesimc\",  Vdq, xx, Wdq, xx, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesimc, 0x6638db18, \"vaesimc\",  Vdq, xx, Wdq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638db18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 32 */\n    {OP_aesenc,  0x6638dc18, \"aesenc\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesenc, 0x6638dc18, \"vaesenc\",  Vdq, xx, Hdq,Wdq, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638dc18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 33 */\n    {OP_aesenclast,  0x6638dd18,\"aesenclast\",Vdq,xx,Wdq,Vdq,xx, mrm|reqp, x, END_LIST},\n    {OP_vaesenclast, 0x6638dd18,\"vaesenclast\",Vdq,xx,Hdq,Wdq,xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638dd18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 34 */\n    {OP_aesdec,  0x6638de18, \"aesdec\",  Vdq, xx, Wdq,Vdq, xx, mrm|reqp, x, END_LIST},\n    {OP_vaesdec, 0x6638de18, \"vaesdec\",  Vdq, xx, Hdq,Wdq, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638de18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 35 */\n    {OP_aesdeclast,  0x6638df18,\"aesdeclast\",Vdq,xx,Wdq,Vdq,xx, mrm|reqp, x, END_LIST},\n    {OP_vaesdeclast, 0x6638df18,\"vaesdeclast\",Vdq,xx,Hdq,Wdq,xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x6638df18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 36 */\n    {OP_pextrb,   0x663a1418, \"pextrb\", Rd_Mb, xx, Vb_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpextrb,  0x663a1418, \"vpextrb\", Rd_Mb, xx, Vb_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 37 */\n    {OP_pextrw,   0x663a1518, \"pextrw\", Rd_Mw, xx, Vw_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpextrw,  0x663a1518, \"vpextrw\", Rd_Mw, xx, Vw_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 38 */\n    {OP_pextrd,   0x663a1618, \"pextrd\",  Ed_q, xx, Vd_q_dq, Ib, xx, mrm|reqp, x, END_LIST},/*\"pextrq\" with rex.w*/\n    {OP_vpextrd,  0x663a1618, \"vpextrd\",  Ed_q, xx, Vd_q_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},/*\"vpextrq\" with rex.w*/\n    {INVALID, 0x663a1618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 39 */\n    {OP_extractps,  0x663a1718, \"extractps\", Ed, xx, Vd_dq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vextractps, 0x663a1718, \"vextractps\", Ed, xx, Vd_dq, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1718, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 40 */\n    {OP_roundps,  0x663a0818, \"roundps\",  Vdq, xx, Wdq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundps, 0x663a0818, \"vroundps\",  Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 41 */\n    {OP_roundpd,  0x663a0918, \"roundpd\",  Vdq, xx, Wdq, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundpd, 0x663a0918, \"vroundpd\",  Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 42 */\n    {OP_roundss,  0x663a0a18, \"roundss\",  Vss, xx, Wss, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundss, 0x663a0a18, \"vroundss\",  Vdq, xx, H12_dq, Wss, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 43 */\n    {OP_roundsd,  0x663a0b18, \"roundsd\",  Vsd, xx, Wsd, Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vroundsd, 0x663a0b18, \"vroundsd\",  Vdq, xx, Hsd, Wsd, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0b18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 44 */\n    {OP_blendps,  0x663a0c18, \"blendps\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vblendps, 0x663a0c18, \"vblendps\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 45 */\n    {OP_blendpd,  0x663a0d18, \"blendpd\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vblendpd, 0x663a0d18, \"vblendpd\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 46 */\n    {OP_pblendw,  0x663a0e18, \"pblendw\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vpblendw, 0x663a0e18, \"vpblendw\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 47 */\n    /* FIXME i#1388: pinsrb actually reads only bottom byte of reg */\n    {OP_pinsrb,   0x663a2018, \"pinsrb\",   Vb_dq, xx, Rd_Mb,  Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vpinsrb,  0x663a2018, \"vpinsrb\",   Vdq, xx, H15_dq, Rd_Mb, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a2018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 48 */\n    {OP_insertps, 0x663a2118, \"insertps\", Vdq,xx,Udq_Md,Ib, xx, mrm|reqp, x, END_LIST},\n    {OP_vinsertps,0x663a2118, \"vinsertps\", Vdq,xx,Hdq,Udq_Md,Ib, mrm|vex|reqp|reqL0, x, END_LIST},\n    {INVALID, 0x663a2118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 49 */\n    {OP_pinsrd,   0x663a2218, \"pinsrd\",   Vd_q_dq, xx, Ed_q,Ib, xx, mrm|reqp, x, END_LIST},/*\"pinsrq\" with rex.w*/\n    {OP_vpinsrd,  0x663a2218, \"vpinsrd\",   Vdq, xx, H12_8_dq, Ed_q, Ib, mrm|vex|reqp, x, END_LIST},/*\"vpinsrq\" with rex.w*/\n    {INVALID, 0x663a2218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 50 */\n    {OP_dpps,     0x663a4018, \"dpps\",     Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vdpps,    0x663a4018, \"vdpps\",     Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 51 */\n    {OP_dppd,     0x663a4118, \"dppd\",     Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vdppd,    0x663a4118, \"vdppd\",     Vdq, xx, Hdq, Wdq, Ib, mrm|vex|reqp|reqL0, x, END_LIST},\n    {INVALID, 0x663a4118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 52 */\n    {OP_mpsadbw,  0x663a4218, \"mpsadbw\",  Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vmpsadbw, 0x663a4218, \"vmpsadbw\",  Vx, xx, Hx, Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 53 */\n    {OP_pcmpestrm, 0x663a6018, \"pcmpestrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|reqp|xop, fW6, exop[8]},\n    {OP_vpcmpestrm,0x663a6018, \"vpcmpestrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|vex|reqp|xop, fW6, exop[11]},\n    {INVALID, 0x663a6018, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 54 */\n    {OP_pcmpestri, 0x663a6118, \"pcmpestri\",ecx, xx, Vdq, Wdq, Ib, mrm|reqp|xop, fW6, exop[9]},\n    {OP_vpcmpestri,0x663a6118, \"vpcmpestri\",ecx, xx, Vdq, Wdq, Ib, mrm|vex|reqp|xop, fW6, exop[12]},\n    {INVALID, 0x663a6118, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 55 */\n    {OP_pcmpistrm, 0x663a6218, \"pcmpistrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|reqp, fW6, END_LIST},\n    {OP_vpcmpistrm,0x663a6218, \"vpcmpistrm\",xmm0, xx, Vdq, Wdq, Ib, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x663a6218, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 56 */\n    {OP_pcmpistri, 0x663a6318, \"pcmpistri\",ecx, xx, Vdq, Wdq, Ib, mrm|reqp, fW6, END_LIST},\n    {OP_vpcmpistri,0x663a6318, \"vpcmpistri\",ecx, xx, Vdq, Wdq, Ib, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x663a6318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 57 */\n    {OP_pclmulqdq, 0x663a4418, \"pclmulqdq\", Vdq, xx, Wdq, Ib, Vdq, mrm|reqp, x, END_LIST},\n    {OP_vpclmulqdq,0x663a4418, \"vpclmulqdq\", Vdq, xx, Hdq, Wdq, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a4418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 58 */\n    {OP_aeskeygenassist, 0x663adf18, \"aeskeygenassist\",Vdq,xx,Wdq,Ib,xx,mrm|reqp,x,END_LIST},\n    {OP_vaeskeygenassist,0x663adf18, \"vaeskeygenassist\",Vdq,xx,Wdq,Ib,xx,mrm|vex|reqp,x,END_LIST},\n    {INVALID, 0x663adf18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 59 */\n    {INVALID,   0x66380e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vtestps, 0x66380e18, \"vtestps\", xx, xx, Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66380e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 60 */\n    {INVALID,   0x66380f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vtestpd, 0x66380f18, \"vtestpd\", xx, xx, Vx,Wx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {INVALID, 0x66380f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 61 */\n    {OP_ldmxcsr, 0x0fae32, \"ldmxcsr\", xx, xx, Md, xx, xx, mrm, x, END_LIST},\n    {OP_vldmxcsr, 0x0fae32, \"vldmxcsr\", xx, xx, Md, xx, xx, mrm|vex|reqL0, x, END_LIST},\n    {INVALID, 0x0fae32, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 62 */\n    {OP_stmxcsr, 0x0fae33, \"stmxcsr\", Md, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_vstmxcsr, 0x0fae33, \"vstmxcsr\", Md, xx, xx, xx, xx, mrm|vex, x, END_LIST},\n    {INVALID, 0x0fae33, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 63 */\n    {INVALID,   0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtph2ps, 0x66381318, \"vcvtph2ps\", Vx, xx, Wx, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66381318, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 64 */\n    {INVALID,   0x66381818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastss, 0x66381818, \"vbroadcastss\", Vx, xx, Wd_dq, xx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66381818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 65 */\n    {INVALID,   0x66381918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastsd, 0x66381918, \"vbroadcastsd\", Vqq, xx, Wq_dq, xx, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x66381918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 66 */\n    {INVALID,   0x66381a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vbroadcastf128, 0x66381a18, \"vbroadcastf128\", Vqq, xx, Mdq, xx, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x66381a18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 67 */\n    {INVALID,   0x66382c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovps, 0x66382c18, \"vmaskmovps\", Vx, xx, Hx,Mx, xx, mrm|vex|reqp|predcx, x, tvex[69][1]},\n    {INVALID, 0x66382c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 68 */\n    {INVALID,   0x66382d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovpd, 0x66382d18, \"vmaskmovpd\", Vx, xx, Hx,Mx, xx, mrm|vex|reqp|predcx, x, tvex[70][1]},\n    {INVALID, 0x66382d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 69 */\n    {INVALID,   0x66382e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovps, 0x66382e18, \"vmaskmovps\", Mx, xx, Hx,Vx, xx, mrm|vex|reqp|predcx, x, END_LIST},\n    {INVALID, 0x66382e18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 70 */\n    {INVALID,   0x66382f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmaskmovpd, 0x66382f18, \"vmaskmovpd\", Mx, xx, Hx,Vx, xx, mrm|vex|reqp|predcx, x, END_LIST},\n    {INVALID, 0x66382f18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 71 */\n    {INVALID,   0x663a0418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilps, 0x663a0418, \"vpermilps\", Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, tvex[77][1]},\n    {INVALID, 0x663a0418, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 72 */\n    {INVALID,   0x663a0518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilpd, 0x663a0518, \"vpermilpd\", Vx, xx, Wx, Ib, xx, mrm|vex|reqp, x, tvex[78][1]},\n    {INVALID, 0x663a0518, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 73 */\n    {INVALID,   0x663a0618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vperm2f128, 0x663a0618, \"vperm2f128\", Vx, xx, Hx,Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a0618, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 74 */\n    {INVALID,   0x663a1818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vinsertf128, 0x663a1818, \"vinsertf128\", Vx, xx, Hx,Wx, Ib, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1818, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 75 */\n    {INVALID,   0x663a1918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vextractf128, 0x663a1918, \"vextractf128\", Wdq, xx, Vdq_qq, Ib, xx, mrm|vex|reqp|reqL1, x, END_LIST},\n    {INVALID, 0x663a1918, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 76 */\n    {INVALID,   0x663a1d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vcvtps2ph, 0x663a1d18, \"vcvtps2ph\", Wx, xx, Vx, Ib, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x663a1d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 77 */\n    {INVALID,   0x66380c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilps, 0x66380c18, \"vpermilps\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66380c18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 78 */\n    {INVALID,   0x66380d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vpermilpd, 0x66380d18, \"vpermilpd\", Vx, xx, Hx,Wx, xx, mrm|vex|reqp, x, END_LIST},\n    {INVALID, 0x66380d18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 79 */\n    {OP_seto,    0x0f9010,             \"seto\", Eb, xx, xx, xx, xx, mrm, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f9010, \"(prefix ext 144)\", xx, xx, xx, xx, xx, mrm,   x, 144},\n    {INVALID, 0x0f9010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 80 */\n    {OP_setno,   0x0f9110,            \"setno\", Eb, xx, xx, xx, xx, mrm, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f9110, \"(prefix ext 145)\", xx, xx, xx, xx, xx, mrm,   x, 145},\n    {INVALID, 0x0f9110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 81 */\n    {OP_setb,    0x0f9210,             \"setb\", Eb, xx, xx, xx, xx, mrm, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f9210, \"(prefix ext 146)\", xx, xx, xx, xx, xx, mrm,   x, 146},\n    {INVALID, 0x0f9210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 82 */\n    {OP_setnb,   0x0f9310,            \"setnb\", Eb, xx, xx, xx, xx, mrm, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f9310, \"(prefix ext 147)\", xx, xx, xx, xx, xx, mrm,   x, 147},\n    {INVALID, 0x0f9310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 83 */\n    {OP_cmovno,  0x0f4110,           \"cmovno\", Gv, xx, Ev, xx, xx, mrm|predcc, fRO, END_LIST},\n    {PREFIX_EXT, 0x0f4110, \"(prefix ext 148)\", xx, xx, xx, xx, xx, mrm,         x, 148},\n    {INVALID, 0x0f4110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 84 */\n    {OP_cmovb,   0x0f4210,            \"cmovb\", Gv, xx, Ev, xx, xx, mrm|predcc, fRC, END_LIST},\n    {PREFIX_EXT, 0x0f4210, \"(prefix ext 149)\", xx, xx, xx, xx, xx, mrm,          x, 149},\n    {INVALID, 0x0f4210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 85 */\n    {OP_cmovnp,  0x0f4b10,           \"cmovnp\", Gv, xx, Ev, xx, xx, mrm|predcc, fRP, END_LIST},\n    {PREFIX_EXT, 0x0f4b10, \"(prefix ext 150)\", xx, xx, xx, xx, xx, mrm,          x, 150},\n    {INVALID, 0x0f4b10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 86 */\n    {OP_cmovz,   0x0f4410,            \"cmovz\", Gv, xx, Ev, xx, xx, mrm|predcc, fRZ, END_LIST},\n    {PREFIX_EXT, 0x0f4410, \"(prefix ext 151)\", xx, xx, xx, xx, xx, mrm,          x, 151},\n    {INVALID, 0x0f4410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 87 */\n    {OP_cmovnz,  0x0f4510,           \"cmovnz\", Gv, xx, Ev, xx, xx, mrm|predcc, fRZ, END_LIST},\n    {PREFIX_EXT, 0x0f4510, \"(prefix ext 152)\", xx, xx, xx, xx, xx, mrm,          x, 152},\n    {INVALID, 0x0f4510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 88 */\n    {OP_cmovbe,  0x0f4610,           \"cmovbe\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRC|fRZ), END_LIST},\n    {PREFIX_EXT, 0x0f4610, \"(prefix ext 153)\", xx, xx, xx, xx, xx, mrm,                x, 153},\n    {INVALID, 0x0f4610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 89 */\n    {OP_cmovnbe, 0x0f4710,          \"cmovnbe\", Gv, xx, Ev, xx, xx, mrm|predcc, (fRC|fRZ), END_LIST},\n    {PREFIX_EXT, 0x0f4710, \"(prefix ext 154)\", xx, xx, xx, xx, xx, mrm,                x, 154},\n    {INVALID, 0x0f4710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 90 */\n    {OP_cmovp,   0x0f4a10,            \"cmovp\", Gv, xx, Ev, xx, xx, mrm|predcc, fRP, END_LIST},\n    {PREFIX_EXT, 0x0f4a10, \"(prefix ext 155)\", xx, xx, xx, xx, xx, mrm,          x, 155},\n    {INVALID, 0x0f4a10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 91 */\n    {OP_sets,    0x0f9810,             \"sets\", Eb, xx, xx, xx, xx, mrm, fRS, END_LIST},\n    {PREFIX_EXT, 0x0f9810, \"(prefix ext 156)\", xx, xx, xx, xx, xx, mrm,    x, 156},\n    {INVALID, 0x0f9810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  }, { /* e_vex ext 92 */\n    {OP_setns,   0x0f9910,            \"setns\", Eb, xx, xx, xx, xx, mrm, fRS, END_LIST},\n    {PREFIX_EXT, 0x0f9910, \"(prefix ext 157)\", xx, xx, xx, xx, xx, mrm,   x, 157},\n    {INVALID, 0x0f9910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on mod and rm bits in modrm byte\n * For mod, entry 0 is all mem ref mod values (0,1,2) while entry 1 is 3.\n * For the mem ref, we give just one of the 3 possible modrm bytes\n * (we only use it when encoding so we don't need all 3).\n */\nconst instr_info_t mod_extensions[][2] = {\n  { /* mod extension 0 */\n    {OP_sgdt, 0x0f0130, \"sgdt\", Ms, xx, xx, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,  0x0f0171, \"(group 7 mod + rm ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n  },\n  { /* mod extension 1 */\n    {OP_sidt, 0x0f0131, \"sidt\",  Ms, xx, xx, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,  0x0f0171, \"(group 7 mod + rm ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n  },\n  { /* mod extension 2 */\n    {OP_invlpg, 0x0f0137, \"invlpg\", xx, xx, Mm, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0177, \"(group 7 mod + rm ext 2)\", xx, xx, xx, xx, xx, mrm, x, 2},\n  },\n  { /* mod extension 3 */\n    {OP_clflush, 0x0fae37, \"clflush\", xx, xx, Mb, xx, xx, mrm, x, END_LIST},\n    {OP_sfence,  0xf80fae77, \"sfence\",  xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 4 */\n    {OP_lidt,   0x0f0133, \"lidt\",  xx, xx, Ms, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0173, \"(group 7 mod + rm ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n  },\n  { /* mod extension 5 */\n    {OP_lgdt,   0x0f0132, \"lgdt\",  xx, xx, Ms, xx, xx, mrm, x, END_LIST},\n    {RM_EXT,    0x0f0172, \"(group 7 mod + rm ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n  },\n  { /* mod extension 6 */\n    {REX_W_EXT, 0x0fae35, \"(rex.w ext 3)\", xx, xx, xx, xx, xx, mrm, x, 3},\n    /* note that gdb thinks e9-ef are \"lfence (bad)\" (PR 239920) */\n    {OP_lfence, 0xe80fae75, \"lfence\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 7 */\n    {REX_W_EXT,   0x0fae36, \"(rex.w ext 4)\", xx, xx, xx, xx, xx, mrm, x, 4},\n    {OP_mfence,   0xf00fae76, \"mfence\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n  },\n  { /* mod extension 8 */\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vss, xx, Wss,  xx, xx, mrm|vex, x, modx[10][0]},\n    {OP_vmovss,  0xf30f1010, \"vmovss\",  Vdq, xx, H12_dq, Uss, xx, mrm|vex, x, modx[10][1]},\n  },\n  { /* mod extension 9 */\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vsd, xx, Wsd,  xx, xx, mrm|vex, x, modx[11][0]},\n    {OP_vmovsd,  0xf20f1010, \"vmovsd\",  Vdq, xx, Hsd, Usd, xx, mrm|vex, x, modx[11][1]},\n  },\n  { /* mod extension 10 */\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Wss, xx, Vss,  xx, xx, mrm|vex, x, modx[ 8][1]},\n    {OP_vmovss,  0xf30f1110, \"vmovss\",  Udq, xx, H12_dq, Vss, xx, mrm|vex, x, END_LIST},\n  },\n  { /* mod extension 11 */\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Wsd, xx, Vsd,  xx, xx, mrm|vex, x, modx[ 9][1]},\n    {OP_vmovsd,  0xf20f1110, \"vmovsd\",  Udq, xx, Hsd, Vsd, xx, mrm|vex, x, END_LIST},\n  },\n  { /* mod extension 12 */\n    {PREFIX_EXT, 0x0fc736, \"(prefix ext 137)\", xx, xx, xx, xx, xx, no, x, 137},\n    {OP_rdrand,  0x0fc736, \"rdrand\", Rv, xx, xx, xx, xx, mrm, fW6, END_LIST},\n  },\n  { /* mod extension 13 */\n    /* The latest Intel table implies 0x66 prefix makes invalid instr but not worth\n     * explicitly encoding that until we have more information.\n     */\n    {OP_vmptrst, 0x0fc737, \"vmptrst\", Mq, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_rdseed,  0x0fc737, \"rdseed\", Rv, xx, xx, xx, xx, mrm, fW6, END_LIST},\n  },\n  { /* mod extension 14 */\n    {REX_W_EXT,  0x0fae30, \"(rex.w ext 0)\", xx, xx, xx, xx, xx, mrm, x, 0},\n    /* Using reqp to avoid having to create a whole prefix_ext entry for one opcode.\n     * Ditto below.\n     */\n    {OP_rdfsbase,0xf30fae30, \"rdfsbase\", Ry, xx, xx, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 15 */\n    {REX_W_EXT,  0x0fae31, \"(rex.w ext 1)\", xx, xx, xx, xx, xx, mrm, x, 1},\n    {OP_rdgsbase,0xf30fae31, \"rdgsbase\", Ry, xx, xx, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 16 */\n    {E_VEX_EXT,    0x0fae32, \"(e_vex ext 61)\", xx, xx, xx, xx, xx, mrm, x, 61},\n    {OP_wrfsbase,0xf30fae32, \"wrfsbase\", xx, xx, Ry, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 17 */\n    {E_VEX_EXT,    0x0fae33, \"(e_vex ext 62)\", xx, xx, xx, xx, xx, mrm, x, 62},\n    {OP_wrgsbase,0xf30fae33, \"wrgsbase\", xx, xx, Ry, xx, xx, mrm|o64|reqp, x, END_LIST},\n  },\n  { /* mod extension 18 */\n    /* load from memory zeroes top bits */\n    {OP_movss,  0xf30f1010, \"movss\",  Vdq, xx, Mss, xx, xx, mrm, x, modx[18][1]},\n    {OP_movss,  0xf30f1010, \"movss\",  Vss, xx, Uss, xx, xx, mrm, x, tpe[1][1]},\n  },\n  { /* mod extension 19 */\n    /* load from memory zeroes top bits */\n    {OP_movsd,  0xf20f1010, \"movsd\",  Vdq, xx, Msd, xx, xx, mrm, x, modx[19][1]},\n    {OP_movsd,  0xf20f1010, \"movsd\",  Vsd, xx, Usd, xx, xx, mrm, x, tpe[1][3]},\n  },\n};\n\n/* Naturally all of these have modrm bytes even if they have no explicit operands */\nconst instr_info_t rm_extensions[][8] = {\n  { /* rm extension 0 */\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmcall,   0xc10f0171, \"vmcall\",   xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmlaunch, 0xc20f0171, \"vmlaunch\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmresume, 0xc30f0171, \"vmresume\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_vmxoff,   0xc40f0171, \"vmxoff\",   xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 1 */\n    {OP_monitor, 0xc80f0171, \"monitor\",  xx, xx, eax, ecx, edx, mrm, x, END_LIST},\n    {OP_mwait,   0xc90f0171, \"mwait\",  xx, xx, eax, ecx, xx, mrm, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 2 */\n    {OP_swapgs, 0xf80f0177, \"swapgs\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    {OP_rdtscp, 0xf90f0177, \"rdtscp\", edx, eax, xx, xx, xx, mrm|xop, x, exop[10]},/*AMD-only*/\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n  { /* rm extension 3 */\n    {OP_vmrun,  0xd80f0173, \"vmrun\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_vmmcall,0xd90f0173, \"vmmcall\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_vmload, 0xda0f0173, \"vmload\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_vmsave, 0xdb0f0173, \"vmsave\", xx, xx, axAX, xx, xx, mrm, x, END_LIST},\n    {OP_stgi,   0xdc0f0173, \"stgi\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_clgi,   0xdd0f0173, \"clgi\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_skinit, 0xde0f0173, \"skinit\", xx, xx, eax, xx, xx, mrm, x, END_LIST},\n    {OP_invlpga,0xdf0f0173, \"invlpga\", xx, xx, axAX, ecx, xx, mrm, x, END_LIST},\n  },\n  { /* rm extension 4 */\n    {OP_xgetbv, 0xd00f0172, \"xgetbv\", edx, eax, ecx, xx, xx, mrm, x, END_LIST},\n    {OP_xsetbv, 0xd10f0172, \"xsetbv\", xx, xx, ecx, edx, eax, mrm, x, END_LIST},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n    {OP_vmfunc, 0xd40f0172, \"vmfunc\", xx, xx, xx, xx, xx, mrm|o64, x, END_LIST},\n    /* Only if the transaction fails does xend write to eax => predcx.\n     * XXX i#1314: on failure eip is also written to.\n     */\n    {OP_xend,   0xd50f0172, \"xend\", eax, xx, xx, xx, xx, mrm|predcx, x, NA},\n    {OP_xtest,  0xd60f0172, \"xtest\", xx, xx, xx, xx, xx, mrm, fW6, NA},\n    {INVALID,   0x0f0131, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether in 64-bit mode\n */\n\nconst instr_info_t x64_extensions[][2] = {\n  {    /* x64_ext 0 */\n    {OP_inc,  0x400000, \"inc\", zAX, xx, zAX, xx, xx, i64, (fW6&(~fWC)), t64e[1][0]},\n    {PREFIX,  0x400000, \"rex\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_GENERAL},\n  }, { /* x64_ext 1 */\n    {OP_inc,  0x410000, \"inc\", zCX, xx, zCX, xx, xx, i64, (fW6&(~fWC)), t64e[2][0]},\n    {PREFIX,  0x410000, \"rex.b\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_B},\n  }, { /* x64_ext 2 */\n    {OP_inc,  0x420000, \"inc\", zDX, xx, zDX, xx, xx, i64, (fW6&(~fWC)), t64e[3][0]},\n    {PREFIX,  0x420000, \"rex.x\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_X},\n  }, { /* x64_ext 3 */\n    {OP_inc,  0x430000, \"inc\", zBX, xx, zBX, xx, xx, i64, (fW6&(~fWC)), t64e[4][0]},\n    {PREFIX,  0x430000, \"rex.xb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 4 */\n    {OP_inc,  0x440000, \"inc\", zSP, xx, zSP, xx, xx, i64, (fW6&(~fWC)), t64e[5][0]},\n    {PREFIX,  0x440000, \"rex.r\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R},\n  }, { /* x64_ext 5 */\n    {OP_inc,  0x450000, \"inc\", zBP, xx, zBP, xx, xx, i64, (fW6&(~fWC)), t64e[6][0]},\n    {PREFIX,  0x450000, \"rex.rb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_B},\n  }, { /* x64_ext 6 */\n    {OP_inc,  0x460000, \"inc\", zSI, xx, zSI, xx, xx, i64, (fW6&(~fWC)), t64e[7][0]},\n    {PREFIX,  0x460000, \"rex.rx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_X},\n  }, { /* x64_ext 7 */\n    {OP_inc,  0x470000, \"inc\", zDI, xx, zDI, xx, xx, i64, (fW6&(~fWC)), tex[12][0]},\n    {PREFIX,  0x470000, \"rex.rxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_R|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 8 */\n    {OP_dec,  0x480000, \"dec\", zAX, xx, zAX, xx, xx, i64, (fW6&(~fWC)), t64e[9][0]},\n    {PREFIX,  0x480000, \"rex.w\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W},\n  }, { /* x64_ext 9 */\n    {OP_dec,  0x490000, \"dec\", zCX, xx, zCX, xx, xx, i64, (fW6&(~fWC)), t64e[10][0]},\n    {PREFIX,  0x490000, \"rex.wb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_B},\n  }, { /* x64_ext 10 */\n    {OP_dec,  0x4a0000, \"dec\", zDX, xx, zDX, xx, xx, i64, (fW6&(~fWC)), t64e[11][0]},\n    {PREFIX,  0x4a0000, \"rex.wx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_X},\n  }, { /* x64_ext 11 */\n    {OP_dec,  0x4b0000, \"dec\", zBX, xx, zBX, xx, xx, i64, (fW6&(~fWC)), t64e[12][0]},\n    {PREFIX,  0x4b0000, \"rex.wxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 12 */\n    {OP_dec,  0x4c0000, \"dec\", zSP, xx, zSP, xx, xx, i64, (fW6&(~fWC)), t64e[13][0]},\n    {PREFIX,  0x4c0000, \"rex.wr\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R},\n  }, { /* x64_ext 13 */\n    {OP_dec,  0x4d0000, \"dec\", zBP, xx, zBP, xx, xx, i64, (fW6&(~fWC)), t64e[14][0]},\n    {PREFIX,  0x4d0000, \"rex.wrb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_B},\n  }, { /* x64_ext 14 */\n    {OP_dec,  0x4e0000, \"dec\", zSI, xx, zSI, xx, xx, i64, (fW6&(~fWC)), t64e[15][0]},\n    {PREFIX,  0x4e0000, \"rex.wrx\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_X},\n  }, { /* x64_ext 15 */\n    {OP_dec,  0x4f0000, \"dec\", zDI, xx, zDI, xx, xx, i64, (fW6&(~fWC)), tex[12][1]},\n    {PREFIX,  0x4f0000, \"rex.wrxb\", xx, xx, xx, xx, xx, no, x, PREFIX_REX_W|PREFIX_REX_R|PREFIX_REX_X|PREFIX_REX_B},\n  }, { /* x64_ext 16 */\n    {OP_arpl,   0x630000, \"arpl\", Ew, xx, Gw, xx, xx, mrm|i64, fWZ, END_LIST},\n    {OP_movsxd, 0x630000, \"movsxd\", Gv, xx, Ed, xx, xx, mrm|o64, x, END_LIST},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on the first two bits of the 2nd byte,\n * or whether in x64 mode.\n */\nconst instr_info_t vex_prefix_extensions[][2] = {\n  {    /* vex_prefix_ext 0 */\n    {OP_les,  0xc40000, \"les\", Gz, es, Mp, xx, xx, mrm|i64, x, END_LIST},\n    {PREFIX,  0xc40000, \"vex+2b\", xx, xx, xx, xx, xx, no, x, PREFIX_VEX_3B},\n  }, { /* vex_prefix_ext 1 */\n    {OP_lds,  0xc50000, \"lds\", Gz, ds, Mp, xx, xx, mrm|i64, x, END_LIST},\n    {PREFIX,  0xc50000, \"vex+1b\", xx, xx, xx, xx, xx, no, x, PREFIX_VEX_2B},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on bits 4 and 5 of the 2nd byte.\n */\nconst instr_info_t xop_prefix_extensions[][2] = {\n  {    /* xop_prefix_ext 0 */\n    {EXTENSION, 0x8f0000, \"(group 1d)\", xx, xx, xx, xx, xx, mrm, x, 26},\n    {PREFIX,    0x8f0000, \"xop\", xx, xx, xx, xx, xx, no, x, PREFIX_XOP},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether vex-encoded and vex.L\n * Index 0 = no vex, 1 = vex and vex.L=0, 2 = vex and vex.L=1\n */\nconst instr_info_t vex_L_extensions[][3] = {\n  {    /* vex_L_ext 0 */\n    {OP_emms,       0x0f7710, \"emms\", xx, xx, xx, xx, xx, no, x, END_LIST},\n    {OP_vzeroupper, 0x0f7710, \"vzeroupper\", xx, xx, xx, xx, xx, vex, x, END_LIST},\n    {OP_vzeroall,   0x0f7790, \"vzeroall\", xx, xx, xx, xx, xx, vex, x, END_LIST},\n  },\n};\n\n/****************************************************************************\n* Instructions that differ depending on whether evex-encoded.\n* Index 0 = no evex, 1 = evex\n*/\n\nconst instr_info_t evex_prefix_extensions[][2] = {\n  {   /* evex_prefix_ext */\n    {OP_bound, 0x620000, \"bound\", xx, xx, Gv, Ma, xx, mrm|i64, x, END_LIST},\n    {PREFIX,   0x620000, \"(evex prefix)\", xx, xx, xx, xx, xx, no, x, PREFIX_EVEX},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on whether a rex prefix is present.\n */\n\n/* Instructions that differ depending on whether rex.b in is present.\n * The table is indexed by rex.b: index 0 is for no rex.b.\n */\nconst instr_info_t rex_b_extensions[][2] = {\n  { /* rex.b extension 0 */\n    {OP_nop,  0x900000, \"nop\", xx, xx, xx, xx, xx, no, x, tpe[103][2]},\n    /* For decoding we avoid needing new operand types by only getting\n     * here if rex.b is set.  For encode, we would need either to take\n     * REQUIRES_REX + OPCODE_SUFFIX or a new operand type for registers that\n     * must be extended (could also try to list r8 instead of eax but\n     * have to make sure all decode/encode routines can handle that as most\n     * assume the registers listed here are 32-bit base): that's too\n     * much effort for a corner case that we're not 100% certain works on\n     * all x64 processors, so we just don't list in the encoding chain.\n     */\n    {OP_xchg, 0x900000, \"xchg\", eAX_x, eAX, eAX_x, eAX, xx, o64, x, END_LIST},\n  },\n};\n\n/* Instructions that differ depending on whether rex.w in is present.\n * The table is indexed by rex.w: index 0 is for no rex.w.\n */\nconst instr_info_t rex_w_extensions[][2] = {\n  { /* rex.w extension 0 */\n    {OP_fxsave32, 0x0fae30, \"fxsave\",   Me, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_fxsave64, 0x0fae30, \"fxsave64\", Me, xx, xx, xx, xx, mrm|rex, x, END_LIST},\n  },\n  { /* rex.w extension 1 */\n    {OP_fxrstor32, 0x0fae31, \"fxrstor\",   xx, xx, Me, xx, xx, mrm, x, END_LIST},\n    {OP_fxrstor64, 0x0fae31, \"fxrstor64\", xx, xx, Me, xx, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 2 */\n    {OP_xsave32,   0x0fae34, \"xsave\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsave64,   0x0fae34, \"xsave64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 3 */\n    {OP_xrstor32, 0x0fae35, \"xrstor\",   xx, xx, Mxsave, edx, eax, mrm, x, END_LIST},\n    {OP_xrstor64, 0x0fae35, \"xrstor64\", xx, xx, Mxsave, edx, eax, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 4 */\n    {OP_xsaveopt32, 0x0fae36, \"xsaveopt\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsaveopt64, 0x0fae36, \"xsaveopt64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n  { /* rex.w extension 5 */\n    {OP_xsavec32, 0x0fc734, \"xsavec\",   Mxsave, xx, edx, eax, xx, mrm, x, END_LIST},\n    {OP_xsavec64, 0x0fc734, \"xsavec64\", Mxsave, xx, edx, eax, xx, mrm|rex, o64, END_LIST},\n  },\n};\n\n/****************************************************************************\n * 3-byte-opcode instructions: 0x0f 0x38 and 0x0f 0x3a.\n * SSSE3 and SSE4.\n *\n * XXX: if they add more 2nd byte possibilities, we could switch to one\n * large table here and one extension type with indices into which subtable.\n * For now we have two separate tables.\n *\n * N.B.: if any are added here that do not take modrm bytes, or whose\n * size can vary based on data16 or addr16, we need to modify our\n * decode_fast table assumptions!\n *\n * Many of these only come in Vdq,Wdq forms, yet still require the 0x66 prefix.\n * Rather than waste space in the prefix_extensions table for 4 entries 3 of which\n * are invalid, and need another layer of lookup, we use the new REQUIRES_PREFIX\n * flag (\"reqp\").\n *\n * Since large parts of the opcode space are empty, we save space by having a\n * table of 256 indices instead of 256 instr_info_t structs.\n */\nconst byte third_byte_38_index[256] = {\n  /* 0   1   2   3    4   5   6   7    8   9   A   B    C   D   E   F */\n     1,  2,  3,  4,   5,  6,  7,  8,   9, 10, 11, 12,  96, 97, 56, 57,  /* 0 */\n    16,  0,  0, 88,  17, 18,111, 19,  89, 90, 91,  0,  13, 14, 15,  0,  /* 1 */\n    20, 21, 22, 23,  24, 25,  0,  0,  26, 27, 28, 29,  92, 93, 94, 95,  /* 2 */\n    30, 31, 32, 33,  34, 35,112, 36,  37, 38, 39, 40,  41, 42, 43, 44,  /* 3 */\n    45, 46,  0,  0,   0,113,114,115,   0,  0,  0,  0,   0,  0,  0,  0,  /* 4 */\n     0,  0,  0,  0,   0,  0,  0,  0, 118,119,108,  0,   0,  0,  0,  0,  /* 5 */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* 6 */\n     0,  0,  0,  0,   0,  0,  0,  0, 116,117,  0,  0,   0,  0,  0,  0,  /* 7 */\n    49, 50,103,  0,   0,  0,  0,  0,   0,  0,  0,  0, 109,  0,110,  0,  /* 8 */\n   104,105,106,107,   0,  0, 58, 59,  60, 61, 62, 63,  64, 65, 66, 67,  /* 9 */\n     0,  0,  0,  0,   0,  0, 68, 69,  70, 71, 72, 73,  74, 75, 76, 77,  /* A */\n     0,  0,  0,  0,   0,  0, 78, 79,  80, 81, 82, 83,  84, 85, 86, 87,  /* B */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* C */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0, 51,  52, 53, 54, 55,  /* D */\n     0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,   0,  0,  0,  0,  /* E */\n    47, 48,100, 99,   0,101,102, 98,   0,  0,  0,  0,   0,  0,  0,  0   /* F */\n};\n\nconst instr_info_t third_byte_38[] = {\n  {INVALID,     0x38ff18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},              /* 0*/\n  /**** SSSE3 ****/\n  {PREFIX_EXT,  0x380018,   \"(prefix ext 118)\", xx, xx, xx, xx, xx, mrm, x, 118},/* 1*/\n  {PREFIX_EXT,  0x380118,   \"(prefix ext 119)\", xx, xx, xx, xx, xx, mrm, x, 119},/* 2*/\n  {PREFIX_EXT,  0x380218,   \"(prefix ext 120)\", xx, xx, xx, xx, xx, mrm, x, 120},/* 3*/\n  {PREFIX_EXT,  0x380318,   \"(prefix ext 121)\", xx, xx, xx, xx, xx, mrm, x, 121},/* 4*/\n  {PREFIX_EXT,  0x380418,   \"(prefix ext 122)\", xx, xx, xx, xx, xx, mrm, x, 122},/* 5*/\n  {PREFIX_EXT,  0x380518,   \"(prefix ext 123)\", xx, xx, xx, xx, xx, mrm, x, 123},/* 6*/\n  {PREFIX_EXT,  0x380618,   \"(prefix ext 124)\", xx, xx, xx, xx, xx, mrm, x, 124},/* 7*/\n  {PREFIX_EXT,  0x380718,   \"(prefix ext 125)\", xx, xx, xx, xx, xx, mrm, x, 125},/* 8*/\n  {PREFIX_EXT,  0x380818,   \"(prefix ext 126)\", xx, xx, xx, xx, xx, mrm, x, 126},/* 9*/\n  {PREFIX_EXT,  0x380918,   \"(prefix ext 127)\", xx, xx, xx, xx, xx, mrm, x, 127},/*10*/\n  {PREFIX_EXT,  0x380a18,   \"(prefix ext 128)\", xx, xx, xx, xx, xx, mrm, x, 128},/*11*/\n  {PREFIX_EXT,  0x380b18,   \"(prefix ext 129)\", xx, xx, xx, xx, xx, mrm, x, 129},/*12*/\n  {PREFIX_EXT,  0x381c18,   \"(prefix ext 130)\", xx, xx, xx, xx, xx, mrm, x, 130},/*13*/\n  {PREFIX_EXT,  0x381d18,   \"(prefix ext 131)\", xx, xx, xx, xx, xx, mrm, x, 131},/*14*/\n  {PREFIX_EXT,  0x381e18,   \"(prefix ext 132)\", xx, xx, xx, xx, xx, mrm, x, 132},/*15*/\n  /**** SSE4 ****/\n  {OP_pblendvb, 0x66381018, \"pblendvb\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*16*/\n  {OP_blendvps, 0x66381418, \"blendvps\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*17*/\n  {OP_blendvpd, 0x66381518, \"blendvpd\", Vdq, xx, Wdq,xmm0,Vdq, mrm|reqp,x, END_LIST},/*18*/\n  {E_VEX_EXT,  0x66381718, \"(e_vex ext  3)\", xx, xx, xx, xx, xx, mrm, x,  3},/*19*/\n  /* 20 */\n  {E_VEX_EXT,  0x66382018, \"(e_vex ext  4)\", xx, xx, xx, xx, xx, mrm, x,  4},/*20*/\n  {E_VEX_EXT,  0x66382118, \"(e_vex ext  5)\", xx, xx, xx, xx, xx, mrm, x,  5},/*21*/\n  {E_VEX_EXT,  0x66382218, \"(e_vex ext  6)\", xx, xx, xx, xx, xx, mrm, x,  6},/*22*/\n  {E_VEX_EXT,  0x66382318, \"(e_vex ext  7)\", xx, xx, xx, xx, xx, mrm, x,  7},/*23*/\n  {E_VEX_EXT,  0x66382418, \"(e_vex ext  8)\", xx, xx, xx, xx, xx, mrm, x,  8},/*24*/\n  {E_VEX_EXT,  0x66382518, \"(e_vex ext  9)\", xx, xx, xx, xx, xx, mrm, x,  9},/*25*/\n  {E_VEX_EXT,  0x66382818, \"(e_vex ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},/*26*/\n  {E_VEX_EXT,  0x66382918, \"(e_vex ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},/*27*/\n  {E_VEX_EXT,  0x66382a18, \"(e_vex ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},/*28*/\n  {E_VEX_EXT,  0x66382b18, \"(e_vex ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},/*29*/\n  /* 30 */\n  {E_VEX_EXT,  0x66383018, \"(e_vex ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},/*30*/\n  {E_VEX_EXT,  0x66383118, \"(e_vex ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},/*31*/\n  {E_VEX_EXT,  0x66383218, \"(e_vex ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},/*32*/\n  {E_VEX_EXT,  0x66383318, \"(e_vex ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},/*33*/\n  {E_VEX_EXT,  0x66383418, \"(e_vex ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},/*34*/\n  {E_VEX_EXT,  0x66383518, \"(e_vex ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},/*35*/\n  {E_VEX_EXT,  0x66383718, \"(e_vex ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},/*36*/\n  {E_VEX_EXT,  0x66383818, \"(e_vex ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},/*37*/\n  {E_VEX_EXT,  0x66383918, \"(e_vex ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},/*38*/\n  {E_VEX_EXT,  0x66383a18, \"(e_vex ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},/*39*/\n  {E_VEX_EXT,  0x66383b18, \"(e_vex ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},/*40*/\n  {E_VEX_EXT,  0x66383c18, \"(e_vex ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},/*41*/\n  {E_VEX_EXT,  0x66383d18, \"(e_vex ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},/*42*/\n  {E_VEX_EXT,  0x66383e18, \"(e_vex ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},/*43*/\n  {E_VEX_EXT,  0x66383f18, \"(e_vex ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},/*44*/\n  /* 40 */\n  {E_VEX_EXT,  0x66384018, \"(e_vex ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},/*45*/\n  {E_VEX_EXT,  0x66384118, \"(e_vex ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},/*46*/\n  /* f0 */\n  {PREFIX_EXT,  0x38f018,   \"(prefix ext 138)\", xx, xx, xx, xx, xx, mrm, x, 138},/*47*/\n  {PREFIX_EXT,  0x38f118,   \"(prefix ext 139)\", xx, xx, xx, xx, xx, mrm, x, 139},/*48*/\n  /* 80 */\n  {OP_invept,   0x66388018, \"invept\",   xx, xx, Gr, Mdq, xx, mrm|reqp, x, END_LIST},/*49*/\n  {OP_invvpid,  0x66388118, \"invvpid\",  xx, xx, Gr, Mdq, xx, mrm|reqp, x, END_LIST},/*50*/\n  /* db-df */\n  {E_VEX_EXT,  0x6638db18, \"(e_vex ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},/*51*/\n  {E_VEX_EXT,  0x6638dc18, \"(e_vex ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},/*52*/\n  {E_VEX_EXT,  0x6638dd18, \"(e_vex ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},/*53*/\n  {E_VEX_EXT,  0x6638de18, \"(e_vex ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},/*54*/\n  {E_VEX_EXT,  0x6638df18, \"(e_vex ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},/*55*/\n  /* AVX */\n  {E_VEX_EXT,  0x66380e18, \"(e_vex ext 59)\", xx, xx, xx, xx, xx, mrm, x, 59},/*56*/\n  {E_VEX_EXT,  0x66380f18, \"(e_vex ext 60)\", xx, xx, xx, xx, xx, mrm, x, 60},/*57*/\n  /* FMA 96-9f */\n  {VEX_W_EXT, 0x66389618, \"(vex_W ext  6)\", xx, xx, xx, xx, xx, mrm, x,  6},/*58*/\n  {VEX_W_EXT, 0x66389718, \"(vex_W ext  9)\", xx, xx, xx, xx, xx, mrm, x,  9},/*59*/\n  {VEX_W_EXT, 0x66389818, \"(vex_W ext  0)\", xx, xx, xx, xx, xx, mrm, x,  0},/*60*/\n  {VEX_W_EXT, 0x66389918, \"(vex_W ext  3)\", xx, xx, xx, xx, xx, mrm, x,  3},/*61*/\n  {VEX_W_EXT, 0x66389a18, \"(vex_W ext 12)\", xx, xx, xx, xx, xx, mrm, x, 12},/*62*/\n  {VEX_W_EXT, 0x66389b18, \"(vex_W ext 15)\", xx, xx, xx, xx, xx, mrm, x, 15},/*63*/\n  {VEX_W_EXT, 0x66389c18, \"(vex_W ext 18)\", xx, xx, xx, xx, xx, mrm, x, 18},/*64*/\n  {VEX_W_EXT, 0x66389d18, \"(vex_W ext 21)\", xx, xx, xx, xx, xx, mrm, x, 21},/*65*/\n  {VEX_W_EXT, 0x66389e18, \"(vex_W ext 24)\", xx, xx, xx, xx, xx, mrm, x, 24},/*66*/\n  {VEX_W_EXT, 0x66389f18, \"(vex_W ext 27)\", xx, xx, xx, xx, xx, mrm, x, 27},/*67*/\n  /* FMA a6-af */\n  {VEX_W_EXT, 0x6638a618, \"(vex_W ext  7)\", xx, xx, xx, xx, xx, mrm, x,  7},/*68*/\n  {VEX_W_EXT, 0x6638a718, \"(vex_W ext 10)\", xx, xx, xx, xx, xx, mrm, x, 10},/*69*/\n  {VEX_W_EXT, 0x6638a818, \"(vex_W ext  1)\", xx, xx, xx, xx, xx, mrm, x,  1},/*70*/\n  {VEX_W_EXT, 0x6638a918, \"(vex_W ext  4)\", xx, xx, xx, xx, xx, mrm, x,  4},/*71*/\n  {VEX_W_EXT, 0x6638aa18, \"(vex_W ext 13)\", xx, xx, xx, xx, xx, mrm, x, 13},/*72*/\n  {VEX_W_EXT, 0x6638ab18, \"(vex_W ext 16)\", xx, xx, xx, xx, xx, mrm, x, 16},/*73*/\n  {VEX_W_EXT, 0x6638ac18, \"(vex_W ext 19)\", xx, xx, xx, xx, xx, mrm, x, 19},/*74*/\n  {VEX_W_EXT, 0x6638ad18, \"(vex_W ext 22)\", xx, xx, xx, xx, xx, mrm, x, 22},/*75*/\n  {VEX_W_EXT, 0x6638ae18, \"(vex_W ext 25)\", xx, xx, xx, xx, xx, mrm, x, 25},/*76*/\n  {VEX_W_EXT, 0x6638af18, \"(vex_W ext 28)\", xx, xx, xx, xx, xx, mrm, x, 28},/*77*/\n  /* FMA b6-bf */\n  {VEX_W_EXT, 0x6638b618, \"(vex_W ext  8)\", xx, xx, xx, xx, xx, mrm, x,  8},/*78*/\n  {VEX_W_EXT, 0x6638b718, \"(vex_W ext 11)\", xx, xx, xx, xx, xx, mrm, x, 11},/*79*/\n  {VEX_W_EXT, 0x6638b818, \"(vex_W ext  2)\", xx, xx, xx, xx, xx, mrm, x,  2},/*80*/\n  {VEX_W_EXT, 0x6638b918, \"(vex_W ext  5)\", xx, xx, xx, xx, xx, mrm, x,  5},/*81*/\n  {VEX_W_EXT, 0x6638ba18, \"(vex_W ext 14)\", xx, xx, xx, xx, xx, mrm, x, 14},/*82*/\n  {VEX_W_EXT, 0x6638bb18, \"(vex_W ext 17)\", xx, xx, xx, xx, xx, mrm, x, 17},/*83*/\n  {VEX_W_EXT, 0x6638bc18, \"(vex_W ext 20)\", xx, xx, xx, xx, xx, mrm, x, 20},/*84*/\n  {VEX_W_EXT, 0x6638bd18, \"(vex_W ext 23)\", xx, xx, xx, xx, xx, mrm, x, 23},/*85*/\n  {VEX_W_EXT, 0x6638be18, \"(vex_W ext 26)\", xx, xx, xx, xx, xx, mrm, x, 26},/*86*/\n  {VEX_W_EXT, 0x6638bf18, \"(vex_W ext 29)\", xx, xx, xx, xx, xx, mrm, x, 29},/*87*/\n  /* AVX overlooked in original pass */\n  {E_VEX_EXT, 0x66381318, \"(e_vex ext 63)\", xx, xx, xx, xx, xx, mrm, x, 63},/*88*/\n  {E_VEX_EXT, 0x66381818, \"(e_vex ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},/*89*/\n  {E_VEX_EXT, 0x66381918, \"(e_vex ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},/*90*/\n  {E_VEX_EXT, 0x66381a18, \"(e_vex ext 66)\", xx, xx, xx, xx, xx, mrm, x, 66},/*91*/\n  {E_VEX_EXT, 0x66382c18, \"(e_vex ext 67)\", xx, xx, xx, xx, xx, mrm, x, 67},/*92*/\n  {E_VEX_EXT, 0x66382d18, \"(e_vex ext 68)\", xx, xx, xx, xx, xx, mrm, x, 68},/*93*/\n  {E_VEX_EXT, 0x66382e18, \"(e_vex ext 69)\", xx, xx, xx, xx, xx, mrm, x, 69},/*94*/\n  {E_VEX_EXT, 0x66382f18, \"(e_vex ext 70)\", xx, xx, xx, xx, xx, mrm, x, 70},/*95*/\n  {E_VEX_EXT, 0x66380c18, \"(e_vex ext 77)\", xx, xx, xx, xx, xx, mrm, x, 77},/*96*/\n  {E_VEX_EXT, 0x66380d18, \"(e_vex ext 78)\", xx, xx, xx, xx, xx, mrm, x, 78},/*97*/\n  /* TBM */\n  {PREFIX_EXT, 0x38f718, \"(prefix ext 141)\", xx, xx, xx, xx, xx, mrm, x, 141},  /*98*/\n  /* BMI1 */\n  {EXTENSION, 0x38f318, \"(group 17)\", By, xx, Ey, xx, xx, mrm|vex, x, 31},      /*99*/\n  /* marked reqp b/c it should have no prefix (prefixes for future opcodes) */\n  {OP_andn, 0x38f218, \"andn\", Gy, xx, By, Ey, xx, mrm|vex|reqp, fW6, END_LIST},/*100*/\n  /* BMI2 */\n  {PREFIX_EXT, 0x38f518, \"(prefix ext 142)\", xx, xx, xx, xx, xx, mrm, x, 142}, /*101*/\n  {PREFIX_EXT, 0x38f618, \"(prefix ext 143)\", xx, xx, xx, xx, xx, mrm, x, 143}, /*102*/\n  {OP_invpcid, 0x66388218, \"invpcid\",  xx, xx, Gy, Mdq, xx, mrm|reqp, x, END_LIST},/*103*/\n  /* AVX2 */\n  {VEX_W_EXT, 0x66389018, \"(vex_W ext 66)\", xx, xx, xx, xx, xx, mrm|vex, x, 66},/*104*/\n  {VEX_W_EXT, 0x66389118, \"(vex_W ext 67)\", xx, xx, xx, xx, xx, mrm|vex, x, 67},/*105*/\n  {VEX_W_EXT, 0x66389218, \"(vex_W ext 68)\", xx, xx, xx, xx, xx, mrm|vex, x, 68},/*106*/\n  {VEX_W_EXT, 0x66389318, \"(vex_W ext 69)\", xx, xx, xx, xx, xx, mrm|vex, x, 69},/*107*/\n  {OP_vbroadcasti128,0x66385a18, \"vbroadcasti128\",Vqq,xx,Mdq,xx,xx,mrm|vex|reqp,x,END_LIST},/*108*/\n  {VEX_W_EXT, 0x66388c18, \"(vex_W ext 70)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 70},/*109*/\n  {VEX_W_EXT, 0x66388e18, \"(vex_W ext 71)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 71},/*110*/\n  /* Following Intel and not marking as packed float vs ints: just \"qq\". */\n  {OP_vpermps,0x66381618, \"vpermps\",Vqq,xx,Hqq,Wqq,xx, mrm|vex|reqp,x,END_LIST}, /*111*/\n  {OP_vpermd, 0x66383618, \"vpermd\", Vqq,xx,Hqq,Wqq,xx, mrm|vex|reqp,x,END_LIST}, /*112*/\n  {VEX_W_EXT, 0x66384518, \"(vex_W ext 72)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 72},/*113*/\n  {OP_vpsravd,0x66384618, \"vpsravd\", Vx,xx,Hx,Wx,xx, mrm|vex|reqp, x, END_LIST}, /*114*/\n  {VEX_W_EXT, 0x66384718, \"(vex_W ext 73)\", xx,xx,xx,xx,xx, mrm|vex|reqp, x, 73},/*115*/\n  {OP_vpbroadcastb, 0x66387818, \"vpbroadcastb\", Vx, xx, Wb_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*116*/\n  {OP_vpbroadcastw, 0x66387918, \"vpbroadcastw\", Vx, xx, Ww_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*117*/\n  {OP_vpbroadcastd, 0x66385818, \"vpbroadcastd\", Vx, xx, Wd_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*118*/\n  {OP_vpbroadcastq, 0x66385918, \"vpbroadcastq\", Vx, xx, Wq_dq, xx, xx, mrm|vex|reqp, x, END_LIST},/*119*/\n};\n\n/* N.B.: every 0x3a instr so far has an immediate.  If a version w/o an immed\n * comes along we'll have to add a threebyte_3a_vex_extra[] table to decode_fast.c.\n */\nconst byte third_byte_3a_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n    59,60,61, 0, 28,29,30, 0,  6, 7, 8, 9, 10,11,12, 1,  /* 0 */\n     0, 0, 0, 0,  2, 3, 4, 5, 31,32, 0, 0,  0,33, 0, 0,  /* 1 */\n    13,14,15, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n    63,64,65,66,  0, 0, 0, 0, 57,58, 0, 0,  0, 0, 0, 0,  /* 3 */\n    16,17,18, 0, 23, 0,62, 0, 54,55,25,26, 27, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 34,35,36,37,  /* 5 */\n    19,20,21,22,  0, 0, 0, 0, 38,39,40,41, 42,43,44,45,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0, 46,47,48,49, 50,51,52,53,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0,24,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n    56, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst instr_info_t third_byte_3a[] = {\n  {INVALID,     0x3aff18, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},                 /* 0*/\n  /**** SSSE3 ****/\n  {PREFIX_EXT,  0x3a0f18, \"(prefix ext 133)\", xx, xx, xx, xx, xx, mrm, x, 133},    /* 1*/\n  /**** SSE4 ****/\n  {E_VEX_EXT,  0x663a1418, \"(e_vex ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},/* 2*/\n  {E_VEX_EXT,  0x663a1518, \"(e_vex ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},/* 3*/\n  {E_VEX_EXT,  0x663a1618, \"(e_vex ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},/* 4*/\n  {E_VEX_EXT,  0x663a1718, \"(e_vex ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},/* 5*/\n  {E_VEX_EXT,  0x663a0818, \"(e_vex ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},/* 6*/\n  {E_VEX_EXT,  0x663a0918, \"(e_vex ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},/* 7*/\n  {E_VEX_EXT,  0x663a0a18, \"(e_vex ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},/* 8*/\n  {E_VEX_EXT,  0x663a0b18, \"(e_vex ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},/* 9*/\n  {E_VEX_EXT,  0x663a0c18, \"(e_vex ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},/*10*/\n  {E_VEX_EXT,  0x663a0d18, \"(e_vex ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},/*11*/\n  {E_VEX_EXT,  0x663a0e18, \"(e_vex ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},/*12*/\n  /* 20 */\n  {E_VEX_EXT,  0x663a2018, \"(e_vex ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},/*13*/\n  {E_VEX_EXT,  0x663a2118, \"(e_vex ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},/*14*/\n  {E_VEX_EXT,  0x663a2218, \"(e_vex ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},/*15*/\n  /* 40 */\n  {E_VEX_EXT,  0x663a4018, \"(e_vex ext 50)\", xx, xx, xx, xx, xx, mrm, x, 50},/*16*/\n  {E_VEX_EXT,  0x663a4118, \"(e_vex ext 51)\", xx, xx, xx, xx, xx, mrm, x, 51},/*17*/\n  {E_VEX_EXT,  0x663a4218, \"(e_vex ext 52)\", xx, xx, xx, xx, xx, mrm, x, 52},/*18*/\n  /* 60 */\n  {E_VEX_EXT,  0x663a6018, \"(e_vex ext 53)\", xx, xx, xx, xx, xx, mrm, x, 53},/*19*/\n  {E_VEX_EXT,  0x663a6118, \"(e_vex ext 54)\", xx, xx, xx, xx, xx, mrm, x, 54},/*20*/\n  {E_VEX_EXT,  0x663a6218, \"(e_vex ext 55)\", xx, xx, xx, xx, xx, mrm, x, 55},/*21*/\n  {E_VEX_EXT,  0x663a6318, \"(e_vex ext 56)\", xx, xx, xx, xx, xx, mrm, x, 56},/*22*/\n  {E_VEX_EXT,  0x663a4418, \"(e_vex ext 57)\", xx, xx, xx, xx, xx, mrm, x, 57},/*23*/\n  {E_VEX_EXT,  0x663adf18, \"(e_vex ext 58)\", xx, xx, xx, xx, xx, mrm, x, 58},/*24*/\n  /* AVX overlooked in original pass */\n  {E_VEX_EXT,  0x663a4a18, \"(e_vex ext  0)\", xx, xx, xx, xx, xx, mrm, x,  0},/*25*/\n  {E_VEX_EXT,  0x663a4b18, \"(e_vex ext  1)\", xx, xx, xx, xx, xx, mrm, x,  1},/*26*/\n  {E_VEX_EXT,  0x663a4c18, \"(e_vex ext  2)\", xx, xx, xx, xx, xx, mrm, x,  2},/*27*/\n  {E_VEX_EXT,  0x663a0418, \"(e_vex ext 71)\", xx, xx, xx, xx, xx, mrm, x, 71},/*28*/\n  {E_VEX_EXT,  0x663a0518, \"(e_vex ext 72)\", xx, xx, xx, xx, xx, mrm, x, 72},/*29*/\n  {E_VEX_EXT,  0x663a0618, \"(e_vex ext 73)\", xx, xx, xx, xx, xx, mrm, x, 73},/*30*/\n  {E_VEX_EXT,  0x663a1818, \"(e_vex ext 74)\", xx, xx, xx, xx, xx, mrm, x, 74},/*31*/\n  {E_VEX_EXT,  0x663a1918, \"(e_vex ext 75)\", xx, xx, xx, xx, xx, mrm, x, 75},/*32*/\n  {E_VEX_EXT,  0x663a1d18, \"(e_vex ext 76)\", xx, xx, xx, xx, xx, mrm, x, 76},/*33*/\n  /* FMA4 */\n  {VEX_W_EXT,0x663a5c18, \"(vex_W ext 30)\", xx, xx, xx, xx, xx, mrm, x, 30},/*34*/\n  {VEX_W_EXT,0x663a5d18, \"(vex_W ext 31)\", xx, xx, xx, xx, xx, mrm, x, 31},/*35*/\n  {VEX_W_EXT,0x663a5e18, \"(vex_W ext 32)\", xx, xx, xx, xx, xx, mrm, x, 32},/*36*/\n  {VEX_W_EXT,0x663a5f18, \"(vex_W ext 33)\", xx, xx, xx, xx, xx, mrm, x, 33},/*37*/\n  {VEX_W_EXT,0x663a6818, \"(vex_W ext 34)\", xx, xx, xx, xx, xx, mrm, x, 34},/*38*/\n  {VEX_W_EXT,0x663a6918, \"(vex_W ext 35)\", xx, xx, xx, xx, xx, mrm, x, 35},/*39*/\n  {VEX_W_EXT,0x663a6a18, \"(vex_W ext 36)\", xx, xx, xx, xx, xx, mrm, x, 36},/*40*/\n  {VEX_W_EXT,0x663a6b18, \"(vex_W ext 37)\", xx, xx, xx, xx, xx, mrm, x, 37},/*41*/\n  {VEX_W_EXT,0x663a6c18, \"(vex_W ext 38)\", xx, xx, xx, xx, xx, mrm, x, 38},/*42*/\n  {VEX_W_EXT,0x663a6d18, \"(vex_W ext 39)\", xx, xx, xx, xx, xx, mrm, x, 39},/*43*/\n  {VEX_W_EXT,0x663a6e18, \"(vex_W ext 40)\", xx, xx, xx, xx, xx, mrm, x, 40},/*44*/\n  {VEX_W_EXT,0x663a6f18, \"(vex_W ext 41)\", xx, xx, xx, xx, xx, mrm, x, 41},/*45*/\n  {VEX_W_EXT,0x663a7818, \"(vex_W ext 42)\", xx, xx, xx, xx, xx, mrm, x, 42},/*46*/\n  {VEX_W_EXT,0x663a7918, \"(vex_W ext 43)\", xx, xx, xx, xx, xx, mrm, x, 43},/*47*/\n  {VEX_W_EXT,0x663a7a18, \"(vex_W ext 44)\", xx, xx, xx, xx, xx, mrm, x, 44},/*48*/\n  {VEX_W_EXT,0x663a7b18, \"(vex_W ext 45)\", xx, xx, xx, xx, xx, mrm, x, 45},/*49*/\n  {VEX_W_EXT,0x663a7c18, \"(vex_W ext 46)\", xx, xx, xx, xx, xx, mrm, x, 46},/*50*/\n  {VEX_W_EXT,0x663a7d18, \"(vex_W ext 47)\", xx, xx, xx, xx, xx, mrm, x, 47},/*51*/\n  {VEX_W_EXT,0x663a7e18, \"(vex_W ext 48)\", xx, xx, xx, xx, xx, mrm, x, 48},/*52*/\n  {VEX_W_EXT,0x663a7f18, \"(vex_W ext 49)\", xx, xx, xx, xx, xx, mrm, x, 49},/*53*/\n  /* XOP */\n  {VEX_W_EXT,0x663a4818, \"(vex_W ext 64)\", xx, xx, xx, xx, xx, mrm, x, 64},/*54*/\n  {VEX_W_EXT,0x663a4918, \"(vex_W ext 65)\", xx, xx, xx, xx, xx, mrm, x, 65},/*55*/\n  /* BMI2 */\n  {OP_rorx,  0xf23af018, \"rorx\",  Gy, xx, Ey, Ib, xx, mrm|vex|reqp, x, END_LIST},/*56*/\n  /* AVX2 */\n  {OP_vinserti128,0x663a3818,\"vinserti128\",Vqq,xx,Hqq,Wdq,Ib,mrm|vex|reqp,x,END_LIST},/*57*/\n  {OP_vextracti128,0x663a3918,\"vextracti128\",Wdq,xx,Vqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*58*/\n  {OP_vpermq, 0x663a0058, \"vpermq\", Vqq,xx,Wqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*59*/\n  /* Following Intel and not marking as packed float vs ints: just \"qq\". */\n  {OP_vpermpd,0x663a0158, \"vpermpd\",Vqq,xx,Wqq,Ib,xx,mrm|vex|reqp,x,END_LIST},/*60*/\n  {OP_vpblendd,0x663a0218,\"vpblendd\",Vx,xx,Hx,Wx,Ib, mrm|vex|reqp,x,END_LIST},/*61*/\n  {OP_vperm2i128,0x663a4618,\"vperm2i128\",Vqq,xx,Hqq,Wqq,Ib, mrm|vex|reqp,x,END_LIST},/*62*/\n  /* AVX-512, VEX prefix. */\n  {VEX_W_EXT,0x660f3010, \"(vex_W ext 102)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 102},/*63*/\n  {VEX_W_EXT,0x660f3110, \"(vex_W ext 103)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 103},/*64*/\n  {VEX_W_EXT,0x660f3210, \"(vex_W ext 100)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 100},/*65*/\n  {VEX_W_EXT,0x660f3310, \"(vex_W ext 101)\", xx, xx, xx, xx, xx, mrm|vex|reqp, x, 101},/*66*/\n};\n\n/****************************************************************************\n * Instructions that differ depending on vex.W\n * Index is vex.W value\n */\nconst instr_info_t vex_W_extensions[][2] = {\n  {    /* vex_W_ext 0 */\n    {OP_vfmadd132ps,0x66389818,\"vfmadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd132pd,0x66389858,\"vfmadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 1 */\n    {OP_vfmadd213ps,0x6638a818,\"vfmadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd213pd,0x6638a858,\"vfmadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 2 */\n    {OP_vfmadd231ps,0x6638b818,\"vfmadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd231pd,0x6638b858,\"vfmadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 3 */\n    {OP_vfmadd132ss,0x66389918,\"vfmadd132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd132sd,0x66389958,\"vfmadd132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 4 */\n    {OP_vfmadd213ss,0x6638a918,\"vfmadd213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd213sd,0x6638a958,\"vfmadd213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 5 */\n    {OP_vfmadd231ss,0x6638b918,\"vfmadd231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmadd231sd,0x6638b958,\"vfmadd231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 6 */\n    {OP_vfmaddsub132ps,0x66389618,\"vfmaddsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub132pd,0x66389658,\"vfmaddsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 7 */\n    {OP_vfmaddsub213ps,0x6638a618,\"vfmaddsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub213pd,0x6638a658,\"vfmaddsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 8 */\n    {OP_vfmaddsub231ps,0x6638b618,\"vfmaddsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmaddsub231pd,0x6638b658,\"vfmaddsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 9 */\n    {OP_vfmsubadd132ps,0x66389718,\"vfmsubadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd132pd,0x66389758,\"vfmsubadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 10 */\n    {OP_vfmsubadd213ps,0x6638a718,\"vfmsubadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd213pd,0x6638a758,\"vfmsubadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 11 */\n    {OP_vfmsubadd231ps,0x6638b718,\"vfmsubadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsubadd231pd,0x6638b758,\"vfmsubadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 12 */\n    {OP_vfmsub132ps,0x66389a18,\"vfmsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub132pd,0x66389a58,\"vfmsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 13 */\n    {OP_vfmsub213ps,0x6638aa18,\"vfmsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub213pd,0x6638aa58,\"vfmsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 14 */\n    {OP_vfmsub231ps,0x6638ba18,\"vfmsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub231pd,0x6638ba58,\"vfmsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 15 */\n    {OP_vfmsub132ss,0x66389b18,\"vfmsub132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub132sd,0x66389b58,\"vfmsub132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 16 */\n    {OP_vfmsub213ss,0x6638ab18,\"vfmsub213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub213sd,0x6638ab58,\"vfmsub213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 17 */\n    {OP_vfmsub231ss,0x6638bb18,\"vfmsub231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfmsub231sd,0x6638bb58,\"vfmsub231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 18 */\n    {OP_vfnmadd132ps,0x66389c18,\"vfnmadd132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd132pd,0x66389c58,\"vfnmadd132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 19 */\n    {OP_vfnmadd213ps,0x6638ac18,\"vfnmadd213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd213pd,0x6638ac58,\"vfnmadd213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 20 */\n    {OP_vfnmadd231ps,0x6638bc18,\"vfnmadd231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd231pd,0x6638bc58,\"vfnmadd231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 21 */\n    {OP_vfnmadd132ss,0x66389d18,\"vfnmadd132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd132sd,0x66389d58,\"vfnmadd132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 22 */\n    {OP_vfnmadd213ss,0x6638ad18,\"vfnmadd213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd213sd,0x6638ad58,\"vfnmadd213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 23 */\n    {OP_vfnmadd231ss,0x6638bd18,\"vfnmadd231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmadd231sd,0x6638bd58,\"vfnmadd231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 24 */\n    {OP_vfnmsub132ps,0x66389e18,\"vfnmsub132ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub132pd,0x66389e58,\"vfnmsub132pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 25 */\n    {OP_vfnmsub213ps,0x6638ae18,\"vfnmsub213ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub213pd,0x6638ae58,\"vfnmsub213pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 26 */\n    {OP_vfnmsub231ps,0x6638be18,\"vfnmsub231ps\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub231pd,0x6638be58,\"vfnmsub231pd\",Vvs,xx,Hvs,Wvs,Vvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 27 */\n    {OP_vfnmsub132ss,0x66389f18,\"vfnmsub132ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub132sd,0x66389f58,\"vfnmsub132sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 28 */\n    {OP_vfnmsub213ss,0x6638af18,\"vfnmsub213ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub213sd,0x6638af58,\"vfnmsub213sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 29 */\n    {OP_vfnmsub231ss,0x6638bf18,\"vfnmsub231ss\",Vss,xx,Hss,Wss,Vss,mrm|vex|reqp,x,END_LIST},\n    {OP_vfnmsub231sd,0x6638bf58,\"vfnmsub231sd\",Vsd,xx,Hsd,Wsd,Vsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 30 */\n    {OP_vfmaddsubps,0x663a5c18,\"vfmaddsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[30][1]},\n    {OP_vfmaddsubps,0x663a5c58,\"vfmaddsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 31 */\n    {OP_vfmaddsubpd,0x663a5d18,\"vfmaddsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[31][1]},\n    {OP_vfmaddsubpd,0x663a5d58,\"vfmaddsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 32 */\n    {OP_vfmsubaddps,0x663a5e18,\"vfmsubaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[32][1]},\n    {OP_vfmsubaddps,0x663a5e58,\"vfmsubaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 33 */\n    {OP_vfmsubaddpd,0x663a5f18,\"vfmsubaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[33][1]},\n    {OP_vfmsubaddpd,0x663a5f58,\"vfmsubaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 34 */\n    {OP_vfmaddps,0x663a6818,\"vfmaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[34][1]},\n    {OP_vfmaddps,0x663a6858,\"vfmaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 35 */\n    {OP_vfmaddpd,0x663a6918,\"vfmaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[35][1]},\n    {OP_vfmaddpd,0x663a6958,\"vfmaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 36 */\n    {OP_vfmaddss,0x663a6a18,\"vfmaddss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[36][1]},\n    {OP_vfmaddss,0x663a6a58,\"vfmaddss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 37 */\n    {OP_vfmaddsd,0x663a6b18,\"vfmaddsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[37][1]},\n    {OP_vfmaddsd,0x663a6b58,\"vfmaddsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 38 */\n    {OP_vfmsubps,0x663a6c18,\"vfmsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[38][1]},\n    {OP_vfmsubps,0x663a6c58,\"vfmsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 39 */\n    {OP_vfmsubpd,0x663a6d18,\"vfmsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[39][1]},\n    {OP_vfmsubpd,0x663a6d58,\"vfmsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 40 */\n    {OP_vfmsubss,0x663a6e18,\"vfmsubss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[40][1]},\n    {OP_vfmsubss,0x663a6e58,\"vfmsubss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 41 */\n    {OP_vfmsubsd,0x663a6f18,\"vfmsubsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[41][1]},\n    {OP_vfmsubsd,0x663a6f58,\"vfmsubsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 42 */\n    {OP_vfnmaddps,0x663a7818,\"vfnmaddps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[42][1]},\n    {OP_vfnmaddps,0x663a7858,\"vfnmaddps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 43 */\n    {OP_vfnmaddpd,0x663a7918,\"vfnmaddpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[43][1]},\n    {OP_vfnmaddpd,0x663a7958,\"vfnmaddpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 44 */\n    {OP_vfnmaddss,0x663a7a18,\"vfnmaddss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[44][1]},\n    {OP_vfnmaddss,0x663a7a58,\"vfnmaddss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 45 */\n    {OP_vfnmaddsd,0x663a7b18,\"vfnmaddsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[45][1]},\n    {OP_vfnmaddsd,0x663a7b58,\"vfnmaddsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 46 */\n    {OP_vfnmsubps,0x663a7c18,\"vfnmsubps\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[46][1]},\n    {OP_vfnmsubps,0x663a7c58,\"vfnmsubps\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 47 */\n    {OP_vfnmsubpd,0x663a7d18,\"vfnmsubpd\",Vvs,xx,Lvs,Wvs,Hvs,mrm|vex|reqp,x,tvexw[47][1]},\n    {OP_vfnmsubpd,0x663a7d58,\"vfnmsubpd\",Vvs,xx,Lvs,Hvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 48 */\n    {OP_vfnmsubss,0x663a7e18,\"vfnmsubss\",Vdq,xx,Lss,Wss,Hss,mrm|vex|reqp,x,tvexw[48][1]},\n    {OP_vfnmsubss,0x663a7e58,\"vfnmsubss\",Vdq,xx,Lss,Hss,Wss,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 49 */\n    {OP_vfnmsubsd,0x663a7f18,\"vfnmsubsd\",Vdq,xx,Lsd,Wsd,Hsd,mrm|vex|reqp,x,tvexw[49][1]},\n    {OP_vfnmsubsd,0x663a7f58,\"vfnmsubsd\",Vdq,xx,Lsd,Hsd,Wsd,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 50 */\n    {OP_vpcmov,    0x08a218,\"vpcmov\",    Vvs,xx,Hvs,Wvs,Lvs,mrm|vex,x,tvexw[50][1]},\n    {OP_vpcmov,    0x08a258,\"vpcmov\",    Vvs,xx,Hvs,Lvs,Wvs,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 51 */\n    {OP_vpperm,    0x08a318,\"vpperm\",    Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,tvexw[51][1]},\n    {OP_vpperm,    0x08a358,\"vpperm\",    Vdq,xx,Hdq,Ldq,Wdq,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 52 */\n    {OP_vprotb,    0x099018,\"vprotb\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[52][1]},\n    {OP_vprotb,    0x099058,\"vprotb\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 53 */\n    {OP_vprotw,    0x099118,\"vprotw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[53][1]},\n    {OP_vprotw,    0x099158,\"vprotw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 54 */\n    {OP_vprotd,    0x099218,\"vprotd\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[54][1]},\n    {OP_vprotd,    0x099258,\"vprotd\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 55 */\n    {OP_vprotq,    0x099318,\"vprotq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[55][1]},\n    {OP_vprotq,    0x099358,\"vprotq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 56 */\n    {OP_vpshlb,    0x099418,\"vpshlb\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[56][1]},\n    {OP_vpshlb,    0x099458,\"vpshlb\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 57 */\n    {OP_vpshlw,    0x099518,\"vpshlw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[57][1]},\n    {OP_vpshlw,    0x099558,\"vpshlw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 58 */\n    {OP_vpshld,    0x099618,\"vpshld\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[58][1]},\n    {OP_vpshld,    0x099658,\"vpshld\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 59 */\n    {OP_vpshlq,    0x099718,\"vpshlq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[59][1]},\n    {OP_vpshlq,    0x099758,\"vpshlq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 60 */\n    {OP_vpshab,    0x099818,\"vpshab\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[60][1]},\n    {OP_vpshab,    0x099858,\"vpshab\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 61 */\n    {OP_vpshaw,    0x099918,\"vpshaw\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[61][1]},\n    {OP_vpshaw,    0x099958,\"vpshaw\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 62 */\n    {OP_vpshad,    0x099a18,\"vpshad\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[62][1]},\n    {OP_vpshad,    0x099a58,\"vpshad\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 63 */\n    {OP_vpshaq,    0x099b18,\"vpshaq\",    Vdq,xx,Wdq,Hdq,xx,mrm|vex,x,tvexw[63][1]},\n    {OP_vpshaq,    0x099b58,\"vpshaq\",    Vdq,xx,Hdq,Wdq,xx,mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 64 */\n    {OP_vpermil2ps,0x663a4818,\"vpermil2ps\",Vvs,xx,Hvs,Wvs,Lvs,mrm|vex|reqp,x,tvexw[64][1]},\n    {OP_vpermil2ps,0x663a4858,\"vpermil2ps\",Vvs,xx,Hvs,Lvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 65 */\n    {OP_vpermil2pd,0x663a4918,\"vpermil2pd\",Vvs,xx,Hvs,Wvs,Lvs,mrm|vex|reqp,x,tvexw[65][1]},\n    {OP_vpermil2pd,0x663a4958,\"vpermil2pd\",Vvs,xx,Hvs,Lvs,Wvs,mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 66 */\n    /* XXX: OP_v*gather* raise #UD if any pair of the index, mask, or destination\n     * registers are identical.  We don't bother trying to detect that.\n     */\n    {OP_vpgatherdd,0x66389018,\"vpgatherdd\",Vx,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpgatherdq,0x66389058,\"vpgatherdq\",Vx,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 67 */\n    {OP_vpgatherqd,0x66389118,\"vpgatherqd\",Vx,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpgatherqq,0x66389158,\"vpgatherqq\",Vx,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 68 */\n    {OP_vgatherdps,0x66389218,\"vgatherdps\",Vvs,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vgatherdpd,0x66389258,\"vgatherdpd\",Vvd,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 69 */\n    {OP_vgatherqps,0x66389318,\"vgatherqps\",Vvs,Hx,MVd,Hx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vgatherqpd,0x66389358,\"vgatherqpd\",Vvd,Hx,MVq,Hx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 70 */\n    {OP_vpmaskmovd,0x66388c18,\"vpmaskmovd\",Vx,xx,Hx,Mx,xx, mrm|vex|reqp|predcx,x,tvexw[71][0]},\n    {OP_vpmaskmovq,0x66388c58,\"vpmaskmovq\",Vx,xx,Hx,Mx,xx, mrm|vex|reqp|predcx,x,tvexw[71][1]},\n  }, { /* vex_W_ext 71 */\n    /* Conditional store => predcx */\n    {OP_vpmaskmovd,0x66388e18,\"vpmaskmovd\",Mx,xx,Vx,Hx,xx, mrm|vex|reqp|predcx,x,END_LIST},\n    {OP_vpmaskmovq,0x66388e58,\"vpmaskmovq\",Mx,xx,Vx,Hx,xx, mrm|vex|reqp|predcx,x,END_LIST},\n  }, { /* vex_W_ext 72 */\n    {OP_vpsrlvd,0x66384518,\"vpsrlvd\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpsrlvq,0x66384558,\"vpsrlvq\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 73 */\n    {OP_vpsllvd,0x66384718,\"vpsllvd\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n    {OP_vpsllvq,0x66384758,\"vpsllvq\",Vx,xx,Hx,Wx,xx, mrm|vex|reqp,x,END_LIST},\n  }, { /* vex_W_ext 74 */\n    {OP_kmovw,0x0f9010,\"kmovw\",KPw,xx,KQw,xx,xx, mrm|vex,x,tvexw[76][0]},\n    {OP_kmovq,0x0f9050,\"kmovq\",KPq,xx,KQq,xx,xx, mrm|vex,x,tvexw[76][1]},\n  }, { /* vex_W_ext 75 */\n    {OP_kmovb,0x660f9010,\"kmovb\",KPb,xx,KQb,xx,xx, mrm|vex,x,tvexw[77][0]},\n    {OP_kmovd,0x660f9050,\"kmovd\",KPd,xx,KQd,xx,xx, mrm|vex,x,tvexw[77][1]},\n  }, { /* vex_W_ext 76 */\n    {OP_kmovw,0x0f9110,\"kmovw\",KQw,xx,KPw,xx,xx, mrm|vex,x,tvexw[78][0]},\n    {OP_kmovq,0x0f9150,\"kmovq\",KQq,xx,KPq,xx,xx, mrm|vex,x,tvexw[106][1]},\n  }, { /* vex_W_ext 77 */\n    {OP_kmovb,0x660f9110,\"kmovb\",KQb,xx,KPb,xx,xx, mrm|vex,x,tvexw[79][0]},\n    {OP_kmovd,0x660f9150,\"kmovd\",KQd,xx,KPd,xx,xx, mrm|vex,x,tvexw[106][0]},\n  }, { /* vex_W_ext 78 */\n    {OP_kmovw,0x0f9210,\"kmovw\",KPw,xx,Ry,xx,xx, mrm|vex,x,tvexw[80][0]},\n    {INVALID, 0x0f9250,\"(bad)\", xx,xx,xx,xx,xx,      no,x,NA},\n  }, { /* vex_W_ext 79 */\n    {OP_kmovb,0x660f9210,\"kmovb\",KPb,xx,Ry,xx,xx, mrm|vex,x,tvexw[81][0]},\n    {INVALID, 0x660f9250,\"(bad)\", xx,xx,xx,xx,xx,      no,x,NA},\n  }, { /* vex_W_ext 80 */\n    {OP_kmovw,0x0f9310,\"kmovw\",  Gd,xx,KRw,xx,xx, mrm|vex,x,END_LIST},\n    {INVALID, 0x0f9450,\"(bad)\", xx,xx,xx,xx,xx,        no,x,NA},\n  }, { /* vex_W_ext 81 */\n    {OP_kmovb,0x660f9310,\"kmovb\",Gd,xx,KRb,xx,xx, mrm|vex,x,END_LIST},\n    {INVALID, 0x660f9350,\"(bad)\",xx,xx,xx,xx,xx,       no,x,NA},\n  }, { /* vex_W_ext 82 */\n    {OP_kandw,0x0f4110,\"kandw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kandq,0x0f4150,\"kandq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 83 */\n    {OP_kandb,0x660f4110,\"kandb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kandd,0x660f4150,\"kandd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 84 */\n    {OP_kandnw,0x0f4210,\"kandnw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kandnq,0x0f4250,\"kandnq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 85 */\n    {OP_kandnb,0x660f4210,\"kandnb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kandnd,0x660f4250,\"kandnd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 86 */\n    {OP_kunpckwd,0x0f4b10,\"kunpckwd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n    {OP_kunpckdq,0x0f4b50,\"kunpckdq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 87 */\n    {OP_kunpckbw,0x660f4b10,\"kunpckbw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {INVALID,    0x660f4b50,   \"(bad)\", xx,xx, xx,  xx,xx,     no,x,NA},\n  }, { /* vex_W_ext 88 */\n    {OP_knotw,0x0f4410,\"knotw\",KPw,xx,KRw,xx,xx, mrm|vex,x,END_LIST},\n    {OP_knotq,0x0f4450,\"knotq\",KPq,xx,KRq,xx,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 89 */\n    {OP_knotb,0x660f4410,\"knotb\",KPb,xx,KRb,xx,xx, mrm|vex,x,END_LIST},\n    {OP_knotd,0x660f4450,\"knotd\",KPd,xx,KRd,xx,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 90 */\n    {OP_korw,0x0f4510,\"korw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_korq,0x0f4550,\"korq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 91 */\n    {OP_korb,0x660f4510,\"korb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kord,0x660f4550,\"kord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 92 */\n    {OP_kxnorw,0x0f4610,\"kxnorw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kxnorq,0x0f4650,\"kxnorq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 93 */\n    {OP_kxnorb,0x660f4610,\"kxnorb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kxnord,0x660f4650,\"kxnord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 94 */\n    {OP_kxorw,0x0f4710,\"kxorw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kxorq,0x0f4750,\"kxorq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 95 */\n    {OP_kxorb,0x660f4710,\"kxorb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kxord,0x660f4750,\"kxord\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 96 */\n    {OP_kaddw,0x0f4a10,\"kaddw\",KPw,xx,KVw,KRw,xx, mrm|vex,x,END_LIST},\n    {OP_kaddq,0x0f4a50,\"kaddq\",KPq,xx,KVq,KRq,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 97 */\n    {OP_kaddb,0x660f4a10,\"kaddb\",KPb,xx,KVb,KRb,xx, mrm|vex,x,END_LIST},\n    {OP_kaddd,0x660f4a50,\"kaddd\",KPd,xx,KVd,KRd,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 98 */\n    {OP_kortestw,0x0f9810,\"kortestw\",KPw,xx,KRw,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n    {OP_kortestq,0x0f9850,\"kortestq\",KPq,xx,KRq,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n  }, { /* vex_W_ext 99 */\n    {OP_kortestb,0x660f9810,\"kortestb\",KPb,xx,KRb,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n    {OP_kortestd,0x660f9850,\"kortestd\",KPd,xx,KRd,xx,xx, mrm|vex,(fWC|fWZ),END_LIST},\n  }, { /* vex_W_ext 100 */\n    {OP_kshiftlb,0x663a3208,\"kshiftlb\",KPb,xx,KRb,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftlw,0x663a3248,\"kshiftlw\",KPw,xx,KRw,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 101 */\n    {OP_kshiftld,0x663a3308,\"kshiftld\",KPd,xx,KRd,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftlq,0x663a3348,\"kshiftlq\",KPq,xx,KRq,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 102 */\n    {OP_kshiftrb,0x663a3008,\"kshiftrb\",KPb,xx,KRb,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftrw,0x663a3048,\"kshiftrw\",KPw,xx,KRw,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 103 */\n    {OP_kshiftrd,0x663a3108,\"kshiftrd\",KPd,xx,KRd,Ib,xx, mrm|vex,x,END_LIST},\n    {OP_kshiftrq,0x663a3148,\"kshiftrq\",KPq,xx,KRq,Ib,xx, mrm|vex,x,END_LIST},\n  }, { /* vex_W_ext 104 */\n    {OP_ktestw,0x0f9910,\"ktestw\",KPw,xx,KRw,xx,xx, mrm|vex,fW6,END_LIST},\n    {OP_ktestq,0x0f9950,\"ktestq\",KPq,xx,KRq,xx,xx, mrm|vex,fW6,END_LIST},\n  }, { /* vex_W_ext 105 */\n    {OP_ktestb,0x660f9910,\"ktestb\",KPb,xx,KRb,xx,xx, mrm|vex,fW6,END_LIST},\n    {OP_ktestd,0x660f9950,\"ktestd\",KPd,xx,KRd,xx,xx, mrm|vex,fW6,END_LIST},\n  }, { /* vex_W_ext 106 */\n    {OP_kmovd,0xf20f9210,\"kmovd\",KPd,xx,Ry,xx,xx, mrm|vex,x,tvexw[107][0]},\n    {OP_kmovq,0xf20f9250,\"kmovq\",KPq,xx,Ry,xx,xx, mrm|vex,x,tvexw[107][1]},\n  }, { /* vex_W_ext 107 */\n    {OP_kmovd,0xf20f9310,\"kmovd\",  Gd,xx,KRd,xx,xx, mrm|vex,x,END_LIST},\n    {OP_kmovq,0xf20f9350,\"kmovq\",Gd_q,xx,KRq,xx,xx, mrm|vex,x,END_LIST},\n  },\n};\n\n/****************************************************************************\n * Instructions that differ depending on evex.W.\n * Index is evex.W value\n */\nconst instr_info_t evex_W_extensions[][2] = {\n  {    /* evex_W_ext 0 */\n    {OP_vmovups, 0x0f1010,\"vmovups\", Ves,xx,KEd,Wes,xx,mrm|evex,x,tevexw[1][0]},\n    {INVALID, 0x0f1050,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 1 */\n    {OP_vmovups, 0x0f1110,\"vmovups\", Wes,xx,KEd,Ves,xx,mrm|evex,x,END_LIST},\n    {INVALID, 0x0f1150,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 2 */\n    {INVALID, 0x660f1010,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovupd, 0x660f1050,\"vmovupd\", Ved,xx,KEd,Wed,xx,mrm|evex,x,tevexw[3][1]},\n  },\n  {    /* evex_W_ext 3 */\n    {INVALID, 0x660f1110,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovupd, 0x660f1150,\"vmovupd\", Wed,xx,KEd,Ved,xx,mrm|evex,x,END_LIST},\n  },\n  {    /* evex_W_ext 4 */\n    {OP_vmovaps, 0x0f2810,\"vmovaps\", Ves,xx,KEd,Wes,xx,mrm|evex,x,tevexw[5][0]},\n    {INVALID, 0x0f2850,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 5 */\n    {OP_vmovaps, 0x0f2910,\"vmovaps\", Wes,xx,KEd,Ves,xx,mrm|evex,x,END_LIST},\n    {INVALID, 0x0f2950,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n  },\n  {    /* evex_W_ext 6 */\n    {INVALID, 0x660f2810,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovapd, 0x660f2850,\"vmovapd\", Ved,xx,KEd,Wed,xx,mrm|evex,x,tevexw[7][1]},\n  },\n  {    /* evex_W_ext 7 */\n    {INVALID, 0x660f2910,\"(bad)\", xx,xx,xx,xx,xx,no,x,NA},\n    {OP_vmovapd, 0x660f2950,\"vmovapd\", Wed,xx,KEd,Ved,xx,mrm|evex,x,END_LIST},\n  },\n};\n\n/****************************************************************************\n * XOP instructions\n *\n * Since large parts of the opcode space are empty, we save space by having\n * tables of 256 indices instead of tables of 256 instr_info_t structs.\n */\n/* N.B.: all XOP 0x08 are assumed to have an immediate.  If this becomes\n * untrue we'll have to add an xop_8_extra[] table in decode_fast.c.\n */\nconst byte xop_8_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 1, 2, 3,  0, 0, 0, 0,  0, 0, 4, 5,  /* 8 */\n     0, 0, 0, 0,  0, 6, 7, 8,  0, 0, 0, 0,  0, 0, 9,10,  /* 9 */\n     0, 0,11,12,  0, 0,13, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0,14, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n    15,16,17,18,  0, 0, 0, 0,  0, 0, 0, 0, 19,20,21,22,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 23,24,25,26,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst byte xop_9_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0,58,59, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n     0, 0,61, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n    27,28,29,30,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n    31,32,33,34, 35,36,37,38, 39,40,41,42,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0,43,44,45,  0, 0,46,47,  0, 0, 0,48,  0, 0, 0, 0,  /* C */\n     0,49,50,51,  0, 0,52,53,  0, 0, 0,54,  0, 0, 0, 0,  /* D */\n     0,55,56,57,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\n/* N.B.: nothing here for initial XOP but upcoming TBM and LWP have opcodes here */\nconst byte xop_a_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 0 */\n    60, 0,62, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 8 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 9 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* A */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\n\nconst instr_info_t xop_extensions[] = {\n  {INVALID,     0x000000, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},              /* 0*/\n  /* We are out of flags, and we want to share a lot of REQUIRES_VEX, so to\n   * distinguish XOP we just rely on the XOP.map_select being disjoint from\n   * the VEX.m-mmm field.\n   */\n  /* XOP.map_select = 0x08 */\n  {OP_vpmacssww, 0x088518,\"vpmacssww\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 1*/\n  {OP_vpmacsswd, 0x088618,\"vpmacsswd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 2*/\n  {OP_vpmacssdql,0x088718,\"vpmacssdql\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 3*/\n  {OP_vpmacssdd, 0x088e18,\"vpmacssdd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 4*/\n  {OP_vpmacssdqh,0x088f18,\"vpmacssdqh\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 5*/\n  {OP_vpmacsww,  0x089518,\"vpmacsww\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 6*/\n  {OP_vpmacswd,  0x089618,\"vpmacswd\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 7*/\n  {OP_vpmacsdql, 0x089718,\"vpmacsdql\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 8*/\n  {OP_vpmacsdd,  0x089e18,\"vpmacsdd\",  Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /* 9*/\n  {OP_vpmacsdqh, 0x089f18,\"vpmacsdqh\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*10*/\n  {VEX_W_EXT,    0x08a218, \"(vex_W ext 50)\", xx,xx,xx,xx,xx, mrm|vex, x,  50},  /*11*/\n  {VEX_W_EXT,    0x08a318, \"(vex_W ext 51)\", xx,xx,xx,xx,xx, mrm|vex, x,  51},  /*12*/\n  {OP_vpmadcsswd,0x08a618,\"vpmadcsswd\",Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*13*/\n  {OP_vpmadcswd, 0x08b618,\"vpmadcswd\", Vdq,xx,Hdq,Wdq,Ldq,mrm|vex,x,END_LIST},  /*14*/\n  {OP_vprotb,    0x08c018,\"vprotb\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[52][0]},/*15*/\n  {OP_vprotw,    0x08c118,\"vprotw\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[53][0]},/*16*/\n  {OP_vprotd,    0x08c218,\"vprotd\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[54][0]},/*17*/\n  {OP_vprotq,    0x08c318,\"vprotq\",    Vdq,xx,Wdq,Ib,xx,mrm|vex,x,tvexw[55][0]},/*18*/\n  {OP_vpcomb,    0x08cc18,\"vpcomb\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*19*/\n  {OP_vpcomw,    0x08cd18,\"vpcomw\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*20*/\n  {OP_vpcomd,    0x08ce18,\"vpcomd\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*21*/\n  {OP_vpcomq,    0x08cf18,\"vpcomq\",    Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*22*/\n  {OP_vpcomub,   0x08ec18,\"vpcomub\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*23*/\n  {OP_vpcomuw,   0x08ed18,\"vpcomuw\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*24*/\n  {OP_vpcomud,   0x08ee18,\"vpcomud\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*25*/\n  {OP_vpcomuq,   0x08ef18,\"vpcomuq\",   Vdq,xx,Hdq,Wdq,Ib,mrm|vex,x,END_LIST},   /*26*/\n  /* XOP.map_select = 0x09 */\n  {OP_vfrczps,   0x098018,\"vfrczps\",   Vvs,xx,Wvs,xx,xx,mrm|vex,x,END_LIST},    /*27*/\n  {OP_vfrczpd,   0x098118,\"vfrczpd\",   Vvs,xx,Wvs,xx,xx,mrm|vex,x,END_LIST},    /*28*/\n  {OP_vfrczss,   0x098218,\"vfrczss\",   Vss,xx,Wss,xx,xx,mrm|vex,x,END_LIST},    /*29*/\n  {OP_vfrczsd,   0x098318,\"vfrczsd\",   Vsd,xx,Wsd,xx,xx,mrm|vex,x,END_LIST},    /*30*/\n  {VEX_W_EXT,    0x099018, \"(vex_W ext 52)\", xx,xx,xx,xx,xx, mrm|vex, x,  52},  /*31*/\n  {VEX_W_EXT,    0x099118, \"(vex_W ext 53)\", xx,xx,xx,xx,xx, mrm|vex, x,  53},  /*32*/\n  {VEX_W_EXT,    0x099218, \"(vex_W ext 54)\", xx,xx,xx,xx,xx, mrm|vex, x,  54},  /*33*/\n  {VEX_W_EXT,    0x099318, \"(vex_W ext 55)\", xx,xx,xx,xx,xx, mrm|vex, x,  55},  /*34*/\n  {VEX_W_EXT,    0x099418, \"(vex_W ext 56)\", xx,xx,xx,xx,xx, mrm|vex, x,  56},  /*35*/\n  {VEX_W_EXT,    0x099518, \"(vex_W ext 57)\", xx,xx,xx,xx,xx, mrm|vex, x,  57},  /*36*/\n  {VEX_W_EXT,    0x099618, \"(vex_W ext 58)\", xx,xx,xx,xx,xx, mrm|vex, x,  58},  /*37*/\n  {VEX_W_EXT,    0x099718, \"(vex_W ext 59)\", xx,xx,xx,xx,xx, mrm|vex, x,  59},  /*38*/\n  {VEX_W_EXT,    0x099818, \"(vex_W ext 60)\", xx,xx,xx,xx,xx, mrm|vex, x,  60},  /*39*/\n  {VEX_W_EXT,    0x099918, \"(vex_W ext 61)\", xx,xx,xx,xx,xx, mrm|vex, x,  61},  /*40*/\n  {VEX_W_EXT,    0x099a18, \"(vex_W ext 62)\", xx,xx,xx,xx,xx, mrm|vex, x,  62},  /*41*/\n  {VEX_W_EXT,    0x099b18, \"(vex_W ext 63)\", xx,xx,xx,xx,xx, mrm|vex, x,  63},  /*42*/\n  {OP_vphaddbw,  0x09c118,\"vphaddbw\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*43*/\n  {OP_vphaddbd,  0x09c218,\"vphaddbd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*44*/\n  {OP_vphaddbq,  0x09c318,\"vphaddbq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*45*/\n  {OP_vphaddwd,  0x09c618,\"vphaddwd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*46*/\n  {OP_vphaddwq,  0x09c718,\"vphaddwq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*47*/\n  {OP_vphadddq,  0x09cb18,\"vphadddq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*48*/\n  /* AMD decode table erroneously lists this as \"vphaddubwd\" */\n  {OP_vphaddubw, 0x09d118,\"vphaddubw\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*49*/\n  {OP_vphaddubd, 0x09d218,\"vphaddubd\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*50*/\n  {OP_vphaddubq, 0x09d318,\"vphaddubq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*51*/\n  {OP_vphadduwd, 0x09d618,\"vphadduwd\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*52*/\n  {OP_vphadduwq, 0x09d718,\"vphadduwq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*53*/\n  {OP_vphaddudq, 0x09db18,\"vphaddudq\",  Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*54*/\n  {OP_vphsubbw,  0x09e118,\"vphsubbw\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*55*/\n  {OP_vphsubwd,  0x09e218,\"vphsubwd\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*56*/\n  {OP_vphsubdq,  0x09e318,\"vphsubdq\",   Vdq,xx,Wdq,xx,xx,mrm|vex,x,END_LIST},   /*57*/\n  {EXTENSION,    0x090118, \"(XOP group 1)\", xx,xx, xx,xx,xx, mrm|vex, x, 27},   /*58*/\n  {EXTENSION,    0x090218, \"(XOP group 2)\", xx,xx, xx,xx,xx, mrm|vex, x, 28},   /*59*/\n  /* XOP.map_select = 0x0a */\n  {OP_bextr,     0x0a1018, \"bextr\",  Gy,xx,Ey,Id,xx, mrm|vex, fW6, END_LIST},   /*60*/\n  /* Later-added instrs, from various tables */\n  {EXTENSION,    0x091218, \"(XOP group 3)\", xx,xx, xx,xx,xx, mrm|vex, x, 29},   /*61*/\n  {EXTENSION,    0x0a1218, \"(XOP group 4)\", xx,xx, xx,xx,xx, mrm|vex, x, 30},   /*62*/\n};\n\n/****************************************************************************\n * String instructions that differ depending on rep/repne prefix\n *\n * Note that Intel manuals prior to May 2011 claim that for x64 the count\n * register for ins and outs is rcx by default, but for all other rep* is ecx.\n * The AMD manual, and experimental evidence, contradicts this and has rcx\n * as the default count register for all rep*.\n * Furthermore, the Intel manual implies that w/o rex.w edi/esi are used\n * rather than rdi/rsi: which again the AMD manual and experimental\n * evidence contradict.\n */\nconst instr_info_t rep_extensions[][4] = {\n    /* FIXME: ins and outs access \"I/O ports\", are these memory addresses?\n     * if so, change Ib to Ob and change dx to i_dx (move to dest for outs)\n     */\n  { /* rep extension 0 */\n    {OP_ins,      0x6c0000, \"ins\",       Yb, axDI, dx, axDI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_ins,  0xf36c0000, \"rep ins\", Yb, axDI, dx, axDI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf36c0000, \"rep ins\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 1 */\n    {OP_ins,      0x6d0000, \"ins\",       Yz, axDI, dx, axDI, xx, no, fRD, tre[0][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_ins,  0xf36d0000, \"rep ins\", Yz, axDI, dx, axDI, axCX, xop_next, fRD, tre[0][2]},\n    {OP_CONTD,  0xf36d0000, \"rep ins\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 2 */\n    {OP_outs,      0x6e0000, \"outs\",       axSI, xx, Xb, dx, axSI, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_outs,  0xf36e0000, \"rep outs\", axSI, axCX, Xb, dx, axSI, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf36e0000, \"rep outs\", xx, xx, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 3 */\n    {OP_outs,      0x6f0000, \"outs\",       axSI, xx, Xz, dx, axSI, no, fRD, tre[2][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_outs,  0xf36f0000, \"rep outs\", axSI, axCX, Xz, dx, axSI, xop_next, fRD, tre[2][2]},\n    {OP_CONTD,  0xf36f0000, \"rep outs\", xx, xx, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 4 */\n    {OP_movs,      0xa40000, \"movs\",       Yb, axSI, Xb, axSI, axDI, xop_next, fRD, END_LIST},\n    {OP_CONTD,      0xa40000, \"movs\",       axDI, xx, xx, xx, xx, no, fRD, END_LIST},\n    {OP_rep_movs,  0xf3a40000, \"rep movs\", Yb, axSI, Xb, axSI, axDI, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3a40000, \"rep movs\", axDI, axCX, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 5 */\n    {OP_movs,      0xa50000, \"movs\",       Yv, axSI, Xv, axSI, axDI, xop_next, fRD, tre[4][0]},\n    {OP_CONTD,      0xa50000, \"movs\",       axDI, xx, xx, xx, xx, no, fRD, END_LIST},\n    {OP_rep_movs,  0xf3a50000, \"rep movs\", Yv, axSI, Xv, axSI, axDI, xop_next, fRD, tre[4][2]},\n    {OP_CONTD,  0xf3a50000, \"rep movs\", axDI, axCX, axCX, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 6 */\n    {OP_stos,      0xaa0000, \"stos\",       Yb, axDI, al, axDI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_stos,  0xf3aa0000, \"rep stos\", Yb, axDI, al, axDI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3aa0000, \"rep stos\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 7 */\n    {OP_stos,      0xab0000, \"stos\",       Yv, axDI, eAX, axDI, xx, no, fRD, tre[6][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_stos,  0xf3ab0000, \"rep stos\", Yv, axDI, eAX, axDI, axCX, xop_next, fRD, tre[6][2]},\n    {OP_CONTD,  0xf3ab0000, \"rep stos\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 8 */\n    {OP_lods,      0xac0000, \"lods\",       al, axSI, Xb, axSI, xx, no, fRD, END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_lods,  0xf3ac0000, \"rep lods\", al, axSI, Xb, axSI, axCX, xop_next, fRD, END_LIST},\n    {OP_CONTD,  0xf3ac0000, \"rep lods\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n  { /* rep extension 9 */\n    {OP_lods,      0xad0000, \"lods\",       eAX, axSI, Xv, axSI, xx, no, fRD, tre[8][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_lods,  0xf3ad0000, \"rep lods\", eAX, axSI, Xv, axSI, axCX, xop_next, fRD, tre[8][2]},\n    {OP_CONTD,  0xf3ad0000, \"rep lods\", axCX, xx, xx, xx, xx, no, fRD, END_LIST},\n  },\n};\n\nconst instr_info_t repne_extensions[][6] = {\n  { /* repne extension 0 */\n    {OP_cmps,       0xa60000, \"cmps\",         axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD), END_LIST},\n    {OP_CONTD,      0xa60000, \"cmps\",         xx, xx, axDI, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_rep_cmps,   0xf3a60000, \"rep cmps\",   axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf3a60000, \"rep cmps\",   axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_cmps, 0xf2a60000, \"repne cmps\", axSI, axDI, Xb, Yb, axSI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf2a60000, \"repne cmps\", axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 1 */\n    {OP_cmps,       0xa70000, \"cmps\",         axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD), tne[0][0]},\n    {OP_CONTD,      0xa70000, \"cmps\",         xx, xx, axDI, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_rep_cmps,   0xf3a70000, \"rep cmps\",   axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD|fRZ), tne[0][2]},\n    {OP_CONTD,      0xf3a70000, \"rep cmps\",   axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_cmps, 0xf2a70000, \"repne cmps\", axSI, axDI, Xv, Yv, axSI, xop_next, (fW6|fRD|fRZ), tne[0][4]},\n    {OP_CONTD,      0xf2a70000, \"repne cmps\", axCX, xx, axDI, axCX, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 2 */\n    {OP_scas,       0xae0000, \"scas\",         axDI, xx, Yb, al, axDI, no, (fW6|fRD), END_LIST},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_scas,   0xf3ae0000, \"rep scas\",   axDI, axCX, Yb, al, axDI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf3ae0000, \"rep scas\",   xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_scas, 0xf2ae0000, \"repne scas\", axDI, axCX, Yb, al, axDI, xop_next, (fW6|fRD|fRZ), END_LIST},\n    {OP_CONTD,      0xf2ae0000, \"repne scas\", xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n  },\n  { /* repne extension 3 */\n    {OP_scas,       0xaf0000, \"scas\",         axDI, xx, Yv, eAX, axDI, no, (fW6|fRD), tne[2][0]},\n    {INVALID,   0x00000000, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n    {OP_rep_scas,   0xf3af0000, \"rep scas\",   axDI, axCX, Yv, eAX, axDI, xop_next, (fW6|fRD|fRZ), tne[2][2]},\n    {OP_CONTD,      0xf3af0000, \"rep scas\",   xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n    {OP_repne_scas, 0xf2af0000, \"repne scas\", axDI, axCX, Yv, eAX, axDI, xop_next, (fW6|fRD|fRZ), tne[2][4]},\n    {OP_CONTD,      0xf2af0000, \"repne scas\", xx, xx, axCX, xx, xx, no, (fW6|fRD), END_LIST},\n  }\n};\n\n/****************************************************************************\n * Float instructions with ModR/M from 0x00 to 0xbf\n * This is from Tables A-7, A-9, A-11, A-13, A-15, A-17, A-19, A-21\n * I've added my own symbol '+' to indicate a float, and:\n *   'x' to indicate extended real (80 bits)\n *   'y' to indicate 14/28 byte value in memory\n *   'z' to indicate 98/108 byte value in memory\n */\n/* FIXME: I ignore fp stack changes, should we model that? */\nconst instr_info_t float_low_modrm[] = {\n  /* d8 */\n  {OP_fadd,  0xd80020, \"fadd\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x20]}, /* 00 */\n  {OP_fmul,  0xd80021, \"fmul\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x21]},\n  {OP_fcom,  0xd80022, \"fcom\",  xx, xx, Fd, st0, xx, mrm, x, tfl[0x22]},\n  {OP_fcomp, 0xd80023, \"fcomp\", xx, xx, Fd, st0, xx, mrm, x, tfl[0x23]},\n  {OP_fsub,  0xd80024, \"fsub\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x24]},\n  {OP_fsubr, 0xd80025, \"fsubr\", st0, xx, Fd, st0, xx, mrm, x, tfl[0x25]},\n  {OP_fdiv,  0xd80026, \"fdiv\",  st0, xx, Fd, st0, xx, mrm, x, tfl[0x26]},\n  {OP_fdivr, 0xd80027, \"fdivr\", st0, xx, Fd, st0, xx, mrm, x, tfl[0x27]},\n  /*  d9 */\n  {OP_fld,    0xd90020, \"fld\",    st0, xx, Fd, xx, xx, mrm, x, tfl[0x1d]}, /* 08 */\n  {INVALID,   0xd90021, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fst,    0xd90022, \"fst\",    Fd, xx, st0, xx, xx, mrm, x, tfl[0x2a]},\n  {OP_fstp,   0xd90023, \"fstp\",   Fd, xx, st0, xx, xx, mrm, x, tfl[0x1f]},\n  {OP_fldenv, 0xd90024, \"fldenv\", xx, xx, Fy, xx, xx, mrm, x, END_LIST},\n  {OP_fldcw,  0xd90025, \"fldcw\",  xx, xx, Fw, xx, xx, mrm, x, END_LIST},\n  {OP_fnstenv, 0xd90026, \"fnstenv\", Fy, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, this is \"fstenv\"*/\n  {OP_fnstcw,  0xd90027, \"fnstcw\",  Fw, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, this is \"fstcw\"*/\n  /* da */\n  {OP_fiadd,  0xda0020, \"fiadd\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x30]}, /* 10 */\n  {OP_fimul,  0xda0021, \"fimul\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x31]},\n  {OP_ficom,  0xda0022, \"ficom\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x32]},\n  {OP_ficomp, 0xda0023, \"ficomp\", st0, xx, Md, st0, xx, mrm, x, tfl[0x33]},\n  {OP_fisub,  0xda0024, \"fisub\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x34]},\n  {OP_fisubr, 0xda0025, \"fisubr\", st0, xx, Md, st0, xx, mrm, x, tfl[0x35]},\n  {OP_fidiv,  0xda0026, \"fidiv\",  st0, xx, Md, st0, xx, mrm, x, tfl[0x36]},\n  {OP_fidivr, 0xda0027, \"fidivr\", st0, xx, Md, st0, xx, mrm, x, tfl[0x37]},\n  /* db */\n  {OP_fild,  0xdb0020, \"fild\",  st0, xx, Md, xx, xx, mrm, x, tfl[0x38]}, /* 18 */\n  {OP_fisttp, 0xdb0021, \"fisttp\",  Md, xx, st0, xx, xx, no, x, tfl[0x39]},\n  {OP_fist,  0xdb0022, \"fist\",  Md, xx, st0, xx, xx, mrm, x, tfl[0x3a]},\n  {OP_fistp, 0xdb0023, \"fistp\", Md, xx, st0, xx, xx, mrm, x, tfl[0x3b]},\n  {INVALID,  0xdb0024, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fld,   0xdb0025, \"fld\",   st0, xx, Fx, xx, xx, mrm, x, tfl[0x28]},\n  {INVALID,  0xdb0026, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fstp,  0xdb0027, \"fstp\",  Fx, xx, st0, xx, xx, mrm, x, tfl[0x2b]},\n  /* dc */\n  {OP_fadd,  0xdc0020, \"fadd\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x00]}, /* 20 */\n  {OP_fmul,  0xdc0021, \"fmul\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x08]},\n  {OP_fcom,  0xdc0022, \"fcom\",  xx, xx, Fq, st0, xx, mrm, x, tfh[0][0x10]},\n  {OP_fcomp, 0xdc0023, \"fcomp\", xx, xx, Fq, st0, xx, mrm, x, tfh[0][0x18]},\n  {OP_fsub,  0xdc0024, \"fsub\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x20]},\n  {OP_fsubr, 0xdc0025, \"fsubr\", st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x28]},\n  {OP_fdiv,  0xdc0026, \"fdiv\",  st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x30]},\n  {OP_fdivr, 0xdc0027, \"fdivr\", st0, xx, Fq, st0, xx, mrm, x, tfh[0][0x38]},\n  /* dd */\n  {OP_fld,   0xdd0020, \"fld\",    st0, xx, Fq, xx, xx, mrm, x, tfh[1][0x00]}, /* 28 */\n  {OP_fisttp, 0xdd0021, \"fisttp\",  Mq, xx, st0, xx, xx, no, x, tfl[0x19]},\n  {OP_fst,   0xdd0022, \"fst\",    Fq, xx, st0, xx, xx, mrm, x, tfh[5][0x10]},\n  {OP_fstp,  0xdd0023, \"fstp\",   Fq, xx, st0, xx, xx, mrm, x, tfh[5][0x18]},\n  {OP_frstor,0xdd0024, \"frstor\", xx, xx, Fz, xx, xx, mrm, x, END_LIST},\n  {INVALID,  0xdd0025, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n  {OP_fnsave, 0xdd0026, \"fnsave\",  Fz, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME:w/ preceding fwait instr, this is \"fsave\"*/\n  {OP_fnstsw, 0xdd0027, \"fnstsw\",  Fw, xx, xx, xx, xx, mrm, x, tfh[7][0x20]},/*FIXME:w/ preceding fwait instr, this is \"fstsw\"*/\n  /* de */\n  {OP_fiadd,  0xde0020, \"fiadd\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST}, /* 30 */\n  {OP_fimul,  0xde0021, \"fimul\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_ficom,  0xde0022, \"ficom\",  xx, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_ficomp, 0xde0023, \"ficomp\", xx, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fisub,  0xde0024, \"fisub\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fisubr, 0xde0025, \"fisubr\", st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fidiv,  0xde0026, \"fidiv\",  st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  {OP_fidivr, 0xde0027, \"fidivr\", st0, xx, Fw, st0, xx, mrm, x, END_LIST},\n  /* df */\n  {OP_fild,   0xdf0020, \"fild\",    st0, xx, Fw, xx, xx, mrm, x, tfl[0x3d]}, /* 38 */\n  {OP_fisttp, 0xdf0021, \"fisttp\",  Mw, xx, st0, xx, xx, no, x, END_LIST},\n  {OP_fist,   0xdf0022, \"fist\",    Fw, xx, st0, xx, xx, mrm, x, END_LIST},\n  {OP_fistp,  0xdf0023, \"fistp\",   Fw, xx, st0, xx, xx, mrm, x, tfl[0x3f]},\n  {OP_fbld,   0xdf0024, \"fbld\",    st0, xx, Fx, xx, xx, mrm, x, END_LIST},\n  {OP_fild,   0xdf0025, \"fild\",    st0, xx, Fq, xx, xx, mrm, x, END_LIST},\n  {OP_fbstp,  0xdf0026, \"fbstp\",   Fx, xx, st0, xx, xx, mrm, x, END_LIST},\n  {OP_fistp,  0xdf0027, \"fistp\",   Fq, xx, st0, xx, xx, mrm, x, END_LIST},\n};\n\n/****************************************************************************\n * Float instructions with ModR/M above 0xbf\n * This is from Tables A-8, A-10, A-12, A-14, A-16, A-18, A-20, A-22\n */\nconst instr_info_t float_high_modrm[][64] = {\n    { /* d8 = [0] */\n        {OP_fadd, 0xd8c010, \"fadd\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x01]}, /* c0 = [0x00] */\n        {OP_fadd, 0xd8c110, \"fadd\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x02]},\n        {OP_fadd, 0xd8c210, \"fadd\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x03]},\n        {OP_fadd, 0xd8c310, \"fadd\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x04]},\n        {OP_fadd, 0xd8c410, \"fadd\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x05]},\n        {OP_fadd, 0xd8c510, \"fadd\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x06]},\n        {OP_fadd, 0xd8c610, \"fadd\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x07]},\n        {OP_fadd, 0xd8c710, \"fadd\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x00]},\n        {OP_fmul, 0xd8c810, \"fmul\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x09]}, /* c8 = [0x08] */\n        {OP_fmul, 0xd8c910, \"fmul\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x0a]},\n        {OP_fmul, 0xd8ca10, \"fmul\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x0b]},\n        {OP_fmul, 0xd8cb10, \"fmul\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x0c]},\n        {OP_fmul, 0xd8cc10, \"fmul\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x0d]},\n        {OP_fmul, 0xd8cd10, \"fmul\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x0e]},\n        {OP_fmul, 0xd8ce10, \"fmul\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x0f]},\n        {OP_fmul, 0xd8cf10, \"fmul\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x08]},\n        {OP_fcom, 0xd8d010, \"fcom\", xx, xx, st0, st0, xx, mrm, x, tfh[0][0x11]}, /* d0 = [0x10] */\n        {OP_fcom, 0xd8d110, \"fcom\", xx, xx, st0, st1, xx, mrm, x, tfh[0][0x12]},\n        {OP_fcom, 0xd8d210, \"fcom\", xx, xx, st0, st2, xx, mrm, x, tfh[0][0x13]},\n        {OP_fcom, 0xd8d310, \"fcom\", xx, xx, st0, st3, xx, mrm, x, tfh[0][0x14]},\n        {OP_fcom, 0xd8d410, \"fcom\", xx, xx, st0, st4, xx, mrm, x, tfh[0][0x15]},\n        {OP_fcom, 0xd8d510, \"fcom\", xx, xx, st0, st5, xx, mrm, x, tfh[0][0x16]},\n        {OP_fcom, 0xd8d610, \"fcom\", xx, xx, st0, st6, xx, mrm, x, tfh[0][0x17]},\n        {OP_fcom, 0xd8d710, \"fcom\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xd8d810, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, tfh[0][0x19]}, /* d8 = [0x18] */\n        {OP_fcomp, 0xd8d910, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, tfh[0][0x1a]},\n        {OP_fcomp, 0xd8da10, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, tfh[0][0x1b]},\n        {OP_fcomp, 0xd8db10, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, tfh[0][0x1c]},\n        {OP_fcomp, 0xd8dc10, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, tfh[0][0x1d]},\n        {OP_fcomp, 0xd8dd10, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, tfh[0][0x1e]},\n        {OP_fcomp, 0xd8de10, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, tfh[0][0x1f]},\n        {OP_fcomp, 0xd8df10, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsub, 0xd8e010, \"fsub\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x21]}, /* e0 = [0x20] */\n        {OP_fsub, 0xd8e110, \"fsub\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x22]},\n        {OP_fsub, 0xd8e210, \"fsub\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x23]},\n        {OP_fsub, 0xd8e310, \"fsub\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x24]},\n        {OP_fsub, 0xd8e410, \"fsub\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x25]},\n        {OP_fsub, 0xd8e510, \"fsub\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x26]},\n        {OP_fsub, 0xd8e610, \"fsub\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x27]},\n        {OP_fsub, 0xd8e710, \"fsub\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x28]},\n        {OP_fsubr, 0xd8e810, \"fsubr\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x29]}, /* e8 = [0x28] */\n        {OP_fsubr, 0xd8e910, \"fsubr\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x2a]},\n        {OP_fsubr, 0xd8ea10, \"fsubr\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x2b]},\n        {OP_fsubr, 0xd8eb10, \"fsubr\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x2c]},\n        {OP_fsubr, 0xd8ec10, \"fsubr\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x2d]},\n        {OP_fsubr, 0xd8ed10, \"fsubr\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x2e]},\n        {OP_fsubr, 0xd8ee10, \"fsubr\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x2f]},\n        {OP_fsubr, 0xd8ef10, \"fsubr\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x20]},\n        {OP_fdiv, 0xd8f010, \"fdiv\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x31]}, /* f0 = [0x30] */\n        {OP_fdiv, 0xd8f110, \"fdiv\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x32]},\n        {OP_fdiv, 0xd8f210, \"fdiv\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x33]},\n        {OP_fdiv, 0xd8f310, \"fdiv\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x34]},\n        {OP_fdiv, 0xd8f410, \"fdiv\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x35]},\n        {OP_fdiv, 0xd8f510, \"fdiv\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x36]},\n        {OP_fdiv, 0xd8f610, \"fdiv\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x37]},\n        {OP_fdiv, 0xd8f710, \"fdiv\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x38]},\n        {OP_fdivr, 0xd8f810, \"fdivr\", st0, xx, st0, st0, xx, mrm, x, tfh[0][0x39]}, /* f8 = [0x38] */\n        {OP_fdivr, 0xd8f910, \"fdivr\", st0, xx, st1, st0, xx, mrm, x, tfh[0][0x3a]},\n        {OP_fdivr, 0xd8fa10, \"fdivr\", st0, xx, st2, st0, xx, mrm, x, tfh[0][0x3b]},\n        {OP_fdivr, 0xd8fb10, \"fdivr\", st0, xx, st3, st0, xx, mrm, x, tfh[0][0x3c]},\n        {OP_fdivr, 0xd8fc10, \"fdivr\", st0, xx, st4, st0, xx, mrm, x, tfh[0][0x3d]},\n        {OP_fdivr, 0xd8fd10, \"fdivr\", st0, xx, st5, st0, xx, mrm, x, tfh[0][0x3e]},\n        {OP_fdivr, 0xd8fe10, \"fdivr\", st0, xx, st6, st0, xx, mrm, x, tfh[0][0x3f]},\n        {OP_fdivr, 0xd8ff10, \"fdivr\", st0, xx, st7, st0, xx, mrm, x, tfh[4][0x30]},\n   },\n    { /* d9 = [1] */\n        {OP_fld, 0xd9c010, \"fld\", st0, xx, st0, xx, xx, mrm, x, tfh[1][0x01]}, /* c0 = [0x00] */\n        {OP_fld, 0xd9c110, \"fld\", st0, xx, st1, xx, xx, mrm, x, tfh[1][0x02]},\n        {OP_fld, 0xd9c210, \"fld\", st0, xx, st2, xx, xx, mrm, x, tfh[1][0x03]},\n        {OP_fld, 0xd9c310, \"fld\", st0, xx, st3, xx, xx, mrm, x, tfh[1][0x04]},\n        {OP_fld, 0xd9c410, \"fld\", st0, xx, st4, xx, xx, mrm, x, tfh[1][0x05]},\n        {OP_fld, 0xd9c510, \"fld\", st0, xx, st5, xx, xx, mrm, x, tfh[1][0x06]},\n        {OP_fld, 0xd9c610, \"fld\", st0, xx, st6, xx, xx, mrm, x, tfh[1][0x07]},\n        {OP_fld, 0xd9c710, \"fld\", st0, xx, st7, xx, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xd9c810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, tfh[1][0x09]}, /* c8 = [0x08] */\n        {OP_fxch, 0xd9c910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, tfh[1][0x0a]},\n        {OP_fxch, 0xd9ca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, tfh[1][0x0b]},\n        {OP_fxch, 0xd9cb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, tfh[1][0x0c]},\n        {OP_fxch, 0xd9cc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, tfh[1][0x0d]},\n        {OP_fxch, 0xd9cd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, tfh[1][0x0e]},\n        {OP_fxch, 0xd9ce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, tfh[1][0x0f]},\n        {OP_fxch, 0xd9cf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fnop, 0xd9d010, \"fnop\", xx, xx, xx, xx, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {INVALID, 0xd9d110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xd9d710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        /* Undocumented.  On sandpile.org as \"fstp1\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xd9d810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fstp, 0xd9d910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9da10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9db10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9dc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9dd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9de10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xd9df10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fchs,   0xd9e010, \"fchs\",   st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* e0 = [0x20] */\n        {OP_fabs,   0xd9e110, \"fabs\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9e210, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID,   0xd9e310, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_ftst,   0xd9e410, \"ftst\",   st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fxam,   0xd9e510, \"fxam\",   xx, xx, st0, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9e610, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID,   0xd9e710, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fld1,   0xd9e810, \"fld1\",   st0, xx, cF, xx, xx, mrm, x, END_LIST}, /* e8 = [0x28] */\n        {OP_fldl2t, 0xd9e910, \"fldl2t\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldl2e, 0xd9ea10, \"fldl2e\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldpi,  0xd9eb10, \"fldpi\",  st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldlg2, 0xd9ec10, \"fldlg2\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldln2, 0xd9ed10, \"fldln2\", st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {OP_fldz,   0xd9ee10, \"fldz\",   st0, xx, cF, xx, xx, mrm, x, END_LIST},\n        {INVALID,   0xd9ef10, \"(bad)\",  xx, xx, xx, xx, xx, no, x, NA},\n        {OP_f2xm1,  0xd9f010, \"f2xm1\",  st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* f0 = [0x30] */\n        {OP_fyl2x,  0xd9f110, \"fyl2x\",  st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fptan,  0xd9f210, \"fptan\",  st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fpatan, 0xd9f310, \"fpatan\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxtract,0xd9f410, \"fxtract\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fprem1, 0xd9f510, \"fprem1\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fdecstp,0xd9f610, \"fdecstp\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n        {OP_fincstp,0xd9f710, \"fincstp\", xx, xx, xx, xx, xx, mrm, x, END_LIST},\n        {OP_fprem,  0xd9f810, \"fprem\",  st0, st1, st0, st1, xx, mrm, x, END_LIST}, /* f8 = [0x38] */\n        {OP_fyl2xp1,0xd9f910, \"fyl2xp1\",st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fsqrt,  0xd9fa10, \"fsqrt\",  st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fsincos,0xd9fb10, \"fsincos\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_frndint,0xd9fc10, \"frndint\",st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fscale, 0xd9fd10, \"fscale\", st0, xx, st1, st0, xx, mrm, x, END_LIST},\n        {OP_fsin,   0xd9fe10, \"fsin\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fcos,   0xd9ff10, \"fcos\",   st0, xx, st0, xx, xx, mrm, x, END_LIST},\n   },\n    { /* da = [2] */\n        {OP_fcmovb, 0xdac010, \"fcmovb\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x01]}, /* c0 = [0x00] */\n        {OP_fcmovb, 0xdac110, \"fcmovb\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x02]},\n        {OP_fcmovb, 0xdac210, \"fcmovb\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x03]},\n        {OP_fcmovb, 0xdac310, \"fcmovb\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x04]},\n        {OP_fcmovb, 0xdac410, \"fcmovb\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x05]},\n        {OP_fcmovb, 0xdac510, \"fcmovb\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x06]},\n        {OP_fcmovb, 0xdac610, \"fcmovb\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x07]},\n        {OP_fcmovb, 0xdac710, \"fcmovb\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmove, 0xdac810, \"fcmove\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x09]}, /* c8 = [0x08] */\n        {OP_fcmove, 0xdac910, \"fcmove\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0a]},\n        {OP_fcmove, 0xdaca10, \"fcmove\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0b]},\n        {OP_fcmove, 0xdacb10, \"fcmove\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0c]},\n        {OP_fcmove, 0xdacc10, \"fcmove\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0d]},\n        {OP_fcmove, 0xdacd10, \"fcmove\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0e]},\n        {OP_fcmove, 0xdace10, \"fcmove\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x0f]},\n        {OP_fcmove, 0xdacf10, \"fcmove\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovbe, 0xdad010, \"fcmovbe\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x11]}, /* d0 = [0x10] */\n        {OP_fcmovbe, 0xdad110, \"fcmovbe\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x12]},\n        {OP_fcmovbe, 0xdad210, \"fcmovbe\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x13]},\n        {OP_fcmovbe, 0xdad310, \"fcmovbe\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x14]},\n        {OP_fcmovbe, 0xdad410, \"fcmovbe\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x15]},\n        {OP_fcmovbe, 0xdad510, \"fcmovbe\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x16]},\n        {OP_fcmovbe, 0xdad610, \"fcmovbe\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x17]},\n        {OP_fcmovbe, 0xdad710, \"fcmovbe\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovu, 0xdad810, \"fcmovu\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x19]}, /* d8 = [0x18] */\n        {OP_fcmovu, 0xdad910, \"fcmovu\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1a]},\n        {OP_fcmovu, 0xdada10, \"fcmovu\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1b]},\n        {OP_fcmovu, 0xdadb10, \"fcmovu\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1c]},\n        {OP_fcmovu, 0xdadc10, \"fcmovu\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1d]},\n        {OP_fcmovu, 0xdadd10, \"fcmovu\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1e]},\n        {OP_fcmovu, 0xdade10, \"fcmovu\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[2][0x1f]},\n        {OP_fcmovu, 0xdadf10, \"fcmovu\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {INVALID, 0xdae010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e0 = [0x20] */\n        {INVALID, 0xdae110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdae810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e8 = [0x28] */\n        {OP_fucompp, 0xdae910, \"fucompp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {INVALID, 0xdaea10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaeb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaec10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaed10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaee10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaef10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f0 = [0x30] */\n        {INVALID, 0xdaf110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdaf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdafe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdaff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* db = [3] */\n        {OP_fcmovnb, 0xdbc010, \"fcmovnb\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x01]}, /* c0 = [0x00] */\n        {OP_fcmovnb, 0xdbc110, \"fcmovnb\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x02]},\n        {OP_fcmovnb, 0xdbc210, \"fcmovnb\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x03]},\n        {OP_fcmovnb, 0xdbc310, \"fcmovnb\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x04]},\n        {OP_fcmovnb, 0xdbc410, \"fcmovnb\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x05]},\n        {OP_fcmovnb, 0xdbc510, \"fcmovnb\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x06]},\n        {OP_fcmovnb, 0xdbc610, \"fcmovnb\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x07]},\n        {OP_fcmovnb, 0xdbc710, \"fcmovnb\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovne, 0xdbc810, \"fcmovne\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x09]}, /* c8 = [0x08] */\n        {OP_fcmovne, 0xdbc910, \"fcmovne\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0a]},\n        {OP_fcmovne, 0xdbca10, \"fcmovne\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0b]},\n        {OP_fcmovne, 0xdbcb10, \"fcmovne\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0c]},\n        {OP_fcmovne, 0xdbcc10, \"fcmovne\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0d]},\n        {OP_fcmovne, 0xdbcd10, \"fcmovne\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0e]},\n        {OP_fcmovne, 0xdbce10, \"fcmovne\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x0f]},\n        {OP_fcmovne, 0xdbcf10, \"fcmovne\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovnbe, 0xdbd010, \"fcmovnbe\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x12]}, /* d0 = [0x10] */\n        {OP_fcmovnbe, 0xdbd110, \"fcmovnbe\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x12]},\n        {OP_fcmovnbe, 0xdbd210, \"fcmovnbe\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x13]},\n        {OP_fcmovnbe, 0xdbd310, \"fcmovnbe\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x14]},\n        {OP_fcmovnbe, 0xdbd410, \"fcmovnbe\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x15]},\n        {OP_fcmovnbe, 0xdbd510, \"fcmovnbe\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x16]},\n        {OP_fcmovnbe, 0xdbd610, \"fcmovnbe\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x17]},\n        {OP_fcmovnbe, 0xdbd710, \"fcmovnbe\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {OP_fcmovnu, 0xdbd810, \"fcmovnu\", st0, xx, st0, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x19]}, /* d8 = [0x18] */\n        {OP_fcmovnu, 0xdbd910, \"fcmovnu\", st0, xx, st1, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1a]},\n        {OP_fcmovnu, 0xdbda10, \"fcmovnu\", st0, xx, st2, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1b]},\n        {OP_fcmovnu, 0xdbdb10, \"fcmovnu\", st0, xx, st3, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1c]},\n        {OP_fcmovnu, 0xdbdc10, \"fcmovnu\", st0, xx, st4, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1d]},\n        {OP_fcmovnu, 0xdbdd10, \"fcmovnu\", st0, xx, st5, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1e]},\n        {OP_fcmovnu, 0xdbde10, \"fcmovnu\", st0, xx, st6, xx, xx, mrm|predcc, (fRC|fRP|fRZ), tfh[3][0x1f]},\n        {OP_fcmovnu, 0xdbdf10, \"fcmovnu\", st0, xx, st7, xx, xx, mrm|predcc, (fRC|fRP|fRZ), END_LIST},\n        {INVALID, 0xdbe010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* e0 = [0x20] */\n        {INVALID, 0xdbe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fnclex, 0xdbe210, \"fnclex\", xx, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, called \"fclex\"*/\n        {OP_fninit, 0xdbe310, \"fninit\", xx, xx, xx, xx, xx, mrm, x, END_LIST},/*FIXME: w/ preceding fwait instr, called \"finit\"*/\n        {INVALID, 0xdbe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fucomi, 0xdbe810, \"fucomi\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x29]}, /* e8 = [0x28] */\n        {OP_fucomi, 0xdbe910, \"fucomi\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2a]},\n        {OP_fucomi, 0xdbea10, \"fucomi\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2b]},\n        {OP_fucomi, 0xdbeb10, \"fucomi\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2c]},\n        {OP_fucomi, 0xdbec10, \"fucomi\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2d]},\n        {OP_fucomi, 0xdbed10, \"fucomi\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2e]},\n        {OP_fucomi, 0xdbee10, \"fucomi\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x2f]},\n        {OP_fucomi, 0xdbef10, \"fucomi\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {OP_fcomi, 0xdbf010, \"fcomi\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x31]}, /* f0 = [0x30] */\n        {OP_fcomi, 0xdbf110, \"fcomi\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x32]},\n        {OP_fcomi, 0xdbf210, \"fcomi\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x33]},\n        {OP_fcomi, 0xdbf310, \"fcomi\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x34]},\n        {OP_fcomi, 0xdbf410, \"fcomi\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x35]},\n        {OP_fcomi, 0xdbf510, \"fcomi\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x36]},\n        {OP_fcomi, 0xdbf610, \"fcomi\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[3][0x37]},\n        {OP_fcomi, 0xdbf710, \"fcomi\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {INVALID, 0xdbf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdbf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbfe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdbff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* dc = [4] */\n        {OP_fadd, 0xdcc010, \"fadd\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x01]}, /* c0 = [0x00] */\n        {OP_fadd, 0xdcc110, \"fadd\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x02]},\n        {OP_fadd, 0xdcc210, \"fadd\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x03]},\n        {OP_fadd, 0xdcc310, \"fadd\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x04]},\n        {OP_fadd, 0xdcc410, \"fadd\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x05]},\n        {OP_fadd, 0xdcc510, \"fadd\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x06]},\n        {OP_fadd, 0xdcc610, \"fadd\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x07]},\n        {OP_fadd, 0xdcc710, \"fadd\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fmul, 0xdcc810, \"fmul\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x09]}, /* c8 = [0x08] */\n        {OP_fmul, 0xdcc910, \"fmul\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x0a]},\n        {OP_fmul, 0xdcca10, \"fmul\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x0b]},\n        {OP_fmul, 0xdccb10, \"fmul\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x0c]},\n        {OP_fmul, 0xdccc10, \"fmul\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x0d]},\n        {OP_fmul, 0xdccd10, \"fmul\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x0e]},\n        {OP_fmul, 0xdcce10, \"fmul\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x0f]},\n        {OP_fmul, 0xdccf10, \"fmul\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcom2\".  We assume an alias for fcom\n         * and do not include in the encode chain.\n         */\n        {OP_fcom, 0xdcd010, \"fcom\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fcom, 0xdcd110, \"fcom\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd210, \"fcom\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd310, \"fcom\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd410, \"fcom\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd510, \"fcom\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd610, \"fcom\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcom, 0xdcd710, \"fcom\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcomp3\".  We assume an alias for fcomp\n         * and do not include in the encode chain.\n         */\n        {OP_fcomp, 0xdcd810, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fcomp, 0xdcd910, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcda10, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdb10, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdc10, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdd10, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcde10, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xdcdf10, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsubr, 0xdce010, \"fsubr\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x21]}, /* e0 = [0x20] */\n        {OP_fsubr, 0xdce110, \"fsubr\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x22]},\n        {OP_fsubr, 0xdce210, \"fsubr\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x23]},\n        {OP_fsubr, 0xdce310, \"fsubr\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x24]},\n        {OP_fsubr, 0xdce410, \"fsubr\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x25]},\n        {OP_fsubr, 0xdce510, \"fsubr\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x26]},\n        {OP_fsubr, 0xdce610, \"fsubr\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x27]},\n        {OP_fsubr, 0xdce710, \"fsubr\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsub, 0xdce810, \"fsub\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x29]}, /* e8 = [0x28] */\n        {OP_fsub, 0xdce910, \"fsub\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x2a]},\n        {OP_fsub, 0xdcea10, \"fsub\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x2b]},\n        {OP_fsub, 0xdceb10, \"fsub\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x2c]},\n        {OP_fsub, 0xdcec10, \"fsub\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x2d]},\n        {OP_fsub, 0xdced10, \"fsub\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x2e]},\n        {OP_fsub, 0xdcee10, \"fsub\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x2f]},\n        {OP_fsub, 0xdcef10, \"fsub\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivr, 0xdcf010, \"fdivr\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x31]}, /* f0 = [0x30] */\n        {OP_fdivr, 0xdcf110, \"fdivr\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x32]},\n        {OP_fdivr, 0xdcf210, \"fdivr\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x33]},\n        {OP_fdivr, 0xdcf310, \"fdivr\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x34]},\n        {OP_fdivr, 0xdcf410, \"fdivr\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x35]},\n        {OP_fdivr, 0xdcf510, \"fdivr\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x36]},\n        {OP_fdivr, 0xdcf610, \"fdivr\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x37]},\n        {OP_fdivr, 0xdcf710, \"fdivr\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdiv, 0xdcf810, \"fdiv\", st0, xx, st0, st0, xx, mrm, x, tfh[4][0x39]}, /* f8 = [0x38] */\n        {OP_fdiv, 0xdcf910, \"fdiv\", st1, xx, st0, st1, xx, mrm, x, tfh[4][0x3a]},\n        {OP_fdiv, 0xdcfa10, \"fdiv\", st2, xx, st0, st2, xx, mrm, x, tfh[4][0x3b]},\n        {OP_fdiv, 0xdcfb10, \"fdiv\", st3, xx, st0, st3, xx, mrm, x, tfh[4][0x3c]},\n        {OP_fdiv, 0xdcfc10, \"fdiv\", st4, xx, st0, st4, xx, mrm, x, tfh[4][0x3d]},\n        {OP_fdiv, 0xdcfd10, \"fdiv\", st5, xx, st0, st5, xx, mrm, x, tfh[4][0x3e]},\n        {OP_fdiv, 0xdcfe10, \"fdiv\", st6, xx, st0, st6, xx, mrm, x, tfh[4][0x3f]},\n        {OP_fdiv, 0xdcff10, \"fdiv\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n   },\n    { /* dd = [5] */\n        {OP_ffree, 0xddc010, \"ffree\", st0, xx, xx, xx, xx, mrm, x, tfh[5][0x01]}, /* c0 = [0x00] */\n        {OP_ffree, 0xddc110, \"ffree\", st1, xx, xx, xx, xx, mrm, x, tfh[5][0x02]},\n        {OP_ffree, 0xddc210, \"ffree\", st2, xx, xx, xx, xx, mrm, x, tfh[5][0x03]},\n        {OP_ffree, 0xddc310, \"ffree\", st3, xx, xx, xx, xx, mrm, x, tfh[5][0x04]},\n        {OP_ffree, 0xddc410, \"ffree\", st4, xx, xx, xx, xx, mrm, x, tfh[5][0x05]},\n        {OP_ffree, 0xddc510, \"ffree\", st5, xx, xx, xx, xx, mrm, x, tfh[5][0x06]},\n        {OP_ffree, 0xddc610, \"ffree\", st6, xx, xx, xx, xx, mrm, x, tfh[5][0x07]},\n        {OP_ffree, 0xddc710, \"ffree\", st7, xx, xx, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fxch4\".  We assume an alias for fxch\n         * and do not include in the encode chain.\n         */\n        {OP_fxch, 0xddc810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, END_LIST}, /* c8 = [0x08] */\n        {OP_fxch, 0xddc910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xddcf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fst, 0xddd010, \"fst\", st0, xx, st0, xx, xx, mrm, x, tfh[5][0x11]}, /* d0 = [0x10] */\n        {OP_fst, 0xddd110, \"fst\", st1, xx, st0, xx, xx, mrm, x, tfh[5][0x12]},\n        {OP_fst, 0xddd210, \"fst\", st2, xx, st0, xx, xx, mrm, x, tfh[5][0x13]},\n        {OP_fst, 0xddd310, \"fst\", st3, xx, st0, xx, xx, mrm, x, tfh[5][0x14]},\n        {OP_fst, 0xddd410, \"fst\", st4, xx, st0, xx, xx, mrm, x, tfh[5][0x15]},\n        {OP_fst, 0xddd510, \"fst\", st5, xx, st0, xx, xx, mrm, x, tfh[5][0x16]},\n        {OP_fst, 0xddd610, \"fst\", st6, xx, st0, xx, xx, mrm, x, tfh[5][0x17]},\n        {OP_fst, 0xddd710, \"fst\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xddd810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, tfh[5][0x19]}, /* d8 = [0x18] */\n        {OP_fstp, 0xddd910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, tfh[5][0x1a]},\n        {OP_fstp, 0xddda10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, tfh[5][0x1b]},\n        {OP_fstp, 0xdddb10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, tfh[5][0x1c]},\n        {OP_fstp, 0xdddc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, tfh[5][0x1d]},\n        {OP_fstp, 0xdddd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, tfh[5][0x1e]},\n        {OP_fstp, 0xddde10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, tfh[5][0x1f]},\n        {OP_fstp, 0xdddf10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fucom, 0xdde010, \"fucom\", xx, xx, st0, st0, xx, mrm, x, tfh[5][0x21]}, /* e0 = [0x20] */\n        {OP_fucom, 0xdde110, \"fucom\", xx, xx, st1, st0, xx, mrm, x, tfh[5][0x22]},\n        {OP_fucom, 0xdde210, \"fucom\", xx, xx, st2, st0, xx, mrm, x, tfh[5][0x23]},\n        {OP_fucom, 0xdde310, \"fucom\", xx, xx, st3, st0, xx, mrm, x, tfh[5][0x24]},\n        {OP_fucom, 0xdde410, \"fucom\", xx, xx, st4, st0, xx, mrm, x, tfh[5][0x25]},\n        {OP_fucom, 0xdde510, \"fucom\", xx, xx, st5, st0, xx, mrm, x, tfh[5][0x26]},\n        {OP_fucom, 0xdde610, \"fucom\", xx, xx, st6, st0, xx, mrm, x, tfh[5][0x27]},\n        {OP_fucom, 0xdde710, \"fucom\", xx, xx, st7, st0, xx, mrm, x, END_LIST},\n        {OP_fucomp, 0xdde810, \"fucomp\", xx, xx, st0, st0, xx, mrm, x, tfh[5][0x29]}, /* e8 = [0x28] */\n        {OP_fucomp, 0xdde910, \"fucomp\", xx, xx, st1, st0, xx, mrm, x, tfh[5][0x2a]},\n        {OP_fucomp, 0xddea10, \"fucomp\", xx, xx, st2, st0, xx, mrm, x, tfh[5][0x2b]},\n        {OP_fucomp, 0xddeb10, \"fucomp\", xx, xx, st3, st0, xx, mrm, x, tfh[5][0x2c]},\n        {OP_fucomp, 0xddec10, \"fucomp\", xx, xx, st4, st0, xx, mrm, x, tfh[5][0x2d]},\n        {OP_fucomp, 0xdded10, \"fucomp\", xx, xx, st5, st0, xx, mrm, x, tfh[5][0x2e]},\n        {OP_fucomp, 0xddee10, \"fucomp\", xx, xx, st6, st0, xx, mrm, x, tfh[5][0x2f]},\n        {OP_fucomp, 0xddef10, \"fucomp\", xx, xx, st7, st0, xx, mrm, x, END_LIST},\n        {INVALID, 0xddf010, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f0 = [0x30] */\n        {INVALID, 0xddf110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddf810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xddf910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddfe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xddff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n    { /* de = [6]*/\n        {OP_faddp, 0xdec010, \"faddp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x01]}, /* c0 = [0x00] */\n        {OP_faddp, 0xdec110, \"faddp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x02]},\n        {OP_faddp, 0xdec210, \"faddp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x03]},\n        {OP_faddp, 0xdec310, \"faddp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x04]},\n        {OP_faddp, 0xdec410, \"faddp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x05]},\n        {OP_faddp, 0xdec510, \"faddp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x06]},\n        {OP_faddp, 0xdec610, \"faddp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x07]},\n        {OP_faddp, 0xdec710, \"faddp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fmulp, 0xdec810, \"fmulp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x09]}, /* c8 = [0x08] */\n        {OP_fmulp, 0xdec910, \"fmulp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x0a]},\n        {OP_fmulp, 0xdeca10, \"fmulp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x0b]},\n        {OP_fmulp, 0xdecb10, \"fmulp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x0c]},\n        {OP_fmulp, 0xdecc10, \"fmulp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x0d]},\n        {OP_fmulp, 0xdecd10, \"fmulp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x0e]},\n        {OP_fmulp, 0xdece10, \"fmulp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x0f]},\n        {OP_fmulp, 0xdecf10, \"fmulp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fcomp5\".  We assume an alias for fcomp\n         * and do not include in the encode chain.\n         */\n        {OP_fcomp, 0xded010, \"fcomp\", xx, xx, st0, st0, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fcomp, 0xded110, \"fcomp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded210, \"fcomp\", xx, xx, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded310, \"fcomp\", xx, xx, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded410, \"fcomp\", xx, xx, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded510, \"fcomp\", xx, xx, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded610, \"fcomp\", xx, xx, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fcomp, 0xded710, \"fcomp\", xx, xx, st0, st7, xx, mrm, x, END_LIST},\n        {INVALID, 0xded810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* d8 = [0x18] */\n        {OP_fcompp, 0xded910, \"fcompp\", xx, xx, st0, st1, xx, mrm, x, END_LIST},\n        {INVALID, 0xdeda10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdede10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdedf10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fsubrp, 0xdee010, \"fsubrp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x21]}, /* e0 = [0x20] */\n        {OP_fsubrp, 0xdee110, \"fsubrp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x22]},\n        {OP_fsubrp, 0xdee210, \"fsubrp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x23]},\n        {OP_fsubrp, 0xdee310, \"fsubrp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x24]},\n        {OP_fsubrp, 0xdee410, \"fsubrp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x25]},\n        {OP_fsubrp, 0xdee510, \"fsubrp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x26]},\n        {OP_fsubrp, 0xdee610, \"fsubrp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x27]},\n        {OP_fsubrp, 0xdee710, \"fsubrp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fsubp, 0xdee810, \"fsubp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x29]}, /* e8 = [0x28] */\n        {OP_fsubp, 0xdee910, \"fsubp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x2a]},\n        {OP_fsubp, 0xdeea10, \"fsubp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x2b]},\n        {OP_fsubp, 0xdeeb10, \"fsubp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x2c]},\n        {OP_fsubp, 0xdeec10, \"fsubp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x2d]},\n        {OP_fsubp, 0xdeed10, \"fsubp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x2e]},\n        {OP_fsubp, 0xdeee10, \"fsubp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x2f]},\n        {OP_fsubp, 0xdeef10, \"fsubp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivrp, 0xdef010, \"fdivrp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x31]}, /* f0 = [0x30] */\n        {OP_fdivrp, 0xdef110, \"fdivrp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x32]},\n        {OP_fdivrp, 0xdef210, \"fdivrp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x33]},\n        {OP_fdivrp, 0xdef310, \"fdivrp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x34]},\n        {OP_fdivrp, 0xdef410, \"fdivrp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x35]},\n        {OP_fdivrp, 0xdef510, \"fdivrp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x36]},\n        {OP_fdivrp, 0xdef610, \"fdivrp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x37]},\n        {OP_fdivrp, 0xdef710, \"fdivrp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n        {OP_fdivp, 0xdef810, \"fdivp\", st0, xx, st0, st0, xx, mrm, x, tfh[6][0x39]}, /* f8 = [0x38] */\n        {OP_fdivp, 0xdef910, \"fdivp\", st1, xx, st0, st1, xx, mrm, x, tfh[6][0x3a]},\n        {OP_fdivp, 0xdefa10, \"fdivp\", st2, xx, st0, st2, xx, mrm, x, tfh[6][0x3b]},\n        {OP_fdivp, 0xdefb10, \"fdivp\", st3, xx, st0, st3, xx, mrm, x, tfh[6][0x3c]},\n        {OP_fdivp, 0xdefc10, \"fdivp\", st4, xx, st0, st4, xx, mrm, x, tfh[6][0x3d]},\n        {OP_fdivp, 0xdefd10, \"fdivp\", st5, xx, st0, st5, xx, mrm, x, tfh[6][0x3e]},\n        {OP_fdivp, 0xdefe10, \"fdivp\", st6, xx, st0, st6, xx, mrm, x, tfh[6][0x3f]},\n        {OP_fdivp, 0xdeff10, \"fdivp\", st7, xx, st0, st7, xx, mrm, x, END_LIST},\n   },\n    { /* df = [7] */\n        /* Undocumented by Intel, but is on p152 of \"AMD Athlon\n         * Processor x86 Code Optimization Guide.\"\n         */\n        {OP_ffreep, 0xdfc010, \"ffreep\", st0, xx, xx, xx, xx, mrm, x, tfh[7][0x01]}, /* c0 = [0x00] */\n        {OP_ffreep, 0xdfc110, \"ffreep\", st1, xx, xx, xx, xx, mrm, x, tfh[7][0x02]},\n        {OP_ffreep, 0xdfc210, \"ffreep\", st2, xx, xx, xx, xx, mrm, x, tfh[7][0x03]},\n        {OP_ffreep, 0xdfc310, \"ffreep\", st3, xx, xx, xx, xx, mrm, x, tfh[7][0x04]},\n        {OP_ffreep, 0xdfc410, \"ffreep\", st4, xx, xx, xx, xx, mrm, x, tfh[7][0x05]},\n        {OP_ffreep, 0xdfc510, \"ffreep\", st5, xx, xx, xx, xx, mrm, x, tfh[7][0x06]},\n        {OP_ffreep, 0xdfc610, \"ffreep\", st6, xx, xx, xx, xx, mrm, x, tfh[7][0x07]},\n        {OP_ffreep, 0xdfc710, \"ffreep\", st7, xx, xx, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fxch7\".  We assume an alias for fxch\n         * and do not include in the encode chain.\n         */\n        {OP_fxch, 0xdfc810, \"fxch\", st0, st0, st0, st0, xx, mrm, x, END_LIST}, /* c8 = [0x08] */\n        {OP_fxch, 0xdfc910, \"fxch\", st0, st1, st0, st1, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfca10, \"fxch\", st0, st2, st0, st2, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcb10, \"fxch\", st0, st3, st0, st3, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcc10, \"fxch\", st0, st4, st0, st4, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcd10, \"fxch\", st0, st5, st0, st5, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfce10, \"fxch\", st0, st6, st0, st6, xx, mrm, x, END_LIST},\n        {OP_fxch, 0xdfcf10, \"fxch\", st0, st7, st0, st7, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fstp8\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xdfd010, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d0 = [0x10] */\n        {OP_fstp, 0xdfd110, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd210, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd310, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd410, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd510, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd610, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfd710, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        /* Undocumented.  On sandpile.org as \"fstp9\".  We assume an alias for fstp\n         * and do not include in the encode chain.\n         */\n        {OP_fstp, 0xdfd810, \"fstp\", st0, xx, st0, xx, xx, mrm, x, END_LIST}, /* d8 = [0x18] */\n        {OP_fstp, 0xdfd910, \"fstp\", st1, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfda10, \"fstp\", st2, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdb10, \"fstp\", st3, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdc10, \"fstp\", st4, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdd10, \"fstp\", st5, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfde10, \"fstp\", st6, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fstp, 0xdfdf10, \"fstp\", st7, xx, st0, xx, xx, mrm, x, END_LIST},\n        {OP_fnstsw, 0xdfe010, \"fnstsw\", ax, xx, xx, xx, xx, mrm, x, END_LIST}, /* e0 = [0x20] */ /*FIXME:w/ preceding fwait instr, this is \"fstsw\"*/\n        {INVALID, 0xdfe110, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe210, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe310, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe410, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe510, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe610, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfe710, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {OP_fucomip, 0xdfe810, \"fucomip\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x29]}, /* e8 = [0x28] */\n        {OP_fucomip, 0xdfe910, \"fucomip\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2a]},\n        {OP_fucomip, 0xdfea10, \"fucomip\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2b]},\n        {OP_fucomip, 0xdfeb10, \"fucomip\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2c]},\n        {OP_fucomip, 0xdfec10, \"fucomip\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2d]},\n        {OP_fucomip, 0xdfed10, \"fucomip\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2e]},\n        {OP_fucomip, 0xdfee10, \"fucomip\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x2f]},\n        {OP_fucomip, 0xdfef10, \"fucomip\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {OP_fcomip, 0xdff010, \"fcomip\", xx, xx, st0, st0, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x31]}, /* f0 = [0x30] */\n        {OP_fcomip, 0xdff110, \"fcomip\", xx, xx, st0, st1, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x32]},\n        {OP_fcomip, 0xdff210, \"fcomip\", xx, xx, st0, st2, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x33]},\n        {OP_fcomip, 0xdff310, \"fcomip\", xx, xx, st0, st3, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x34]},\n        {OP_fcomip, 0xdff410, \"fcomip\", xx, xx, st0, st4, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x35]},\n        {OP_fcomip, 0xdff510, \"fcomip\", xx, xx, st0, st5, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x36]},\n        {OP_fcomip, 0xdff610, \"fcomip\", xx, xx, st0, st6, xx, mrm, (fWC|fWP|fWZ), tfh[7][0x37]},\n        {OP_fcomip, 0xdff710, \"fcomip\", xx, xx, st0, st7, xx, mrm, (fWC|fWP|fWZ), END_LIST},\n        {INVALID, 0xdff810, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA}, /* f8 = [0x38] */\n        {INVALID, 0xdff910, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffa10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffb10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffc10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffd10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdffe10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n        {INVALID, 0xdfff10, \"(bad)\", xx, xx, xx, xx, xx, no, x, NA},\n   },\n};\n\n/****************************************************************************\n * Suffix extensions: 3DNow! and 3DNow!+\n * Since there are only 24 of them, we save space by having a\n * table of 256 indices instead of 256 instr_info_t structs.\n */\nconst byte suffix_index[256] = {\n  /* 0  1  2  3   4  5  6  7   8  9  A  B   C  D  E  F */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 20,18, 0, 0,  /* 0 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0, 21,19, 0, 0,  /* 1 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 2 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 3 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 4 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 5 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 6 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* 7 */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0,22, 0,  0, 0,23, 0,  /* 8 */\n     4, 0, 0, 0,  7, 0,10,13,  0, 0,16, 0,  0, 0, 2, 0,  /* 9 */\n     5, 0, 0, 0,  8, 0,11,14,  0, 0,17, 0,  0, 0, 3, 0,  /* A */\n     6, 0, 0, 0,  9, 0,12,15,  0, 0, 0,24,  0, 0, 0, 1,  /* B */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* C */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* D */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  /* E */\n     0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0,  0, 0, 0, 0   /* F */\n};\nconst instr_info_t suffix_extensions[] = {\n    /* Rather than forging an exception let's anticipate future additions: we know\n     * (pretty sure anyway) that they'll have the same length and operand structure.\n     * Won't encode properly from Level 4 but that's ok.\n     */\n    {OP_unknown_3dnow, 0x000f0f90, \"unknown 3DNow\",\n                                          Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 0*/\n    {OP_pavgusb , 0xbf0f0f90, \"pavgusb\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 1*/\n    {OP_pfadd   , 0x9e0f0f90, \"pfadd\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 2*/\n    {OP_pfacc   , 0xae0f0f90, \"pfacc\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 3*/\n    {OP_pfcmpge , 0x900f0f90, \"pfcmpge\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 4*/\n    {OP_pfcmpgt , 0xa00f0f90, \"pfcmpgt\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 5*/\n    {OP_pfcmpeq , 0xb00f0f90, \"pfcmpeq\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 6*/\n    {OP_pfmin   , 0x940f0f90, \"pfmin\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 7*/\n    {OP_pfmax   , 0xa40f0f90, \"pfmax\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 8*/\n    {OP_pfmul   , 0xb40f0f90, \"pfmul\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/* 9*/\n    {OP_pfrcp   , 0x960f0f90, \"pfrcp\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*10*/\n    {OP_pfrcpit1, 0xa60f0f90, \"pfrcpit1\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*11*/\n    {OP_pfrcpit2, 0xb60f0f90, \"pfrcpit2\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*12*/\n    {OP_pfrsqrt , 0x970f0f90, \"pfrsqrt\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*13*/\n    {OP_pfrsqit1, 0xa70f0f90, \"pfrsqit1\", Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*14*/\n    {OP_pmulhrw , 0xb70f0f90, \"pmulhrw\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*15*/\n    {OP_pfsub   , 0x9a0f0f90, \"pfsub\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*16*/\n    {OP_pfsubr  , 0xaa0f0f90, \"pfsubr\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*17*/\n    {OP_pi2fd   , 0x0d0f0f90, \"pi2fd\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*18*/\n    {OP_pf2id   , 0x1d0f0f90, \"pf2id\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*19*/\n    {OP_pi2fw   , 0x0c0f0f90, \"pi2fw\"  ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*20*/\n    {OP_pf2iw   , 0x1c0f0f90, \"pf2iw\",    Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*21*/\n    {OP_pfnacc  , 0x8a0f0f90, \"pfnacc\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*22*/\n    {OP_pfpnacc , 0x8e0f0f90, \"pfpnacc\",  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*23*/\n    {OP_pswapd  , 0xbb0f0f90, \"pswapd\" ,  Pq, xx, Qq, Pq, xx, mrm, x, END_LIST},/*24*/\n};\n\n/****************************************************************************\n * To handle more than 2 dests or 3 sources we chain on extra instructions.\n * All cases where we have extra operands are single-encoding-only instructions,\n * so we use the list field to point to here.\n * N.B.: the size of this table is hardcoded in decode.c.\n * Also, only implicit operands are in these instruction extensions!!!\n */\nconst instr_info_t extra_operands[] =\n{\n    /* 0x00 */\n    {OP_CONTD, 0x000000, \"<pusha cont'd>\", xx, xx, eCX, eDX, eBP, xop, x, exop[0x01]},\n    {OP_CONTD, 0x000000, \"<pusha cont'd>\", xx, xx, eSI, eDI, xx, no, x, END_LIST},\n    /* 0x02 */\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eBX, eCX, xx, xx, xx, xop, x, exop[0x03]},\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eDX, eBP, xx, xx, xx, xop, x, exop[0x04]},\n    {OP_CONTD, 0x000000, \"<popa cont'd>\", eSI, eDI, xx, xx, xx, no, x, END_LIST},\n    /* 0x05 */\n    {OP_CONTD, 0x000000, \"<enter cont'd>\", xbp, xx, xbp, xx, xx, no, x, END_LIST},\n    /* 0x06 */\n    {OP_CONTD, 0x000000, \"<cpuid cont'd>\", ecx, edx, xx, xx, xx, no, x, END_LIST},\n    /* 0x07 */\n    {OP_CONTD, 0x000000, \"<cmpxchg8b cont'd>\", eDX, xx, eCX, eBX, xx, mrm, fWZ, END_LIST},\n    {OP_CONTD,0x663a6018, \"<pcmpestrm cont'd\", xx, xx, eax, edx, xx, mrm|reqp, fW6, END_LIST},\n    {OP_CONTD,0x663a6018, \"<pcmpestri cont'd\", xx, xx, eax, edx, xx, mrm|reqp, fW6, END_LIST},\n    /* 10 */\n    {OP_CONTD,0xf90f0177, \"<rdtscp cont'd>\", ecx, xx, xx, xx, xx, mrm, x, END_LIST},\n    {OP_CONTD,0x663a6018, \"<vpcmpestrm cont'd\", xx, xx, eax, edx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {OP_CONTD,0x663a6018, \"<vpcmpestri cont'd\", xx, xx, eax, edx, xx, mrm|vex|reqp, fW6, END_LIST},\n    {OP_CONTD,0x0f3710, \"<getsec cont'd\", ecx, xx, xx, xx, xx, predcx, x, END_LIST},\n};\n\n/* clang-format on */\n", "idx": 1, "id": 16805, "msg": "Per style guide: punctuate as well (end in period, or maybe colon here) and ideally make it a complete sentence. In general, either capitalized + punctuated + complete sentences, or short snippet where we don't care, with preference in new code for the former. So it seems either leave it alone or \"Opcodes added in Intel Skylake:\" or sthg.", "proj": "DynamoRIO-dynamorio", "lang": "c"}
{"patch": "@@ -43,11 +43,12 @@ module RSpec\n \n   extend RSpec::Core::Warnings\n \n-  # Used to ensure examples get reloaded between multiple runs in\n-  # the same process.\n+  # Used to ensure examples get reloaded and user configuration gets reset to\n+  # defaults between multiple runs in the same process.\n   #\n   # Users must invoke this if they want to have the configuration reset when\n-  # they use runner multiple times within the same process.\n+  # they use runner multiple times within the same process. Users must deal\n+  # themselves with re-configuration of RSpec before run.\n   def self.reset\n     @world = nil\n     @configuration = nil", "y": 1, "oldf": "# rubocop:disable Style/GlobalVars\n$_rspec_core_load_started_at = Time.now\n# rubocop:enable Style/GlobalVars\n\nrequire 'rbconfig'\n\nrequire \"rspec/support\"\nRSpec::Support.require_rspec_support \"caller_filter\"\n\nRSpec::Support.define_optimized_require_for_rspec(:core) { |f| require_relative f }\n\n%w[\n  version\n  warnings\n\n  flat_map\n  filter_manager\n  dsl\n  notifications\n  reporter\n\n  hooks\n  memoized_helpers\n  metadata\n  metadata_filter\n  pending\n  formatters\n  ordering\n\n  world\n  configuration\n  option_parser\n  configuration_options\n  runner\n  example\n  shared_example_group\n  example_group\n].each { |name| RSpec::Support.require_rspec_core name }\n\n# Namespace for all core RSpec code.\nmodule RSpec\n  autoload :SharedContext, 'rspec/core/shared_context'\n\n  extend RSpec::Core::Warnings\n\n  # Used to ensure examples get reloaded between multiple runs in\n  # the same process.\n  #\n  # Users must invoke this if they want to have the configuration reset when\n  # they use runner multiple times within the same process.\n  def self.reset\n    @world = nil\n    @configuration = nil\n  end\n\n  # Used to ensure examples get reloaded between multiple runs in the same\n  # process and ensures user configuration is persisted.\n  #\n  # Users must invoke this if they want to clear all examples but preserve\n  # current configuration when they use runner multiple times within the same\n  # process.\n  def self.clear_examples\n    world.reset\n    configuration.reporter.reset\n    configuration.start_time = ::RSpec::Core::Time.now\n    configuration.reset_filters\n  end\n\n  # Returns the global [Configuration](RSpec/Core/Configuration) object. While you\n  # _can_ use this method to access the configuration, the more common\n  # convention is to use [RSpec.configure](RSpec#configure-class_method).\n  #\n  # @example\n  #     RSpec.configuration.drb_port = 1234\n  # @see RSpec.configure\n  # @see Core::Configuration\n  def self.configuration\n    @configuration ||= begin\n                         config = RSpec::Core::Configuration.new\n                         config.expose_dsl_globally = true\n                         config\n                       end\n  end\n\n  # Yields the global configuration to a block.\n  # @yield [Configuration] global configuration\n  #\n  # @example\n  #     RSpec.configure do |config|\n  #       config.add_formatter 'documentation'\n  #     end\n  # @see Core::Configuration\n  def self.configure\n    yield configuration if block_given?\n  end\n\n  # The example being executed.\n  #\n  # The primary audience for this method is library authors who need access\n  # to the example currently being executed and also want to support all\n  # versions of RSpec 2 and 3.\n  #\n  # @example\n  #\n  #     RSpec.configure do |c|\n  #       # context.example is deprecated, but RSpec.current_example is not\n  #       # available until RSpec 3.0.\n  #       fetch_current_example = RSpec.respond_to?(:current_example) ?\n  #         proc { RSpec.current_example } : proc { |context| context.example }\n  #\n  #       c.before(:example) do\n  #         example = fetch_current_example.call(self)\n  #\n  #         # ...\n  #       end\n  #     end\n  #\n  def self.current_example\n    thread_local_metadata[:current_example]\n  end\n\n  # Set the current example being executed.\n  # @api private\n  def self.current_example=(example)\n    thread_local_metadata[:current_example] = example\n  end\n\n  # @private\n  # A single thread local variable so we don't excessively pollute that\n  # namespace.\n  def self.thread_local_metadata\n    Thread.current[:_rspec] ||= {}\n  end\n\n  # @private\n  # Internal container for global non-configuration data\n  def self.world\n    @world ||= RSpec::Core::World.new\n  end\n\n  # Namespace for the rspec-core code.\n  module Core\n    # @private\n    # This avoids issues with reporting time caused by examples that\n    # change the value/meaning of Time.now without properly restoring\n    # it.\n    class Time\n      class << self\n        define_method(:now, &::Time.method(:now))\n      end\n    end\n\n    # @private path to executable file\n    def self.path_to_executable\n      @path_to_executable ||= File.expand_path('../../../exe/rspec', __FILE__)\n    end\n  end\n\n  # @private\n  MODULES_TO_AUTOLOAD = {\n    :Matchers     => \"rspec/expectations\",\n    :Expectations => \"rspec/expectations\",\n    :Mocks        => \"rspec/mocks\"\n  }\n\n  # @private\n  def self.const_missing(name)\n    # Load rspec-expectations when RSpec::Matchers is referenced. This allows\n    # people to define custom matchers (using `RSpec::Matchers.define`) before\n    # rspec-core has loaded rspec-expectations (since it delays the loading of\n    # it to allow users to configure a different assertion/expectation\n    # framework). `autoload` can't be used since it works with ruby's built-in\n    # require (e.g. for files that are available relative to a load path dir),\n    # but not with rubygems' extended require.\n    #\n    # As of rspec 2.14.1, we no longer require `rspec/mocks` and\n    # `rspec/expectations` when `rspec` is required, so we want\n    # to make them available as an autoload.\n    require MODULES_TO_AUTOLOAD.fetch(name) { return super }\n    ::RSpec.const_get(name)\n  end\nend\n", "idx": 1, "id": 14084, "msg": "This should read \"they use the runner\" rather than \"they use runner\" (this was an existing typo, though...).", "proj": "rspec-rspec-core", "lang": "rb"}
{"patch": "@@ -0,0 +1,36 @@\n+import torch.optim as optim\n+\n+\n+def caffe2_initialize(model, cfg):\n+    \"\"\"Initialize lr and weight_decay of the model as caffe2 style.\n+\n+    The lr of bias is 2 times base lr, the weight decay of bias is 0.\n+\n+    Args:\n+        model (obj): network.\n+        cfg (dict): optimizer cfg.\n+\n+    Returns:\n+        optimizer: a optimizer contains param groups based on above setting.\n+    \"\"\"\n+    if hasattr(model, 'module'):\n+        model = model.module\n+\n+    optimizer_cfg = cfg.copy()\n+    optimizer_type = optimizer_cfg.pop('type')\n+\n+    params = []\n+    base_lr = optimizer_cfg.lr\n+    base_weight_decay = optimizer_cfg.weight_decay\n+    for k, v in model.named_parameters():\n+        if not v.requires_grad:\n+            continue\n+        if 'bias' in k:\n+            lr = base_lr * 2\n+            weight_decay = 0\n+        else:\n+            lr = base_lr\n+            weight_decay = base_weight_decay\n+        params.append({\"params\": [v], \"lr\": lr, \"weight_decay\": weight_decay})\n+    return getattr(optim, optimizer_type)(\n+        params, base_lr, momentum=optimizer_cfg.momentum)", "y": 1, "oldf": "", "idx": 1, "id": 17288, "msg": "Using different lr and decay for weight and bias is not specific to caffe2, which has been adopted long ago. The naming is misleading.", "proj": "open-mmlab-mmdetection", "lang": "py"}
{"patch": "@@ -40,6 +40,35 @@ module.exports = function(realmConstructor) {\n     setConstructorOnPrototype(realmConstructor.Results);\n     setConstructorOnPrototype(realmConstructor.Object);\n \n+    //Add async open API\n+    Object.defineProperties(realmConstructor, getOwnPropertyDescriptors({\n+        open(config) {\n+            return new Promise((resolve, reject) => {\n+                realmConstructor._waitForDownload(config, (error) => {\n+                    if (error) {\n+                        reject(error);\n+                    }\n+\n+                    let syncedRealm = new realmConstructor(config);\n+                    //FIXME: RN hangs here. Remove when node's makeCallback alternative is implemented\n+                    setTimeout(() => { resolve(syncedRealm); }, 1);                     \n+                });\n+            });\n+        },\n+\n+        openAsync(config, callback) {\n+                realmConstructor._waitForDownload(config, (error) => {\n+                    if (error) {\n+                        callback(error);\n+                    }\n+\n+                    let syncedRealm = new realmConstructor(config);\n+                    //FIXME: RN hangs here. Remove when node's makeCallback alternative is implemented\n+                    setTimeout(() => { callback(null, syncedRealm); }, 1);\n+                });\n+        },\n+    }));\n+\n     // Add sync methods\n     if (realmConstructor.Sync) {\n         let userMethods = require('./user-methods');", "y": 1, "oldf": "////////////////////////////////////////////////////////////////////////////\n//\n// Copyright 2016 Realm Inc.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n////////////////////////////////////////////////////////////////////////////\n\n'use strict';\n\nlet getOwnPropertyDescriptors = Object.getOwnPropertyDescriptors || function(obj) {\n    return Object.getOwnPropertyNames(obj).reduce(function (descriptors, name) {\n        descriptors[name] = Object.getOwnPropertyDescriptor(obj, name);\n        return descriptors;\n    }, {});\n};\n\nfunction setConstructorOnPrototype(klass) {\n    if (klass.prototype.constructor !== klass) {\n        Object.defineProperty(klass.prototype, 'constructor', { value: klass, configurable: true, writable: true });\n    }\n}\n\nmodule.exports = function(realmConstructor) {\n    // Add the specified Array methods to the Collection prototype.\n    Object.defineProperties(realmConstructor.Collection.prototype, require('./collection-methods'));\n\n    setConstructorOnPrototype(realmConstructor.Collection);\n    setConstructorOnPrototype(realmConstructor.List);\n    setConstructorOnPrototype(realmConstructor.Results);\n    setConstructorOnPrototype(realmConstructor.Object);\n\n    // Add sync methods\n    if (realmConstructor.Sync) {\n        let userMethods = require('./user-methods');\n        Object.defineProperties(realmConstructor.Sync.User, getOwnPropertyDescriptors(userMethods.static));\n        Object.defineProperties(realmConstructor.Sync.User.prototype, getOwnPropertyDescriptors(userMethods.instance));\n        Object.defineProperty(realmConstructor.Sync.User, '_realmConstructor', { value: realmConstructor });\n\n        realmConstructor.Sync.AuthError = require('./errors').AuthError;\n\n        if (realmConstructor.Sync.cleanup) {\n            // FIXME: DOES THIS WORK ON BOTH NODE AND REACT NATIVE?\n            process.on('exit', realmConstructor.Sync.cleanup);\n            process.on('SIGINT', function () {\n                realmConstructor.Sync.cleanup();\n                process.exit(2);\n            });\n            process.on('uncaughtException', function(e) {\n                realmConstructor.Sync.cleanup();\n                /* eslint-disable no-console */\n                console.log(e.stack);\n                process.exit(99);\n            });\n        }\n\n        setConstructorOnPrototype(realmConstructor.Sync.User);\n        setConstructorOnPrototype(realmConstructor.Sync.Session);\n    } else {\n        Object.defineProperty(realmConstructor, 'Sync', {\n            get: function () {\n                throw new Error(\"Realm.Sync is not available. Note that the developer edition of the Node.JS SDK for Realm does not include sync on Linux.\");\n            }\n        })\n    }\n\n    // TODO: Remove this now useless object.\n    var types = Object.freeze({\n        'BOOL': 'bool',\n        'INT': 'int',\n        'FLOAT': 'float',\n        'DOUBLE': 'double',\n        'STRING': 'string',\n        'DATE': 'date',\n        'DATA': 'data',\n        'OBJECT': 'object',\n        'LIST': 'list',\n    });\n    Object.defineProperty(realmConstructor, 'Types', {\n        get: function() {\n            if (typeof console != 'undefined') {\n                /* global console */\n                /* eslint-disable no-console */\n                var stack = new Error().stack.split(\"\\n\").slice(2).join(\"\\n\");\n                var msg = '`Realm.Types` is deprecated! Please specify the type name as lowercase string instead!\\n'+stack;\n                if (console.warn != undefined) {\n                    console.warn(msg);\n                }\n                else {\n                    console.log(msg);\n                }\n                /* eslint-enable no-console */\n            }\n            return types;\n        },\n        configurable: true\n    });\n}\n", "idx": 1, "id": 16024, "msg": "we should really just get the makeCallback pr implemented/merged - think the work is already done?", "proj": "realm-realm-js", "lang": "js"}
{"patch": "@@ -89,7 +89,7 @@ func (tun *nativeTun) Close() error {\n }\n \n func (tun *nativeTun) Flush() error {\n-\treturn tun.Flush()\n+\treturn nil\n }\n \n func (tun *nativeTun) MTU() (int, error) {", "y": 1, "oldf": "// +build windows\n\n/*\n * Copyright (C) 2019 The \"MysteriumNetwork/node\" Authors.\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\npackage userspace\n\nimport (\n\t\"net\"\n\t\"os\"\n\n\t\"github.com/pkg/errors\"\n\t\"github.com/songgao/water\"\n\t\"golang.zx2c4.com/wireguard/device\"\n\t\"golang.zx2c4.com/wireguard/tun\"\n)\n\ntype nativeTun struct {\n\ttun    *water.Interface\n\tevents chan tun.Event\n}\n\n// CreateTUN creates native TUN device for wireguard.\nfunc CreateTUN(name string, subnet net.IPNet) (tun.Device, error) {\n\ttunDevice, err := water.New(water.Config{\n\t\tDeviceType: water.TUN,\n\t\tPlatformSpecificParams: water.PlatformSpecificParams{\n\t\t\tComponentID: \"tap0901\",\n\t\t\tNetwork:     subnet.String(),\n\t\t},\n\t})\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to create new TUN device\")\n\t}\n\n\tif err := assignIP(tunDevice.Name(), subnet); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to assign IP address\")\n\t}\n\n\tif tunDevice.Name() != name {\n\t\tif err := renameInterface(tunDevice.Name(), name); err != nil {\n\t\t\treturn nil, errors.Wrap(err, \"failed to rename network interface\")\n\t\t}\n\t}\n\n\treturn &nativeTun{\n\t\ttun:    tunDevice,\n\t\tevents: make(chan tun.Event, 10),\n\t}, nil\n}\n\nfunc (tun *nativeTun) Name() (string, error) {\n\treturn tun.tun.Name(), nil\n}\n\nfunc (tun *nativeTun) File() *os.File {\n\treturn nil\n}\n\nfunc (tun *nativeTun) Events() chan tun.Event {\n\treturn tun.events\n}\n\nfunc (tun *nativeTun) Read(buff []byte, offset int) (int, error) {\n\treturn tun.tun.Read(buff[offset:])\n}\n\nfunc (tun *nativeTun) Write(buff []byte, offset int) (int, error) {\n\treturn tun.tun.Write(buff[offset:])\n}\n\nfunc (tun *nativeTun) Close() error {\n\tclose(tun.events)\n\treturn tun.tun.Close()\n}\n\nfunc (tun *nativeTun) Flush() error {\n\treturn tun.Flush()\n}\n\nfunc (tun *nativeTun) MTU() (int, error) {\n\treturn device.DefaultMTU, nil\n}\n", "idx": 1, "id": 15220, "msg": "We don't need to flush any more?", "proj": "mysteriumnetwork-node", "lang": "go"}
{"patch": "@@ -115,7 +115,7 @@ func AccountStateAtHeight(sr protocol.StateReader, encodedAddr string, height ui\n \t}\n \tpkHash := hash.BytesToHash160(addr.Bytes())\n \tvar account state.Account\n-\tif err := sr.State(pkHash, &account, protocol.BlockHeightOption(height)); err != nil {\n+\tif _, err := sr.State(&account, protocol.BlockHeightOption(height), protocol.LegacyKeyOption(pkHash)); err != nil {\n \t\tif errors.Cause(err) == state.ErrStateNotExist {\n \t\t\taccount = state.EmptyAccount()\n \t\t\treturn &account, nil", "y": 0, "oldf": "// Copyright (c) 2018 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage accountutil\n\nimport (\n\t\"math/big\"\n\n\t\"github.com/pkg/errors\"\n\n\t\"github.com/iotexproject/go-pkgs/hash\"\n\t\"github.com/iotexproject/iotex-address/address\"\n\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\t\"github.com/iotexproject/iotex-core/state\"\n)\n\ntype noncer interface {\n\tNonce() uint64\n}\n\n// SetNonce sets nonce for account\nfunc SetNonce(i noncer, state *state.Account) {\n\tif i.Nonce() > state.Nonce {\n\t\tstate.Nonce = i.Nonce()\n\t}\n}\n\n// LoadOrCreateAccount either loads an account state or creates an account state\nfunc LoadOrCreateAccount(sm protocol.StateManager, encodedAddr string) (*state.Account, error) {\n\tvar account state.Account\n\taddr, err := address.FromString(encodedAddr)\n\tif err != nil {\n\t\taccount = state.EmptyAccount()\n\t\treturn &account, errors.Wrap(err, \"failed to get address public key hash from encoded address\")\n\t}\n\taddrHash := hash.BytesToHash160(addr.Bytes())\n\terr = sm.State(addrHash, &account)\n\tif err == nil {\n\t\treturn &account, nil\n\t}\n\tif errors.Cause(err) == state.ErrStateNotExist {\n\t\taccount.Balance = big.NewInt(0)\n\t\taccount.VotingWeight = big.NewInt(0)\n\t\tif err := sm.PutState(addrHash, account); err != nil {\n\t\t\treturn nil, errors.Wrapf(err, \"failed to put state for account %x\", addrHash)\n\t\t}\n\t\treturn &account, nil\n\t}\n\treturn nil, err\n}\n\n// LoadAccount loads an account state\nfunc LoadAccount(sm protocol.StateReader, addrHash hash.Hash160) (*state.Account, error) {\n\tvar account state.Account\n\tif err := sm.State(addrHash, &account); err != nil {\n\t\tif errors.Cause(err) == state.ErrStateNotExist {\n\t\t\taccount = state.EmptyAccount()\n\t\t\treturn &account, nil\n\t\t}\n\t\treturn nil, err\n\t}\n\treturn &account, nil\n}\n\n// StoreAccount puts updated account state to trie\nfunc StoreAccount(sm protocol.StateManager, encodedAddr string, account *state.Account) error {\n\taddr, err := address.FromString(encodedAddr)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to get address public key hash from encoded address\")\n\t}\n\taddrHash := hash.BytesToHash160(addr.Bytes())\n\treturn sm.PutState(addrHash, account)\n}\n\n// Recorded tests if an account has been actually stored\nfunc Recorded(sm protocol.StateReader, addr address.Address) (bool, error) {\n\tvar account state.Account\n\terr := sm.State(hash.BytesToHash160(addr.Bytes()), &account)\n\tif err == nil {\n\t\treturn true, nil\n\t}\n\tif errors.Cause(err) == state.ErrStateNotExist {\n\t\treturn false, nil\n\t}\n\treturn false, err\n}\n\n// AccountState returns the confirmed account state on the chain\nfunc AccountState(sr protocol.StateReader, encodedAddr string) (*state.Account, error) {\n\taddr, err := address.FromString(encodedAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"error when getting the pubkey hash\")\n\t}\n\tpkHash := hash.BytesToHash160(addr.Bytes())\n\tvar account state.Account\n\tif err := sr.State(pkHash, &account); err != nil {\n\t\tif errors.Cause(err) == state.ErrStateNotExist {\n\t\t\taccount = state.EmptyAccount()\n\t\t\treturn &account, nil\n\t\t}\n\t\treturn nil, errors.Wrapf(err, \"error when loading state of %x\", pkHash)\n\t}\n\treturn &account, nil\n}\n\n// AccountStateAtHeight returns the confirmed account state on the chain\nfunc AccountStateAtHeight(sr protocol.StateReader, encodedAddr string, height uint64) (*state.Account, error) {\n\taddr, err := address.FromString(encodedAddr)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"error when getting the pubkey hash\")\n\t}\n\tpkHash := hash.BytesToHash160(addr.Bytes())\n\tvar account state.Account\n\tif err := sr.State(pkHash, &account, protocol.BlockHeightOption(height)); err != nil {\n\t\tif errors.Cause(err) == state.ErrStateNotExist {\n\t\t\taccount = state.EmptyAccount()\n\t\t\treturn &account, nil\n\t\t}\n\t\treturn nil, errors.Wrapf(err, \"error when loading state of %x\", pkHash)\n\t}\n\treturn &account, nil\n}\n", "idx": 5, "id": 20832, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go"}
{"patch": "@@ -81,5 +81,7 @@ namespace Nethermind.Blockchain.Receipts\n         public long MigratedBlockNumber { get; set; }\n \n         public int Count => _transactions.Count;\n+        \n+        public event EventHandler<ReceiptsEventArgs> ReceiptsInserted;\n     }\n }", "y": 0, "oldf": "//  Copyright (c) 2021 Demerzel Solutions Limited\n//  This file is part of the Nethermind library.\n// \n//  The Nethermind library is free software: you can redistribute it and/or modify\n//  it under the terms of the GNU Lesser General Public License as published by\n//  the Free Software Foundation, either version 3 of the License, or\n//  (at your option) any later version.\n// \n//  The Nethermind library is distributed in the hope that it will be useful,\n//  but WITHOUT ANY WARRANTY; without even the implied warranty of\n//  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n//  GNU Lesser General Public License for more details.\n// \n//  You should have received a copy of the GNU Lesser General Public License\n//  along with the Nethermind. If not, see <http://www.gnu.org/licenses/>.\n\nusing System.Collections.Concurrent;\nusing Nethermind.Core;\nusing Nethermind.Core.Crypto;\nusing Nethermind.Db;\nusing Nethermind.Serialization.Rlp;\n\nnamespace Nethermind.Blockchain.Receipts\n{\n    public class InMemoryReceiptStorage : IReceiptStorage\n    {\n        private readonly bool _allowReceiptIterator;\n        private readonly ConcurrentDictionary<Keccak, TxReceipt[]> _receipts = new ConcurrentDictionary<Keccak, TxReceipt[]>();\n        \n        private readonly ConcurrentDictionary<Keccak, TxReceipt> _transactions = new ConcurrentDictionary<Keccak, TxReceipt>();\n\n        public InMemoryReceiptStorage(bool allowReceiptIterator = true)\n        {\n            _allowReceiptIterator = allowReceiptIterator;\n        }\n\n        public Keccak FindBlockHash(Keccak txHash)\n        {\n            _transactions.TryGetValue(txHash, out var receipt);\n            return receipt?.BlockHash;\n        }\n\n        public TxReceipt[] Get(Block block) => Get(block.Hash);\n\n        public TxReceipt[] Get(Keccak blockHash)\n        {\n            _receipts.TryGetValue(blockHash, out var receipts);\n            return receipts;\n        }\n\n        public bool CanGetReceiptsByHash(long blockNumber) => true;\n        public bool TryGetReceiptsIterator(long blockNumber, Keccak blockHash, out ReceiptsIterator iterator)\n        {\n            if (_allowReceiptIterator && _receipts.TryGetValue(blockHash, out var receipts))\n            {\n#pragma warning disable 618\n                iterator = new ReceiptsIterator(ReceiptStorageDecoder.Instance.Encode(receipts, RlpBehaviors.Storage | RlpBehaviors.Eip658Receipts).Bytes, new MemDb());\n#pragma warning restore 618\n                return true;\n            }\n            else\n            {\n                iterator = new ReceiptsIterator();\n                return false;\n            }\n        }\n\n        public void Insert(Block block, params TxReceipt[] txReceipts)\n        {\n            _receipts[block.Hash] = txReceipts;\n            for (int i = 0; i < txReceipts.Length; i++)\n            {\n                var txReceipt = txReceipts[i];\n                txReceipt.BlockHash = block.Hash;\n                _transactions[txReceipt.TxHash] = txReceipt;\n            }\n        }\n\n        public long? LowestInsertedReceiptBlockNumber { get; set; }\n\n        public long MigratedBlockNumber { get; set; }\n\n        public int Count => _transactions.Count;\n    }\n}\n", "idx": 3, "id": 25018, "msg": "", "proj": "NethermindEth-nethermind", "lang": ".cs"}
{"patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Copyright ConsenSys AG.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+ * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+ * specific language governing permissions and limitations under the License.\n+ *\n+ * SPDX-License-Identifier: Apache-2.0\n+ */\n+package org.hyperledger.besu.plugin.services.storage;\n+\n+public interface PrivacyKeyValueStorageFactory extends KeyValueStorageFactory {\n+  /**\n+   * Retrieves the schema version of the key-value storage factory.\n+   *\n+   * @return the schema version of the key-value storage factory.\n+   */\n+  int getSchemaVersion();\n+}", "y": 1, "oldf": "", "idx": 1, "id": 20279, "msg": "Could that be more than the \"schema\" version? I think database version would be a better name ...", "proj": "hyperledger-besu", "lang": "java"}
{"patch": "@@ -1192,15 +1192,16 @@ type BlockServer interface {\n \t// If this returns a BServerErrorOverQuota, with Throttled=false,\n \t// the caller can treat it as informational and otherwise ignore\n \t// the error.\n-\tPut(ctx context.Context, tlfID tlf.ID, id BlockID, context BlockContext,\n+\tPut(ctx context.Context, tlfID tlf.ID, id kbfsblock.ID, context kbfsblock.Context,\n \t\tbuf []byte, serverHalf kbfscrypto.BlockCryptKeyServerHalf) error\n \n \t// AddBlockReference adds a new reference to the given block,\n-\t// defined by the given context (which should contain a non-zero\n-\t// BlockRefNonce).  (Contexts with a BlockRefNonce of zero should\n-\t// be used when putting the block for the first time via Put().)\n-\t// Returns a BServerErrorBlockNonExistent if id is unknown within\n-\t// this folder.\n+\t// defined by the given context (which should contain a\n+\t// non-zero kbfsblock.RefNonce).  (Contexts with a\n+\t// kbfsblock.RefNonce of zero should be used when putting the\n+\t// block for the first time via Put().)  Returns a\n+\t// BServerErrorBlockNonExistent if id is unknown within this\n+\t// folder.\n \t//\n \t// AddBlockReference should be idempotent, although it should\n \t// also return an error if, for a given ID and refnonce, any", "y": 0, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"time\"\n\n\t\"github.com/keybase/client/go/libkb\"\n\t\"github.com/keybase/client/go/logger\"\n\t\"github.com/keybase/client/go/protocol/keybase1\"\n\t\"github.com/keybase/kbfs/kbfscodec\"\n\t\"github.com/keybase/kbfs/kbfscrypto\"\n\t\"github.com/keybase/kbfs/tlf\"\n\tmetrics \"github.com/rcrowley/go-metrics\"\n\t\"golang.org/x/net/context\"\n)\n\n// Block just needs to be (de)serialized using msgpack\ntype Block interface {\n\t// GetEncodedSize returns the encoded size of this block, but only\n\t// if it has been previously set; otherwise it returns 0.\n\tGetEncodedSize() uint32\n\t// SetEncodedSize sets the encoded size of this block, locally\n\t// caching it.  The encoded size is not serialized.\n\tSetEncodedSize(size uint32)\n\t// DataVersion returns the data version for this block\n\tDataVersion() DataVer\n\t// NewEmpty returns a new block of the same type as this block\n\tNewEmpty() Block\n\t// Set sets this block to the same value as the passed-in block\n\tSet(other Block, codec kbfscodec.Codec)\n}\n\n// NodeID is a unique but transient ID for a Node. That is, two Node\n// objects in memory at the same time represent the same file or\n// directory if and only if their NodeIDs are equal (by pointer).\ntype NodeID interface {\n\t// ParentID returns the NodeID of the directory containing the\n\t// pointed-to file or directory, or nil if none exists.\n\tParentID() NodeID\n}\n\n// Node represents a direct pointer to a file or directory in KBFS.\n// It is somewhat like an inode in a regular file system.  Users of\n// KBFS can use Node as a handle when accessing files or directories\n// they have previously looked up.\ntype Node interface {\n\t// GetID returns the ID of this Node. This should be used as a\n\t// map key instead of the Node itself.\n\tGetID() NodeID\n\t// GetFolderBranch returns the folder ID and branch for this Node.\n\tGetFolderBranch() FolderBranch\n\t// GetBasename returns the current basename of the node, or \"\"\n\t// if the node has been unlinked.\n\tGetBasename() string\n}\n\n// KBFSOps handles all file system operations.  Expands all indirect\n// pointers.  Operations that modify the server data change all the\n// block IDs along the path, and so must return a path with the new\n// BlockIds so the caller can update their references.\n//\n// KBFSOps implementations must guarantee goroutine-safety of calls on\n// a per-top-level-folder basis.\n//\n// There are two types of operations that could block:\n//   * remote-sync operations, that need to synchronously update the\n//     MD for the corresponding top-level folder.  When these\n//     operations return successfully, they will have guaranteed to\n//     have successfully written the modification to the KBFS servers.\n//   * remote-access operations, that don't sync any modifications to KBFS\n//     servers, but may block on reading data from the servers.\n//\n// KBFSOps implementations are supposed to give git-like consistency\n// semantics for modification operations; they will be visible to\n// other clients immediately after the remote-sync operations succeed,\n// if and only if there was no other intervening modification to the\n// same folder.  If not, the change will be sync'd to the server in a\n// special per-device \"unmerged\" area before the operation succeeds.\n// In this case, the modification will not be visible to other clients\n// until the KBFS code on this device performs automatic conflict\n// resolution in the background.\n//\n// All methods take a Context (see https://blog.golang.org/context),\n// and if that context is cancelled during the operation, KBFSOps will\n// abort any blocking calls and return ctx.Err(). Any notifications\n// resulting from an operation will also include this ctx (or a\n// Context derived from it), allowing the caller to determine whether\n// the notification is a result of their own action or an external\n// action.\ntype KBFSOps interface {\n\t// GetFavorites returns the logged-in user's list of favorite\n\t// top-level folders.  This is a remote-access operation.\n\tGetFavorites(ctx context.Context) ([]Favorite, error)\n\t// RefreshCachedFavorites tells the instances to forget any cached\n\t// favorites list and fetch a new list from the server.  The\n\t// effects are asychronous; if there's an error refreshing the\n\t// favorites, the cached favorites will become empty.\n\tRefreshCachedFavorites(ctx context.Context)\n\t// AddFavorite adds the favorite to both the server and\n\t// the local cache.\n\tAddFavorite(ctx context.Context, fav Favorite) error\n\t// DeleteFavorite deletes the favorite from both the server and\n\t// the local cache.  Idempotent, so it succeeds even if the folder\n\t// isn't favorited.\n\tDeleteFavorite(ctx context.Context, fav Favorite) error\n\n\t// GetTLFCryptKeys gets crypt key of all generations as well as\n\t// TLF ID for tlfHandle. The returned keys (the keys slice) are ordered by\n\t// generation, starting with the key for FirstValidKeyGen.\n\tGetTLFCryptKeys(ctx context.Context, tlfHandle *TlfHandle) (\n\t\tkeys []kbfscrypto.TLFCryptKey, id tlf.ID, err error)\n\n\t// GetTLFID gets the TLF ID for tlfHandle.\n\tGetTLFID(ctx context.Context, tlfHandle *TlfHandle) (tlf.ID, error)\n\n\t// GetOrCreateRootNode returns the root node and root entry\n\t// info associated with the given TLF handle and branch, if\n\t// the logged-in user has read permissions to the top-level\n\t// folder. It creates the folder if one doesn't exist yet (and\n\t// branch == MasterBranch), and the logged-in user has write\n\t// permissions to the top-level folder.  This is a\n\t// remote-access operation.\n\tGetOrCreateRootNode(\n\t\tctx context.Context, h *TlfHandle, branch BranchName) (\n\t\tnode Node, ei EntryInfo, err error)\n\t// GetRootNode is like GetOrCreateRootNode but if the root node\n\t// does not exist it will return a nil Node and not create it.\n\tGetRootNode(\n\t\tctx context.Context, h *TlfHandle, branch BranchName) (\n\t\tnode Node, ei EntryInfo, err error)\n\t// GetDirChildren returns a map of children in the directory,\n\t// mapped to their EntryInfo, if the logged-in user has read\n\t// permission for the top-level folder.  This is a remote-access\n\t// operation.\n\tGetDirChildren(ctx context.Context, dir Node) (map[string]EntryInfo, error)\n\t// Lookup returns the Node and entry info associated with a\n\t// given name in a directory, if the logged-in user has read\n\t// permissions to the top-level folder.  The returned Node is nil\n\t// if the name is a symlink.  This is a remote-access operation.\n\tLookup(ctx context.Context, dir Node, name string) (Node, EntryInfo, error)\n\t// Stat returns the entry info associated with a\n\t// given Node, if the logged-in user has read permissions to the\n\t// top-level folder.  This is a remote-access operation.\n\tStat(ctx context.Context, node Node) (EntryInfo, error)\n\t// CreateDir creates a new subdirectory under the given node, if\n\t// the logged-in user has write permission to the top-level\n\t// folder.  Returns the new Node for the created subdirectory, and\n\t// its new entry info.  This is a remote-sync operation.\n\tCreateDir(ctx context.Context, dir Node, name string) (\n\t\tNode, EntryInfo, error)\n\t// CreateFile creates a new file under the given node, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// Returns the new Node for the created file, and its new\n\t// entry info. excl (when implemented) specifies whether this is an exclusive\n\t// create.  Semantically setting excl to WithExcl is like O_CREAT|O_EXCL in a\n\t// Unix open() call.\n\t//\n\t// This is a remote-sync operation.\n\tCreateFile(ctx context.Context, dir Node, name string, isExec bool, excl Excl) (\n\t\tNode, EntryInfo, error)\n\t// CreateLink creates a new symlink under the given node, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// Returns the new entry info for the created symlink.  This\n\t// is a remote-sync operation.\n\tCreateLink(ctx context.Context, dir Node, fromName string, toPath string) (\n\t\tEntryInfo, error)\n\t// RemoveDir removes the subdirectory represented by the given\n\t// node, if the logged-in user has write permission to the\n\t// top-level folder.  Will return an error if the subdirectory is\n\t// not empty.  This is a remote-sync operation.\n\tRemoveDir(ctx context.Context, dir Node, dirName string) error\n\t// RemoveEntry removes the directory entry represented by the\n\t// given node, if the logged-in user has write permission to the\n\t// top-level folder.  This is a remote-sync operation.\n\tRemoveEntry(ctx context.Context, dir Node, name string) error\n\t// Rename performs an atomic rename operation with a given\n\t// top-level folder if the logged-in user has write permission to\n\t// that folder, and will return an error if nodes from different\n\t// folders are passed in.  Also returns an error if the new name\n\t// already has an entry corresponding to an existing directory\n\t// (only non-dir types may be renamed over).  This is a\n\t// remote-sync operation.\n\tRename(ctx context.Context, oldParent Node, oldName string, newParent Node,\n\t\tnewName string) error\n\t// Read fills in the given buffer with data from the file at the\n\t// given node starting at the given offset, if the logged-in user\n\t// has read permission to the top-level folder.  The read data\n\t// reflects any outstanding writes and truncates to that file that\n\t// have been written through this KBFSOps object, even if those\n\t// writes have not yet been sync'd.  There is no guarantee that\n\t// Read returns all of the requested data; it will return the\n\t// number of bytes that it wrote to the dest buffer.  Reads on an\n\t// unlinked file may or may not succeed, depending on whether or\n\t// not the data has been cached locally.  If (0, nil) is returned,\n\t// that means EOF has been reached. This is a remote-access\n\t// operation.\n\tRead(ctx context.Context, file Node, dest []byte, off int64) (int64, error)\n\t// Write modifies the file at the given node, by writing the given\n\t// buffer at the given offset within the file, if the logged-in\n\t// user has write permission to the top-level folder.  It\n\t// overwrites any data already there, and extends the file size as\n\t// necessary to accomodate the new data.  It guarantees to write\n\t// the entire buffer in one operation.  Writes on an unlinked file\n\t// may or may not succeed as no-ops, depending on whether or not\n\t// the necessary blocks have been locally cached.  This is a\n\t// remote-access operation.\n\tWrite(ctx context.Context, file Node, data []byte, off int64) error\n\t// Truncate modifies the file at the given node, by either\n\t// shrinking or extending its size to match the given size, if the\n\t// logged-in user has write permission to the top-level folder.\n\t// If extending the file, it pads the new data with 0s.  Truncates\n\t// on an unlinked file may or may not succeed as no-ops, depending\n\t// on whether or not the necessary blocks have been locally\n\t// cached.  This is a remote-access operation.\n\tTruncate(ctx context.Context, file Node, size uint64) error\n\t// SetEx turns on or off the executable bit on the file\n\t// represented by a given node, if the logged-in user has write\n\t// permissions to the top-level folder.  This is a remote-sync\n\t// operation.\n\tSetEx(ctx context.Context, file Node, ex bool) error\n\t// SetMtime sets the modification time on the file represented by\n\t// a given node, if the logged-in user has write permissions to\n\t// the top-level folder.  If mtime is nil, it is a noop.  This is\n\t// a remote-sync operation.\n\tSetMtime(ctx context.Context, file Node, mtime *time.Time) error\n\t// Sync flushes all outstanding writes and truncates for the given\n\t// file to the KBFS servers, if the logged-in user has write\n\t// permissions to the top-level folder.  If done through a file\n\t// system interface, this may include modifications done via\n\t// multiple file handles.  This is a remote-sync operation.\n\tSync(ctx context.Context, file Node) error\n\t// FolderStatus returns the status of a particular folder/branch, along\n\t// with a channel that will be closed when the status has been\n\t// updated (to eliminate the need for polling this method).\n\tFolderStatus(ctx context.Context, folderBranch FolderBranch) (\n\t\tFolderBranchStatus, <-chan StatusUpdate, error)\n\t// Status returns the status of KBFS, along with a channel that will be\n\t// closed when the status has been updated (to eliminate the need for\n\t// polling this method). KBFSStatus can be non-empty even if there is an\n\t// error.\n\tStatus(ctx context.Context) (\n\t\tKBFSStatus, <-chan StatusUpdate, error)\n\t// UnstageForTesting clears out this device's staged state, if\n\t// any, and fast-forwards to the current head of this\n\t// folder-branch.\n\tUnstageForTesting(ctx context.Context, folderBranch FolderBranch) error\n\t// Rekey rekeys this folder.\n\tRekey(ctx context.Context, id tlf.ID) error\n\t// SyncFromServerForTesting blocks until the local client has\n\t// contacted the server and guaranteed that all known updates\n\t// for the given top-level folder have been applied locally\n\t// (and notifications sent out to any observers).  It returns\n\t// an error if this folder-branch is currently unmerged or\n\t// dirty locally.\n\tSyncFromServerForTesting(ctx context.Context, folderBranch FolderBranch) error\n\t// GetUpdateHistory returns a complete history of all the merged\n\t// updates of the given folder, in a data structure that's\n\t// suitable for encoding directly into JSON.  This is an expensive\n\t// operation, and should only be used for ocassional debugging.\n\t// Note that the history does not include any unmerged changes or\n\t// outstanding writes from the local device.\n\tGetUpdateHistory(ctx context.Context, folderBranch FolderBranch) (\n\t\thistory TLFUpdateHistory, err error)\n\t// GetEditHistory returns a clustered list of the most recent file\n\t// edits by each of the valid writers of the given folder.  users\n\t// looking to get updates to this list can register as an observer\n\t// for the folder.\n\tGetEditHistory(ctx context.Context, folderBranch FolderBranch) (\n\t\tedits TlfWriterEdits, err error)\n\n\t// GetNodeMetadata gets metadata associated with a Node.\n\tGetNodeMetadata(ctx context.Context, node Node) (NodeMetadata, error)\n\n\t// Shutdown is called to clean up any resources associated with\n\t// this KBFSOps instance.\n\tShutdown(ctx context.Context) error\n\t// PushConnectionStatusChange updates the status of a service for\n\t// human readable connection status tracking.\n\tPushConnectionStatusChange(service string, newStatus error)\n\t// PushStatusChange causes Status listeners to be notified via closing\n\t// the status channel.\n\tPushStatusChange()\n}\n\n// KeybaseService is an interface for communicating with the keybase\n// service.\ntype KeybaseService interface {\n\t// Resolve, given an assertion, resolves it to a username/UID\n\t// pair. The username <-> UID mapping is trusted and\n\t// immutable, so it can be cached. If the assertion is just\n\t// the username or a UID assertion, then the resolution can\n\t// also be trusted. If the returned pair is equal to that of\n\t// the current session, then it can also be\n\t// trusted. Otherwise, Identify() needs to be called on the\n\t// assertion before the assertion -> (username, UID) mapping\n\t// can be trusted.\n\tResolve(ctx context.Context, assertion string) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n\n\t// Identify, given an assertion, returns a UserInfo struct\n\t// with the user that matches that assertion, or an error\n\t// otherwise. The reason string is displayed on any tracker\n\t// popups spawned.\n\tIdentify(ctx context.Context, assertion, reason string) (UserInfo, error)\n\n\t// LoadUserPlusKeys returns a UserInfo struct for a\n\t// user with the specified UID.\n\t// If you have the UID for a user and don't require Identify to\n\t// validate an assertion or the identity of a user, use this to\n\t// get UserInfo structs as it is much cheaper than Identify.\n\tLoadUserPlusKeys(ctx context.Context, uid keybase1.UID) (UserInfo, error)\n\n\t// LoadUnverifiedKeys returns a list of unverified public keys.  They are the union\n\t// of all known public keys associated with the account and the currently verified\n\t// keys currently part of the user's sigchain.\n\tLoadUnverifiedKeys(ctx context.Context, uid keybase1.UID) (\n\t\t[]keybase1.PublicKey, error)\n\n\t// CurrentSession returns a SessionInfo struct with all the\n\t// information for the current session, or an error otherwise.\n\tCurrentSession(ctx context.Context, sessionID int) (SessionInfo, error)\n\n\t// FavoriteAdd adds the given folder to the list of favorites.\n\tFavoriteAdd(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteAdd removes the given folder from the list of\n\t// favorites.\n\tFavoriteDelete(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteList returns the current list of favorites.\n\tFavoriteList(ctx context.Context, sessionID int) ([]keybase1.Folder, error)\n\n\t// Notify sends a filesystem notification.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification) error\n\n\t// NotifySyncStatus sends a sync status notification.\n\tNotifySyncStatus(ctx context.Context,\n\t\tstatus *keybase1.FSPathSyncStatus) error\n\n\t// FlushUserFromLocalCache instructs this layer to clear any\n\t// KBFS-side, locally-cached information about the given user.\n\t// This does NOT involve communication with the daemon, this is\n\t// just to force future calls loading this user to fall through to\n\t// the daemon itself, rather than being served from the cache.\n\tFlushUserFromLocalCache(ctx context.Context, uid keybase1.UID)\n\n\t// FlushUserUnverifiedKeysFromLocalCache instructs this layer to clear any\n\t// KBFS-side, locally-cached unverified keys for the given user.\n\tFlushUserUnverifiedKeysFromLocalCache(ctx context.Context, uid keybase1.UID)\n\n\t// TODO: Add CryptoClient methods, too.\n\n\t// EstablishMountDir asks the service for the current mount path\n\t// and sets it if not established.\n\tEstablishMountDir(ctx context.Context) (string, error)\n\n\t// Shutdown frees any resources associated with this\n\t// instance. No other methods may be called after this is\n\t// called.\n\tShutdown()\n}\n\n// KeybaseServiceCn defines methods needed to construct KeybaseService\n// and Crypto implementations.\ntype KeybaseServiceCn interface {\n\tNewKeybaseService(config Config, params InitParams, ctx Context, log logger.Logger) (KeybaseService, error)\n\tNewCrypto(config Config, params InitParams, ctx Context, log logger.Logger) (Crypto, error)\n}\n\ntype resolver interface {\n\t// Resolve, given an assertion, resolves it to a username/UID\n\t// pair. The username <-> UID mapping is trusted and\n\t// immutable, so it can be cached. If the assertion is just\n\t// the username or a UID assertion, then the resolution can\n\t// also be trusted. If the returned pair is equal to that of\n\t// the current session, then it can also be\n\t// trusted. Otherwise, Identify() needs to be called on the\n\t// assertion before the assertion -> (username, UID) mapping\n\t// can be trusted.\n\tResolve(ctx context.Context, assertion string) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n}\n\ntype identifier interface {\n\t// Identify resolves an assertion (which could also be a\n\t// username) to a UserInfo struct, spawning tracker popups if\n\t// necessary.  The reason string is displayed on any tracker\n\t// popups spawned.\n\tIdentify(ctx context.Context, assertion, reason string) (UserInfo, error)\n}\n\ntype normalizedUsernameGetter interface {\n\t// GetNormalizedUsername returns the normalized username\n\t// corresponding to the given UID.\n\tGetNormalizedUsername(ctx context.Context, uid keybase1.UID) (libkb.NormalizedUsername, error)\n}\n\ntype currentInfoGetter interface {\n\t// GetCurrentToken gets the current keybase session token.\n\tGetCurrentToken(ctx context.Context) (string, error)\n\t// GetCurrentUserInfo gets the name and UID of the current\n\t// logged-in user.\n\tGetCurrentUserInfo(ctx context.Context) (\n\t\tlibkb.NormalizedUsername, keybase1.UID, error)\n\t// GetCurrentCryptPublicKey gets the crypt public key for the\n\t// currently-active device.\n\tGetCurrentCryptPublicKey(ctx context.Context) (\n\t\tkbfscrypto.CryptPublicKey, error)\n\t// GetCurrentVerifyingKey gets the public key used for signing for the\n\t// currently-active device.\n\tGetCurrentVerifyingKey(ctx context.Context) (\n\t\tkbfscrypto.VerifyingKey, error)\n}\n\n// KBPKI interacts with the Keybase daemon to fetch user info.\ntype KBPKI interface {\n\tcurrentInfoGetter\n\tresolver\n\tidentifier\n\tnormalizedUsernameGetter\n\n\t// HasVerifyingKey returns nil if the given user has the given\n\t// VerifyingKey, and an error otherwise.\n\tHasVerifyingKey(ctx context.Context, uid keybase1.UID,\n\t\tverifyingKey kbfscrypto.VerifyingKey,\n\t\tatServerTime time.Time) error\n\n\t// HasUnverifiedVerifyingKey returns nil if the given user has the given\n\t// unverified VerifyingKey, and an error otherwise.  Note that any match\n\t// is with a key not verified to be currently connected to the user via\n\t// their sigchain.  This is currently only used to verify finalized or\n\t// reset TLFs.  Further note that unverified keys is a super set of\n\t// verified keys.\n\tHasUnverifiedVerifyingKey(ctx context.Context, uid keybase1.UID,\n\t\tverifyingKey kbfscrypto.VerifyingKey) error\n\n\t// GetCryptPublicKeys gets all of a user's crypt public keys (including\n\t// paper keys).\n\tGetCryptPublicKeys(ctx context.Context, uid keybase1.UID) (\n\t\t[]kbfscrypto.CryptPublicKey, error)\n\n\t// TODO: Split the methods below off into a separate\n\t// FavoriteOps interface.\n\n\t// FavoriteAdd adds folder to the list of the logged in user's\n\t// favorite folders.  It is idempotent.\n\tFavoriteAdd(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteDelete deletes folder from the list of the logged in user's\n\t// favorite folders.  It is idempotent.\n\tFavoriteDelete(ctx context.Context, folder keybase1.Folder) error\n\n\t// FavoriteList returns the list of all favorite folders for\n\t// the logged in user.\n\tFavoriteList(ctx context.Context) ([]keybase1.Folder, error)\n\n\t// Notify sends a filesystem notification.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification) error\n}\n\n// KeyMetadata is an interface for something that holds key\n// information. This is usually implemented by RootMetadata.\ntype KeyMetadata interface {\n\t// TlfID returns the ID of the TLF for which this object holds\n\t// key info.\n\tTlfID() tlf.ID\n\n\t// LatestKeyGeneration returns the most recent key generation\n\t// with key data in this object, or PublicKeyGen if this TLF\n\t// is public.\n\tLatestKeyGeneration() KeyGen\n\n\t// GetTlfHandle returns the handle for the TLF. It must not\n\t// return nil.\n\t//\n\t// TODO: Remove the need for this function in this interface,\n\t// so that BareRootMetadata can implement this interface\n\t// fully.\n\tGetTlfHandle() *TlfHandle\n\n\t// HasKeyForUser returns whether or not the given user has\n\t// keys for at least one device at the given key\n\t// generation. Returns false if the TLF is public, or if the\n\t// given key generation is invalid.\n\tHasKeyForUser(keyGen KeyGen, user keybase1.UID) bool\n\n\t// GetTLFCryptKeyParams returns all the necessary info to\n\t// construct the TLF crypt key for the given key generation,\n\t// user, and device (identified by its crypt public key), or\n\t// false if not found. This returns an error if the TLF is\n\t// public.\n\tGetTLFCryptKeyParams(\n\t\tkeyGen KeyGen, user keybase1.UID,\n\t\tkey kbfscrypto.CryptPublicKey) (\n\t\tkbfscrypto.TLFEphemeralPublicKey,\n\t\tEncryptedTLFCryptKeyClientHalf,\n\t\tTLFCryptKeyServerHalfID, bool, error)\n\n\t// StoresHistoricTLFCryptKeys returns whether or not history keys are\n\t// symmetrically encrypted; if not, they're encrypted per-device.\n\tStoresHistoricTLFCryptKeys() bool\n\n\t// GetHistoricTLFCryptKey attempts to symmetrically decrypt the key at the given\n\t// generation using the current generation's TLFCryptKey.\n\tGetHistoricTLFCryptKey(c cryptoPure, keyGen KeyGen,\n\t\tcurrentKey kbfscrypto.TLFCryptKey) (\n\t\tkbfscrypto.TLFCryptKey, error)\n}\n\ntype encryptionKeyGetter interface {\n\t// GetTLFCryptKeyForEncryption gets the crypt key to use for\n\t// encryption (i.e., with the latest key generation) for the\n\t// TLF with the given metadata.\n\tGetTLFCryptKeyForEncryption(ctx context.Context, kmd KeyMetadata) (\n\t\tkbfscrypto.TLFCryptKey, error)\n}\n\ntype mdDecryptionKeyGetter interface {\n\t// GetTLFCryptKeyForMDDecryption gets the crypt key to use for the\n\t// TLF with the given metadata to decrypt the private portion of\n\t// the metadata.  It finds the appropriate key from mdWithKeys\n\t// (which in most cases is the same as mdToDecrypt) if it's not\n\t// already cached.\n\tGetTLFCryptKeyForMDDecryption(ctx context.Context,\n\t\tkmdToDecrypt, kmdWithKeys KeyMetadata) (\n\t\tkbfscrypto.TLFCryptKey, error)\n}\n\n// KeyManager fetches and constructs the keys needed for KBFS file\n// operations.\ntype KeyManager interface {\n\tencryptionKeyGetter\n\tmdDecryptionKeyGetter\n\n\t// GetTLFCryptKeyForBlockDecryption gets the crypt key to use\n\t// for the TLF with the given metadata to decrypt the block\n\t// pointed to by the given pointer.\n\tGetTLFCryptKeyForBlockDecryption(ctx context.Context, kmd KeyMetadata,\n\t\tblockPtr BlockPointer) (kbfscrypto.TLFCryptKey, error)\n\n\t// GetTLFCryptKeyOfAllGenerations gets the crypt keys of all generations\n\t// for current devices. keys contains crypt keys from all generations, in\n\t// order, starting from FirstValidKeyGen.\n\tGetTLFCryptKeyOfAllGenerations(ctx context.Context, kmd KeyMetadata) (\n\t\tkeys []kbfscrypto.TLFCryptKey, err error)\n\n\t// Rekey checks the given MD object, if it is a private TLF,\n\t// against the current set of device keys for all valid\n\t// readers and writers.  If there are any new devices, it\n\t// updates all existing key generations to include the new\n\t// devices.  If there are devices that have been removed, it\n\t// creates a new epoch of keys for the TLF.  If there was an\n\t// error, or the RootMetadata wasn't changed, it returns false.\n\t// Otherwise, it returns true. If a new key generation is\n\t// added the second return value points to this new key. This\n\t// is to allow for caching of the TLF crypt key only after a\n\t// successful merged write of the metadata. Otherwise we could\n\t// prematurely pollute the key cache.\n\t//\n\t// If the given MD object is a public TLF, it simply updates\n\t// the TLF's handle with any newly-resolved writers.\n\t//\n\t// If promptPaper is set, prompts for any unlocked paper keys.\n\t// promptPaper shouldn't be set if md is for a public TLF.\n\tRekey(ctx context.Context, md *RootMetadata, promptPaper bool) (\n\t\tbool, *kbfscrypto.TLFCryptKey, error)\n}\n\n// Reporter exports events (asynchronously) to any number of sinks\ntype Reporter interface {\n\t// ReportErr records that a given error happened.\n\tReportErr(ctx context.Context, tlfName CanonicalTlfName, public bool,\n\t\tmode ErrorModeType, err error)\n\t// AllKnownErrors returns all errors known to this Reporter.\n\tAllKnownErrors() []ReportedError\n\t// Notify sends the given notification to any sink.\n\tNotify(ctx context.Context, notification *keybase1.FSNotification)\n\t// NotifySyncStatus sends the given path sync status to any sink.\n\tNotifySyncStatus(ctx context.Context, status *keybase1.FSPathSyncStatus)\n\t// Shutdown frees any resources allocated by a Reporter.\n\tShutdown()\n}\n\n// MDCache gets and puts plaintext top-level metadata into the cache.\ntype MDCache interface {\n\t// Get gets the metadata object associated with the given TLF ID,\n\t// revision number, and branch ID (NullBranchID for merged MD).\n\tGet(tlf tlf.ID, rev MetadataRevision, bid BranchID) (ImmutableRootMetadata, error)\n\t// Put stores the metadata object.\n\tPut(md ImmutableRootMetadata) error\n\t// Delete removes the given metadata object from the cache if it exists.\n\tDelete(tlf tlf.ID, rev MetadataRevision, bid BranchID)\n\t// Replace replaces the entry matching the md under the old branch\n\t// ID with the new one.  If the old entry doesn't exist, this is\n\t// equivalent to a Put.\n\tReplace(newRmd ImmutableRootMetadata, oldBID BranchID) error\n}\n\n// KeyCache handles caching for both TLFCryptKeys and BlockCryptKeys.\ntype KeyCache interface {\n\t// GetTLFCryptKey gets the crypt key for the given TLF.\n\tGetTLFCryptKey(tlf.ID, KeyGen) (kbfscrypto.TLFCryptKey, error)\n\t// PutTLFCryptKey stores the crypt key for the given TLF.\n\tPutTLFCryptKey(tlf.ID, KeyGen, kbfscrypto.TLFCryptKey) error\n}\n\n// BlockCacheLifetime denotes the lifetime of an entry in BlockCache.\ntype BlockCacheLifetime int\n\nconst (\n\t// TransientEntry means that the cache entry may be evicted at\n\t// any time.\n\tTransientEntry BlockCacheLifetime = iota\n\t// PermanentEntry means that the cache entry must remain until\n\t// explicitly removed from the cache.\n\tPermanentEntry\n)\n\n// BlockCache gets and puts plaintext dir blocks and file blocks into\n// a cache.  These blocks are immutable and identified by their\n// content hash.\ntype BlockCache interface {\n\t// Get gets the block associated with the given block ID.\n\tGet(ptr BlockPointer) (Block, error)\n\t// CheckForKnownPtr sees whether this cache has a transient\n\t// entry for the given file block, which must be a direct file\n\t// block containing data).  Returns the full BlockPointer\n\t// associated with that ID, including key and data versions.\n\t// If no ID is known, return an uninitialized BlockPointer and\n\t// a nil error.\n\tCheckForKnownPtr(tlf tlf.ID, block *FileBlock) (BlockPointer, error)\n\t// Put stores the final (content-addressable) block associated\n\t// with the given block ID. If lifetime is TransientEntry,\n\t// then it is assumed that the block exists on the server and\n\t// the entry may be evicted from the cache at any time. If\n\t// lifetime is PermanentEntry, then it is assumed that the\n\t// block doesn't exist on the server and must remain in the\n\t// cache until explicitly removed. As an intermediary state,\n\t// as when a block is being sent to the server, the block may\n\t// be put into the cache both with TransientEntry and\n\t// PermanentEntry -- these are two separate entries. This is\n\t// fine, since the block should be the same.\n\tPut(ptr BlockPointer, tlf tlf.ID, block Block,\n\t\tlifetime BlockCacheLifetime) error\n\t// DeleteTransient removes the transient entry for the given\n\t// pointer from the cache, as well as any cached IDs so the block\n\t// won't be reused.\n\tDeleteTransient(ptr BlockPointer, tlf tlf.ID) error\n\t// Delete removes the permanent entry for the non-dirty block\n\t// associated with the given block ID from the cache.  No\n\t// error is returned if no block exists for the given ID.\n\tDeletePermanent(id BlockID) error\n\t// DeleteKnownPtr removes the cached ID for the given file\n\t// block. It does not remove the block itself.\n\tDeleteKnownPtr(tlf tlf.ID, block *FileBlock) error\n\n\t// SetCleanBytesCapacity atomically sets clean bytes capacity for block\n\t// cache.\n\tSetCleanBytesCapacity(capacity uint64)\n\n\t// GetCleanBytesCapacity atomically gets clean bytes capacity for block\n\t// cache.\n\tGetCleanBytesCapacity() (capacity uint64)\n}\n\n// DirtyPermChan is a channel that gets closed when the holder has\n// permission to write.  We are forced to define it as a type due to a\n// bug in mockgen that can't handle return values with a chan\n// struct{}.\ntype DirtyPermChan <-chan struct{}\n\n// DirtyBlockCache gets and puts plaintext dir blocks and file blocks\n// into a cache, which have been modified by the application and not\n// yet committed on the KBFS servers.  They are identified by a\n// (potentially random) ID that may not have any relationship with\n// their context, along with a Branch in case the same TLF is being\n// modified via multiple branches.  Dirty blocks are never evicted,\n// they must be deleted explicitly.\ntype DirtyBlockCache interface {\n\t// Get gets the block associated with the given block ID.  Returns\n\t// the dirty block for the given ID, if one exists.\n\tGet(tlfID tlf.ID, ptr BlockPointer, branch BranchName) (Block, error)\n\t// Put stores a dirty block currently identified by the\n\t// given block pointer and branch name.\n\tPut(tlfID tlf.ID, ptr BlockPointer, branch BranchName, block Block) error\n\t// Delete removes the dirty block associated with the given block\n\t// pointer and branch from the cache.  No error is returned if no\n\t// block exists for the given ID.\n\tDelete(tlfID tlf.ID, ptr BlockPointer, branch BranchName) error\n\t// IsDirty states whether or not the block associated with the\n\t// given block pointer and branch name is dirty in this cache.\n\tIsDirty(tlfID tlf.ID, ptr BlockPointer, branch BranchName) bool\n\t// IsAnyDirty returns whether there are any dirty blocks in the\n\t// cache. tlfID may be ignored.\n\tIsAnyDirty(tlfID tlf.ID) bool\n\t// RequestPermissionToDirty is called whenever a user wants to\n\t// write data to a file.  The caller provides an estimated number\n\t// of bytes that will become dirty -- this is difficult to know\n\t// exactly without pre-fetching all the blocks involved, but in\n\t// practice we can just use the number of bytes sent in via the\n\t// Write. It returns a channel that blocks until the cache is\n\t// ready to receive more dirty data, at which point the channel is\n\t// closed.  The user must call\n\t// `UpdateUnsyncedBytes(-estimatedDirtyBytes)` once it has\n\t// completed its write and called `UpdateUnsyncedBytes` for all\n\t// the exact dirty block sizes.\n\tRequestPermissionToDirty(ctx context.Context, tlfID tlf.ID,\n\t\testimatedDirtyBytes int64) (DirtyPermChan, error)\n\t// UpdateUnsyncedBytes is called by a user, who has already been\n\t// granted permission to write, with the delta in block sizes that\n\t// were dirtied as part of the write.  So for example, if a\n\t// newly-dirtied block of 20 bytes was extended by 5 bytes, they\n\t// should send 25.  If on the next write (before any syncs), bytes\n\t// 10-15 of that same block were overwritten, they should send 0\n\t// over the channel because there were no new bytes.  If an\n\t// already-dirtied block is truncated, or if previously requested\n\t// bytes have now been updated more accurately in previous\n\t// requests, newUnsyncedBytes may be negative.  wasSyncing should\n\t// be true if `BlockSyncStarted` has already been called for this\n\t// block.\n\tUpdateUnsyncedBytes(tlfID tlf.ID, newUnsyncedBytes int64, wasSyncing bool)\n\t// UpdateSyncingBytes is called when a particular block has\n\t// started syncing, or with a negative number when a block is no\n\t// longer syncing due to an error (and BlockSyncFinished will\n\t// never be called).\n\tUpdateSyncingBytes(tlfID tlf.ID, size int64)\n\t// BlockSyncFinished is called when a particular block has\n\t// finished syncing, though the overall sync might not yet be\n\t// complete.  This lets the cache know it might be able to grant\n\t// more permission to writers.\n\tBlockSyncFinished(tlfID tlf.ID, size int64)\n\t// SyncFinished is called when a complete sync has completed and\n\t// its dirty blocks have been removed from the cache.  This lets\n\t// the cache know it might be able to grant more permission to\n\t// writers.\n\tSyncFinished(tlfID tlf.ID, size int64)\n\t// ShouldForceSync returns true if the sync buffer is full enough\n\t// to force all callers to sync their data immediately.\n\tShouldForceSync(tlfID tlf.ID) bool\n\n\t// Shutdown frees any resources associated with this instance.  It\n\t// returns an error if there are any unsynced blocks.\n\tShutdown() error\n}\n\n// cryptoPure contains all methods of Crypto that don't depend on\n// implicit state, i.e. they're pure functions of the input.\ntype cryptoPure interface {\n\t// MakeRandomTlfID generates a dir ID using a CSPRNG.\n\tMakeRandomTlfID(isPublic bool) (tlf.ID, error)\n\n\t// MakeRandomBranchID generates a per-device branch ID using a\n\t// CSPRNG.  It will not return LocalSquashBranchID or\n\t// NullBranchID.\n\tMakeRandomBranchID() (BranchID, error)\n\n\t// MakeMdID computes the MD ID of a RootMetadata object.\n\t// TODO: This should move to BareRootMetadata. Note though, that some mock tests\n\t// rely on it being part of the config and crypto_measured.go uses it to keep\n\t// statistics on time spent hashing.\n\tMakeMdID(md BareRootMetadata) (MdID, error)\n\n\t// MakeMerkleHash computes the hash of a RootMetadataSigned object\n\t// for inclusion into the KBFS Merkle tree.\n\tMakeMerkleHash(md *RootMetadataSigned) (MerkleHash, error)\n\n\t// MakeTemporaryBlockID generates a temporary block ID using a\n\t// CSPRNG. This is used for indirect blocks before they're\n\t// committed to the server.\n\tMakeTemporaryBlockID() (BlockID, error)\n\n\t// MakePermanentBlockID computes the permanent ID of a block\n\t// given its encoded and encrypted contents.\n\tMakePermanentBlockID(encodedEncryptedData []byte) (BlockID, error)\n\n\t// VerifyBlockID verifies that the given block ID is the\n\t// permanent block ID for the given encoded and encrypted\n\t// data.\n\tVerifyBlockID(encodedEncryptedData []byte, id BlockID) error\n\n\t// MakeRefNonce generates a block reference nonce using a\n\t// CSPRNG. This is used for distinguishing different references to\n\t// the same BlockID.\n\tMakeBlockRefNonce() (BlockRefNonce, error)\n\n\t// MakeRandomTLFEphemeralKeys generates ephemeral keys using a\n\t// CSPRNG for a TLF. These keys can then be used to key/rekey\n\t// the TLF.\n\tMakeRandomTLFEphemeralKeys() (kbfscrypto.TLFEphemeralPublicKey,\n\t\tkbfscrypto.TLFEphemeralPrivateKey, error)\n\n\t// MakeRandomTLFKeys generates keys using a CSPRNG for a\n\t// single key generation of TLF.\n\tMakeRandomTLFKeys() (kbfscrypto.TLFPublicKey,\n\t\tkbfscrypto.TLFPrivateKey, kbfscrypto.TLFCryptKey, error)\n\t// MakeRandomTLFCryptKeyServerHalf generates the server-side of a\n\t// top-level folder crypt key.\n\tMakeRandomTLFCryptKeyServerHalf() (\n\t\tkbfscrypto.TLFCryptKeyServerHalf, error)\n\t// MakeRandomBlockCryptKeyServerHalf generates the server-side of\n\t// a block crypt key.\n\tMakeRandomBlockCryptKeyServerHalf() (\n\t\tkbfscrypto.BlockCryptKeyServerHalf, error)\n\n\t// MaskTLFCryptKey returns the client-side of a top-level folder crypt key.\n\tMaskTLFCryptKey(serverHalf kbfscrypto.TLFCryptKeyServerHalf,\n\t\tkey kbfscrypto.TLFCryptKey) (\n\t\tkbfscrypto.TLFCryptKeyClientHalf, error)\n\t// UnmaskTLFCryptKey returns the top-level folder crypt key.\n\tUnmaskTLFCryptKey(serverHalf kbfscrypto.TLFCryptKeyServerHalf,\n\t\tclientHalf kbfscrypto.TLFCryptKeyClientHalf) (\n\t\tkbfscrypto.TLFCryptKey, error)\n\t// UnmaskBlockCryptKey returns the block crypt key.\n\tUnmaskBlockCryptKey(serverHalf kbfscrypto.BlockCryptKeyServerHalf,\n\t\ttlfCryptKey kbfscrypto.TLFCryptKey) (\n\t\tkbfscrypto.BlockCryptKey, error)\n\n\t// Verify verifies that sig matches msg being signed with the\n\t// private key that corresponds to verifyingKey.\n\tVerify(msg []byte, sigInfo kbfscrypto.SignatureInfo) error\n\n\t// EncryptTLFCryptKeyClientHalf encrypts a TLFCryptKeyClientHalf\n\t// using both a TLF's ephemeral private key and a device pubkey.\n\tEncryptTLFCryptKeyClientHalf(\n\t\tprivateKey kbfscrypto.TLFEphemeralPrivateKey,\n\t\tpublicKey kbfscrypto.CryptPublicKey,\n\t\tclientHalf kbfscrypto.TLFCryptKeyClientHalf) (\n\t\tEncryptedTLFCryptKeyClientHalf, error)\n\n\t// EncryptPrivateMetadata encrypts a PrivateMetadata object.\n\tEncryptPrivateMetadata(\n\t\tpmd PrivateMetadata, key kbfscrypto.TLFCryptKey) (\n\t\tEncryptedPrivateMetadata, error)\n\t// DecryptPrivateMetadata decrypts a PrivateMetadata object.\n\tDecryptPrivateMetadata(\n\t\tencryptedPMD EncryptedPrivateMetadata,\n\t\tkey kbfscrypto.TLFCryptKey) (PrivateMetadata, error)\n\n\t// EncryptBlocks encrypts a block. plainSize is the size of the encoded\n\t// block; EncryptBlock() must guarantee that plainSize <=\n\t// len(encryptedBlock).\n\tEncryptBlock(block Block, key kbfscrypto.BlockCryptKey) (\n\t\tplainSize int, encryptedBlock EncryptedBlock, err error)\n\n\t// DecryptBlock decrypts a block. Similar to EncryptBlock(),\n\t// DecryptBlock() must guarantee that (size of the decrypted\n\t// block) <= len(encryptedBlock).\n\tDecryptBlock(encryptedBlock EncryptedBlock,\n\t\tkey kbfscrypto.BlockCryptKey, block Block) error\n\n\t// GetTLFCryptKeyServerHalfID creates a unique ID for this particular\n\t// kbfscrypto.TLFCryptKeyServerHalf.\n\tGetTLFCryptKeyServerHalfID(\n\t\tuser keybase1.UID, devicePubKey kbfscrypto.CryptPublicKey,\n\t\tserverHalf kbfscrypto.TLFCryptKeyServerHalf) (\n\t\tTLFCryptKeyServerHalfID, error)\n\n\t// VerifyTLFCryptKeyServerHalfID verifies the ID is the proper HMAC result.\n\tVerifyTLFCryptKeyServerHalfID(serverHalfID TLFCryptKeyServerHalfID,\n\t\tuser keybase1.UID, deviceKID keybase1.KID,\n\t\tserverHalf kbfscrypto.TLFCryptKeyServerHalf) error\n\n\t// EncryptMerkleLeaf encrypts a Merkle leaf node with the TLFPublicKey.\n\tEncryptMerkleLeaf(leaf MerkleLeaf, pubKey kbfscrypto.TLFPublicKey,\n\t\tnonce *[24]byte, ePrivKey kbfscrypto.TLFEphemeralPrivateKey) (\n\t\tEncryptedMerkleLeaf, error)\n\n\t// DecryptMerkleLeaf decrypts a Merkle leaf node with the TLFPrivateKey.\n\tDecryptMerkleLeaf(encryptedLeaf EncryptedMerkleLeaf,\n\t\tprivKey kbfscrypto.TLFPrivateKey, nonce *[24]byte,\n\t\tePubKey kbfscrypto.TLFEphemeralPublicKey) (*MerkleLeaf, error)\n\n\t// MakeTLFWriterKeyBundleID hashes a TLFWriterKeyBundleV3 to create an ID.\n\tMakeTLFWriterKeyBundleID(wkb TLFWriterKeyBundleV3) (TLFWriterKeyBundleID, error)\n\n\t// MakeTLFReaderKeyBundleID hashes a TLFReaderKeyBundleV3 to create an ID.\n\tMakeTLFReaderKeyBundleID(rkb TLFReaderKeyBundleV3) (TLFReaderKeyBundleID, error)\n\n\t// EncryptTLFCryptKeys encrypts an array of historic TLFCryptKeys.\n\tEncryptTLFCryptKeys(oldKeys []kbfscrypto.TLFCryptKey,\n\t\tkey kbfscrypto.TLFCryptKey) (EncryptedTLFCryptKeys, error)\n\n\t// DecryptTLFCryptKeys decrypts an array of historic TLFCryptKeys.\n\tDecryptTLFCryptKeys(\n\t\tencKeys EncryptedTLFCryptKeys, key kbfscrypto.TLFCryptKey) (\n\t\t[]kbfscrypto.TLFCryptKey, error)\n}\n\n// Crypto signs, verifies, encrypts, and decrypts stuff.\ntype Crypto interface {\n\tcryptoPure\n\n\t// Duplicate kbfscrypto.Signer here to work around gomock's\n\t// limitations.\n\tSign(context.Context, []byte) (kbfscrypto.SignatureInfo, error)\n\tSignForKBFS(context.Context, []byte) (kbfscrypto.SignatureInfo, error)\n\tSignToString(context.Context, []byte) (string, error)\n\n\t// DecryptTLFCryptKeyClientHalf decrypts a\n\t// kbfscrypto.TLFCryptKeyClientHalf using the current device's\n\t// private key and the TLF's ephemeral public key.\n\tDecryptTLFCryptKeyClientHalf(ctx context.Context,\n\t\tpublicKey kbfscrypto.TLFEphemeralPublicKey,\n\t\tencryptedClientHalf EncryptedTLFCryptKeyClientHalf) (\n\t\tkbfscrypto.TLFCryptKeyClientHalf, error)\n\n\t// DecryptTLFCryptKeyClientHalfAny decrypts one of the\n\t// kbfscrypto.TLFCryptKeyClientHalf using the available\n\t// private keys and the ephemeral public key.  If promptPaper\n\t// is true, the service will prompt the user for any unlocked\n\t// paper keys.\n\tDecryptTLFCryptKeyClientHalfAny(ctx context.Context,\n\t\tkeys []EncryptedTLFCryptKeyClientAndEphemeral,\n\t\tpromptPaper bool) (\n\t\tkbfscrypto.TLFCryptKeyClientHalf, int, error)\n\n\t// Shutdown frees any resources associated with this instance.\n\tShutdown()\n}\n\n// MDOps gets and puts root metadata to an MDServer.  On a get, it\n// verifies the metadata is signed by the metadata's signing key.\ntype MDOps interface {\n\t// GetForHandle returns the current metadata object\n\t// corresponding to the given top-level folder's handle and\n\t// merge status, if the logged-in user has read permission on\n\t// the folder.  It creates the folder if one doesn't exist\n\t// yet, and the logged-in user has permission to do so.\n\t//\n\t// If there is no returned error, then the returned ID must\n\t// always be non-null. An empty ImmutableRootMetadata may be\n\t// returned, but if it is non-empty, then its ID must match\n\t// the returned ID.\n\tGetForHandle(\n\t\tctx context.Context, handle *TlfHandle, mStatus MergeStatus) (\n\t\ttlf.ID, ImmutableRootMetadata, error)\n\n\t// GetForTLF returns the current metadata object\n\t// corresponding to the given top-level folder, if the logged-in\n\t// user has read permission on the folder.\n\tGetForTLF(ctx context.Context, id tlf.ID) (ImmutableRootMetadata, error)\n\n\t// GetUnmergedForTLF is the same as the above but for unmerged\n\t// metadata.\n\tGetUnmergedForTLF(ctx context.Context, id tlf.ID, bid BranchID) (\n\t\tImmutableRootMetadata, error)\n\n\t// GetRange returns a range of metadata objects corresponding to\n\t// the passed revision numbers (inclusive).\n\tGetRange(ctx context.Context, id tlf.ID, start, stop MetadataRevision) (\n\t\t[]ImmutableRootMetadata, error)\n\n\t// GetUnmergedRange is the same as the above but for unmerged\n\t// metadata history (inclusive).\n\tGetUnmergedRange(ctx context.Context, id tlf.ID, bid BranchID,\n\t\tstart, stop MetadataRevision) ([]ImmutableRootMetadata, error)\n\n\t// Put stores the metadata object for the given\n\t// top-level folder.\n\tPut(ctx context.Context, rmd *RootMetadata) (MdID, error)\n\n\t// PutUnmerged is the same as the above but for unmerged\n\t// metadata history.\n\tPutUnmerged(ctx context.Context, rmd *RootMetadata) (MdID, error)\n\n\t// PruneBranch prunes all unmerged history for the given TLF\n\t// branch.\n\tPruneBranch(ctx context.Context, id tlf.ID, bid BranchID) error\n\n\t// ResolveBranch prunes all unmerged history for the given TLF\n\t// branch, and also deletes any blocks in `blocksToDelete` that\n\t// are still in the local journal.  It also appends the given MD\n\t// to the journal.\n\tResolveBranch(ctx context.Context, id tlf.ID, bid BranchID,\n\t\tblocksToDelete []BlockID, rmd *RootMetadata) (MdID, error)\n\n\t// GetLatestHandleForTLF returns the server's idea of the latest handle for the TLF,\n\t// which may not yet be reflected in the MD if the TLF hasn't been rekeyed since it\n\t// entered into a conflicting state.\n\tGetLatestHandleForTLF(ctx context.Context, id tlf.ID) (\n\t\ttlf.Handle, error)\n}\n\n// KeyOps fetches server-side key halves from the key server.\ntype KeyOps interface {\n\t// GetTLFCryptKeyServerHalf gets a server-side key half for a\n\t// device given the key half ID.\n\tGetTLFCryptKeyServerHalf(ctx context.Context,\n\t\tserverHalfID TLFCryptKeyServerHalfID,\n\t\tcryptPublicKey kbfscrypto.CryptPublicKey) (\n\t\tkbfscrypto.TLFCryptKeyServerHalf, error)\n\n\t// PutTLFCryptKeyServerHalves stores a server-side key halves for a\n\t// set of users and devices.\n\tPutTLFCryptKeyServerHalves(ctx context.Context,\n\t\tkeyServerHalves UserDeviceKeyServerHalves) error\n\n\t// DeleteTLFCryptKeyServerHalf deletes a server-side key half for a\n\t// device given the key half ID.\n\tDeleteTLFCryptKeyServerHalf(ctx context.Context,\n\t\tuid keybase1.UID, kid keybase1.KID,\n\t\tserverHalfID TLFCryptKeyServerHalfID) error\n}\n\n// BlockOps gets and puts data blocks to a BlockServer. It performs\n// the necessary crypto operations on each block.\ntype BlockOps interface {\n\t// Get gets the block associated with the given block pointer\n\t// (which belongs to the TLF with the given key metadata),\n\t// decrypts it if necessary, and fills in the provided block\n\t// object with its contents, if the logged-in user has read\n\t// permission for that block.\n\tGet(ctx context.Context, kmd KeyMetadata, blockPtr BlockPointer,\n\t\tblock Block) error\n\n\t// Ready turns the given block (which belongs to the TLF with\n\t// the given key metadata) into encoded (and encrypted) data,\n\t// and calculates its ID and size, so that we can do a bunch\n\t// of block puts in parallel for every write. Ready() must\n\t// guarantee that plainSize <= readyBlockData.QuotaSize().\n\tReady(ctx context.Context, kmd KeyMetadata, block Block) (\n\t\tid BlockID, plainSize int, readyBlockData ReadyBlockData, err error)\n\n\t// Delete instructs the server to delete the given block references.\n\t// It returns the number of not-yet deleted references to\n\t// each block reference\n\tDelete(ctx context.Context, tlfID tlf.ID, ptrs []BlockPointer) (\n\t\tliveCounts map[BlockID]int, err error)\n\n\t// Archive instructs the server to mark the given block references\n\t// as \"archived\"; that is, they are not being used in the current\n\t// view of the folder, and shouldn't be served to anyone other\n\t// than folder writers.\n\tArchive(ctx context.Context, tlfID tlf.ID, ptrs []BlockPointer) error\n\n\t// Shutdown shuts down all the workers performing Get operations\n\tShutdown()\n}\n\n// Duplicate kbfscrypto.AuthTokenRefreshHandler here to work around\n// gomock's limitations.\ntype authTokenRefreshHandler interface {\n\tRefreshAuthToken(context.Context)\n}\n\n// MDServer gets and puts metadata for each top-level directory.  The\n// instantiation should be able to fetch session/user details via KBPKI.  On a\n// put, the server is responsible for 1) ensuring the user has appropriate\n// permissions for whatever modifications were made; 2) ensuring that\n// LastModifyingWriter and LastModifyingUser are updated appropriately; and 3)\n// detecting conflicting writes based on the previous root block ID (i.e., when\n// it supports strict consistency).  On a get, it verifies the logged-in user\n// has read permissions.\n//\n// TODO: Add interface for searching by time\ntype MDServer interface {\n\tauthTokenRefreshHandler\n\n\t// GetForHandle returns the current (signed/encrypted) metadata\n\t// object corresponding to the given top-level folder's handle, if\n\t// the logged-in user has read permission on the folder.  It\n\t// creates the folder if one doesn't exist yet, and the logged-in\n\t// user has permission to do so.\n\t//\n\t// If there is no returned error, then the returned ID must\n\t// always be non-null. A nil *RootMetadataSigned may be\n\t// returned, but if it is non-nil, then its ID must match the\n\t// returned ID.\n\tGetForHandle(ctx context.Context, handle tlf.Handle,\n\t\tmStatus MergeStatus) (tlf.ID, *RootMetadataSigned, error)\n\n\t// GetForTLF returns the current (signed/encrypted) metadata object\n\t// corresponding to the given top-level folder, if the logged-in\n\t// user has read permission on the folder.\n\tGetForTLF(ctx context.Context, id tlf.ID, bid BranchID, mStatus MergeStatus) (\n\t\t*RootMetadataSigned, error)\n\n\t// GetRange returns a range of (signed/encrypted) metadata objects\n\t// corresponding to the passed revision numbers (inclusive).\n\tGetRange(ctx context.Context, id tlf.ID, bid BranchID, mStatus MergeStatus,\n\t\tstart, stop MetadataRevision) ([]*RootMetadataSigned, error)\n\n\t// Put stores the (signed/encrypted) metadata object for the given\n\t// top-level folder. Note: If the unmerged bit is set in the metadata\n\t// block's flags bitmask it will be appended to the unmerged per-device\n\t// history.\n\tPut(ctx context.Context, rmds *RootMetadataSigned, extra ExtraMetadata) error\n\n\t// PruneBranch prunes all unmerged history for the given TLF branch.\n\tPruneBranch(ctx context.Context, id tlf.ID, bid BranchID) error\n\n\t// RegisterForUpdate tells the MD server to inform the caller when\n\t// there is a merged update with a revision number greater than\n\t// currHead, which did NOT originate from this same MD server\n\t// session.  This method returns a chan which can receive only a\n\t// single error before it's closed.  If the received err is nil,\n\t// then there is updated MD ready to fetch which didn't originate\n\t// locally; if it is non-nil, then the previous registration\n\t// cannot send the next notification (e.g., the connection to the\n\t// MD server may have failed). In either case, the caller must\n\t// re-register to get a new chan that can receive future update\n\t// notifications.\n\tRegisterForUpdate(ctx context.Context, id tlf.ID,\n\t\tcurrHead MetadataRevision) (<-chan error, error)\n\n\t// CheckForRekeys initiates the rekey checking process on the\n\t// server.  The server is allowed to delay this request, and so it\n\t// returns a channel for returning the error. Actual rekey\n\t// requests are expected to come in asynchronously.\n\tCheckForRekeys(ctx context.Context) <-chan error\n\n\t// TruncateLock attempts to take the history truncation lock for\n\t// this folder, for a TTL defined by the server.  Returns true if\n\t// the lock was successfully taken.\n\tTruncateLock(ctx context.Context, id tlf.ID) (bool, error)\n\t// TruncateUnlock attempts to release the history truncation lock\n\t// for this folder.  Returns true if the lock was successfully\n\t// released.\n\tTruncateUnlock(ctx context.Context, id tlf.ID) (bool, error)\n\n\t// DisableRekeyUpdatesForTesting disables processing rekey updates\n\t// received from the mdserver while testing.\n\tDisableRekeyUpdatesForTesting()\n\n\t// Shutdown is called to shutdown an MDServer connection.\n\tShutdown()\n\n\t// IsConnected returns whether the MDServer is connected.\n\tIsConnected() bool\n\n\t// GetLatestHandleForTLF returns the server's idea of the latest handle for the TLF,\n\t// which may not yet be reflected in the MD if the TLF hasn't been rekeyed since it\n\t// entered into a conflicting state.  For the highest level of confidence, the caller\n\t// should verify the mapping with a Merkle tree lookup.\n\tGetLatestHandleForTLF(ctx context.Context, id tlf.ID) (\n\t\ttlf.Handle, error)\n\n\t// OffsetFromServerTime is the current estimate for how off our\n\t// local clock is from the mdserver clock.  Add this to any\n\t// mdserver-provided timestamps to get the \"local\" time of the\n\t// corresponding event.  If the returned bool is false, then we\n\t// don't have a current estimate for the offset.\n\tOffsetFromServerTime() (time.Duration, bool)\n\n\t// GetKeyBundles looks up the key bundles for the given key\n\t// bundle IDs. tlfID must be non-zero but either or both wkbID\n\t// and rkbID can be zero, in which case nil will be returned\n\t// for the respective bundle. If a bundle cannot be found, an\n\t// error is returned and nils are returned for both bundles.\n\tGetKeyBundles(ctx context.Context, tlfID tlf.ID,\n\t\twkbID TLFWriterKeyBundleID, rkbID TLFReaderKeyBundleID) (\n\t\t*TLFWriterKeyBundleV3, *TLFReaderKeyBundleV3, error)\n}\n\ntype mdServerLocal interface {\n\tMDServer\n\taddNewAssertionForTest(\n\t\tuid keybase1.UID, newAssertion keybase1.SocialAssertion) error\n\tgetCurrentMergedHeadRevision(ctx context.Context, id tlf.ID) (\n\t\trev MetadataRevision, err error)\n\tisShutdown() bool\n\tcopy(config mdServerLocalConfig) mdServerLocal\n}\n\n// BlockServer gets and puts opaque data blocks.  The instantiation\n// should be able to fetch session/user details via KBPKI.  On a\n// put/delete, the server is reponsible for: 1) checking that the ID\n// matches the hash of the buffer; and 2) enforcing writer quotas.\ntype BlockServer interface {\n\tauthTokenRefreshHandler\n\n\t// Get gets the (encrypted) block data associated with the given\n\t// block ID and context, uses the provided block key to decrypt\n\t// the block, and fills in the provided block object with its\n\t// contents, if the logged-in user has read permission for that\n\t// block.\n\tGet(ctx context.Context, tlfID tlf.ID, id BlockID, context BlockContext) (\n\t\t[]byte, kbfscrypto.BlockCryptKeyServerHalf, error)\n\t// Put stores the (encrypted) block data under the given ID and\n\t// context on the server, along with the server half of the block\n\t// key.  context should contain a BlockRefNonce of zero.  There\n\t// will be an initial reference for this block for the given\n\t// context.\n\t//\n\t// Put should be idempotent, although it should also return an\n\t// error if, for a given ID, any of the other arguments differ\n\t// from previous Put calls with the same ID.\n\t//\n\t// If this returns a BServerErrorOverQuota, with Throttled=false,\n\t// the caller can treat it as informational and otherwise ignore\n\t// the error.\n\tPut(ctx context.Context, tlfID tlf.ID, id BlockID, context BlockContext,\n\t\tbuf []byte, serverHalf kbfscrypto.BlockCryptKeyServerHalf) error\n\n\t// AddBlockReference adds a new reference to the given block,\n\t// defined by the given context (which should contain a non-zero\n\t// BlockRefNonce).  (Contexts with a BlockRefNonce of zero should\n\t// be used when putting the block for the first time via Put().)\n\t// Returns a BServerErrorBlockNonExistent if id is unknown within\n\t// this folder.\n\t//\n\t// AddBlockReference should be idempotent, although it should\n\t// also return an error if, for a given ID and refnonce, any\n\t// of the other fields of context differ from previous\n\t// AddBlockReference calls with the same ID and refnonce.\n\t//\n\t// If this returns a BServerErrorOverQuota, with Throttled=false,\n\t// the caller can treat it as informational and otherwise ignore\n\t// the error.\n\tAddBlockReference(ctx context.Context, tlfID tlf.ID, id BlockID,\n\t\tcontext BlockContext) error\n\t// RemoveBlockReferences removes the references to the given block\n\t// ID defined by the given contexts.  If no references to the block\n\t// remain after this call, the server is allowed to delete the\n\t// corresponding block permanently.  If the reference defined by\n\t// the count has already been removed, the call is a no-op.\n\t// It returns the number of remaining not-yet-deleted references after this\n\t// reference has been removed\n\tRemoveBlockReferences(ctx context.Context, tlfID tlf.ID,\n\t\tcontexts map[BlockID][]BlockContext) (liveCounts map[BlockID]int, err error)\n\n\t// ArchiveBlockReferences marks the given block references as\n\t// \"archived\"; that is, they are not being used in the current\n\t// view of the folder, and shouldn't be served to anyone other\n\t// than folder writers.\n\t//\n\t// For a given ID/refnonce pair, ArchiveBlockReferences should\n\t// be idempotent, although it should also return an error if\n\t// any of the other fields of the context differ from previous\n\t// calls with the same ID/refnonce pair.\n\tArchiveBlockReferences(ctx context.Context, tlfID tlf.ID,\n\t\tcontexts map[BlockID][]BlockContext) error\n\n\t// IsUnflushed returns whether a given block is being queued\n\t// locally for later flushing to another block server.\n\tIsUnflushed(ctx context.Context, tlfID tlf.ID, id BlockID) (bool, error)\n\n\t// Shutdown is called to shutdown a BlockServer connection.\n\tShutdown()\n\n\t// GetUserQuotaInfo returns the quota for the user.\n\tGetUserQuotaInfo(ctx context.Context) (info *UserQuotaInfo, err error)\n}\n\n// blockServerLocal is the interface for BlockServer implementations\n// that store data locally.\ntype blockServerLocal interface {\n\tBlockServer\n\t// getAllRefsForTest returns all the known block references\n\t// for the given TLF, and should only be used during testing.\n\tgetAllRefsForTest(ctx context.Context, tlfID tlf.ID) (\n\t\tmap[BlockID]blockRefMap, error)\n}\n\n// BlockSplitter decides when a file or directory block needs to be split\ntype BlockSplitter interface {\n\t// CopyUntilSplit copies data into the block until we reach the\n\t// point where we should split, but only if writing to the end of\n\t// the last block.  If this is writing into the middle of a file,\n\t// just copy everything that will fit into the block, and assume\n\t// that block boundaries will be fixed later. Return how much was\n\t// copied.\n\tCopyUntilSplit(\n\t\tblock *FileBlock, lastBlock bool, data []byte, off int64) int64\n\n\t// CheckSplit, given a block, figures out whether it ends at the\n\t// right place.  If so, return 0.  If not, return either the\n\t// offset in the block where it should be split, or -1 if more\n\t// bytes from the next block should be appended.\n\tCheckSplit(block *FileBlock) int64\n\n\t// ShouldEmbedBlockChanges decides whether we should keep the\n\t// block changes embedded in the MD or not.\n\tShouldEmbedBlockChanges(bc *BlockChanges) bool\n}\n\n// KeyServer fetches/writes server-side key halves from/to the key server.\ntype KeyServer interface {\n\t// GetTLFCryptKeyServerHalf gets a server-side key half for a\n\t// device given the key half ID.\n\tGetTLFCryptKeyServerHalf(ctx context.Context,\n\t\tserverHalfID TLFCryptKeyServerHalfID,\n\t\tcryptPublicKey kbfscrypto.CryptPublicKey) (\n\t\tkbfscrypto.TLFCryptKeyServerHalf, error)\n\n\t// PutTLFCryptKeyServerHalves stores a server-side key halves for a\n\t// set of users and devices.\n\tPutTLFCryptKeyServerHalves(ctx context.Context,\n\t\tkeyServerHalves UserDeviceKeyServerHalves) error\n\n\t// DeleteTLFCryptKeyServerHalf deletes a server-side key half for a\n\t// device given the key half ID.\n\tDeleteTLFCryptKeyServerHalf(ctx context.Context,\n\t\tuid keybase1.UID, kid keybase1.KID,\n\t\tserverHalfID TLFCryptKeyServerHalfID) error\n\n\t// Shutdown is called to free any KeyServer resources.\n\tShutdown()\n}\n\n// NodeChange represents a change made to a node as part of an atomic\n// file system operation.\ntype NodeChange struct {\n\tNode Node\n\t// Basenames of entries added/removed.\n\tDirUpdated  []string\n\tFileUpdated []WriteRange\n}\n\n// Observer can be notified that there is an available update for a\n// given directory.  The notification callbacks should not block, or\n// make any calls to the Notifier interface.  Nodes passed to the\n// observer should not be held past the end of the notification\n// callback.\ntype Observer interface {\n\t// LocalChange announces that the file at this Node has been\n\t// updated locally, but not yet saved at the server.\n\tLocalChange(ctx context.Context, node Node, write WriteRange)\n\t// BatchChanges announces that the nodes have all been updated\n\t// together atomically.  Each NodeChange in changes affects the\n\t// same top-level folder and branch.\n\tBatchChanges(ctx context.Context, changes []NodeChange)\n\t// TlfHandleChange announces that the handle of the corresponding\n\t// folder branch has changed, likely due to previously-unresolved\n\t// assertions becoming resolved.  This indicates that the listener\n\t// should switch over any cached paths for this folder-branch to\n\t// the new name.  Nodes that were acquired under the old name will\n\t// still continue to work, but new lookups on the old name may\n\t// either encounter alias errors or entirely new TLFs (in the case\n\t// of conflicts).\n\tTlfHandleChange(ctx context.Context, newHandle *TlfHandle)\n}\n\n// Notifier notifies registrants of directory changes\ntype Notifier interface {\n\t// RegisterForChanges declares that the given Observer wants to\n\t// subscribe to updates for the given top-level folders.\n\tRegisterForChanges(folderBranches []FolderBranch, obs Observer) error\n\t// UnregisterFromChanges declares that the given Observer no\n\t// longer wants to subscribe to updates for the given top-level\n\t// folders.\n\tUnregisterFromChanges(folderBranches []FolderBranch, obs Observer) error\n}\n\n// Clock is an interface for getting the current time\ntype Clock interface {\n\t// Now returns the current time.\n\tNow() time.Time\n}\n\n// ConflictRenamer deals with names for conflicting directory entries.\ntype ConflictRenamer interface {\n\t// ConflictRename returns the appropriately modified filename.\n\tConflictRename(ctx context.Context, op op, original string) (\n\t\tstring, error)\n}\n\n// Config collects all the singleton instance instantiations needed to\n// run KBFS in one place.  The methods below are self-explanatory and\n// do not require comments.\ntype Config interface {\n\tKBFSOps() KBFSOps\n\tSetKBFSOps(KBFSOps)\n\tKBPKI() KBPKI\n\tSetKBPKI(KBPKI)\n\tKeyManager() KeyManager\n\tSetKeyManager(KeyManager)\n\tReporter() Reporter\n\tSetReporter(Reporter)\n\tMDCache() MDCache\n\tSetMDCache(MDCache)\n\tKeyCache() KeyCache\n\tSetKeyBundleCache(KeyBundleCache)\n\tKeyBundleCache() KeyBundleCache\n\tSetKeyCache(KeyCache)\n\tBlockCache() BlockCache\n\tSetBlockCache(BlockCache)\n\tDirtyBlockCache() DirtyBlockCache\n\tSetDirtyBlockCache(DirtyBlockCache)\n\tCrypto() Crypto\n\tSetCrypto(Crypto)\n\tCodec() kbfscodec.Codec\n\tSetCodec(kbfscodec.Codec)\n\tMDOps() MDOps\n\tSetMDOps(MDOps)\n\tKeyOps() KeyOps\n\tSetKeyOps(KeyOps)\n\tBlockOps() BlockOps\n\tSetBlockOps(BlockOps)\n\tMDServer() MDServer\n\tSetMDServer(MDServer)\n\tBlockServer() BlockServer\n\tSetBlockServer(BlockServer)\n\tKeyServer() KeyServer\n\tSetKeyServer(KeyServer)\n\tKeybaseService() KeybaseService\n\tSetKeybaseService(KeybaseService)\n\tBlockSplitter() BlockSplitter\n\tSetBlockSplitter(BlockSplitter)\n\tNotifier() Notifier\n\tSetNotifier(Notifier)\n\tClock() Clock\n\tSetClock(Clock)\n\tConflictRenamer() ConflictRenamer\n\tSetConflictRenamer(ConflictRenamer)\n\tMetadataVersion() MetadataVer\n\tSetMetadataVersion(MetadataVer)\n\tDataVersion() DataVer\n\tRekeyQueue() RekeyQueue\n\tSetRekeyQueue(RekeyQueue)\n\t// ReqsBufSize indicates the number of read or write operations\n\t// that can be buffered per folder\n\tReqsBufSize() int\n\t// MaxFileBytes indicates the maximum supported plaintext size of\n\t// a file in bytes.\n\tMaxFileBytes() uint64\n\t// MaxNameBytes indicates the maximum supported size of a\n\t// directory entry name in bytes.\n\tMaxNameBytes() uint32\n\t// MaxDirBytes indicates the maximum supported plaintext size of a\n\t// directory in bytes.\n\tMaxDirBytes() uint64\n\t// DoBackgroundFlushes says whether we should periodically try to\n\t// flush dirty files, even without a sync from the user.  Should\n\t// be true except for during some testing.\n\tDoBackgroundFlushes() bool\n\tSetDoBackgroundFlushes(bool)\n\t// RekeyWithPromptWaitTime indicates how long to wait, after\n\t// setting the rekey bit, before prompting for a paper key.\n\tRekeyWithPromptWaitTime() time.Duration\n\n\t// GracePeriod specifies a grace period for which a delayed cancellation\n\t// waits before actual cancels the context. This is useful for giving\n\t// critical portion of a slow remote operation some extra time to finish as\n\t// an effort to avoid conflicting. Example include an O_EXCL Create call\n\t// interrupted by ALRM signal actually makes it to the server, while\n\t// application assumes not since EINTR is returned. A delayed cancellation\n\t// allows us to distinguish between successful cancel (where remote operation\n\t// didn't make to server) or failed cancel (where remote operation made to\n\t// the server). However, the optimal value of this depends on the network\n\t// conditions. A long grace period for really good network condition would\n\t// just unnecessarily slow down Ctrl-C.\n\t//\n\t// TODO: make this adaptive and self-change over time based on network\n\t// conditions.\n\tDelayedCancellationGracePeriod() time.Duration\n\tSetDelayedCancellationGracePeriod(time.Duration)\n\t// QuotaReclamationPeriod indicates how often should each TLF\n\t// should check for quota to reclaim.  If the Duration.Seconds()\n\t// == 0, quota reclamation should not run automatically.\n\tQuotaReclamationPeriod() time.Duration\n\t// QuotaReclamationMinUnrefAge indicates the minimum time a block\n\t// must have been unreferenced before it can be reclaimed.\n\tQuotaReclamationMinUnrefAge() time.Duration\n\t// QuotaReclamationMinHeadAge indicates the minimum age of the\n\t// most recently merged MD update before we can run reclamation,\n\t// to avoid conflicting with a currently active writer.\n\tQuotaReclamationMinHeadAge() time.Duration\n\n\t// ResetCaches clears and re-initializes all data and key caches.\n\tResetCaches()\n\n\tMakeLogger(module string) logger.Logger\n\t// MetricsRegistry may be nil, which should be interpreted as\n\t// not using metrics at all. (i.e., as if UseNilMetrics were\n\t// set). This differs from how go-metrics treats nil Registry\n\t// objects, which is to use the default registry.\n\tMetricsRegistry() metrics.Registry\n\tSetMetricsRegistry(metrics.Registry)\n\t// TLFValidDuration is the time TLFs are valid before identification needs to be redone.\n\tTLFValidDuration() time.Duration\n\t// SetTLFValidDuration sets TLFValidDuration.\n\tSetTLFValidDuration(time.Duration)\n\t// Shutdown is called to free config resources.\n\tShutdown(context.Context) error\n\t// CheckStateOnShutdown tells the caller whether or not it is safe\n\t// to check the state of the system on shutdown.\n\tCheckStateOnShutdown() bool\n}\n\n// NodeCache holds Nodes, and allows libkbfs to update them when\n// things change about the underlying KBFS blocks.  It is probably\n// most useful to instantiate this on a per-folder-branch basis, so\n// that it can create a Path with the correct DirId and Branch name.\ntype NodeCache interface {\n\t// GetOrCreate either makes a new Node for the given\n\t// BlockPointer, or returns an existing one. TODO: If we ever\n\t// support hard links, we will have to revisit the \"name\" and\n\t// \"parent\" parameters here.  name must not be empty. Returns\n\t// an error if parent cannot be found.\n\tGetOrCreate(ptr BlockPointer, name string, parent Node) (Node, error)\n\t// Get returns the Node associated with the given ptr if one\n\t// already exists.  Otherwise, it returns nil.\n\tGet(ref BlockRef) Node\n\t// UpdatePointer updates the BlockPointer for the corresponding\n\t// Node.  NodeCache ignores this call when oldRef is not cached in\n\t// any Node.\n\tUpdatePointer(oldRef BlockRef, newPtr BlockPointer)\n\t// Move swaps the parent node for the corresponding Node, and\n\t// updates the node's name.  NodeCache ignores the call when ptr\n\t// is not cached.  Returns an error if newParent cannot be found.\n\t// If newParent is nil, it treats the ptr's corresponding node as\n\t// being unlinked from the old parent completely.\n\tMove(ref BlockRef, newParent Node, newName string) error\n\t// Unlink set the corresponding node's parent to nil and caches\n\t// the provided path in case the node is still open. NodeCache\n\t// ignores the call when ptr is not cached.  The path is required\n\t// because the caller may have made changes to the parent nodes\n\t// already that shouldn't be reflected in the cached path.\n\tUnlink(ref BlockRef, oldPath path)\n\t// PathFromNode creates the path up to a given Node.\n\tPathFromNode(node Node) path\n\t// AllNodes returns the complete set of nodes currently in the cache.\n\tAllNodes() []Node\n}\n\n// fileBlockDeepCopier fetches a file block, makes a deep copy of it\n// (duplicating pointer for any indirect blocks) and generates a new\n// random temporary block ID for it.  It returns the new BlockPointer,\n// and internally saves the block for future uses.\ntype fileBlockDeepCopier func(context.Context, string, BlockPointer) (\n\tBlockPointer, error)\n\n// crAction represents a specific action to take as part of the\n// conflict resolution process.\ntype crAction interface {\n\t// swapUnmergedBlock should be called before do(), and if it\n\t// returns true, the caller must use the merged block\n\t// corresponding to the returned BlockPointer instead of\n\t// unmergedBlock when calling do().  If BlockPointer{} is zeroPtr\n\t// (and true is returned), just swap in the regular mergedBlock.\n\tswapUnmergedBlock(unmergedChains *crChains, mergedChains *crChains,\n\t\tunmergedBlock *DirBlock) (bool, BlockPointer, error)\n\t// do modifies the given merged block in place to resolve the\n\t// conflict, and potential uses the provided blockCopyFetchers to\n\t// obtain copies of other blocks (along with new BlockPointers)\n\t// when requiring a block copy.\n\tdo(ctx context.Context, unmergedCopier fileBlockDeepCopier,\n\t\tmergedCopier fileBlockDeepCopier, unmergedBlock *DirBlock,\n\t\tmergedBlock *DirBlock) error\n\t// updateOps potentially modifies, in place, the slices of\n\t// unmerged and merged operations stored in the corresponding\n\t// crChains for the given unmerged and merged most recent\n\t// pointers.  Eventually, the \"unmerged\" ops will be pushed as\n\t// part of a MD update, and so should contain any necessarily\n\t// operations to fully merge the unmerged data, including any\n\t// conflict resolution.  The \"merged\" ops will be played through\n\t// locally, to notify any caches about the newly-obtained merged\n\t// data (and any changes to local data that were required as part\n\t// of conflict resolution, such as renames).  A few things to note:\n\t// * A particular action's updateOps method may be called more than\n\t//   once for different sets of chains, however it should only add\n\t//   new directory operations (like create/rm/rename) into directory\n\t//   chains.\n\t// * updateOps doesn't necessarily result in correct BlockPointers within\n\t//   each of those ops; that must happen in a later phase.\n\t// * mergedBlock can be nil if the chain is for a file.\n\tupdateOps(unmergedMostRecent BlockPointer, mergedMostRecent BlockPointer,\n\t\tunmergedBlock *DirBlock, mergedBlock *DirBlock,\n\t\tunmergedChains *crChains, mergedChains *crChains) error\n\t// String returns a string representation for this crAction, used\n\t// for debugging.\n\tString() string\n}\n\n// RekeyQueue is a managed queue of folders needing some rekey action taken upon them\n// by the current client.\ntype RekeyQueue interface {\n\t// Enqueue enqueues a folder for rekey action.\n\tEnqueue(tlf.ID) <-chan error\n\t// IsRekeyPending returns true if the given folder is in the rekey queue.\n\tIsRekeyPending(tlf.ID) bool\n\t// GetRekeyChannel will return any rekey completion channel (if pending.)\n\tGetRekeyChannel(id tlf.ID) <-chan error\n\t// Clear cancels all pending rekey actions and clears the queue.\n\tClear()\n\t// Waits for all queued rekeys to finish\n\tWait(ctx context.Context) error\n}\n\n// BareRootMetadata is a read-only interface to the bare serializeable MD that\n// is signed by the reader or writer.\ntype BareRootMetadata interface {\n\t// TlfID returns the ID of the TLF this BareRootMetadata is for.\n\tTlfID() tlf.ID\n\t// LatestKeyGeneration returns the most recent key generation in this\n\t// BareRootMetadata, or PublicKeyGen if this TLF is public.\n\tLatestKeyGeneration() KeyGen\n\t// IsValidRekeyRequest returns true if the current block is a simple rekey wrt\n\t// the passed block.\n\tIsValidRekeyRequest(codec kbfscodec.Codec, prevMd BareRootMetadata,\n\t\tuser keybase1.UID, prevExtra, extra ExtraMetadata) (bool, error)\n\t// MergedStatus returns the status of this update -- has it been\n\t// merged into the main folder or not?\n\tMergedStatus() MergeStatus\n\t// IsRekeySet returns true if the rekey bit is set.\n\tIsRekeySet() bool\n\t// IsWriterMetadataCopiedSet returns true if the bit is set indicating\n\t// the writer metadata was copied.\n\tIsWriterMetadataCopiedSet() bool\n\t// IsFinal returns true if this is the last metadata block for a given\n\t// folder.  This is only expected to be set for folder resets.\n\tIsFinal() bool\n\t// IsWriter returns whether or not the user+device is an authorized writer.\n\tIsWriter(user keybase1.UID, deviceKID keybase1.KID, extra ExtraMetadata) bool\n\t// IsReader returns whether or not the user+device is an authorized reader.\n\tIsReader(user keybase1.UID, deviceKID keybase1.KID, extra ExtraMetadata) bool\n\t// DeepCopy returns a deep copy of the underlying data structure.\n\tDeepCopy(codec kbfscodec.Codec) (MutableBareRootMetadata, error)\n\t// MakeSuccessorCopy returns a newly constructed successor\n\t// copy to this metadata revision.  It differs from DeepCopy\n\t// in that it can perform an up conversion to a new metadata\n\t// version. tlfCryptKeyGetter should be a function that\n\t// returns a list of TLFCryptKeys for all key generations in\n\t// ascending order.\n\tMakeSuccessorCopy(codec kbfscodec.Codec, crypto cryptoPure,\n\t\textra ExtraMetadata, latestMDVer MetadataVer,\n\t\ttlfCryptKeyGetter func() ([]kbfscrypto.TLFCryptKey, error),\n\t\tisReadableAndWriter bool) (mdCopy MutableBareRootMetadata,\n\t\textraCopy ExtraMetadata, err error)\n\t// CheckValidSuccessor makes sure the given BareRootMetadata is a valid\n\t// successor to the current one, and returns an error otherwise.\n\tCheckValidSuccessor(currID MdID, nextMd BareRootMetadata) error\n\t// CheckValidSuccessorForServer is like CheckValidSuccessor but with\n\t// server-specific error messages.\n\tCheckValidSuccessorForServer(currID MdID, nextMd BareRootMetadata) error\n\t// MakeBareTlfHandle makes a tlf.Handle for this\n\t// BareRootMetadata. Should be used only by servers and MDOps.\n\tMakeBareTlfHandle(extra ExtraMetadata) (tlf.Handle, error)\n\t// TlfHandleExtensions returns a list of handle extensions associated with the TLf.\n\tTlfHandleExtensions() (extensions []tlf.HandleExtension)\n\t// GetDeviceKIDs returns the KIDs (of\n\t// kbfscrypto.CryptPublicKeys) for all known devices for the\n\t// given user at the given key generation, if any.  Returns an\n\t// error if the TLF is public, or if the given key generation\n\t// is invalid.\n\tGetDeviceKIDs(keyGen KeyGen, user keybase1.UID, extra ExtraMetadata) (\n\t\t[]keybase1.KID, error)\n\t// HasKeyForUser returns whether or not the given user has keys for at\n\t// least one device at the given key generation. Returns false if the\n\t// TLF is public, or if the given key generation is invalid. Equivalent to:\n\t//\n\t//   kids, err := GetDeviceKIDs(keyGen, user)\n\t//   return (err == nil) && (len(kids) > 0)\n\tHasKeyForUser(keyGen KeyGen, user keybase1.UID, extra ExtraMetadata) bool\n\t// GetTLFCryptKeyParams returns all the necessary info to construct\n\t// the TLF crypt key for the given key generation, user, and device\n\t// (identified by its crypt public key), or false if not found. This\n\t// returns an error if the TLF is public.\n\tGetTLFCryptKeyParams(keyGen KeyGen, user keybase1.UID,\n\t\tkey kbfscrypto.CryptPublicKey, extra ExtraMetadata) (\n\t\tkbfscrypto.TLFEphemeralPublicKey,\n\t\tEncryptedTLFCryptKeyClientHalf,\n\t\tTLFCryptKeyServerHalfID, bool, error)\n\t// IsValidAndSigned verifies the BareRootMetadata, checks the\n\t// writer signature, and returns an error if a problem was\n\t// found. This should be the first thing checked on a BRMD\n\t// retrieved from an untrusted source, and then the signing\n\t// user and key should be validated, either by comparing to\n\t// the current device key (using IsLastModifiedBy), or by\n\t// checking with KBPKI.\n\tIsValidAndSigned(codec kbfscodec.Codec,\n\t\tcrypto cryptoPure, extra ExtraMetadata) error\n\t// IsLastModifiedBy verifies that the BareRootMetadata is\n\t// written by the given user and device (identified by the KID\n\t// of the device verifying key), and returns an error if not.\n\tIsLastModifiedBy(uid keybase1.UID, key kbfscrypto.VerifyingKey) error\n\t// LastModifyingWriter return the UID of the last user to modify the writer metadata.\n\tLastModifyingWriter() keybase1.UID\n\t// LastModifyingUser return the UID of the last user to modify the any of the metadata.\n\tGetLastModifyingUser() keybase1.UID\n\t// RefBytes returns the number of newly referenced bytes introduced by this revision of metadata.\n\tRefBytes() uint64\n\t// UnrefBytes returns the number of newly unreferenced bytes introduced by this revision of metadata.\n\tUnrefBytes() uint64\n\t// DiskUsage returns the estimated disk usage for the folder as of this revision of metadata.\n\tDiskUsage() uint64\n\t// RevisionNumber returns the revision number associated with this metadata structure.\n\tRevisionNumber() MetadataRevision\n\t// BID returns the per-device branch ID associated with this metadata revision.\n\tBID() BranchID\n\t// GetPrevRoot returns the hash of the previous metadata revision.\n\tGetPrevRoot() MdID\n\t// IsUnmergedSet returns true if the unmerged bit is set.\n\tIsUnmergedSet() bool\n\t// GetSerializedPrivateMetadata returns the serialized private metadata as a byte slice.\n\tGetSerializedPrivateMetadata() []byte\n\t// GetSerializedWriterMetadata serializes the underlying writer metadata and returns the result.\n\tGetSerializedWriterMetadata(codec kbfscodec.Codec) ([]byte, error)\n\t// Version returns the metadata version.\n\tVersion() MetadataVer\n\t// GetCurrentTLFPublicKey returns the TLF public key for the\n\t// current key generation.\n\tGetCurrentTLFPublicKey(ExtraMetadata) (kbfscrypto.TLFPublicKey, error)\n\t// AreKeyGenerationsEqual returns true if all key generations in the passed metadata are equal to those\n\t// in this revision.\n\tAreKeyGenerationsEqual(kbfscodec.Codec, BareRootMetadata) (bool, error)\n\t// GetUnresolvedParticipants returns any unresolved readers and writers present in this revision of metadata.\n\tGetUnresolvedParticipants() (readers, writers []keybase1.SocialAssertion)\n\t// GetTLFWriterKeyBundleID returns the ID of the externally-stored writer key bundle, or the zero value if\n\t// this object stores it internally.\n\tGetTLFWriterKeyBundleID() TLFWriterKeyBundleID\n\t// GetTLFReaderKeyBundleID returns the ID of the externally-stored reader key bundle, or the zero value if\n\t// this object stores it internally.\n\tGetTLFReaderKeyBundleID() TLFReaderKeyBundleID\n\t// StoresHistoricTLFCryptKeys returns whether or not history keys are symmetrically encrypted; if not, they're\n\t// encrypted per-device.\n\tStoresHistoricTLFCryptKeys() bool\n\t// GetHistoricTLFCryptKey attempts to symmetrically decrypt the key at the given\n\t// generation using the current generation's TLFCryptKey.\n\tGetHistoricTLFCryptKey(c cryptoPure, keyGen KeyGen,\n\t\tcurrentKey kbfscrypto.TLFCryptKey, extra ExtraMetadata) (\n\t\tkbfscrypto.TLFCryptKey, error)\n\t// GetUserDeviceKeyInfoMaps returns copies of the given user\n\t// device key info maps for the given key generation.\n\tGetUserDeviceKeyInfoMaps(\n\t\tcodec kbfscodec.Codec, keyGen KeyGen, extra ExtraMetadata) (\n\t\treaders, writers UserDeviceKeyInfoMap, err error)\n}\n\n// MutableBareRootMetadata is a mutable interface to the bare serializeable MD that is signed by the reader or writer.\ntype MutableBareRootMetadata interface {\n\tBareRootMetadata\n\n\t// SetRefBytes sets the number of newly referenced bytes introduced by this revision of metadata.\n\tSetRefBytes(refBytes uint64)\n\t// SetUnrefBytes sets the number of newly unreferenced bytes introduced by this revision of metadata.\n\tSetUnrefBytes(unrefBytes uint64)\n\t// SetDiskUsage sets the estimated disk usage for the folder as of this revision of metadata.\n\tSetDiskUsage(diskUsage uint64)\n\t// AddRefBytes increments the number of newly referenced bytes introduced by this revision of metadata.\n\tAddRefBytes(refBytes uint64)\n\t// AddUnrefBytes increments the number of newly unreferenced bytes introduced by this revision of metadata.\n\tAddUnrefBytes(unrefBytes uint64)\n\t// AddDiskUsage increments the estimated disk usage for the folder as of this revision of metadata.\n\tAddDiskUsage(diskUsage uint64)\n\t// ClearRekeyBit unsets any set rekey bit.\n\tClearRekeyBit()\n\t// ClearWriterMetadataCopiedBit unsets any set writer metadata copied bit.\n\tClearWriterMetadataCopiedBit()\n\t// ClearFinalBit unsets any final bit.\n\tClearFinalBit()\n\t// SetUnmerged sets the unmerged bit.\n\tSetUnmerged()\n\t// SetBranchID sets the branch ID for this metadata revision.\n\tSetBranchID(bid BranchID)\n\t// SetPrevRoot sets the hash of the previous metadata revision.\n\tSetPrevRoot(mdID MdID)\n\t// SetSerializedPrivateMetadata sets the serialized private metadata.\n\tSetSerializedPrivateMetadata(spmd []byte)\n\t// SignWriterMetadataInternally signs the writer metadata, for\n\t// versions that store this signature inside the metadata.\n\tSignWriterMetadataInternally(ctx context.Context,\n\t\tcodec kbfscodec.Codec, signer kbfscrypto.Signer) error\n\t// SetLastModifyingWriter sets the UID of the last user to modify the writer metadata.\n\tSetLastModifyingWriter(user keybase1.UID)\n\t// SetLastModifyingUser sets the UID of the last user to modify any of the metadata.\n\tSetLastModifyingUser(user keybase1.UID)\n\t// SetRekeyBit sets the rekey bit.\n\tSetRekeyBit()\n\t// SetFinalBit sets the finalized bit.\n\tSetFinalBit()\n\t// SetWriterMetadataCopiedBit set the writer metadata copied bit.\n\tSetWriterMetadataCopiedBit()\n\t// SetRevision sets the revision number of the underlying metadata.\n\tSetRevision(revision MetadataRevision)\n\t// SetUnresolvedReaders sets the list of unresolved readers associated with this folder.\n\tSetUnresolvedReaders(readers []keybase1.SocialAssertion)\n\t// SetUnresolvedWriters sets the list of unresolved writers associated with this folder.\n\tSetUnresolvedWriters(writers []keybase1.SocialAssertion)\n\t// SetConflictInfo sets any conflict info associated with this metadata revision.\n\tSetConflictInfo(ci *tlf.HandleExtension)\n\t// SetFinalizedInfo sets any finalized info associated with this metadata revision.\n\tSetFinalizedInfo(fi *tlf.HandleExtension)\n\t// SetWriters sets the list of writers associated with this folder.\n\tSetWriters(writers []keybase1.UID)\n\t// SetTlfID sets the ID of the underlying folder in the metadata structure.\n\tSetTlfID(tlf tlf.ID)\n\n\t// addKeyGenerationForTest is like AddKeyGeneration, except\n\t// currCryptKey and nextCryptKey don't have to be zero if\n\t// StoresHistoricTLFCryptKeys is false, and takes in\n\t// pre-filled UserDeviceKeyInfoMaps, and also calls\n\t// FinalizeRekey.\n\taddKeyGenerationForTest(codec kbfscodec.Codec, crypto cryptoPure,\n\t\tprevExtra ExtraMetadata,\n\t\tcurrCryptKey, nextCryptKey kbfscrypto.TLFCryptKey,\n\t\tpubKey kbfscrypto.TLFPublicKey,\n\t\twDkim, rDkim UserDeviceKeyInfoMap) ExtraMetadata\n\n\t// AddKeyGeneration adds a new key generation to this revision\n\t// of metadata. If StoresHistoricTLFCryptKeys is false, then\n\t// currCryptKey and nextCryptKey must be zero. Otherwise,\n\t// nextCryptKey must be non-zero, and currCryptKey must be\n\t// zero if there are no existing key generations, and non-zero\n\t// for otherwise.\n\t//\n\t// AddKeyGeneration must only be called on metadata for\n\t// private TLFs.\n\tAddKeyGeneration(codec kbfscodec.Codec, crypto cryptoPure,\n\t\tprevExtra ExtraMetadata,\n\t\tcurrCryptKey, nextCryptKey kbfscrypto.TLFCryptKey,\n\t\tpubKey kbfscrypto.TLFPublicKey) (ExtraMetadata, error)\n\n\t// UpdateKeyGeneration ensures that every device in the given\n\t// key generation for every writer and reader in the provided\n\t// lists has complete TLF crypt key info, and uses the new\n\t// ephemeral key pair to generate the info if it doesn't yet\n\t// exist.\n\t//\n\t// wKeys and rKeys usually contains the full maps of writers\n\t// to per-device crypt public keys, but for reader rekey,\n\t// wKeys will be empty and rKeys will contain only a single\n\t// entry.\n\t//\n\t// UpdateKeyGeneration must only be called on metadata for\n\t// private TLFs.\n\t//\n\t// TODO: Also handle reader promotion.\n\t//\n\t// TODO: Move the key generation handling into this function.\n\tUpdateKeyGeneration(crypto cryptoPure, keyGen KeyGen,\n\t\textra ExtraMetadata, wKeys, rKeys UserDevicePublicKeys,\n\t\tePubKey kbfscrypto.TLFEphemeralPublicKey,\n\t\tePrivKey kbfscrypto.TLFEphemeralPrivateKey,\n\t\ttlfCryptKey kbfscrypto.TLFCryptKey) (\n\t\tUserDeviceKeyServerHalves, error)\n\n\t// PromoteReader converts the given user from a reader to a writer.\n\tPromoteReader(uid keybase1.UID, extra ExtraMetadata) error\n\n\t// RevokeRemovedDevices removes key info for any device not in\n\t// the given maps, and returns a corresponding map of server\n\t// halves to delete from the server.\n\t//\n\t// Note: the returned server halves may not be for all key\n\t// generations, e.g. for MDv3 it's only for the latest key\n\t// generation.\n\tRevokeRemovedDevices(wKeys, rKeys UserDevicePublicKeys,\n\t\textra ExtraMetadata) (ServerHalfRemovalInfo, error)\n\n\t// FinalizeRekey must be called called after all rekeying work\n\t// has been performed on the underlying metadata.\n\tFinalizeRekey(c cryptoPure, extra ExtraMetadata) error\n}\n\n// KeyBundleCache is an interface to a key bundle cache for use with v3 metadata.\ntype KeyBundleCache interface {\n\t// GetTLFReaderKeyBundle returns the TLFReaderKeyBundleV3 for\n\t// the given TLFReaderKeyBundleID, or nil if there is none.\n\tGetTLFReaderKeyBundle(tlf.ID, TLFReaderKeyBundleID) (*TLFReaderKeyBundleV3, error)\n\t// GetTLFWriterKeyBundle returns the TLFWriterKeyBundleV3 for\n\t// the given TLFWriterKeyBundleID, or nil if there is none.\n\tGetTLFWriterKeyBundle(tlf.ID, TLFWriterKeyBundleID) (*TLFWriterKeyBundleV3, error)\n\t// PutTLFReaderKeyBundle stores the given TLFReaderKeyBundleV3.\n\tPutTLFReaderKeyBundle(tlf.ID, TLFReaderKeyBundleID, TLFReaderKeyBundleV3)\n\t// PutTLFWriterKeyBundle stores the given TLFWriterKeyBundleV3.\n\tPutTLFWriterKeyBundle(tlf.ID, TLFWriterKeyBundleID, TLFWriterKeyBundleV3)\n}\n", "idx": 9, "id": 14935, "msg": "", "proj": "keybase-kbfs", "lang": "go"}
{"patch": "@@ -2156,7 +2156,7 @@ export default function Core(rootElement, userSettings, rootInstanceSymbol = fal\n    * @returns {Array} Array of the column's cell values.\n    */\n   // TODO: Getting data from `sourceData` should work always on physical indexes.\n-  this.getSourceDataAtCol = function(column) {\n+  this.getSourceDataAtCol = function (column) {\n     return dataSource.getAtColumn(column);\n   };\n ", "y": 0, "oldf": "import numbro from 'numbro';\nimport {addClass, empty, isChildOfWebComponentTable, removeClass} from './helpers/dom/element';\nimport {columnFactory} from './helpers/setting';\nimport {isFunction} from './helpers/function';\nimport {isDefined, isUndefined, isRegExp, _injectProductInfo} from './helpers/mixed';\nimport {isMobileBrowser} from './helpers/browser';\nimport DataMap from './dataMap';\nimport EditorManager from './editorManager';\nimport EventManager from './eventManager';\nimport {deepClone, duckSchema, extend, isObject, isObjectEquals, deepObjectSize, hasOwnProperty, createObjectPropListener} from './helpers/object';\nimport {arrayFlatten, arrayMap} from './helpers/array';\nimport {getPlugin} from './plugins';\nimport {getRenderer} from './renderers';\nimport {getValidator} from './validators';\nimport {randomString} from './helpers/string';\nimport {rangeEach} from './helpers/number';\nimport TableView from './tableView';\nimport DataSource from './dataSource';\nimport {translateRowsToColumns, cellMethodLookupFactory, spreadsheetColumnLabel} from './helpers/data';\nimport {getTranslator} from './utils/recordTranslator';\nimport {registerAsRootInstance, hasValidParameter, isRootInstance} from './utils/rootInstance';\nimport {CellCoords, CellRange, ViewportColumnsCalculator} from './3rdparty/walkontable/src';\nimport Hooks from './pluginHooks';\nimport DefaultSettings from './defaultSettings';\nimport {getCellType} from './cellTypes';\n\nlet activeGuid = null;\n\n/**\n * Handsontable constructor\n *\n * @core\n * @dependencies numbro\n * @constructor Core\n * @description\n *\n * After Handsontable is constructed, you can modify the grid behavior using the available public methods.\n *\n * ---\n * ## How to call methods\n *\n * These are 2 equal ways to call a Handsontable method:\n *\n * ```js\n * // all following examples assume that you constructed Handsontable like this\n * var ht = new Handsontable(document.getElementById('example1'), options);\n *\n * // now, to use setDataAtCell method, you can either:\n * ht.setDataAtCell(0, 0, 'new value');\n * ```\n *\n * Alternatively, you can call the method using jQuery wrapper (__obsolete__, requires initialization using our jQuery guide\n * ```js\n *   $('#example1').handsontable('setDataAtCell', 0, 0, 'new value');\n * ```\n * ---\n */\nexport default function Core(rootElement, userSettings, rootInstanceSymbol = false) {\n  var priv,\n    datamap,\n    dataSource,\n    grid,\n    selection,\n    editorManager,\n    instance = this,\n    GridSettings = function() {\n    },\n    eventManager = new EventManager(instance);\n\n  extend(GridSettings.prototype, DefaultSettings.prototype); // create grid settings as a copy of default settings\n  extend(GridSettings.prototype, userSettings); // overwrite defaults with user settings\n  extend(GridSettings.prototype, expandType(userSettings));\n\n  if (hasValidParameter(rootInstanceSymbol)) {\n    registerAsRootInstance(this);\n  }\n\n  this.rootElement = rootElement;\n  this.isHotTableEnv = isChildOfWebComponentTable(this.rootElement);\n  EventManager.isHotTableEnv = this.isHotTableEnv;\n\n  this.container = document.createElement('div');\n  this.renderCall = false;\n\n  rootElement.insertBefore(this.container, rootElement.firstChild);\n\n  if (process.env.HOT_PACKAGE_TYPE !== '\\x63\\x65' && isRootInstance(this)) {\n    _injectProductInfo(userSettings.licenseKey, rootElement);\n  }\n\n  this.guid = `ht_${randomString()}`; // this is the namespace for global events\n\n  const recordTranslator = getTranslator(instance);\n\n  dataSource = new DataSource(instance);\n\n  if (!this.rootElement.id || this.rootElement.id.substring(0, 3) === 'ht_') {\n    this.rootElement.id = this.guid; // if root element does not have an id, assign a random id\n  }\n  priv = {\n    cellSettings: [],\n    columnSettings: [],\n    columnsSettingConflicts: ['data', 'width'],\n    settings: new GridSettings(), // current settings instance\n    selRange: null, // exposed by public method `getSelectedRange`\n    isPopulated: null,\n    scrollable: null,\n    firstRun: true,\n  };\n\n  grid = {\n    /**\n     * Inserts or removes rows and columns\n     *\n     * @memberof Core#\n     * @function alter\n     * @private\n     * @param {String} action Possible values: \"insert_row\", \"insert_col\", \"remove_row\", \"remove_col\"\n     * @param {Number} index\n     * @param {Number} amount\n     * @param {String} [source] Optional. Source of hook runner.\n     * @param {Boolean} [keepEmptyRows] Optional. Flag for preventing deletion of empty rows.\n     */\n    alter(action, index, amount, source, keepEmptyRows) {\n      var delta;\n\n      amount = amount || 1;\n\n      function spliceWith(data, index, count, toInject) {\n        let valueFactory = () => {\n          let result;\n\n          if (toInject === 'array') {\n            result = [];\n\n          } else if (toInject === 'object') {\n            result = {};\n          }\n\n          return result;\n        };\n        let spliceArgs = arrayMap(new Array(count), () => valueFactory());\n\n        spliceArgs.unshift(index, 0);\n        data.splice(...spliceArgs);\n      }\n\n      /* eslint-disable no-case-declarations */\n      switch (action) {\n        case 'insert_row':\n\n          const numberOfSourceRows = instance.countSourceRows();\n\n          if (instance.getSettings().maxRows === numberOfSourceRows) {\n            return;\n          }\n\n          index = (isDefined(index)) ? index : numberOfSourceRows;\n\n          delta = datamap.createRow(index, amount, source);\n          spliceWith(priv.cellSettings, index, amount, 'array');\n\n          if (delta) {\n            if (selection.isSelected() && priv.selRange.from.row >= index) {\n              priv.selRange.from.row += delta;\n              selection.transformEnd(delta, 0); // will call render() internally\n            } else {\n              selection.refreshBorders(); // it will call render and prepare methods\n            }\n          }\n          break;\n\n        case 'insert_col':\n          delta = datamap.createCol(index, amount, source);\n\n          for (let row = 0, len = instance.countSourceRows(); row < len; row++) {\n            if (priv.cellSettings[row]) {\n              spliceWith(priv.cellSettings[row], index, amount);\n            }\n          }\n\n          if (delta) {\n            if (Array.isArray(instance.getSettings().colHeaders)) {\n              var spliceArray = [index, 0];\n              spliceArray.length += delta; // inserts empty (undefined) elements at the end of an array\n              Array.prototype.splice.apply(instance.getSettings().colHeaders, spliceArray); // inserts empty (undefined) elements into the colHeader array\n            }\n\n            if (selection.isSelected() && priv.selRange.from.col >= index) {\n              priv.selRange.from.col += delta;\n              selection.transformEnd(0, delta); // will call render() internally\n            } else {\n              selection.refreshBorders(); // it will call render and prepare methods\n            }\n          }\n          break;\n\n        case 'remove_row':\n          datamap.removeRow(index, amount, source);\n          priv.cellSettings.splice(index, amount);\n\n          var totalRows = instance.countRows();\n          var fixedRowsTop = instance.getSettings().fixedRowsTop;\n          if (fixedRowsTop >= index + 1) {\n            instance.getSettings().fixedRowsTop -= Math.min(amount, fixedRowsTop - index);\n          }\n\n          var fixedRowsBottom = instance.getSettings().fixedRowsBottom;\n          if (fixedRowsBottom && index >= totalRows - fixedRowsBottom) {\n            instance.getSettings().fixedRowsBottom -= Math.min(amount, fixedRowsBottom);\n          }\n\n          grid.adjustRowsAndCols();\n          selection.refreshBorders(); // it will call render and prepare methods\n          break;\n\n        case 'remove_col':\n          let visualColumnIndex = recordTranslator.toPhysicalColumn(index);\n\n          datamap.removeCol(index, amount, source);\n\n          for (let row = 0, len = instance.countSourceRows(); row < len; row++) {\n            if (priv.cellSettings[row]) { // if row hasn't been rendered it wouldn't have cellSettings\n              priv.cellSettings[row].splice(visualColumnIndex, amount);\n            }\n          }\n          var fixedColumnsLeft = instance.getSettings().fixedColumnsLeft;\n\n          if (fixedColumnsLeft >= index + 1) {\n            instance.getSettings().fixedColumnsLeft -= Math.min(amount, fixedColumnsLeft - index);\n          }\n\n          if (Array.isArray(instance.getSettings().colHeaders)) {\n            if (typeof visualColumnIndex === 'undefined') {\n              visualColumnIndex = -1;\n            }\n            instance.getSettings().colHeaders.splice(visualColumnIndex, amount);\n          }\n\n          grid.adjustRowsAndCols();\n          selection.refreshBorders(); // it will call render and prepare methods\n\n          break;\n        default:\n          throw new Error(`There is no such action \"${action}\"`);\n      }\n\n      if (!keepEmptyRows) {\n        grid.adjustRowsAndCols(); // makes sure that we did not add rows that will be removed in next refresh\n      }\n    },\n\n    /**\n     * Makes sure there are empty rows at the bottom of the table\n     */\n    adjustRowsAndCols() {\n      if (priv.settings.minRows) {\n        // should I add empty rows to data source to meet minRows?\n        let rows = instance.countRows();\n\n        if (rows < priv.settings.minRows) {\n          for (let r = 0, minRows = priv.settings.minRows; r < minRows - rows; r++) {\n            datamap.createRow(instance.countRows(), 1, 'auto');\n          }\n        }\n      }\n      if (priv.settings.minSpareRows) {\n        let emptyRows = instance.countEmptyRows(true);\n\n        // should I add empty rows to meet minSpareRows?\n        if (emptyRows < priv.settings.minSpareRows) {\n          for (; emptyRows < priv.settings.minSpareRows && instance.countSourceRows() < priv.settings.maxRows; emptyRows++) {\n            datamap.createRow(instance.countRows(), 1, 'auto');\n          }\n        }\n      }\n      {\n        let emptyCols;\n\n        // count currently empty cols\n        if (priv.settings.minCols || priv.settings.minSpareCols) {\n          emptyCols = instance.countEmptyCols(true);\n        }\n\n        // should I add empty cols to meet minCols?\n        if (priv.settings.minCols && !priv.settings.columns && instance.countCols() < priv.settings.minCols) {\n          for (; instance.countCols() < priv.settings.minCols; emptyCols++) {\n            datamap.createCol(instance.countCols(), 1, 'auto');\n          }\n        }\n        // should I add empty cols to meet minSpareCols?\n        if (priv.settings.minSpareCols && !priv.settings.columns && instance.dataType === 'array' &&\n            emptyCols < priv.settings.minSpareCols) {\n          for (; emptyCols < priv.settings.minSpareCols && instance.countCols() < priv.settings.maxCols; emptyCols++) {\n            datamap.createCol(instance.countCols(), 1, 'auto');\n          }\n        }\n      }\n      let rowCount = instance.countRows();\n      let colCount = instance.countCols();\n\n      if (rowCount === 0 || colCount === 0) {\n        selection.deselect();\n      }\n\n      if (selection.isSelected()) {\n        let selectionChanged = false;\n        let fromRow = priv.selRange.from.row;\n        let fromCol = priv.selRange.from.col;\n        let toRow = priv.selRange.to.row;\n        let toCol = priv.selRange.to.col;\n\n        // if selection is outside, move selection to last row\n        if (fromRow > rowCount - 1) {\n          fromRow = rowCount - 1;\n          selectionChanged = true;\n\n          if (toRow > fromRow) {\n            toRow = fromRow;\n          }\n        } else if (toRow > rowCount - 1) {\n          toRow = rowCount - 1;\n          selectionChanged = true;\n\n          if (fromRow > toRow) {\n            fromRow = toRow;\n          }\n        }\n        // if selection is outside, move selection to last row\n        if (fromCol > colCount - 1) {\n          fromCol = colCount - 1;\n          selectionChanged = true;\n\n          if (toCol > fromCol) {\n            toCol = fromCol;\n          }\n        } else if (toCol > colCount - 1) {\n          toCol = colCount - 1;\n          selectionChanged = true;\n\n          if (fromCol > toCol) {\n            fromCol = toCol;\n          }\n        }\n\n        if (selectionChanged) {\n          instance.selectCell(fromRow, fromCol, toRow, toCol);\n        }\n      }\n      if (instance.view) {\n        instance.view.wt.wtOverlays.adjustElementsSize();\n      }\n    },\n\n    /**\n     * Populate the data from the provided 2d array from the given cell coordinates.\n     *\n     * @private\n     * @param {Object} start Start selection position. Visual indexes.\n     * @param {Array} input 2d data array.\n     * @param {Object} [end] End selection position (only for drag-down mode). Visual indexes.\n     * @param {String} [source=\"populateFromArray\"] Source information string.\n     * @param {String} [method=\"overwrite\"] Populate method. Possible options: `shift_down`, `shift_right`, `overwrite`.\n     * @param {String} direction (left|right|up|down) String specifying the direction.\n     * @param {Array} deltas The deltas array. A difference between values of adjacent cells.\n     *                       Useful **only** when the type of handled cells is `numeric`.\n     * @returns {Object|undefined} ending td in pasted area (only if any cell was changed).\n     */\n    populateFromArray(start, input, end, source, method, direction, deltas) {\n      // TODO: either remove or implement the `direction` argument. Currently it's not working at all.\n      var r,\n        rlen,\n        c,\n        clen,\n        setData = [],\n        current = {};\n\n      rlen = input.length;\n\n      if (rlen === 0) {\n        return false;\n      }\n\n      var repeatCol,\n        repeatRow,\n        cmax,\n        rmax,\n        baseEnd = {\n          row: end === null ? null : end.row,\n          col: end === null ? null : end.col\n        };\n\n      /* eslint-disable no-case-declarations */\n      // insert data with specified pasteMode method\n      switch (method) {\n        case 'shift_down' :\n          repeatCol = end ? end.col - start.col + 1 : 0;\n          repeatRow = end ? end.row - start.row + 1 : 0;\n          input = translateRowsToColumns(input);\n          for (c = 0, clen = input.length, cmax = Math.max(clen, repeatCol); c < cmax; c++) {\n            if (c < clen) {\n              for (r = 0, rlen = input[c].length; r < repeatRow - rlen; r++) {\n                input[c].push(input[c][r % rlen]);\n              }\n              input[c].unshift(start.col + c, start.row, 0);\n              instance.spliceCol(...input[c]);\n            } else {\n              input[c % clen][0] = start.col + c;\n              instance.spliceCol(...input[c % clen]);\n            }\n          }\n          break;\n\n        case 'shift_right':\n          repeatCol = end ? end.col - start.col + 1 : 0;\n          repeatRow = end ? end.row - start.row + 1 : 0;\n          for (r = 0, rlen = input.length, rmax = Math.max(rlen, repeatRow); r < rmax; r++) {\n            if (r < rlen) {\n              for (c = 0, clen = input[r].length; c < repeatCol - clen; c++) {\n                input[r].push(input[r][c % clen]);\n              }\n              input[r].unshift(start.row + r, start.col, 0);\n              instance.spliceRow(...input[r]);\n            } else {\n              input[r % rlen][0] = start.row + r;\n              instance.spliceRow(...input[r % rlen]);\n            }\n          }\n          break;\n\n        case 'overwrite':\n        default:\n          // overwrite and other not specified options\n          current.row = start.row;\n          current.col = start.col;\n\n          let selected = { // selected range\n            row: (end && start) ? (end.row - start.row + 1) : 1,\n            col: (end && start) ? (end.col - start.col + 1) : 1\n          };\n          let skippedRow = 0;\n          let skippedColumn = 0;\n          let pushData = true;\n          let cellMeta;\n\n          let getInputValue = function getInputValue(row, col = null) {\n            let rowValue = input[row % input.length];\n\n            if (col !== null) {\n              return rowValue[col % rowValue.length];\n            }\n\n            return rowValue;\n          };\n          let rowInputLength = input.length;\n          let rowSelectionLength = end ? end.row - start.row + 1 : 0;\n\n          if (end) {\n            rlen = rowSelectionLength;\n          } else {\n            rlen = Math.max(rowInputLength, rowSelectionLength);\n          }\n          for (r = 0; r < rlen; r++) {\n            if ((end && current.row > end.row && rowSelectionLength > rowInputLength) ||\n                (!priv.settings.allowInsertRow && current.row > instance.countRows() - 1) ||\n                (current.row >= priv.settings.maxRows)) {\n              break;\n            }\n            let visualRow = r - skippedRow;\n            let colInputLength = getInputValue(visualRow).length;\n            let colSelectionLength = end ? end.col - start.col + 1 : 0;\n\n            if (end) {\n              clen = colSelectionLength;\n            } else {\n              clen = Math.max(colInputLength, colSelectionLength);\n            }\n            current.col = start.col;\n            cellMeta = instance.getCellMeta(current.row, current.col);\n\n            if ((source === 'CopyPaste.paste' || source === 'Autofill.autofill') && cellMeta.skipRowOnPaste) {\n              skippedRow++;\n              current.row++;\n              rlen++;\n              /* eslint-disable no-continue */\n              continue;\n            }\n            skippedColumn = 0;\n\n            for (c = 0; c < clen; c++) {\n              if ((end && current.col > end.col && colSelectionLength > colInputLength) ||\n                  (!priv.settings.allowInsertColumn && current.col > instance.countCols() - 1) ||\n                  (current.col >= priv.settings.maxCols)) {\n                break;\n              }\n              cellMeta = instance.getCellMeta(current.row, current.col);\n\n              if ((source === 'CopyPaste.paste' || source === 'Autofill.fill') && cellMeta.skipColumnOnPaste) {\n                skippedColumn++;\n                current.col++;\n                clen++;\n                continue;\n              }\n              if (cellMeta.readOnly) {\n                current.col++;\n                /* eslint-disable no-continue */\n                continue;\n              }\n              let visualColumn = c - skippedColumn;\n              let value = getInputValue(visualRow, visualColumn);\n              let orgValue = instance.getDataAtCell(current.row, current.col);\n              let index = {\n                row: visualRow,\n                col: visualColumn\n              };\n\n              if (source === 'Autofill.fill') {\n                let result = instance.runHooks('beforeAutofillInsidePopulate', index, direction, input, deltas, {}, selected);\n\n                if (result) {\n                  value = isUndefined(result.value) ? value : result.value;\n                }\n              }\n              if (value !== null && typeof value === 'object') {\n                if (orgValue === null || typeof orgValue !== 'object') {\n                  pushData = false;\n\n                } else {\n                  let orgValueSchema = duckSchema(orgValue[0] || orgValue);\n                  let valueSchema = duckSchema(value[0] || value);\n\n                  /* eslint-disable max-depth */\n                  if (isObjectEquals(orgValueSchema, valueSchema)) {\n                    value = deepClone(value);\n                  } else {\n                    pushData = false;\n                  }\n                }\n\n              } else if (orgValue !== null && typeof orgValue === 'object') {\n                pushData = false;\n              }\n              if (pushData) {\n                setData.push([current.row, current.col, value]);\n              }\n              pushData = true;\n              current.col++;\n            }\n            current.row++;\n          }\n          instance.setDataAtCell(setData, null, null, source || 'populateFromArray');\n          break;\n      }\n    },\n  };\n\n  /* eslint-disable no-multi-assign */\n  this.selection = selection = { // this public assignment is only temporary\n    inProgress: false,\n\n    selectedHeader: {\n      cols: false,\n      rows: false,\n    },\n\n    /**\n     * @param {Boolean} [rows=false]\n     * @param {Boolean} [cols=false]\n     * @param {Boolean} [corner=false]\n     */\n    setSelectedHeaders(rows = false, cols = false, corner = false) {\n      instance.selection.selectedHeader.rows = rows;\n      instance.selection.selectedHeader.cols = cols;\n      instance.selection.selectedHeader.corner = corner;\n    },\n\n    /**\n     * Sets inProgress to `true`. This enables onSelectionEnd and onSelectionEndByProp to function as desired.\n     */\n    begin() {\n      instance.selection.inProgress = true;\n    },\n\n    /**\n     * Sets inProgress to `false`. Triggers onSelectionEnd and onSelectionEndByProp.\n     */\n    finish() {\n      var sel = instance.getSelected();\n      instance.runHooks('afterSelectionEnd', sel[0], sel[1], sel[2], sel[3]);\n      instance.runHooks('afterSelectionEndByProp', sel[0], instance.colToProp(sel[1]), sel[2], instance.colToProp(sel[3]));\n      instance.selection.inProgress = false;\n    },\n\n    /**\n     * @returns {Boolean}\n     */\n    isInProgress() {\n      return instance.selection.inProgress;\n    },\n\n    /**\n     * Starts selection range on given td object.\n     *\n     * @param {CellCoords} coords Visual coords.\n     * @param keepEditorOpened\n     */\n    setRangeStart(coords, keepEditorOpened) {\n      instance.runHooks('beforeSetRangeStart', coords);\n      priv.selRange = new CellRange(coords, coords, coords);\n      selection.setRangeEnd(coords, null, keepEditorOpened);\n    },\n\n    /**\n     * Starts selection range on given td object.\n     *\n     * @param {CellCoords} coords Visual coords.\n     * @param keepEditorOpened\n     */\n    setRangeStartOnly(coords) {\n      instance.runHooks('beforeSetRangeStartOnly', coords);\n      priv.selRange = new CellRange(coords, coords, coords);\n    },\n\n    /**\n     * Ends selection range on given td object.\n     *\n     * @param {CellCoords} coords Visual coords.\n     * @param {Boolean} [scrollToCell=true] If `true`, viewport will be scrolled to range end\n     * @param {Boolean} [keepEditorOpened] If `true`, cell editor will be still opened after changing selection range\n     */\n    setRangeEnd(coords, scrollToCell, keepEditorOpened) {\n      if (priv.selRange === null) {\n        return;\n      }\n      var disableVisualSelection,\n        isHeaderSelected = false,\n        areCoordsPositive = true;\n\n      var firstVisibleRow = instance.view.wt.wtTable.getFirstVisibleRow();\n      var firstVisibleColumn = instance.view.wt.wtTable.getFirstVisibleColumn();\n      var newRangeCoords = {\n        row: null,\n        col: null,\n      };\n\n      // trigger handlers\n      instance.runHooks('beforeSetRangeEnd', coords);\n      instance.selection.begin();\n\n      newRangeCoords.row = coords.row < 0 ? firstVisibleRow : coords.row;\n      newRangeCoords.col = coords.col < 0 ? firstVisibleColumn : coords.col;\n\n      priv.selRange.to = new CellCoords(newRangeCoords.row, newRangeCoords.col);\n\n      if (!priv.settings.multiSelect) {\n        priv.selRange.from = coords;\n      }\n      // set up current selection\n      instance.view.wt.selections.current.clear();\n\n      disableVisualSelection = instance.getCellMeta(priv.selRange.highlight.row, priv.selRange.highlight.col).disableVisualSelection;\n\n      if (typeof disableVisualSelection === 'string') {\n        disableVisualSelection = [disableVisualSelection];\n      }\n\n      if (disableVisualSelection === false ||\n          Array.isArray(disableVisualSelection) && disableVisualSelection.indexOf('current') === -1) {\n        instance.view.wt.selections.current.add(priv.selRange.highlight);\n      }\n      // set up area selection\n      instance.view.wt.selections.area.clear();\n\n      if ((disableVisualSelection === false ||\n          Array.isArray(disableVisualSelection) && disableVisualSelection.indexOf('area') === -1) &&\n          selection.isMultiple()) {\n        instance.view.wt.selections.area.add(priv.selRange.from);\n        instance.view.wt.selections.area.add(priv.selRange.to);\n      }\n      // set up highlight\n      if (priv.settings.currentHeaderClassName || priv.settings.currentRowClassName || priv.settings.currentColClassName) {\n        instance.view.wt.selections.highlight.clear();\n        instance.view.wt.selections.highlight.add(priv.selRange.from);\n        instance.view.wt.selections.highlight.add(priv.selRange.to);\n      }\n\n      const preventScrolling = createObjectPropListener('value');\n\n      // trigger handlers\n      instance.runHooks('afterSelection',\n        priv.selRange.from.row, priv.selRange.from.col, priv.selRange.to.row, priv.selRange.to.col, preventScrolling);\n      instance.runHooks('afterSelectionByProp',\n        priv.selRange.from.row, datamap.colToProp(priv.selRange.from.col), priv.selRange.to.row, datamap.colToProp(priv.selRange.to.col), preventScrolling);\n\n      if ((priv.selRange.from.row === 0 && priv.selRange.to.row === instance.countRows() - 1 && instance.countRows() > 1) ||\n        (priv.selRange.from.col === 0 && priv.selRange.to.col === instance.countCols() - 1 && instance.countCols() > 1)) {\n        isHeaderSelected = true;\n      }\n\n      if (coords.row < 0 || coords.col < 0) {\n        areCoordsPositive = false;\n      }\n\n      if (preventScrolling.isTouched()) {\n        scrollToCell = !preventScrolling.value;\n      }\n\n      if (scrollToCell !== false && !isHeaderSelected && areCoordsPositive) {\n        if (priv.selRange.from && !selection.isMultiple()) {\n          instance.view.scrollViewport(priv.selRange.from);\n        } else {\n          instance.view.scrollViewport(coords);\n        }\n      }\n\n      if (selection.selectedHeader.rows && selection.selectedHeader.cols) {\n        addClass(instance.rootElement, ['ht__selection--rows', 'ht__selection--columns']);\n\n      } else if (selection.selectedHeader.rows) {\n        removeClass(instance.rootElement, 'ht__selection--columns');\n        addClass(instance.rootElement, 'ht__selection--rows');\n\n      } else if (selection.selectedHeader.cols) {\n        removeClass(instance.rootElement, 'ht__selection--rows');\n        addClass(instance.rootElement, 'ht__selection--columns');\n\n      } else {\n        removeClass(instance.rootElement, ['ht__selection--rows', 'ht__selection--columns']);\n      }\n\n      selection.refreshBorders(null, keepEditorOpened);\n    },\n\n    /**\n     * Destroys editor, redraws borders around cells, prepares editor.\n     *\n     * @param {Boolean} [revertOriginal]\n     * @param {Boolean} [keepEditor]\n     */\n    refreshBorders(revertOriginal, keepEditor) {\n      if (!keepEditor) {\n        editorManager.destroyEditor(revertOriginal);\n      }\n      instance.view.render();\n\n      if (selection.isSelected() && !keepEditor) {\n        editorManager.prepareEditor();\n      }\n    },\n\n    /**\n     * Returns information if we have a multiselection.\n     *\n     * @returns {Boolean}\n     */\n    isMultiple() {\n      var\n        isMultiple = !(priv.selRange.to.col === priv.selRange.from.col && priv.selRange.to.row === priv.selRange.from.row),\n        modifier = instance.runHooks('afterIsMultipleSelection', isMultiple);\n\n      if (isMultiple) {\n        return modifier;\n      }\n    },\n\n    /**\n     * Selects cell relative to current cell (if possible).\n     */\n    transformStart(rowDelta, colDelta, force, keepEditorOpened) {\n      var delta = new CellCoords(rowDelta, colDelta),\n        rowTransformDir = 0,\n        colTransformDir = 0,\n        totalRows,\n        totalCols,\n        coords,\n        fixedRowsBottom;\n\n      instance.runHooks('modifyTransformStart', delta);\n      totalRows = instance.countRows();\n      totalCols = instance.countCols();\n      fixedRowsBottom = instance.getSettings().fixedRowsBottom;\n\n      if (priv.selRange.highlight.row + rowDelta > totalRows - 1) {\n        if (force && priv.settings.minSpareRows > 0 && !(fixedRowsBottom && priv.selRange.highlight.row >= totalRows - fixedRowsBottom - 1)) {\n          instance.alter('insert_row', totalRows);\n          totalRows = instance.countRows();\n\n        } else if (priv.settings.autoWrapCol) {\n          delta.row = 1 - totalRows;\n          delta.col = priv.selRange.highlight.col + delta.col == totalCols - 1 ? 1 - totalCols : 1;\n        }\n      } else if (priv.settings.autoWrapCol && priv.selRange.highlight.row + delta.row < 0 && priv.selRange.highlight.col + delta.col >= 0) {\n        delta.row = totalRows - 1;\n        delta.col = priv.selRange.highlight.col + delta.col == 0 ? totalCols - 1 : -1;\n      }\n\n      if (priv.selRange.highlight.col + delta.col > totalCols - 1) {\n        if (force && priv.settings.minSpareCols > 0) {\n          instance.alter('insert_col', totalCols);\n          totalCols = instance.countCols();\n\n        } else if (priv.settings.autoWrapRow) {\n          delta.row = priv.selRange.highlight.row + delta.row == totalRows - 1 ? 1 - totalRows : 1;\n          delta.col = 1 - totalCols;\n        }\n      } else if (priv.settings.autoWrapRow && priv.selRange.highlight.col + delta.col < 0 && priv.selRange.highlight.row + delta.row >= 0) {\n        delta.row = priv.selRange.highlight.row + delta.row == 0 ? totalRows - 1 : -1;\n        delta.col = totalCols - 1;\n      }\n\n      coords = new CellCoords(priv.selRange.highlight.row + delta.row, priv.selRange.highlight.col + delta.col);\n\n      if (coords.row < 0) {\n        rowTransformDir = -1;\n        coords.row = 0;\n\n      } else if (coords.row > 0 && coords.row >= totalRows) {\n        rowTransformDir = 1;\n        coords.row = totalRows - 1;\n      }\n\n      if (coords.col < 0) {\n        colTransformDir = -1;\n        coords.col = 0;\n\n      } else if (coords.col > 0 && coords.col >= totalCols) {\n        colTransformDir = 1;\n        coords.col = totalCols - 1;\n      }\n      instance.runHooks('afterModifyTransformStart', coords, rowTransformDir, colTransformDir);\n      selection.setRangeStart(coords, keepEditorOpened);\n    },\n\n    /**\n     * Sets selection end cell relative to current selection end cell (if possible).\n     */\n    transformEnd(rowDelta, colDelta) {\n      var delta = new CellCoords(rowDelta, colDelta),\n        rowTransformDir = 0,\n        colTransformDir = 0,\n        totalRows,\n        totalCols,\n        coords;\n\n      instance.runHooks('modifyTransformEnd', delta);\n\n      totalRows = instance.countRows();\n      totalCols = instance.countCols();\n      coords = new CellCoords(priv.selRange.to.row + delta.row, priv.selRange.to.col + delta.col);\n\n      if (coords.row < 0) {\n        rowTransformDir = -1;\n        coords.row = 0;\n\n      } else if (coords.row > 0 && coords.row >= totalRows) {\n        rowTransformDir = 1;\n        coords.row = totalRows - 1;\n      }\n\n      if (coords.col < 0) {\n        colTransformDir = -1;\n        coords.col = 0;\n\n      } else if (coords.col > 0 && coords.col >= totalCols) {\n        colTransformDir = 1;\n        coords.col = totalCols - 1;\n      }\n      instance.runHooks('afterModifyTransformEnd', coords, rowTransformDir, colTransformDir);\n      selection.setRangeEnd(coords, true);\n    },\n\n    /**\n     * Returns `true` if currently there is a selection on screen, `false` otherwise.\n     *\n     * @returns {Boolean}\n     */\n    isSelected() {\n      return (priv.selRange !== null);\n    },\n\n    /**\n     * Returns `true` if coords is within current selection coords.\n     *\n     * @param {CellCoords} coords\n     * @returns {Boolean}\n     */\n    inInSelection(coords) {\n      if (!selection.isSelected()) {\n        return false;\n      }\n\n      return priv.selRange.includes(coords);\n    },\n\n    /**\n     * Deselects all selected cells\n     */\n    deselect() {\n      if (!selection.isSelected()) {\n        return;\n      }\n      instance.selection.inProgress = false; // needed by HT inception\n      priv.selRange = null;\n      instance.view.wt.selections.current.clear();\n      instance.view.wt.selections.area.clear();\n      if (priv.settings.currentHeaderClassName || priv.settings.currentRowClassName || priv.settings.currentColClassName) {\n        instance.view.wt.selections.highlight.clear();\n      }\n      editorManager.destroyEditor();\n      selection.refreshBorders();\n      removeClass(instance.rootElement, ['ht__selection--rows', 'ht__selection--columns']);\n      instance.runHooks('afterDeselect');\n    },\n\n    /**\n     * Select all cells\n     */\n    selectAll() {\n      if (!priv.settings.multiSelect) {\n        return;\n      }\n      selection.setSelectedHeaders(true, true, true);\n      selection.setRangeStart(new CellCoords(0, 0));\n      selection.setRangeEnd(new CellCoords(instance.countRows() - 1, instance.countCols() - 1), false);\n    },\n\n    /**\n     * Deletes data from selected cells\n     */\n    empty() {\n      if (!selection.isSelected()) {\n        return;\n      }\n      var topLeft = priv.selRange.getTopLeftCorner();\n      var bottomRight = priv.selRange.getBottomRightCorner();\n      var r,\n        c,\n        changes = [];\n\n      for (r = topLeft.row; r <= bottomRight.row; r++) {\n        for (c = topLeft.col; c <= bottomRight.col; c++) {\n          if (!instance.getCellMeta(r, c).readOnly) {\n            changes.push([r, c, '']);\n          }\n        }\n      }\n      instance.setDataAtCell(changes);\n    },\n  };\n\n  this.init = function() {\n    dataSource.setData(priv.settings.data);\n    instance.runHooks('beforeInit');\n\n    if (isMobileBrowser()) {\n      addClass(instance.rootElement, 'mobile');\n    }\n\n    this.updateSettings(priv.settings, true);\n\n    this.view = new TableView(this);\n    editorManager = new EditorManager(instance, priv, selection, datamap);\n\n    this.forceFullRender = true; // used when data was changed\n\n    instance.runHooks('init');\n    this.view.render();\n\n    if (typeof priv.firstRun === 'object') {\n      instance.runHooks('afterChange', priv.firstRun[0], priv.firstRun[1]);\n      priv.firstRun = false;\n    }\n    instance.runHooks('afterInit');\n  };\n\n  function ValidatorsQueue() { // moved this one level up so it can be used in any function here. Probably this should be moved to a separate file\n    var resolved = false;\n\n    return {\n      validatorsInQueue: 0,\n      valid: true,\n      addValidatorToQueue() {\n        this.validatorsInQueue++;\n        resolved = false;\n      },\n      removeValidatorFormQueue() {\n        this.validatorsInQueue = this.validatorsInQueue - 1 < 0 ? 0 : this.validatorsInQueue - 1;\n        this.checkIfQueueIsEmpty();\n      },\n      onQueueEmpty(valid) {\n      },\n      checkIfQueueIsEmpty() {\n        if (this.validatorsInQueue == 0 && resolved == false) {\n          resolved = true;\n          this.onQueueEmpty(this.valid);\n        }\n      }\n    };\n  }\n\n  function validateChanges(changes, source, callback) {\n    var waitingForValidator = new ValidatorsQueue();\n    waitingForValidator.onQueueEmpty = resolve;\n\n    for (var i = changes.length - 1; i >= 0; i--) {\n      if (changes[i] === null) {\n        changes.splice(i, 1);\n      } else {\n        var row = changes[i][0];\n        var col = datamap.propToCol(changes[i][1]);\n\n        var cellProperties = instance.getCellMeta(row, col);\n\n        if (cellProperties.type === 'numeric' && typeof changes[i][3] === 'string') {\n          if (changes[i][3].length > 0 && (/^-?[\\d\\s]*(\\.|,)?\\d*$/.test(changes[i][3]) || cellProperties.format)) {\n            var len = changes[i][3].length;\n\n            if (isUndefined(cellProperties.language)) {\n              numbro.culture('en-US');\n\n            } else if (changes[i][3].indexOf('.') === len - 3 && changes[i][3].indexOf(',') === -1) {\n              // this input in format XXXX.XX is likely to come from paste. Let's parse it using international rules\n              numbro.culture('en-US');\n            } else {\n\n              numbro.culture(cellProperties.language);\n            }\n\n            const {delimiters} = numbro.cultureData(numbro.culture());\n\n            // try to parse to float - https://github.com/foretagsplatsen/numbro/pull/183\n            if (numbro.validate(changes[i][3]) && !isNaN(changes[i][3])) {\n              changes[i][3] = parseFloat(changes[i][3]);\n\n            } else {\n              changes[i][3] = numbro().unformat(changes[i][3]) || changes[i][3];\n            }\n          }\n        }\n\n        /* eslint-disable no-loop-func */\n        if (instance.getCellValidator(cellProperties)) {\n          waitingForValidator.addValidatorToQueue();\n          instance.validateCell(changes[i][3], cellProperties, (function(i, cellProperties) {\n            return function(result) {\n              if (typeof result !== 'boolean') {\n                throw new Error('Validation error: result is not boolean');\n              }\n              if (result === false && cellProperties.allowInvalid === false) {\n                changes.splice(i, 1); // cancel the change\n                cellProperties.valid = true; // we cancelled the change, so cell value is still valid\n                const cell = instance.getCell(cellProperties.visualRow, cellProperties.visualCol);\n                removeClass(cell, instance.getSettings().invalidCellClassName);\n                --i;\n              }\n              waitingForValidator.removeValidatorFormQueue();\n            };\n          }(i, cellProperties)), source);\n        }\n      }\n    }\n    waitingForValidator.checkIfQueueIsEmpty();\n\n    function resolve() {\n      var beforeChangeResult;\n\n      if (changes.length) {\n        beforeChangeResult = instance.runHooks('beforeChange', changes, source);\n        if (isFunction(beforeChangeResult)) {\n          console.warn('Your beforeChange callback returns a function. It\\'s not supported since Handsontable 0.12.1 (and the returned function will not be executed).');\n        } else if (beforeChangeResult === false) {\n          changes.splice(0, changes.length); // invalidate all changes (remove everything from array)\n        }\n      }\n      callback(); // called when async validators are resolved and beforeChange was not async\n    }\n  }\n\n  /**\n   * Internal function to apply changes. Called after validateChanges\n   *\n   * @private\n   * @param {Array} changes Array in form of [row, prop, oldValue, newValue]\n   * @param {String} source String that identifies how this change will be described in changes array (useful in onChange callback)\n   * @fires Hooks#beforeChangeRender\n   * @fires Hooks#afterChange\n   */\n  function applyChanges(changes, source) {\n    let i = changes.length - 1;\n\n    if (i < 0) {\n      return;\n    }\n\n    for (; i >= 0; i--) {\n      let skipThisChange = false;\n\n      if (changes[i] === null) {\n        changes.splice(i, 1);\n        /* eslint-disable no-continue */\n        continue;\n      }\n\n      if (changes[i][2] == null && changes[i][3] == null) {\n        /* eslint-disable no-continue */\n        continue;\n      }\n\n      if (priv.settings.allowInsertRow) {\n        while (changes[i][0] > instance.countRows() - 1) {\n          let numberOfCreatedRows = datamap.createRow(void 0, void 0, source);\n\n          if (numberOfCreatedRows === 0) {\n            skipThisChange = true;\n            break;\n          }\n        }\n      }\n\n      if (skipThisChange) {\n        /* eslint-disable no-continue */\n        continue;\n      }\n\n      if (instance.dataType === 'array' && (!priv.settings.columns || priv.settings.columns.length === 0) && priv.settings.allowInsertColumn) {\n        while (datamap.propToCol(changes[i][1]) > instance.countCols() - 1) {\n          datamap.createCol(void 0, void 0, source);\n        }\n      }\n\n      datamap.set(changes[i][0], changes[i][1], changes[i][3]);\n    }\n\n    instance.forceFullRender = true; // used when data was changed\n    grid.adjustRowsAndCols();\n    instance.runHooks('beforeChangeRender', changes, source);\n    selection.refreshBorders(null, true);\n    instance.view.wt.wtOverlays.adjustElementsSize();\n    instance.runHooks('afterChange', changes, source || 'edit');\n\n    let activeEditor = instance.getActiveEditor();\n\n    if (activeEditor && isDefined(activeEditor.refreshValue)) {\n      activeEditor.refreshValue();\n    }\n  }\n\n  this.validateCell = function(value, cellProperties, callback, source) {\n    var validator = instance.getCellValidator(cellProperties);\n\n    function done(valid) {\n      var col = cellProperties.visualCol,\n        row = cellProperties.visualRow,\n        td = instance.getCell(row, col, true);\n\n      if (td && td.nodeName != 'TH') {\n        instance.view.wt.wtSettings.settings.cellRenderer(row, col, td);\n      }\n      callback(valid);\n    }\n\n    if (isRegExp(validator)) {\n      validator = (function(validator) {\n        return function(value, callback) {\n          callback(validator.test(value));\n        };\n      }(validator));\n    }\n\n    if (isFunction(validator)) {\n\n      value = instance.runHooks('beforeValidate', value, cellProperties.visualRow, cellProperties.prop, source);\n\n      // To provide consistent behaviour, validation should be always asynchronous\n      instance._registerTimeout(setTimeout(() => {\n        validator.call(cellProperties, value, (valid) => {\n          valid = instance.runHooks('afterValidate', valid, value, cellProperties.visualRow, cellProperties.prop, source);\n          cellProperties.valid = valid;\n\n          done(valid);\n          instance.runHooks('postAfterValidate', valid, value, cellProperties.visualRow, cellProperties.prop, source);\n        });\n      }, 0));\n\n    } else {\n      // resolve callback even if validator function was not found\n      instance._registerTimeout(setTimeout(() => {\n        cellProperties.valid = true;\n        done(cellProperties.valid);\n      }, 0));\n    }\n  };\n\n  function setDataInputToArray(row, propOrCol, value) {\n    if (typeof row === 'object') { // is it an array of changes\n      return row;\n    }\n    return [\n      [row, propOrCol, value]\n    ];\n  }\n\n  /**\n   * @description\n   * Set new value to a cell. To change many cells at once, pass an array of `changes` in format `[[row, col, value], ...]` as\n   * the only parameter. `col` is the index of a __visible__ column (note that if columns were reordered,\n   * the current visible order will be used). `source` is a flag for before/afterChange events. If you pass only array of\n   * changes then `source` could be set as second parameter.\n   *\n   * @memberof Core#\n   * @function setDataAtCell\n   * @param {Number|Array} row Visual row index or array of changes in format `[[row, col, value], ...]`.\n   * @param {Number} col Visual column index.\n   * @param {String} value New value.\n   * @param {String} [source] String that identifies how this change will be described in the changes array (useful in onAfterChange or onBeforeChange callback).\n   */\n  this.setDataAtCell = function(row, col, value, source) {\n    var\n      input = setDataInputToArray(row, col, value),\n      i,\n      ilen,\n      changes = [],\n      prop;\n\n    for (i = 0, ilen = input.length; i < ilen; i++) {\n      if (typeof input[i] !== 'object') {\n        throw new Error('Method `setDataAtCell` accepts row number or changes array of arrays as its first parameter');\n      }\n      if (typeof input[i][1] !== 'number') {\n        throw new Error('Method `setDataAtCell` accepts row and column number as its parameters. If you want to use object property name, use method `setDataAtRowProp`');\n      }\n      prop = datamap.colToProp(input[i][1]);\n      changes.push([\n        input[i][0],\n        prop,\n        dataSource.getAtCell(recordTranslator.toPhysicalRow(input[i][0]), input[i][1]),\n        input[i][2],\n      ]);\n    }\n\n    if (!source && typeof row === 'object') {\n      source = col;\n    }\n\n    instance.runHooks('afterSetDataAtCell', changes, source);\n\n    validateChanges(changes, source, () => {\n      applyChanges(changes, source);\n    });\n  };\n\n  /**\n   * @description\n   * Set new value to a cell. To change many cells at once, pass an array of `changes` in format `[[row, prop, value], ...]` as\n   * the only parameter. `prop` is the name of the object property (e.g. `first.name`). `source` is a flag for before/afterChange events.\n   * If you pass only array of changes then `source` could be set as second parameter.\n   *\n   * @memberof Core#\n   * @function setDataAtRowProp\n   * @param {Number|Array} row Visual row index or array of changes in format `[[row, prop, value], ...]`.\n   * @param {String} prop Property name or the source string.\n   * @param {String} value Value to be set.\n   * @param {String} [source] String that identifies how this change will be described in changes array (useful in onChange callback).\n   */\n  this.setDataAtRowProp = function(row, prop, value, source) {\n    var input = setDataInputToArray(row, prop, value),\n      i,\n      ilen,\n      changes = [];\n\n    for (i = 0, ilen = input.length; i < ilen; i++) {\n      changes.push([\n        input[i][0],\n        input[i][1],\n        dataSource.getAtCell(recordTranslator.toPhysicalRow(input[i][0]), input[i][1]),\n        input[i][2],\n      ]);\n    }\n\n    if (!source && typeof row === 'object') {\n      source = prop;\n    }\n\n    instance.runHooks('afterSetDataAtRowProp', changes, source);\n\n    validateChanges(changes, source, () => {\n      applyChanges(changes, source);\n    });\n  };\n\n  /**\n   * Listen to the keyboard input on document body.\n   *\n   * @memberof Core#\n   * @function listen\n   * @since 0.11\n   */\n  this.listen = function() {\n    let invalidActiveElement = !document.activeElement || (document.activeElement && document.activeElement.nodeName === void 0);\n\n    if (document.activeElement && document.activeElement !== document.body && !invalidActiveElement) {\n      document.activeElement.blur();\n\n    } else if (invalidActiveElement) { // IE\n      document.body.focus();\n    }\n\n    activeGuid = instance.guid;\n  };\n\n  /**\n   * Stop listening to keyboard input on the document body.\n   *\n   * @memberof Core#\n   * @function unlisten\n   * @since 0.11\n   */\n  this.unlisten = function() {\n    if (this.isListening()) {\n      activeGuid = null;\n    }\n  };\n\n  /**\n   * Returns `true` if the current Handsontable instance is listening to keyboard input on document body.\n   *\n   * @memberof Core#\n   * @function isListening\n   * @since 0.11\n   * @returns {Boolean} `true` if the instance is listening, `false` otherwise.\n   */\n  this.isListening = function() {\n    return activeGuid === instance.guid;\n  };\n\n  /**\n   * Destroys the current editor, renders and selects the current cell.\n   *\n   * @memberof Core#\n   * @function destroyEditor\n   * @param {Boolean} [revertOriginal] If != `true`, edited data is saved. Otherwise the previous value is restored.\n   */\n  this.destroyEditor = function(revertOriginal) {\n    selection.refreshBorders(revertOriginal);\n  };\n\n  /**\n   * Populate cells at position with 2D input array (e.g. `[[1, 2], [3, 4]]`).\n   * Use `endRow`, `endCol` when you want to cut input when a certain row is reached.\n   * Optional `source` parameter (default value \"populateFromArray\") is used to identify this call in the resulting events (beforeChange, afterChange).\n   * Optional `populateMethod` parameter (default value \"overwrite\", possible values \"shift_down\" and \"shift_right\")\n   * has the same effect as pasteMode option {@link Options#pasteMode}\n   *\n   * @memberof Core#\n   * @function populateFromArray\n   * @since 0.9.0\n   * @param {Number} row Start visual row index.\n   * @param {Number} col Start visual column index.\n   * @param {Array} input 2d array\n   * @param {Number} [endRow] End visual row index (use when you want to cut input when certain row is reached).\n   * @param {Number} [endCol] End visual column index (use when you want to cut input when certain column is reached).\n   * @param {String} [source=\"populateFromArray\"] Source string.\n   * @param {String} [method=\"overwrite\"] Populate method. Possible options: `shift_down`, `shift_right`, `overwrite`.\n   * @param {String} direction Populate direction. (left|right|up|down)\n   * @param {Array} deltas Deltas array.\n   * @returns {Object|undefined} The ending TD element in pasted area (only if any cells were changed).\n   */\n  this.populateFromArray = function(row, col, input, endRow, endCol, source, method, direction, deltas) {\n    var c;\n\n    if (!(typeof input === 'object' && typeof input[0] === 'object')) {\n      throw new Error('populateFromArray parameter `input` must be an array of arrays'); // API changed in 0.9-beta2, let's check if you use it correctly\n    }\n    c = typeof endRow === 'number' ? new CellCoords(endRow, endCol) : null;\n\n    return grid.populateFromArray(new CellCoords(row, col), input, c, source, method, direction, deltas);\n  };\n\n  /**\n   * Adds/removes data from the column. This function is modelled after Array.splice.\n   * Parameter `col` is the index of the column in which do you want to do splice.\n   * Parameter `index` is the row index at which to start changing the array.\n   * If negative, will begin that many elements from the end. Parameter `amount`, is the number of the old array elements to remove.\n   * If the amount is 0, no elements are removed. Fourth and further parameters are the `elements` to add to the array.\n   * If you don't specify any elements, spliceCol simply removes elements from the array.\n   * {@link DataMap#spliceCol}\n   *\n   * @memberof Core#\n   * @function spliceCol\n   * @since 0.9-beta2\n   * @param {Number} col Index of the column in which do you want to do splice.\n   * @param {Number} index Index at which to start changing the array. If negative, will begin that many elements from the end.\n   * @param {Number} amount An integer indicating the number of old array elements to remove. If amount is 0, no elements are removed.\n   * @param {*} [elements] The elements to add to the array. If you don't specify any elements, spliceCol simply removes elements from the array.\n   */\n  this.spliceCol = function(col, index, amount/* , elements... */) {\n    return datamap.spliceCol(...arguments);\n  };\n\n  /**\n   * Adds/removes data from the row. This function works is modelled after Array.splice.\n   * Parameter `row` is the index of row in which do you want to do splice.\n   * Parameter `index` is the column index at which to start changing the array.\n   * If negative, will begin that many elements from the end. Parameter `amount`, is the number of old array elements to remove.\n   * If the amount is 0, no elements are removed. Fourth and further parameters are the `elements` to add to the array.\n   * If you don't specify any elements, spliceCol simply removes elements from the array.\n   * {@link DataMap#spliceRow}\n   *\n   * @memberof Core#\n   * @function spliceRow\n   * @since 0.11\n   * @param {Number} row Index of column in which do you want to do splice.\n   * @param {Number} index Index at which to start changing the array. If negative, will begin that many elements from the end.\n   * @param {Number} amount An integer indicating the number of old array elements to remove. If amount is 0, no elements are removed.\n   * @param {*} [elements] The elements to add to the array. If you don't specify any elements, spliceCol simply removes elements from the array.\n   */\n  this.spliceRow = function(row, index, amount/* , elements... */) {\n    return datamap.spliceRow(...arguments);\n  };\n\n  /**\n   * Returns indexes of the currently selected cells as an array `[startRow, startCol, endRow, endCol]`.\n   *\n   * Start row and start col are the coordinates of the active cell (where the selection was started).\n   *\n   * @memberof Core#\n   * @function getSelected\n   * @returns {Array} Array of the selection's indexes.\n   */\n  this.getSelected = function() { // https://github.com/handsontable/handsontable/issues/44  //cjl\n    if (selection.isSelected()) {\n      return [priv.selRange.from.row, priv.selRange.from.col, priv.selRange.to.row, priv.selRange.to.col];\n    }\n  };\n\n  /**\n   * Returns the current selection as a CellRange object.\n   *\n   * @memberof Core#\n   * @function getSelectedRange\n   * @since 0.11\n   * @returns {CellRange} Selected range object or undefined` if there is no selection.\n   */\n  this.getSelectedRange = function() { // https://github.com/handsontable/handsontable/issues/44  //cjl\n    if (selection.isSelected()) {\n      return priv.selRange;\n    }\n  };\n\n  /**\n   * Rerender the table.\n   *\n   * @memberof Core#\n   * @function render\n   */\n  this.render = function() {\n    if (instance.view) {\n      instance.renderCall = true;\n      instance.forceFullRender = true; // used when data was changed\n      selection.refreshBorders(null, true);\n    }\n  };\n\n  /**\n   * Reset all cells in the grid to contain data from the data array.\n   *\n   * @memberof Core#\n   * @function loadData\n   * @param {Array} data Array of arrays or array of objects containing data.\n   * @fires Hooks#afterLoadData\n   * @fires Hooks#afterChange\n   */\n  this.loadData = function(data) {\n    if (Array.isArray(priv.settings.dataSchema)) {\n      instance.dataType = 'array';\n    } else if (isFunction(priv.settings.dataSchema)) {\n      instance.dataType = 'function';\n    } else {\n      instance.dataType = 'object';\n    }\n\n    if (datamap) {\n      datamap.destroy();\n    }\n    datamap = new DataMap(instance, priv, GridSettings);\n\n    if (typeof data === 'object' && data !== null) {\n      if (!(data.push && data.splice)) { // check if data is array. Must use duck-type check so Backbone Collections also pass it\n        // when data is not an array, attempt to make a single-row array of it\n        data = [data];\n      }\n\n    } else if (data === null) {\n      data = [];\n      var row;\n      var r = 0;\n      var rlen = 0;\n      var dataSchema = datamap.getSchema();\n\n      for (r = 0, rlen = priv.settings.startRows; r < rlen; r++) {\n        if ((instance.dataType === 'object' || instance.dataType === 'function') && priv.settings.dataSchema) {\n          row = deepClone(dataSchema);\n          data.push(row);\n\n        } else if (instance.dataType === 'array') {\n          row = deepClone(dataSchema[0]);\n          data.push(row);\n\n        } else {\n          row = [];\n\n          for (var c = 0, clen = priv.settings.startCols; c < clen; c++) {\n            row.push(null);\n          }\n\n          data.push(row);\n        }\n      }\n\n    } else {\n      throw new Error(`loadData only accepts array of objects or array of arrays (${typeof data} given)`);\n    }\n\n    priv.isPopulated = false;\n    GridSettings.prototype.data = data;\n\n    if (Array.isArray(data[0])) {\n      instance.dataType = 'array';\n    }\n\n    datamap.dataSource = data;\n    dataSource.data = data;\n    dataSource.dataType = instance.dataType;\n    dataSource.colToProp = datamap.colToProp.bind(datamap);\n    dataSource.propToCol = datamap.propToCol.bind(datamap);\n\n    clearCellSettingCache();\n\n    grid.adjustRowsAndCols();\n    instance.runHooks('afterLoadData', priv.firstRun);\n\n    if (priv.firstRun) {\n      priv.firstRun = [null, 'loadData'];\n    } else {\n      instance.runHooks('afterChange', null, 'loadData');\n      instance.render();\n    }\n    priv.isPopulated = true;\n\n    function clearCellSettingCache() {\n      priv.cellSettings.length = 0;\n    }\n  };\n\n  /**\n   * Returns the current data object (the same one that was passed by `data` configuration option or `loadData` method,\n   * unless the `modifyRow` hook was used to trim some of the rows. If that's the case - use the {@link Core#getSourceData} method.).\n   * Optionally you can provide cell range by defining `row`, `col`, `row2`, `col2` to get only a fragment of grid data.\n   *\n   * Note: getData functionality changed with the release of version 0.20. If you're looking for the previous functionality,\n   * you should use the {@link Core#getSourceData} method.\n   *\n   * @memberof Core#\n   * @function getData\n   * @param {Number} [r] From visual row index.\n   * @param {Number} [c] From visual column index.\n   * @param {Number} [r2] To visual row index.\n   * @param {Number} [c2] To visual column index.\n   * @returns {Array} Array with the data.\n   */\n  this.getData = function(r, c, r2, c2) {\n    if (isUndefined(r)) {\n      return datamap.getAll();\n    }\n    return datamap.getRange(new CellCoords(r, c), new CellCoords(r2, c2), datamap.DESTINATION_RENDERER);\n\n  };\n\n  /**\n   * Returns a string value of the selected range. Each column is separated by tab, each row is separated by a new line character.\n   * {@link DataMap#getCopyableText}\n   *\n   * @memberof Core#\n   * @function getCopyableText\n   * @since 0.11\n   * @param {Number} startRow From visual row index.\n   * @param {Number} startCol From visual column index.\n   * @param {Number} endRow To visual row index.\n   * @param {Number} endCol To visual column index.\n   * @returns {String}\n   */\n  this.getCopyableText = function(startRow, startCol, endRow, endCol) {\n    return datamap.getCopyableText(new CellCoords(startRow, startCol), new CellCoords(endRow, endCol));\n  };\n\n  /**\n   * Returns the data's copyable value at specified row and column index ({@link DataMap#getCopyable}).\n   *\n   * @memberof Core#\n   * @function getCopyableData\n   * @since 0.19.0\n   * @param {Number} row Visual row index.\n   * @param {Number} column Visual column index.\n   * @returns {String}\n   */\n  this.getCopyableData = function(row, column) {\n    return datamap.getCopyable(row, datamap.colToProp(column));\n  };\n\n  /**\n   * Returns schema provided by constructor settings. If it doesn't exist then it returns the schema based on the data\n   * structure in the first row.\n   *\n   * @memberof Core#\n   * @function getSchema\n   * @since 0.13.2\n   * @returns {Object} Schema object.\n   */\n  this.getSchema = function() {\n    return datamap.getSchema();\n  };\n\n  /**\n   * Use it if you need to change configuration after initialization. The `settings` parameter is an object containing the new\n   * settings, declared the same way as in the initial settings object.\n   * Note, that although the `updateSettings` method doesn't overwrite the previously declared settings, it might reset\n   * the settings made post-initialization. (for example - ignore changes made using the columnResize feature).\n   *\n   * @memberof Core#\n   * @function updateSettings\n   * @param {Object} settings New settings object.\n   * @param {Boolean} init Calls this method in the initialization mode. Internal use only.\n   *                       Used by API could be cause of the unexpected behaviour of the Handsontable.\n   * @example\n   * ```js\n   * hot.updateSettings({\n   *    contextMenu: true,\n   *    colHeaders: true,\n   *    fixedRowsTop: 2\n   * });\n   * ```\n   * @fires Hooks#afterCellMetaReset\n   * @fires Hooks#afterUpdateSettings\n   */\n  this.updateSettings = function(settings, init) {\n    let columnsAsFunc = false;\n    let i;\n    let j;\n    let clen;\n\n    if (isDefined(settings.rows)) {\n      throw new Error('\"rows\" setting is no longer supported. do you mean startRows, minRows or maxRows?');\n    }\n    if (isDefined(settings.cols)) {\n      throw new Error('\"cols\" setting is no longer supported. do you mean startCols, minCols or maxCols?');\n    }\n\n    for (i in settings) {\n      if (i === 'data') {\n        /* eslint-disable no-continue */\n        continue; // loadData will be triggered later\n\n      } else if (Hooks.getSingleton().getRegistered().indexOf(i) > -1) {\n        if (isFunction(settings[i]) || Array.isArray(settings[i])) {\n          settings[i].initialHook = true;\n          instance.addHook(i, settings[i]);\n        }\n\n      } else if (!init && hasOwnProperty(settings, i)) { // Update settings\n        GridSettings.prototype[i] = settings[i];\n      }\n    }\n\n    // Load data or create data map\n    if (settings.data === void 0 && priv.settings.data === void 0) {\n      instance.loadData(null); // data source created just now\n\n    } else if (settings.data !== void 0) {\n      instance.loadData(settings.data); // data source given as option\n\n    } else if (settings.columns !== void 0) {\n      datamap.createMap();\n    }\n\n    clen = instance.countCols();\n\n    const columnSetting = settings.columns || GridSettings.prototype.columns;\n\n    // Init columns constructors configuration\n    if (columnSetting && isFunction(columnSetting)) {\n      clen = instance.countSourceCols();\n      columnsAsFunc = true;\n    }\n\n    // Clear cellSettings cache\n    if (settings.cell !== void 0 || settings.cells !== void 0 || settings.columns !== void 0) {\n      priv.cellSettings.length = 0;\n    }\n\n    if (clen > 0) {\n      let proto;\n      let column;\n\n      for (i = 0, j = 0; i < clen; i++) {\n        if (columnsAsFunc && !columnSetting(i)) {\n          /* eslint-disable no-continue */\n          continue;\n        }\n        priv.columnSettings[j] = columnFactory(GridSettings, priv.columnsSettingConflicts);\n\n        // shortcut for prototype\n        proto = priv.columnSettings[j].prototype;\n\n        // Use settings provided by user\n        if (columnSetting) {\n          if (columnsAsFunc) {\n            column = columnSetting(i);\n\n          } else {\n            column = columnSetting[j];\n          }\n\n          if (column) {\n            extend(proto, column);\n            extend(proto, expandType(column));\n          }\n        }\n\n        j++;\n      }\n    }\n\n    if (isDefined(settings.cell)) {\n      for (let key in settings.cell) {\n        if (hasOwnProperty(settings.cell, key)) {\n          let cell = settings.cell[key];\n\n          instance.setCellMetaObject(cell.row, cell.col, cell);\n        }\n      }\n    }\n\n    instance.runHooks('afterCellMetaReset');\n\n    if (isDefined(settings.className)) {\n      if (GridSettings.prototype.className) {\n        removeClass(instance.rootElement, GridSettings.prototype.className);\n      }\n      if (settings.className) {\n        addClass(instance.rootElement, settings.className);\n      }\n    }\n\n    let currentHeight = instance.rootElement.style.height;\n    if (currentHeight !== '') {\n      currentHeight = parseInt(instance.rootElement.style.height, 10);\n    }\n\n    let height = settings.height;\n    if (isFunction(height)) {\n      height = height();\n    }\n\n    if (init) {\n      let initialStyle = instance.rootElement.getAttribute('style');\n\n      if (initialStyle) {\n        instance.rootElement.setAttribute('data-initialstyle', instance.rootElement.getAttribute('style'));\n      }\n    }\n\n    if (height === null) {\n      let initialStyle = instance.rootElement.getAttribute('data-initialstyle');\n\n      if (initialStyle && (initialStyle.indexOf('height') > -1 || initialStyle.indexOf('overflow') > -1)) {\n        instance.rootElement.setAttribute('style', initialStyle);\n\n      } else {\n        instance.rootElement.style.height = '';\n        instance.rootElement.style.overflow = '';\n      }\n\n    } else if (height !== void 0) {\n      instance.rootElement.style.height = `${height}px`;\n      instance.rootElement.style.overflow = 'hidden';\n    }\n\n    if (typeof settings.width !== 'undefined') {\n      var width = settings.width;\n\n      if (isFunction(width)) {\n        width = width();\n      }\n\n      instance.rootElement.style.width = `${width}px`;\n    }\n\n    if (!init) {\n      datamap.clearLengthCache(); // force clear cache length on updateSettings() #3416\n\n      if (instance.view) {\n        instance.view.wt.wtViewport.resetHasOversizedColumnHeadersMarked();\n      }\n\n      instance.runHooks('afterUpdateSettings', settings);\n    }\n\n    grid.adjustRowsAndCols();\n    if (instance.view && !priv.firstRun) {\n      instance.forceFullRender = true; // used when data was changed\n      selection.refreshBorders(null, true);\n    }\n\n    if (!init && instance.view && (currentHeight === '' || height === '' || height === void 0) && currentHeight !== height) {\n      instance.view.wt.wtOverlays.updateMainScrollableElements();\n    }\n  };\n\n  /**\n   * Get value from the selected cell.\n   *\n   * @memberof Core#\n   * @function getValue\n   * @since 0.11\n   * @returns {*} Value of selected cell.\n   */\n  this.getValue = function() {\n    var sel = instance.getSelected();\n    if (GridSettings.prototype.getValue) {\n      if (isFunction(GridSettings.prototype.getValue)) {\n        return GridSettings.prototype.getValue.call(instance);\n      } else if (sel) {\n        return instance.getData()[sel[0]][GridSettings.prototype.getValue];\n      }\n    } else if (sel) {\n      return instance.getDataAtCell(sel[0], sel[1]);\n    }\n  };\n\n  function expandType(obj) {\n    if (!hasOwnProperty(obj, 'type')) {\n      // ignore obj.prototype.type\n      return;\n    }\n\n    var type,\n      expandedType = {};\n\n    if (typeof obj.type === 'object') {\n      type = obj.type;\n    } else if (typeof obj.type === 'string') {\n      type = getCellType(obj.type);\n    }\n\n    for (var i in type) {\n      if (hasOwnProperty(type, i) && !hasOwnProperty(obj, i)) {\n        expandedType[i] = type[i];\n      }\n    }\n\n    return expandedType;\n\n  }\n\n  /**\n   * Returns the object settings.\n   *\n   * @memberof Core#\n   * @function getSettings\n   * @returns {Object} Object containing the current grid settings.\n   */\n  this.getSettings = function() {\n    return priv.settings;\n  };\n\n  /**\n   * Clears the data from the grid. (The table settings remain intact.)\n   *\n   * @memberof Core#\n   * @function clear\n   * @since 0.11\n   */\n  this.clear = function() {\n    selection.selectAll();\n    selection.empty();\n  };\n\n  /**\n   * @memberof Core#\n   * @function alter\n   * @param {String} action See grid.alter for possible values: `\"insert_row\"`, `\"insert_col\"`, `\"remove_row\"`, `\"remove_col\"`\n   * @param {Number} index Visual index of the row/column before which the new row/column will be inserted/removed.\n   * @param {Number} [amount = 1] Amound of rows/columns to be inserted/removed.\n   * @param {String} [source] Source indicator.\n   * @param {Boolean} [keepEmptyRows] Flag for preventing deletion of empty rows.\n   * @description\n   *\n   * Allows altering the table structure by either inserting/removing rows or inserting/removing columns:\n   *\n   * Insert new row(s) above the row with a given `index`. If index is `null` or `undefined`, the new row will be\n   * added after the last row.\n   * ```js\n   * var hot = new Handsontable(document.getElementById('example'));\n   * hot.alter('insert_row', 10);\n   * ```\n   *\n   * Insert new column(s) before the column with a given `index`. If index is `null` or `undefined`, the new column\n   * will be added after the last column.\n   * ```js\n   * var hot = new Handsontable(document.getElementById('example'));\n   * hot.alter('insert_col', 10);\n   * ```\n   *\n   * Remove the row(s) at the given `index`.\n   * ```js\n   * var hot = new Handsontable(document.getElementById('example'));\n   * hot.alter('remove_row', 10);\n   * ```\n   *\n   * Remove the column(s) at the given `index`.\n   * ```js\n   * var hot = new Handsontable(document.getElementById('example'));\n   * hot.alter('remove_col', 10);\n   * ```\n   */\n  this.alter = function(action, index, amount, source, keepEmptyRows) {\n    grid.alter(action, index, amount, source, keepEmptyRows);\n  };\n\n  /**\n   * Returns a TD element for the given `row` and `col` arguments, if it is rendered on screen.\n   * Returns `null` if the TD is not rendered on screen (probably because that part of the table is not visible).\n   *\n   * @memberof Core#\n   * @function getCell\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @param {Boolean} topmost If set to true, it returns the TD element from the topmost overlay. For example,\n   * if the wanted cell is in the range of fixed rows, it will return a TD element from the `top` overlay.\n   * @returns {Element} The cell's TD element.\n   */\n  this.getCell = function(row, col, topmost) {\n    return instance.view.getCellAtCoords(new CellCoords(row, col), topmost);\n  };\n\n  /**\n   * Returns the coordinates of the cell, provided as a HTML Element.\n   *\n   * @memberof Core#\n   * @function getCoords\n   * @param {Element} elem The HTML Element representing the cell.\n   * @returns {CellCoords} Visual coordinates object.\n   */\n  this.getCoords = function(elem) {\n    return this.view.wt.wtTable.getCoords.call(this.view.wt.wtTable, elem);\n  };\n\n  /**\n   * Returns the property name that corresponds with the given column index. {@link DataMap#colToProp}\n   * If the data source is an array of arrays, it returns the columns index.\n   *\n   * @memberof Core#\n   * @function colToProp\n   * @param {Number} col Visual column index.\n   * @returns {String|Number} Column property or physical column index.\n   */\n  this.colToProp = function(col) {\n    return datamap.colToProp(col);\n  };\n\n  /**\n   * Returns column index that corresponds with the given property. {@link DataMap#propToCol}\n   *\n   * @memberof Core#\n   * @function propToCol\n   * @param {String|Number} prop Property name or physical column index.\n   * @returns {Number} Visual column index.\n   */\n  this.propToCol = function(prop) {\n    return datamap.propToCol(prop);\n  };\n\n  /**\n   * Translate physical row index into visual.\n   *\n   * @since 0.29.0\n   * @memberof Core#\n   * @function toVisualRow\n   * @param {Number} row Physical row index.\n   * @returns {Number} Returns visual row index.\n   */\n  this.toVisualRow = (row) => recordTranslator.toVisualRow(row);\n\n  /**\n   * Translate physical column index into visual.\n   *\n   * @since 0.29.0\n   * @memberof Core#\n   * @function toVisualColumn\n   * @param {Number} column Physical column index.\n   * @returns {Number} Returns visual column index.\n   */\n  this.toVisualColumn = (column) => recordTranslator.toVisualColumn(column);\n\n  /**\n   * Translate visual row index into physical.\n   * If displayed rows order is different than the order of rows stored in memory (i.e. sorting is applied)\n   * to retrieve valid physical row index you can use this method.\n   *\n   * @since 0.29.0\n   * @memberof Core#\n   * @function toPhysicalRow\n   * @param {Number} row Visual row index.\n   * @returns {Number} Returns physical row index.\n   */\n  this.toPhysicalRow = (row) => recordTranslator.toPhysicalRow(row);\n\n  /**\n   * Translate visual column index into physical.\n   * If displayed columns order is different than the order of columns stored in memory (i.e. manual column move is applied)\n   * to retrieve valid physical column index you can use this method.\n   *\n   * @since 0.29.0\n   * @memberof Core#\n   * @function toPhysicalColumn\n   * @param {Number} column Visual column index.\n   * @returns {Number} Returns physical column index.\n   */\n  this.toPhysicalColumn = (column) => recordTranslator.toPhysicalColumn(column);\n\n  /**\n   * @description\n   * Returns the cell value at `row`, `col`. `row` and `col` are the __visible__ indexes (note, that if columns were reordered or sorted,\n   * the currently visible order will be used).\n   *\n   * @memberof Core#\n   * @function getDataAtCell\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @returns {String|Boolean|null} Data at cell.\n   */\n  this.getDataAtCell = function(row, col) {\n    return datamap.get(row, datamap.colToProp(col));\n  };\n\n  /**\n   * Return value at `row`, `prop`. (Uses {@link DataMap#get})\n   *\n   * @memberof Core#\n   * @function getDataAtRowProp\n   * @param {Number} row Visual row index.\n   * @param {String} prop Property name.\n   * @returns {*} Cell value.\n   */\n  this.getDataAtRowProp = function(row, prop) {\n    return datamap.get(row, prop);\n  };\n\n  /**\n   * @description\n   * Returns array of column values from the data source. `col` is the __visible__ index of the column.\n   * Note, that if columns were reordered or sorted, the currently visible order will be used.\n   *\n   * @memberof Core#\n   * @function getDataAtCol\n   * @since 0.9-beta2\n   * @param {Number} col Visual column index.\n   * @returns {Array} Array of cell values.\n   */\n  this.getDataAtCol = function(col) {\n    var out = [];\n    return out.concat(...datamap.getRange(\n      new CellCoords(0, col), new CellCoords(priv.settings.data.length - 1, col), datamap.DESTINATION_RENDERER));\n  };\n\n  /**\n   * Given the object property name (e.g. `'first.name'`), returns an array of column's values from the data source.\n   * You can also provide a column index as the first argument.\n   *\n   * @memberof Core#\n   * @function getDataAtProp\n   * @since 0.9-beta2\n   * @param {String|Number} prop Property name / physical column index.\n   * @returns {Array} Array of cell values.\n   */\n  // TODO: Getting data from `datamap` should work on visual indexes.\n  this.getDataAtProp = function(prop) {\n    var out = [],\n      range;\n\n    range = datamap.getRange(\n      new CellCoords(0, datamap.propToCol(prop)),\n      new CellCoords(priv.settings.data.length - 1, datamap.propToCol(prop)),\n      datamap.DESTINATION_RENDERER);\n\n    return out.concat(...range);\n  };\n\n  /**\n   * Returns the source data object (the same that was passed by `data` configuration option or `loadData` method).\n   * Optionally you can provide a cell range by using the `row`, `col`, `row2`, `col2` arguments, to get only a fragment of grid data.\n   *\n   * @memberof Core#\n   * @function getSourceData\n   * @since 0.20.0\n   * @param {Number} [r] From physical row index.\n   * @param {Number} [c] From physical column index (or visual index, if data type is an array of objects).\n   * @param {Number} [r2] To physical row index.\n   * @param {Number} [c2] To physical column index (or visual index, if data type is an array of objects).\n   * @returns {Array} Array of grid data.\n   */\n  this.getSourceData = function(r, c, r2, c2) {\n    let data;\n\n    if (r === void 0) {\n      data = dataSource.getData();\n    } else {\n      data = dataSource.getByRange(new CellCoords(r, c), new CellCoords(r2, c2));\n    }\n\n    return data;\n  };\n\n  /**\n   * Returns the source data object as an arrays of arrays format even when source data was provided in another format.\n   * Optionally you can provide a cell range by using the `row`, `col`, `row2`, `col2` arguments, to get only a fragment of grid data.\n   *\n   * @memberof Core#\n   * @function getSourceDataArray\n   * @since 0.28.0\n   * @param {Number} [r] From physical row index.\n   * @param {Number} [c] From physical column index (or visual index, if data type is an array of objects).\n   * @param {Number} [r2] To physical row index.\n   * @param {Number} [c2] To physical column index (or visual index, if data type is an array of objects).\n   * @returns {Array} An array of arrays.\n   */\n  this.getSourceDataArray = function(r, c, r2, c2) {\n    let data;\n\n    if (r === void 0) {\n      data = dataSource.getData(true);\n    } else {\n      data = dataSource.getByRange(new CellCoords(r, c), new CellCoords(r2, c2), true);\n    }\n\n    return data;\n  };\n\n  /**\n   * Returns an array of column values from the data source. `col` is the index of the row in the data source.\n   *\n   * @memberof Core#\n   * @function getSourceDataAtCol\n   * @since 0.11.0-beta3\n   * @param {Number} column Visual column index.\n   * @returns {Array} Array of the column's cell values.\n   */\n  // TODO: Getting data from `sourceData` should work always on physical indexes.\n  this.getSourceDataAtCol = function(column) {\n    return dataSource.getAtColumn(column);\n  };\n\n  /**\n   * Returns a single row of the data (array or object, depending on what you have). `row` is the index of the row in the data source.\n   *\n   * @memberof Core#\n   * @function getSourceDataAtRow\n   * @since 0.11.0-beta3\n   * @param {Number} row Physical row index.\n   * @returns {Array|Object} Single row of data.\n   */\n  this.getSourceDataAtRow = function(row) {\n    return dataSource.getAtRow(row);\n  };\n\n  /**\n   * Returns a single value from the data source.\n   *\n   * @memberof Core#\n   * @function getSourceDataAtCell\n   * @param {Number} row Physical row index.\n   * @param {Number} column Visual column index.\n   * @returns {*} Cell data.\n   * @since 0.20.0\n   */\n  // TODO: Getting data from `sourceData` should work always on physical indexes.\n  this.getSourceDataAtCell = function(row, column) {\n    return dataSource.getAtCell(row, column);\n  };\n\n  /**\n   * @description\n   * Returns a single row of the data. The `row` argument is the __visible__ index of the row.\n   *\n   * @memberof Core#\n   * @function getDataAtRow\n   * @param {Number} row Visual row index.\n   * @returns {Array} Array of row's cell data.\n   * @since 0.9-beta2\n   */\n  this.getDataAtRow = function(row) {\n    var data = datamap.getRange(new CellCoords(row, 0), new CellCoords(row, this.countCols() - 1), datamap.DESTINATION_RENDERER);\n\n    return data[0] || [];\n  };\n\n  /**\n   * @description\n   * Returns a data type defined in the Handsontable settings under the `type` key ([Options#type](http://docs.handsontable.com/Options.html#type)).\n   * If there are cells with different types in the selected range, it returns `'mixed'`.\n   *\n   * @since 0.18.1\n   * @memberof Core#\n   * @function getDataType\n   * @param {Number} rowFrom From visual row index.\n   * @param {Number} columnFrom From visual column index.\n   * @param {Number} rowTo To visual row index.\n   * @param {Number} columnTo To visual column index.\n   * @returns {String} Cell type (e.q: `'mixed'`, `'text'`, `'numeric'`, `'autocomplete'`).\n   */\n  this.getDataType = function(rowFrom, columnFrom, rowTo, columnTo) {\n    let previousType = null;\n    let currentType = null;\n\n    if (rowFrom === void 0) {\n      rowFrom = 0;\n      rowTo = this.countRows();\n      columnFrom = 0;\n      columnTo = this.countCols();\n    }\n    if (rowTo === void 0) {\n      rowTo = rowFrom;\n    }\n    if (columnTo === void 0) {\n      columnTo = columnFrom;\n    }\n    let type = 'mixed';\n\n    rangeEach(Math.min(rowFrom, rowTo), Math.max(rowFrom, rowTo), (row) => {\n      let isTypeEqual = true;\n\n      rangeEach(Math.min(columnFrom, columnTo), Math.max(columnFrom, columnTo), (column) => {\n        let cellType = this.getCellMeta(row, column);\n\n        currentType = cellType.type;\n\n        if (previousType) {\n          isTypeEqual = previousType === currentType;\n        } else {\n          previousType = currentType;\n        }\n\n        return isTypeEqual;\n      });\n      type = isTypeEqual ? currentType : 'mixed';\n\n      return isTypeEqual;\n    });\n\n    return type;\n  };\n\n  /**\n   * Remove a property defined by the `key` argument from the cell meta object for the provided `row` and `col` coordinates.\n   *\n   * @memberof Core#\n   * @function removeCellMeta\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @param {String} key Property name.\n   * @fires Hooks#beforeRemoveCellMeta\n   * @fires Hooks#afterRemoveCellMeta\n   */\n  this.removeCellMeta = function(row, col, key) {\n    const [physicalRow, physicalColumn] = recordTranslator.toPhysical(row, col);\n    let cachedValue = priv.cellSettings[physicalRow][physicalColumn][key];\n\n    const hookResult = instance.runHooks('beforeRemoveCellMeta', row, col, key, cachedValue);\n\n    if (hookResult !== false) {\n      delete priv.cellSettings[physicalRow][physicalColumn][key];\n\n      instance.runHooks('afterRemoveCellMeta', row, col, key, cachedValue);\n    }\n\n    cachedValue = null;\n  };\n\n  /**\n   * Remove one or more rows from the cell meta object.\n   *\n   * @since 0.30.0\n   * @param {Number} index An integer that specifies at what position to add/remove items, Use negative values to specify the position from the end of the array.\n   * @param {Number} deleteAmount The number of items to be removed. If set to 0, no items will be removed.\n   * @param {Array} items The new items to be added to the array.\n   */\n  this.spliceCellsMeta = function(index, deleteAmount, ...items) {\n    priv.cellSettings.splice(index, deleteAmount, ...items);\n  };\n\n  /**\n   * Set cell meta data object defined by `prop` to the corresponding params `row` and `col`.\n   *\n   * @memberof Core#\n   * @function setCellMetaObject\n   * @since 0.11\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @param {Object} prop Meta object.\n   */\n  this.setCellMetaObject = function(row, col, prop) {\n    if (typeof prop === 'object') {\n      for (var key in prop) {\n        if (hasOwnProperty(prop, key)) {\n          var value = prop[key];\n          this.setCellMeta(row, col, key, value);\n        }\n      }\n    }\n  };\n\n  /**\n   * Sets a property defined by the `key` object to the meta object of a cell corresponding to params `row` and `col`.\n   *\n   * @memberof Core#\n   * @function setCellMeta\n   * @since 0.11\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @param {String} key Property name.\n   * @param {String} val Property value.\n   * @fires Hooks#afterSetCellMeta\n   */\n  this.setCellMeta = function(row, col, key, val) {\n    const [physicalRow, physicalColumn] = recordTranslator.toPhysical(row, col);\n\n    if (!priv.columnSettings[physicalColumn]) {\n      priv.columnSettings[physicalColumn] = columnFactory(GridSettings, priv.columnsSettingConflicts);\n    }\n\n    if (!priv.cellSettings[physicalRow]) {\n      priv.cellSettings[physicalRow] = [];\n    }\n    if (!priv.cellSettings[physicalRow][physicalColumn]) {\n      priv.cellSettings[physicalRow][physicalColumn] = new priv.columnSettings[physicalColumn]();\n    }\n    priv.cellSettings[physicalRow][physicalColumn][key] = val;\n    instance.runHooks('afterSetCellMeta', row, col, key, val);\n  };\n\n  /**\n   * Get all the cells meta settings at least once generated in the table (in order of cell initialization).\n   *\n   * @since 0.19.0\n   * @returns {Array} Returns Array of ColumnSettings object.\n   */\n  this.getCellsMeta = function() {\n    return arrayFlatten(priv.cellSettings);\n  };\n\n  /**\n   * Returns the cell properties object for the given `row` and `col` coordinates.\n   *\n   * @memberof Core#\n   * @function getCellMeta\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @returns {Object} The cell properties object.\n   * @fires Hooks#beforeGetCellMeta\n   * @fires Hooks#afterGetCellMeta\n   */\n  this.getCellMeta = function(row, col) {\n    const prop = datamap.colToProp(col);\n    let cellProperties;\n\n    const [physicalRow, physicalColumn] = recordTranslator.toPhysical(row, col);\n\n    if (!priv.columnSettings[physicalColumn]) {\n      priv.columnSettings[physicalColumn] = columnFactory(GridSettings, priv.columnsSettingConflicts);\n    }\n\n    if (!priv.cellSettings[physicalRow]) {\n      priv.cellSettings[physicalRow] = [];\n    }\n    if (!priv.cellSettings[physicalRow][physicalColumn]) {\n      priv.cellSettings[physicalRow][physicalColumn] = new priv.columnSettings[physicalColumn]();\n    }\n\n    cellProperties = priv.cellSettings[physicalRow][physicalColumn]; // retrieve cellProperties from cache\n\n    cellProperties.row = physicalRow;\n    cellProperties.col = physicalColumn;\n    cellProperties.visualRow = row;\n    cellProperties.visualCol = col;\n    cellProperties.prop = prop;\n    cellProperties.instance = instance;\n\n    instance.runHooks('beforeGetCellMeta', row, col, cellProperties);\n    extend(cellProperties, expandType(cellProperties)); // for `type` added in beforeGetCellMeta\n\n    if (cellProperties.cells) {\n      const settings = cellProperties.cells.call(cellProperties, physicalRow, physicalColumn, prop);\n\n      if (settings) {\n        extend(cellProperties, settings);\n        extend(cellProperties, expandType(settings)); // for `type` added in cells\n      }\n    }\n\n    instance.runHooks('afterGetCellMeta', row, col, cellProperties);\n\n    return cellProperties;\n  };\n\n  /**\n   * Returns a row off the cell meta array.\n   *\n   * @memberof Core#\n   * @function getCellMetaAtRow\n   * @since 0.30.0\n   * @param {Number} row Physical index of the row to return cell meta for.\n   * @returns {Array}\n   */\n  this.getCellMetaAtRow = function(row) {\n    return priv.cellSettings[row];\n  };\n\n  /**\n   * Checks if the data format and config allows user to modify the column structure.\n   * @returns {boolean}\n   */\n  this.isColumnModificationAllowed = function() {\n    return !(instance.dataType === 'object' || instance.getSettings().columns);\n  };\n\n  const rendererLookup = cellMethodLookupFactory('renderer');\n\n  /**\n   * Returns the cell renderer function by given `row` and `col` arguments.\n   *\n   * @memberof Core#\n   * @function getCellRenderer\n   * @since 0.11\n   * @param {Number|Object} row Visual row index or cell meta object.\n   * @param {Number} [col] Visual column index.\n   * @returns {Function} The renderer function.\n   */\n  this.getCellRenderer = function(row, col) {\n    return getRenderer(rendererLookup.call(this, row, col));\n  };\n\n  /**\n   * Returns the cell editor by the provided `row` and `col` arguments.\n   *\n   * @memberof Core#\n   * @function getCellEditor\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @returns {Object} The Editor object.\n   */\n  this.getCellEditor = cellMethodLookupFactory('editor');\n\n  const validatorLookup = cellMethodLookupFactory('validator');\n\n  /**\n   * Returns the cell validator by `row` and `col`, provided a validator is defined. If not - it doesn't return anything.\n   *\n   * @memberof Core#\n   * @function getCellValidator\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @returns {Function|RegExp|undefined} The validator function.\n   */\n  this.getCellValidator = function(row, col) {\n    let validator = validatorLookup.call(this, row, col);\n\n    if (typeof validator === 'string') {\n      validator = getValidator(validator);\n    }\n\n    return validator;\n  };\n\n  /**\n   * Validates all cells using their validator functions and calls callback when finished.\n   *\n   * If one of the cells is invalid, the callback will be fired with `'valid'` arguments as `false` - otherwise it would equal `true`.\n   *\n   * @memberof Core#\n   * @function validateCells\n   * @param {Function} [callback] The callback function.\n   */\n  this.validateCells = function(callback) {\n    var waitingForValidator = new ValidatorsQueue();\n\n    if (callback) {\n      waitingForValidator.onQueueEmpty = callback;\n    }\n\n    let i = instance.countRows() - 1;\n\n    while (i >= 0) {\n      let j = instance.countCols() - 1;\n\n      while (j >= 0) {\n        waitingForValidator.addValidatorToQueue();\n\n        instance.validateCell(instance.getDataAtCell(i, j), instance.getCellMeta(i, j), (result) => {\n          if (typeof result !== 'boolean') {\n            throw new Error('Validation error: result is not boolean');\n          }\n          if (result === false) {\n            waitingForValidator.valid = false;\n          }\n          waitingForValidator.removeValidatorFormQueue();\n        }, 'validateCells');\n        j--;\n      }\n      i--;\n    }\n    waitingForValidator.checkIfQueueIsEmpty();\n  };\n\n  /**\n   * Returns an array of row headers' values (if they are enabled). If param `row` was given, it returns the header of the given row as a string.\n   *\n   * @memberof Core#\n   * @function getRowHeader\n   * @param {Number} [row] Visual row index.\n   * @fires Hooks#modifyRowHeader\n   * @returns {Array|String} Array of header values / single header value.\n   */\n  this.getRowHeader = function(row) {\n    let rowHeader = priv.settings.rowHeaders;\n\n    if (row !== void 0) {\n      row = instance.runHooks('modifyRowHeader', row);\n    }\n    if (row === void 0) {\n      rowHeader = [];\n      rangeEach(instance.countRows() - 1, (i) => {\n        rowHeader.push(instance.getRowHeader(i));\n      });\n\n    } else if (Array.isArray(rowHeader) && rowHeader[row] !== void 0) {\n      rowHeader = rowHeader[row];\n\n    } else if (isFunction(rowHeader)) {\n      rowHeader = rowHeader(row);\n\n    } else if (rowHeader && typeof rowHeader !== 'string' && typeof rowHeader !== 'number') {\n      rowHeader = row + 1;\n    }\n\n    return rowHeader;\n  };\n\n  /**\n   * Returns information about if this table is configured to display row headers.\n   *\n   * @memberof Core#\n   * @function hasRowHeaders\n   * @returns {Boolean} `true` if the instance has the row headers enabled, `false` otherwise.\n   * @since 0.11\n   */\n  this.hasRowHeaders = function() {\n    return !!priv.settings.rowHeaders;\n  };\n\n  /**\n   * Returns information about if this table is configured to display column headers.\n   *\n   * @memberof Core#\n   * @function hasColHeaders\n   * @since 0.11\n   * @returns {Boolean} `True` if the instance has the column headers enabled, `false` otherwise.\n   */\n  this.hasColHeaders = function() {\n    if (priv.settings.colHeaders !== void 0 && priv.settings.colHeaders !== null) { // Polymer has empty value = null\n      return !!priv.settings.colHeaders;\n    }\n    for (var i = 0, ilen = instance.countCols(); i < ilen; i++) {\n      if (instance.getColHeader(i)) {\n        return true;\n      }\n    }\n\n    return false;\n  };\n\n  /**\n   * Returns an array of column headers (in string format, if they are enabled). If param `col` is given, it returns the header at the given column as a string.\n   *\n   * @memberof Core#\n   * @function getColHeader\n   * @param {Number} [col] Visual column index.\n   * @fires Hooks#modifyColHeader\n   * @returns {Array|String} The column header(s).\n   */\n  this.getColHeader = function(col) {\n    let columnsAsFunc = priv.settings.columns && isFunction(priv.settings.columns);\n    let result = priv.settings.colHeaders;\n\n    col = instance.runHooks('modifyColHeader', col);\n\n    if (col === void 0) {\n      let out = [];\n      let ilen = columnsAsFunc ? instance.countSourceCols() : instance.countCols();\n\n      for (let i = 0; i < ilen; i++) {\n        out.push(instance.getColHeader(i));\n      }\n\n      result = out;\n\n    } else {\n      let translateVisualIndexToColumns = function(col) {\n        let arr = [];\n        let columnsLen = instance.countSourceCols();\n        let index = 0;\n\n        for (; index < columnsLen; index++) {\n          if (isFunction(instance.getSettings().columns) && instance.getSettings().columns(index)) {\n            arr.push(index);\n          }\n        }\n\n        return arr[col];\n      };\n      let baseCol = col;\n      col = instance.runHooks('modifyCol', col);\n\n      let prop = translateVisualIndexToColumns(col);\n\n      if (priv.settings.columns && isFunction(priv.settings.columns) && priv.settings.columns(prop) && priv.settings.columns(prop).title) {\n        result = priv.settings.columns(prop).title;\n\n      } else if (priv.settings.columns && priv.settings.columns[col] && priv.settings.columns[col].title) {\n        result = priv.settings.columns[col].title;\n\n      } else if (Array.isArray(priv.settings.colHeaders) && priv.settings.colHeaders[col] !== void 0) {\n        result = priv.settings.colHeaders[col];\n\n      } else if (isFunction(priv.settings.colHeaders)) {\n        result = priv.settings.colHeaders(col);\n\n      } else if (priv.settings.colHeaders && typeof priv.settings.colHeaders !== 'string' && typeof priv.settings.colHeaders !== 'number') {\n        result = spreadsheetColumnLabel(baseCol); // see #1458\n\n      }\n    }\n\n    return result;\n  };\n\n  /**\n   * Return column width from settings (no guessing). Private use intended.\n   *\n   * @private\n   * @memberof Core#\n   * @function _getColWidthFromSettings\n   * @param {Number} col Visual col index.\n   * @returns {Number}\n   */\n  this._getColWidthFromSettings = function(col) {\n    var cellProperties = instance.getCellMeta(0, col);\n    var width = cellProperties.width;\n\n    if (width === void 0 || width === priv.settings.width) {\n      width = cellProperties.colWidths;\n    }\n    if (width !== void 0 && width !== null) {\n      switch (typeof width) {\n        case 'object': // array\n          width = width[col];\n          break;\n\n        case 'function':\n          width = width(col);\n          break;\n        default:\n          break;\n      }\n      if (typeof width === 'string') {\n        width = parseInt(width, 10);\n      }\n    }\n\n    return width;\n  };\n\n  /**\n   * Returns the width of the requested column.\n   *\n   * @memberof Core#\n   * @function getColWidth\n   * @since 0.11\n   * @param {Number} col Visual column index.\n   * @returns {Number} Column width.\n   * @fires Hooks#modifyColWidth\n   */\n  this.getColWidth = function(col) {\n    let width = instance._getColWidthFromSettings(col);\n\n    width = instance.runHooks('modifyColWidth', width, col);\n\n    if (width === void 0) {\n      width = ViewportColumnsCalculator.DEFAULT_WIDTH;\n    }\n\n    return width;\n  };\n\n  /**\n   * Return row height from settings (no guessing). Private use intended.\n   *\n   * @private\n   * @memberof Core#\n   * @function _getRowHeightFromSettings\n   * @param {Number} row Visual row index.\n   * @returns {Number}\n   */\n  this._getRowHeightFromSettings = function(row) {\n    // let cellProperties = instance.getCellMeta(row, 0);\n    // let height = cellProperties.height;\n    //\n    // if (height === void 0 || height === priv.settings.height) {\n    //  height = cellProperties.rowHeights;\n    // }\n    var height = priv.settings.rowHeights;\n\n    if (height !== void 0 && height !== null) {\n      switch (typeof height) {\n        case 'object': // array\n          height = height[row];\n          break;\n\n        case 'function':\n          height = height(row);\n          break;\n        default:\n          break;\n      }\n      if (typeof height === 'string') {\n        height = parseInt(height, 10);\n      }\n    }\n\n    return height;\n  };\n\n  /**\n   * Returns the row height.\n   *\n   * @memberof Core#\n   * @function getRowHeight\n   * @since 0.11\n   * @param {Number} row Visual row index.\n   * @returns {Number} The given row's height.\n   * @fires Hooks#modifyRowHeight\n   */\n  this.getRowHeight = function(row) {\n    var height = instance._getRowHeightFromSettings(row);\n\n    height = instance.runHooks('modifyRowHeight', height, row);\n\n    return height;\n  };\n\n  /**\n   * Returns the total number of rows in the data source.\n   *\n   * @memberof Core#\n   * @function countSourceRows\n   * @since 0.20.0\n   * @returns {Number} Total number in rows in data source.\n   */\n  this.countSourceRows = function() {\n    let sourceLength = instance.runHooks('modifySourceLength');\n    return sourceLength || (instance.getSourceData() ? instance.getSourceData().length : 0);\n  };\n\n  /**\n   * Returns the total number of columns in the data source.\n   *\n   * @memberof Core#\n   * @function countSourceCols\n   * @since 0.26.1\n   * @returns {Number} Total number in columns in data source.\n   */\n  this.countSourceCols = function() {\n    let len = 0;\n    let obj = instance.getSourceData() && instance.getSourceData()[0] ? instance.getSourceData()[0] : [];\n\n    if (isObject(obj)) {\n      len = deepObjectSize(obj);\n\n    } else {\n      len = obj.length || 0;\n    }\n\n    return len;\n  };\n\n  /**\n   * Returns the total number of rows in the grid.\n   *\n   * @memberof Core#\n   * @function countRows\n   * @returns {Number} Total number in rows the grid.\n   */\n  this.countRows = function() {\n    return datamap.getLength();\n  };\n\n  /**\n   * Returns the total number of columns in the grid.\n   *\n   * @memberof Core#\n   * @function countCols\n   * @returns {Number} Total number of columns.\n   */\n  this.countCols = function() {\n    const maxCols = this.getSettings().maxCols;\n    let dataHasLength = false;\n    let dataLen = 0;\n\n    if (instance.dataType === 'array') {\n      dataHasLength = priv.settings.data && priv.settings.data[0] && priv.settings.data[0].length;\n    }\n\n    if (dataHasLength) {\n      dataLen = priv.settings.data[0].length;\n    }\n\n    if (priv.settings.columns) {\n      let columnsIsFunction = isFunction(priv.settings.columns);\n\n      if (columnsIsFunction) {\n        if (instance.dataType === 'array') {\n          let columnLen = 0;\n\n          for (let i = 0; i < dataLen; i++) {\n            if (priv.settings.columns(i)) {\n              columnLen++;\n            }\n          }\n\n          dataLen = columnLen;\n        } else if (instance.dataType === 'object' || instance.dataType === 'function') {\n          dataLen = datamap.colToPropCache.length;\n        }\n\n      } else {\n        dataLen = priv.settings.columns.length;\n      }\n\n    } else if (instance.dataType === 'object' || instance.dataType === 'function') {\n      dataLen = datamap.colToPropCache.length;\n    }\n\n    return Math.min(maxCols, dataLen);\n  };\n\n  /**\n   * Returns an visual index of the first rendered row.\n   *\n   * @memberof Core#\n   * @function rowOffset\n   * @returns {Number} Visual index of first rendered row.\n   */\n  this.rowOffset = function() {\n    return instance.view.wt.wtTable.getFirstRenderedRow();\n  };\n\n  /**\n   * Returns the visual index of the first rendered column.\n   *\n   * @memberof Core#\n   * @function colOffset\n   * @returns {Number} Visual index of the first visible column.\n   */\n  this.colOffset = function() {\n    return instance.view.wt.wtTable.getFirstRenderedColumn();\n  };\n\n  /**\n   * Returns the number of rendered rows (including rows partially or fully rendered outside viewport).\n   *\n   * @memberof Core#\n   * @function countRenderedRows\n   * @returns {Number} Returns -1 if table is not visible.\n   */\n  this.countRenderedRows = function() {\n    return instance.view.wt.drawn ? instance.view.wt.wtTable.getRenderedRowsCount() : -1;\n  };\n\n  /**\n   * Returns the number of visible rows (rendered rows that fully fit inside viewport).\n   *\n   * @memberof Core#\n   * @function countVisibleRows\n   * @returns {Number} Number of visible rows or -1.\n   */\n  this.countVisibleRows = function() {\n    return instance.view.wt.drawn ? instance.view.wt.wtTable.getVisibleRowsCount() : -1;\n  };\n\n  /**\n   * Returns the number of rendered columns (including columns partially or fully rendered outside viewport).\n   *\n   * @memberof Core#\n   * @function countRenderedCols\n   * @returns {Number} Returns -1 if table is not visible.\n   */\n  this.countRenderedCols = function() {\n    return instance.view.wt.drawn ? instance.view.wt.wtTable.getRenderedColumnsCount() : -1;\n  };\n\n  /**\n   * Returns the number of visible columns. Returns -1 if table is not visible\n   *\n   * @memberof Core#\n   * @function countVisibleCols\n   * @return {Number} Number of visible columns or -1.\n   */\n  this.countVisibleCols = function() {\n    return instance.view.wt.drawn ? instance.view.wt.wtTable.getVisibleColumnsCount() : -1;\n  };\n\n  /**\n   * Returns the number of empty rows. If the optional ending parameter is `true`, returns the\n   * number of empty rows at the bottom of the table.\n   *\n   * @memberof Core#\n   * @function countEmptyRows\n   * @param {Boolean} [ending] If `true`, will only count empty rows at the end of the data source.\n   * @returns {Number} Count empty rows\n   * @fires Hooks#modifyRow\n   */\n  this.countEmptyRows = function(ending) {\n    var i = instance.countRows() - 1,\n      empty = 0,\n      row;\n\n    while (i >= 0) {\n      row = instance.runHooks('modifyRow', i);\n\n      if (instance.isEmptyRow(row)) {\n        empty++;\n\n      } else if (ending) {\n        break;\n      }\n      i--;\n    }\n\n    return empty;\n  };\n\n  /**\n   * Returns the number of empty columns. If the optional ending parameter is `true`, returns the number of empty\n   * columns at right hand edge of the table.\n   *\n   * @memberof Core#\n   * @function countEmptyCols\n   * @param {Boolean} [ending] If `true`, will only count empty columns at the end of the data source row.\n   * @returns {Number} Count empty cols\n   */\n  this.countEmptyCols = function(ending) {\n    if (instance.countRows() < 1) {\n      return 0;\n    }\n    var i = instance.countCols() - 1,\n      empty = 0;\n\n    while (i >= 0) {\n      if (instance.isEmptyCol(i)) {\n        empty++;\n      } else if (ending) {\n        break;\n      }\n      i--;\n    }\n\n    return empty;\n  };\n\n  /**\n   * Check if all cells in the row declared by the `row` argument are empty.\n   *\n   * @memberof Core#\n   * @function isEmptyRow\n   * @param {Number} row Row index.\n   * @returns {Boolean} `true` if the row at the given `row` is empty, `false` otherwise.\n   */\n  this.isEmptyRow = function(row) {\n    return priv.settings.isEmptyRow.call(instance, row);\n  };\n\n  /**\n   * Check if all cells in the the column declared by the `col` argument are empty.\n   *\n   * @memberof Core#\n   * @function isEmptyCol\n   * @param {Number} col Column index.\n   * @returns {Boolean} `true` if the column at the given `col` is empty, `false` otherwise.\n   */\n  this.isEmptyCol = function(col) {\n    return priv.settings.isEmptyCol.call(instance, col);\n  };\n\n  /**\n   * Select cell specified by `row` and `col` values or a range of cells finishing at `endRow`, `endCol`.\n   * By default, viewport will be scrolled to selection.\n   * After the `selectCell` method had finished, the instance will be listening to keyboard input on the document.\n   *\n   * @memberof Core#\n   * @function selectCell\n   * @param {Number} row Visual row index.\n   * @param {Number} col Visual column index.\n   * @param {Number} [endRow] Visual end row index (if selecting a range).\n   * @param {Number} [endCol] Visual end column index (if selecting a range).\n   * @param {Boolean} [scrollToCell=true] If `true`, the viewport will be scrolled to the selection.\n   * @param {Boolean} [changeListener=true] If `false`, Handsontable will not change keyboard events listener to himself.\n   * @returns {Boolean} `true` if selection was successful, `false` otherwise.\n   */\n  this.selectCell = function(row, col, endRow, endCol, scrollToCell, changeListener) {\n    var coords;\n\n    changeListener = isUndefined(changeListener) || changeListener === true;\n\n    if (typeof row !== 'number' || row < 0 || row >= instance.countRows()) {\n      return false;\n    }\n    if (typeof col !== 'number' || col < 0 || col >= instance.countCols()) {\n      return false;\n    }\n    if (isDefined(endRow)) {\n      if (typeof endRow !== 'number' || endRow < 0 || endRow >= instance.countRows()) {\n        return false;\n      }\n      if (typeof endCol !== 'number' || endCol < 0 || endCol >= instance.countCols()) {\n        return false;\n      }\n    }\n    coords = new CellCoords(row, col);\n    priv.selRange = new CellRange(coords, coords, coords);\n\n    if (changeListener) {\n      instance.listen();\n    }\n\n    if (isUndefined(endRow)) {\n      selection.setRangeEnd(priv.selRange.from, scrollToCell);\n\n    } else {\n      selection.setRangeEnd(new CellCoords(endRow, endCol), scrollToCell);\n    }\n    instance.selection.finish();\n\n    return true;\n  };\n\n  /**\n   * Select the cell specified by the `row` and `prop` arguments, or a range finishing at `endRow`, `endProp`.\n   * By default, viewport will be scrolled to selection.\n   *\n   * @memberof Core#\n   * @function selectCellByProp\n   * @param {Number} row Visual row index.\n   * @param {String} prop Property name.\n   * @param {Number} [endRow] visual end row index (if selecting a range).\n   * @param {String} [endProp] End property name (if selecting a range).\n   * @param {Boolean} [scrollToCell=true] If `true`, viewport will be scrolled to the selection.\n   * @returns {Boolean} `true` if selection was successful, `false` otherwise.\n   */\n  this.selectCellByProp = function(row, prop, endRow, endProp, scrollToCell) {\n    arguments[1] = datamap.propToCol(arguments[1]);\n\n    if (isDefined(arguments[3])) {\n      arguments[3] = datamap.propToCol(arguments[3]);\n    }\n\n    return instance.selectCell(...arguments);\n  };\n\n  /**\n   * Deselects the current cell selection on grid.\n   *\n   * @memberof Core#\n   * @function deselectCell\n   */\n  this.deselectCell = function() {\n    selection.deselect();\n  };\n\n  /**\n   * Scroll viewport to coords specified by the `row` and `column` arguments.\n   *\n   * @since 0.24.3\n   * @memberof Core#\n   * @function scrollViewportTo\n   * @param {Number} [row] Visual row index.\n   * @param {Number} [column] Visual column index.\n   * @param {Boolean} [snapToBottom = false] If `true`, viewport is scrolled to show the cell on the bottom of the table.\n   * @param {Boolean} [snapToRight = false] If `true`, viewport is scrolled to show the cell on the right side of the table.\n   * @returns {Boolean} `true` if scroll was successful, `false` otherwise.\n   */\n  this.scrollViewportTo = function(row, column, snapToBottom = false, snapToRight = false) {\n    if (row !== void 0 && (row < 0 || row >= instance.countRows())) {\n      return false;\n    }\n    if (column !== void 0 && (column < 0 || column >= instance.countCols())) {\n      return false;\n    }\n\n    let result = false;\n\n    if (row !== void 0 && column !== void 0) {\n      instance.view.wt.wtOverlays.topOverlay.scrollTo(row, snapToBottom);\n      instance.view.wt.wtOverlays.leftOverlay.scrollTo(column, snapToRight);\n\n      result = true;\n    }\n    if (typeof row === 'number' && typeof column !== 'number') {\n      instance.view.wt.wtOverlays.topOverlay.scrollTo(row, snapToBottom);\n\n      result = true;\n    }\n    if (typeof column === 'number' && typeof row !== 'number') {\n      instance.view.wt.wtOverlays.leftOverlay.scrollTo(column, snapToRight);\n\n      result = true;\n    }\n\n    return result;\n  };\n\n  /**\n   * Removes grid from the DOM.\n   *\n   * @memberof Core#\n   * @function destroy\n   * @fires Hooks#afterDestroy\n   */\n  this.destroy = function() {\n\n    instance._clearTimeouts();\n    if (instance.view) { // in case HT is destroyed before initialization has finished\n      instance.view.destroy();\n    }\n    if (dataSource) {\n      dataSource.destroy();\n    }\n    dataSource = null;\n\n    if (process.env.HOT_PACKAGE_TYPE !== '\\x63\\x65' && isRootInstance(instance)) {\n      const licenseInfo = document.querySelector('#hot-display-license-info');\n\n      if (licenseInfo) {\n        licenseInfo.parentNode.removeChild(licenseInfo);\n      }\n    }\n    empty(instance.rootElement);\n    eventManager.destroy();\n\n    instance.runHooks('afterDestroy');\n    Hooks.getSingleton().destroy(instance);\n\n    for (var i in instance) {\n      if (hasOwnProperty(instance, i)) {\n        // replace instance methods with post mortem\n        if (isFunction(instance[i])) {\n          instance[i] = postMortem;\n\n        } else if (i !== 'guid') {\n          // replace instance properties with null (restores memory)\n          // it should not be necessary but this prevents a memory leak side effects that show itself in Jasmine tests\n          instance[i] = null;\n        }\n      }\n    }\n\n    // replace private properties with null (restores memory)\n    // it should not be necessary but this prevents a memory leak side effects that show itself in Jasmine tests\n    if (datamap) {\n      datamap.destroy();\n    }\n    datamap = null;\n    priv = null;\n    grid = null;\n    selection = null;\n    editorManager = null;\n    instance = null;\n    GridSettings = null;\n  };\n\n  /**\n   * Replacement for all methods after Handsotnable was destroyed.\n   *\n   * @private\n   */\n  function postMortem() {\n    throw new Error('This method cannot be called because this Handsontable instance has been destroyed');\n  }\n\n  /**\n   * Returns the active editor object.\n   *\n   * @memberof Core#\n   * @function getActiveEditor\n   * @returns {Object} The active editor object.\n   */\n  this.getActiveEditor = function() {\n    return editorManager.getActiveEditor();\n  };\n\n  /**\n   * Returns plugin instance using the plugin name provided.\n   *\n   * @memberof Core#\n   * @function getPlugin\n   * @param {String} pluginName The plugin name.\n   * @returns {*} The plugin instance.\n   * @since 0.15.0\n   */\n  this.getPlugin = function(pluginName) {\n    return getPlugin(this, pluginName);\n  };\n\n  /**\n   * Returns the Handsontable instance.\n   *\n   * @memberof Core#\n   * @function getInstance\n   * @returns {Handsontable} The Handsontable instance.\n   */\n  this.getInstance = function() {\n    return instance;\n  };\n\n  /**\n   * Adds listener to the specified hook name (only for this Handsontable instance).\n   *\n   * @memberof Core#\n   * @function addHook\n   * @see Hooks#add\n   * @param {String} key Hook name.\n   * @param {Function|Array} callback Function or array of Functions.\n   *\n   * @example\n   * ```js\n   * hot.addHook('beforeInit', myCallback);\n   * ```\n   */\n  this.addHook = function(key, callback) {\n    Hooks.getSingleton().add(key, callback, instance);\n  };\n\n  /**\n   * Check if for a specified hook name there are added listeners (only for this Handsontable instance).\n   *\n   * @memberof Core#\n   * @function hasHook\n   * @see Hooks#has\n   * @param {String} key Hook name\n   * @return {Boolean}\n   *\n   * @example\n   * ```js\n   * var hasBeforeInitListeners = hot.hasHook('beforeInit');\n   * ```\n   */\n  this.hasHook = function(key) {\n    return Hooks.getSingleton().has(key, instance);\n  };\n\n  /**\n   * Adds listener to specified hook name (only for this Handsontable instance).\n   * After the listener is triggered, it will be automatically removed.\n   *\n   * @memberof Core#\n   * @function addHookOnce\n   * @see Hooks#once\n   * @param {String} key Hook name.\n   * @param {Function|Array} callback Function or array of Functions.\n   *\n   * @example\n   * ```js\n   * hot.addHookOnce('beforeInit', myCallback);\n   * ```\n   */\n  this.addHookOnce = function(key, callback) {\n    Hooks.getSingleton().once(key, callback, instance);\n  };\n\n  /**\n   * Removes the hook listener previously registered with {@link Core#addHook}.\n   *\n   * @memberof Core#\n   * @function removeHook\n   * @see Hooks#remove\n   * @param {String} key Hook name.\n   * @param {Function} callback Function which have been registered via {@link Core#addHook}.\n   *\n   * @example\n   * ```js\n   * hot.removeHook('beforeInit', myCallback);\n   * ```\n   */\n  this.removeHook = function(key, callback) {\n    Hooks.getSingleton().remove(key, callback, instance);\n  };\n\n  /**\n   * Run the callbacks for the hook provided in the `key` argument using the parameters given in the other arguments.\n   *\n   * @memberof Core#\n   * @function runHooks\n   * @see Hooks#run\n   * @param {String} key Hook name.\n   * @param {*} [p1] Argument passed to the callback.\n   * @param {*} [p2] Argument passed to the callback.\n   * @param {*} [p3] Argument passed to the callback.\n   * @param {*} [p4] Argument passed to the callback.\n   * @param {*} [p5] Argument passed to the callback.\n   * @param {*} [p6] Argument passed to the callback.\n   * @returns {*}\n   *\n   * @example\n   * ```js\n   * hot.runHooks('beforeInit');\n   * ```\n   */\n  this.runHooks = function(key, p1, p2, p3, p4, p5, p6) {\n    return Hooks.getSingleton().run(instance, key, p1, p2, p3, p4, p5, p6);\n  };\n\n  this.timeouts = [];\n\n  /**\n   * Sets timeout. Purpose of this method is to clear all known timeouts when `destroy` method is called.\n   *\n   * @param {*} handle\n   * @private\n   */\n  this._registerTimeout = function(handle) {\n    this.timeouts.push(handle);\n  };\n\n  /**\n   * Clears all known timeouts.\n   *\n   * @private\n   */\n  this._clearTimeouts = function() {\n    for (var i = 0, ilen = this.timeouts.length; i < ilen; i++) {\n      clearTimeout(this.timeouts[i]);\n    }\n  };\n\n  /**\n   * Handsontable version\n   *\n   * @type {String}\n   */\n  // this.version = Handsontable.version;\n\n  Hooks.getSingleton().run(instance, 'construct');\n};\n", "idx": 79, "id": 14501, "msg": "", "proj": "handsontable-handsontable", "lang": "js"}
{"patch": "@@ -238,8 +238,11 @@ class DequeQueue(collections.deque):\n     deque wrapper implementing the Queue interface.\n     \"\"\"\n \n-    put = collections.deque.append\n-    get = collections.deque.pop\n+    def put(self, *args, **kwargs):\n+        return self.append(*args)\n+\n+    def get(self, **kwargs):\n+        return self.pop()\n \n \n class AsyncCompletionException(Exception):", "y": 1, "oldf": "# -*- coding: utf-8 -*-\n#\n# Copyright 2012-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThe worker communicates with the scheduler and does two things:\n\n1. Sends all tasks that has to be run\n2. Gets tasks from the scheduler that should be run\n\nWhen running in local mode, the worker talks directly to a :py:class:`~luigi.scheduler.CentralPlannerScheduler` instance.\nWhen you run a central server, the worker will talk to the scheduler using a :py:class:`~luigi.rpc.RemoteScheduler` instance.\n\"\"\"\n\nimport collections\nimport getpass\nimport logging\nimport multiprocessing  # Note: this seems to have some stability issues: https://github.com/spotify/luigi/pull/438\nimport os\nimport signal\n\ntry:\n    import Queue\nexcept ImportError:\n    import queue as Queue\nimport random\nimport socket\nimport threading\nimport time\nimport traceback\nimport types\n\nfrom luigi import six\n\nfrom luigi import notifications\nfrom luigi.event import Event\nfrom luigi.task_register import load_task\nfrom luigi.scheduler import DISABLED, DONE, FAILED, PENDING, CentralPlannerScheduler\nfrom luigi.target import Target\nfrom luigi.task import Task, flatten, getpaths, Config\nfrom luigi.task_register import TaskClassException\nfrom luigi.task_status import RUNNING\nfrom luigi.parameter import FloatParameter, IntParameter, BoolParameter\n\ntry:\n    import simplejson as json\nexcept ImportError:\n    import json\n\nlogger = logging.getLogger('luigi-interface')\n\n# Prevent fork() from being called during a C-level getaddrinfo() which uses a process-global mutex,\n# that may not be unlocked in child process, resulting in the process being locked indefinitely.\nfork_lock = threading.Lock()\n\n# Why we assert on _WAIT_INTERVAL_EPS:\n# multiprocessing.Queue.get() is undefined for timeout=0 it seems:\n# https://docs.python.org/3.4/library/multiprocessing.html#multiprocessing.Queue.get.\n# I also tried with really low epsilon, but then ran into the same issue where\n# the test case \"test_external_dependency_worker_is_patient\" got stuck. So I\n# unscientifically just set the final value to a floating point number that\n# \"worked for me\".\n_WAIT_INTERVAL_EPS = 0.00001\n\n\nclass TaskException(Exception):\n    pass\n\n\nclass TaskProcess(multiprocessing.Process):\n\n    \"\"\" Wrap all task execution in this class.\n\n    Mainly for convenience since this is run in a separate process. \"\"\"\n\n    def __init__(self, task, worker_id, result_queue, random_seed=False, worker_timeout=0,\n                 tracking_url_callback=None):\n        super(TaskProcess, self).__init__()\n        self.task = task\n        self.worker_id = worker_id\n        self.result_queue = result_queue\n        self.random_seed = random_seed\n        self.tracking_url_callback = tracking_url_callback\n        if task.worker_timeout is not None:\n            worker_timeout = task.worker_timeout\n        self.timeout_time = time.time() + worker_timeout if worker_timeout else None\n\n    def _run_get_new_deps(self):\n        try:\n            task_gen = self.task.run(tracking_url_callback=self.tracking_url_callback)\n        except TypeError as ex:\n            if 'unexpected keyword argument' not in getattr(ex, 'message', ex.args[0]):\n                raise\n            task_gen = self.task.run()\n        if not isinstance(task_gen, types.GeneratorType):\n            return None\n\n        next_send = None\n        while True:\n            try:\n                if next_send is None:\n                    requires = six.next(task_gen)\n                else:\n                    requires = task_gen.send(next_send)\n            except StopIteration:\n                return None\n\n            new_req = flatten(requires)\n            new_deps = [(t.task_module, t.task_family, t.to_str_params())\n                        for t in new_req]\n            if all(t.complete() for t in new_req):\n                next_send = getpaths(requires)\n            else:\n                return new_deps\n\n    def run(self):\n        logger.info('[pid %s] Worker %s running   %s', os.getpid(), self.worker_id, self.task.task_id)\n\n        if self.random_seed:\n            # Need to have different random seeds if running in separate processes\n            random.seed((os.getpid(), time.time()))\n\n        status = FAILED\n        expl = ''\n        missing = []\n        new_deps = []\n        try:\n            # Verify that all the tasks are fulfilled!\n            missing = [dep.task_id for dep in self.task.deps() if not dep.complete()]\n            if missing:\n                deps = 'dependency' if len(missing) == 1 else 'dependencies'\n                raise RuntimeError('Unfulfilled %s at run time: %s' % (deps, ', '.join(missing)))\n            self.task.trigger_event(Event.START, self.task)\n            t0 = time.time()\n            status = None\n\n            if self.task.run == NotImplemented:\n                # External task\n                # TODO(erikbern): We should check for task completeness after non-external tasks too!\n                # This will resolve #814 and make things a lot more consistent\n                status = DONE if self.task.complete() else FAILED\n            else:\n                new_deps = self._run_get_new_deps()\n                status = DONE if not new_deps else PENDING\n\n            if new_deps:\n                logger.info(\n                    '[pid %s] Worker %s new requirements      %s',\n                    os.getpid(), self.worker_id, self.task.task_id)\n            elif status == DONE:\n                self.task.trigger_event(\n                    Event.PROCESSING_TIME, self.task, time.time() - t0)\n                expl = json.dumps(self.task.on_success())\n                logger.info('[pid %s] Worker %s done      %s', os.getpid(),\n                            self.worker_id, self.task.task_id)\n                self.task.trigger_event(Event.SUCCESS, self.task)\n\n        except KeyboardInterrupt:\n            raise\n        except BaseException as ex:\n            status = FAILED\n            logger.exception(\"[pid %s] Worker %s failed    %s\", os.getpid(), self.worker_id, self.task)\n            self.task.trigger_event(Event.FAILURE, self.task, ex)\n            raw_error_message = self.task.on_failure(ex)\n            expl = json.dumps(raw_error_message)\n            self._send_error_notification(raw_error_message)\n        finally:\n            self.result_queue.put(\n                (self.task.task_id, status, expl, missing, new_deps))\n\n    def _send_error_notification(self, raw_error_message):\n        subject = \"Luigi: %s FAILED\" % self.task\n        notification_error_message = notifications.wrap_traceback(raw_error_message)\n        formatted_error_message = notifications.format_task_error(subject, self.task,\n                                                                  formatted_exception=notification_error_message)\n        notifications.send_error_email(subject, formatted_error_message, self.task.owner_email)\n\n    def _recursive_terminate(self):\n        import psutil\n\n        try:\n            parent = psutil.Process(self.pid)\n            children = parent.children(recursive=True)\n\n            # terminate parent. Give it a chance to clean up\n            super(TaskProcess, self).terminate()\n            parent.wait()\n\n            # terminate children\n            for child in children:\n                try:\n                    child.terminate()\n                except psutil.NoSuchProcess:\n                    continue\n        except psutil.NoSuchProcess:\n            return\n\n    def terminate(self):\n        \"\"\"Terminate this process and its subprocesses.\"\"\"\n        # default terminate() doesn't cleanup child processes, it orphans them.\n        try:\n            return self._recursive_terminate()\n        except ImportError:\n            return super(TaskProcess, self).terminate()\n\n\nclass SingleProcessPool(object):\n    \"\"\"\n    Dummy process pool for using a single processor.\n\n    Imitates the api of multiprocessing.Pool using single-processor equivalents.\n    \"\"\"\n\n    def apply_async(self, function, args):\n        return function(*args)\n\n    def close(self):\n        pass\n\n    def join(self):\n        pass\n\n\nclass DequeQueue(collections.deque):\n    \"\"\"\n    deque wrapper implementing the Queue interface.\n    \"\"\"\n\n    put = collections.deque.append\n    get = collections.deque.pop\n\n\nclass AsyncCompletionException(Exception):\n    \"\"\"\n    Exception indicating that something went wrong with checking complete.\n    \"\"\"\n\n    def __init__(self, trace):\n        self.trace = trace\n\n\nclass TracebackWrapper(object):\n    \"\"\"\n    Class to wrap tracebacks so we can know they're not just strings.\n    \"\"\"\n\n    def __init__(self, trace):\n        self.trace = trace\n\n\ndef check_complete(task, out_queue):\n    \"\"\"\n    Checks if task is complete, puts the result to out_queue.\n    \"\"\"\n    logger.debug(\"Checking if %s is complete\", task)\n    try:\n        is_complete = task.complete()\n    except BaseException:\n        is_complete = TracebackWrapper(traceback.format_exc())\n    out_queue.put((task, is_complete))\n\n\nclass worker(Config):\n\n    ping_interval = FloatParameter(default=1.0,\n                                   config_path=dict(section='core', name='worker-ping-interval'))\n    keep_alive = BoolParameter(default=False,\n                               config_path=dict(section='core', name='worker-keep-alive'))\n    count_uniques = BoolParameter(default=False,\n                                  config_path=dict(section='core', name='worker-count-uniques'),\n                                  description='worker-count-uniques means that we will keep a '\n                                  'worker alive only if it has a unique pending task, as '\n                                  'well as having keep-alive true')\n    wait_interval = FloatParameter(default=1.0,\n                                   config_path=dict(section='core', name='worker-wait-interval'))\n    wait_jitter = FloatParameter(default=5.0)\n\n    max_reschedules = IntParameter(default=1,\n                                   config_path=dict(section='core', name='worker-max-reschedules'))\n    timeout = IntParameter(default=0,\n                           config_path=dict(section='core', name='worker-timeout'))\n    task_limit = IntParameter(default=None,\n                              config_path=dict(section='core', name='worker-task-limit'))\n    retry_external_tasks = BoolParameter(default=False,\n                                         config_path=dict(section='core', name='retry-external-tasks'),\n                                         description='If true, incomplete external tasks will be '\n                                         'retested for completion while Luigi is running.')\n\n\nclass KeepAliveThread(threading.Thread):\n    \"\"\"\n    Periodically tell the scheduler that the worker still lives.\n    \"\"\"\n\n    def __init__(self, scheduler, worker_id, ping_interval):\n        super(KeepAliveThread, self).__init__()\n        self._should_stop = threading.Event()\n        self._scheduler = scheduler\n        self._worker_id = worker_id\n        self._ping_interval = ping_interval\n\n    def stop(self):\n        self._should_stop.set()\n\n    def run(self):\n        while True:\n            self._should_stop.wait(self._ping_interval)\n            if self._should_stop.is_set():\n                logger.info(\"Worker %s was stopped. Shutting down Keep-Alive thread\" % self._worker_id)\n                break\n            with fork_lock:\n                try:\n                    self._scheduler.ping(worker=self._worker_id)\n                except:  # httplib.BadStatusLine:\n                    logger.warning('Failed pinging scheduler')\n\n\nclass Worker(object):\n    \"\"\"\n    Worker object communicates with a scheduler.\n\n    Simple class that talks to a scheduler and:\n\n    * tells the scheduler what it has to do + its dependencies\n    * asks for stuff to do (pulls it in a loop and runs it)\n    \"\"\"\n\n    def __init__(self, scheduler=None, worker_id=None, worker_processes=1, assistant=False, **kwargs):\n        if scheduler is None:\n            scheduler = CentralPlannerScheduler()\n\n        self.worker_processes = int(worker_processes)\n        self._worker_info = self._generate_worker_info()\n\n        if not worker_id:\n            worker_id = 'Worker(%s)' % ', '.join(['%s=%s' % (k, v) for k, v in self._worker_info])\n\n        self._config = worker(**kwargs)\n\n        assert self._config.wait_interval >= _WAIT_INTERVAL_EPS, \"[worker] wait_interval must be positive\"\n        assert self._config.wait_jitter >= 0.0, \"[worker] wait_jitter must be equal or greater than zero\"\n\n        self._id = worker_id\n        self._scheduler = scheduler\n        self._assistant = assistant\n        self._stop_requesting_work = False\n\n        self.host = socket.gethostname()\n        self._scheduled_tasks = {}\n        self._suspended_tasks = {}\n\n        self._first_task = None\n\n        self.add_succeeded = True\n        self.run_succeeded = True\n        self.unfulfilled_counts = collections.defaultdict(int)\n\n        try:\n            signal.signal(signal.SIGUSR1, self.handle_interrupt)\n        except AttributeError:\n            pass\n\n        self._keep_alive_thread = KeepAliveThread(self._scheduler, self._id, self._config.ping_interval)\n        self._keep_alive_thread.daemon = True\n        self._keep_alive_thread.start()\n\n        # Keep info about what tasks are running (could be in other processes)\n        self._task_result_queue = multiprocessing.Queue()\n        self._running_tasks = {}\n\n        # Stuff for execution_summary\n        self._add_task_history = []\n        self._get_work_response_history = []\n\n    def _add_task(self, *args, **kwargs):\n        \"\"\"\n        Call ``self._scheduler.add_task``, but store the values too so we can\n        implement :py:func:`luigi.execution_summary.summary`.\n        \"\"\"\n        task_id = kwargs['task_id']\n        status = kwargs['status']\n        runnable = kwargs['runnable']\n        task = self._scheduled_tasks.get(task_id)\n        if task:\n            msg = (task, status, runnable)\n            self._add_task_history.append(msg)\n        self._scheduler.add_task(*args, **kwargs)\n\n        logger.info('Informed scheduler that task   %s   has status   %s', task_id, status)\n\n    def stop(self):\n        \"\"\"\n        Stop the KeepAliveThread associated with this Worker.\n\n        This should be called whenever you are done with a worker instance to clean up.\n\n        Warning: this should _only_ be performed if you are sure this worker\n        is not performing any work or will perform any work after this has been called\n\n        TODO: also kill all currently running tasks\n\n        TODO (maybe): Worker should be/have a context manager to enforce calling this\n            whenever you stop using a Worker instance\n        \"\"\"\n        self._keep_alive_thread.stop()\n        self._keep_alive_thread.join()\n        for task in self._running_tasks.values():\n            if task.is_alive():\n                task.terminate()\n\n    def _generate_worker_info(self):\n        # Generate as much info as possible about the worker\n        # Some of these calls might not be available on all OS's\n        args = [('salt', '%09d' % random.randrange(0, 999999999)),\n                ('workers', self.worker_processes)]\n        try:\n            args += [('host', socket.gethostname())]\n        except BaseException:\n            pass\n        try:\n            args += [('username', getpass.getuser())]\n        except BaseException:\n            pass\n        try:\n            args += [('pid', os.getpid())]\n        except BaseException:\n            pass\n        try:\n            sudo_user = os.getenv(\"SUDO_USER\")\n            if sudo_user:\n                args.append(('sudo_user', sudo_user))\n        except BaseException:\n            pass\n        return args\n\n    def _validate_task(self, task):\n        if not isinstance(task, Task):\n            raise TaskException('Can not schedule non-task %s' % task)\n\n        if not task.initialized():\n            # we can't get the repr of it since it's not initialized...\n            raise TaskException('Task of class %s not initialized. Did you override __init__ and forget to call super(...).__init__?' % task.__class__.__name__)\n\n    def _log_complete_error(self, task, tb):\n        log_msg = \"Will not schedule {task} or any dependencies due to error in complete() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)\n\n    def _log_dependency_error(self, task, tb):\n        log_msg = \"Will not schedule {task} or any dependencies due to error in deps() method:\\n{tb}\".format(task=task, tb=tb)\n        logger.warning(log_msg)\n\n    def _log_unexpected_error(self, task):\n        logger.exception(\"Luigi unexpected framework error while scheduling %s\", task)  # needs to be called from within except clause\n\n    def _email_complete_error(self, task, formatted_traceback):\n        # like logger.exception but with WARNING level\n        subject = \"Luigi: {task} failed scheduling. Host: {host}\".format(task=task, host=self.host)\n        headline = \"Will not schedule task or any dependencies due to error in complete() method\"\n\n        message = notifications.format_task_error(headline, task, formatted_traceback)\n        notifications.send_error_email(subject, message, task.owner_email)\n\n    def _email_dependency_error(self, task, formatted_traceback):\n        subject = \"Luigi: {task} failed scheduling. Host: {host}\".format(task=task, host=self.host)\n        headline = \"Will not schedule task or any dependencies due to error in deps() method\"\n\n        message = notifications.format_task_error(headline, task, formatted_traceback)\n        notifications.send_error_email(subject, message, task.owner_email)\n\n    def _email_unexpected_error(self, task, formatted_traceback):\n        subject = \"Luigi: Framework error while scheduling {task}. Host: {host}\".format(task=task, host=self.host)\n        headline = \"Luigi framework error\"\n\n        message = notifications.format_task_error(headline, task, formatted_traceback)\n        notifications.send_error_email(subject, message, task.owner_email)\n\n    def add(self, task, multiprocess=False):\n        \"\"\"\n        Add a Task for the worker to check and possibly schedule and run.\n\n        Returns True if task and its dependencies were successfully scheduled or completed before.\n        \"\"\"\n        if self._first_task is None and hasattr(task, 'task_id'):\n            self._first_task = task.task_id\n        self.add_succeeded = True\n        if multiprocess:\n            queue = multiprocessing.Manager().Queue()\n            pool = multiprocessing.Pool()\n        else:\n            queue = DequeQueue()\n            pool = SingleProcessPool()\n        self._validate_task(task)\n        pool.apply_async(check_complete, [task, queue])\n\n        # we track queue size ourselves because len(queue) won't work for multiprocessing\n        queue_size = 1\n        try:\n            seen = set([task.task_id])\n            while queue_size:\n                current = queue.get()\n                queue_size -= 1\n                item, is_complete = current\n                for next in self._add(item, is_complete):\n                    if next.task_id not in seen:\n                        self._validate_task(next)\n                        seen.add(next.task_id)\n                        pool.apply_async(check_complete, [next, queue])\n                        queue_size += 1\n        except (KeyboardInterrupt, TaskException):\n            raise\n        except Exception as ex:\n            self.add_succeeded = False\n            formatted_traceback = traceback.format_exc()\n            self._log_unexpected_error(task)\n            task.trigger_event(Event.BROKEN_TASK, task, ex)\n            self._email_unexpected_error(task, formatted_traceback)\n        finally:\n            pool.close()\n            pool.join()\n        return self.add_succeeded\n\n    def _add(self, task, is_complete):\n        if self._config.task_limit is not None and len(self._scheduled_tasks) >= self._config.task_limit:\n            logger.warning('Will not schedule %s or any dependencies due to exceeded task-limit of %d', task, self._config.task_limit)\n            return\n\n        formatted_traceback = None\n        try:\n            self._check_complete_value(is_complete)\n        except KeyboardInterrupt:\n            raise\n        except AsyncCompletionException as ex:\n            formatted_traceback = ex.trace\n        except BaseException:\n            formatted_traceback = traceback.format_exc()\n\n        if formatted_traceback is not None:\n            self.add_succeeded = False\n            self._log_complete_error(task, formatted_traceback)\n            task.trigger_event(Event.DEPENDENCY_MISSING, task)\n            self._email_complete_error(task, formatted_traceback)\n            # abort, i.e. don't schedule any subtasks of a task with\n            # failing complete()-method since we don't know if the task\n            # is complete and subtasks might not be desirable to run if\n            # they have already ran before\n            return\n\n        if is_complete:\n            deps = None\n            status = DONE\n            runnable = False\n\n            task.trigger_event(Event.DEPENDENCY_PRESENT, task)\n        elif task.run == NotImplemented:\n            deps = None\n            status = PENDING\n            runnable = worker().retry_external_tasks\n\n            task.trigger_event(Event.DEPENDENCY_MISSING, task)\n            logger.warning('Data for %s does not exist (yet?). The task is an '\n                           'external data depedency, so it can not be run from'\n                           ' this luigi process.', task.task_id)\n\n        else:\n            try:\n                deps = task.deps()\n            except Exception:\n                formatted_traceback = traceback.format_exc()\n                self.add_succeeded = False\n                self._log_dependency_error(task, formatted_traceback)\n                self._email_dependency_error(task, formatted_traceback)\n                return\n            status = PENDING\n            runnable = True\n\n        if task.disabled:\n            status = DISABLED\n\n        if deps:\n            for d in deps:\n                self._validate_dependency(d)\n                task.trigger_event(Event.DEPENDENCY_DISCOVERED, task, d)\n                yield d  # return additional tasks to add\n\n            deps = [d.task_id for d in deps]\n\n        self._scheduled_tasks[task.task_id] = task\n        self._add_task(worker=self._id, task_id=task.task_id, status=status,\n                       deps=deps, runnable=runnable, priority=task.priority,\n                       resources=task.process_resources(),\n                       params=task.to_str_params(),\n                       family=task.task_family,\n                       module=task.task_module)\n\n    def _validate_dependency(self, dependency):\n        if isinstance(dependency, Target):\n            raise Exception('requires() can not return Target objects. Wrap it in an ExternalTask class')\n        elif not isinstance(dependency, Task):\n            raise Exception('requires() must return Task objects')\n\n    def _check_complete_value(self, is_complete):\n        if is_complete not in (True, False):\n            if isinstance(is_complete, TracebackWrapper):\n                raise AsyncCompletionException(is_complete.trace)\n            raise Exception(\"Return value of Task.complete() must be boolean (was %r)\" % is_complete)\n\n    def _add_worker(self):\n        self._worker_info.append(('first_task', self._first_task))\n        self._scheduler.add_worker(self._id, self._worker_info)\n\n    def _log_remote_tasks(self, running_tasks, n_pending_tasks, n_unique_pending):\n        logger.info(\"Done\")\n        logger.info(\"There are no more tasks to run at this time\")\n        if running_tasks:\n            for r in running_tasks:\n                logger.info('%s is currently run by worker %s', r['task_id'], r['worker'])\n        elif n_pending_tasks:\n            logger.info(\"There are %s pending tasks possibly being run by other workers\", n_pending_tasks)\n            if n_unique_pending:\n                logger.info(\"There are %i pending tasks unique to this worker\", n_unique_pending)\n\n    def _get_work(self):\n        if self._stop_requesting_work:\n            return None, 0, 0, 0\n        logger.debug(\"Asking scheduler for work...\")\n        r = self._scheduler.get_work(\n            worker=self._id,\n            host=self.host,\n            assistant=self._assistant,\n            current_tasks=list(self._running_tasks.keys()),\n        )\n        n_pending_tasks = r['n_pending_tasks']\n        task_id = r['task_id']\n        running_tasks = r['running_tasks']\n        n_unique_pending = r['n_unique_pending']\n\n        self._get_work_response_history.append(dict(\n            task_id=task_id,\n            running_tasks=running_tasks,\n        ))\n\n        if task_id is not None and task_id not in self._scheduled_tasks:\n            logger.info('Did not schedule %s, will load it dynamically', task_id)\n\n            try:\n                # TODO: we should obtain the module name from the server!\n                self._scheduled_tasks[task_id] = \\\n                    load_task(module=r.get('task_module'),\n                              task_name=r['task_family'],\n                              params_str=r['task_params'])\n            except TaskClassException as ex:\n                msg = 'Cannot find task for %s' % task_id\n                logger.exception(msg)\n                subject = 'Luigi: %s' % msg\n                error_message = notifications.wrap_traceback(ex)\n                notifications.send_error_email(subject, error_message)\n                self._add_task(worker=self._id, task_id=task_id, status=FAILED, runnable=False,\n                               assistant=self._assistant)\n                task_id = None\n                self.run_succeeded = False\n\n        return task_id, running_tasks, n_pending_tasks, n_unique_pending\n\n    def _run_task(self, task_id):\n        task = self._scheduled_tasks[task_id]\n\n        p = self._create_task_process(task)\n\n        self._running_tasks[task_id] = p\n\n        if self.worker_processes > 1:\n            with fork_lock:\n                p.start()\n        else:\n            # Run in the same process\n            p.run()\n\n    def _create_task_process(self, task):\n        def update_tracking_url(tracking_url):\n            self._scheduler.add_task(\n                task_id=task.task_id,\n                worker=self._id,\n                status=RUNNING,\n                tracking_url=tracking_url,\n            )\n\n        return TaskProcess(\n            task, self._id, self._task_result_queue,\n            random_seed=bool(self.worker_processes > 1),\n            worker_timeout=self._config.timeout,\n            tracking_url_callback=update_tracking_url,\n        )\n\n    def _purge_children(self):\n        \"\"\"\n        Find dead children and put a response on the result queue.\n\n        :return:\n        \"\"\"\n        for task_id, p in six.iteritems(self._running_tasks):\n            if not p.is_alive() and p.exitcode:\n                error_msg = 'Worker task %s died unexpectedly with exit code %s' % (task_id, p.exitcode)\n            elif p.timeout_time is not None and time.time() > float(p.timeout_time) and p.is_alive():\n                p.terminate()\n                error_msg = 'Worker task %s timed out and was terminated.' % task_id\n            else:\n                continue\n\n            logger.info(error_msg)\n            self._task_result_queue.put((task_id, FAILED, error_msg, [], []))\n\n    def _handle_next_task(self):\n        \"\"\"\n        We have to catch three ways a task can be \"done\":\n\n        1. normal execution: the task runs/fails and puts a result back on the queue,\n        2. new dependencies: the task yielded new deps that were not complete and\n           will be rescheduled and dependencies added,\n        3. child process dies: we need to catch this separately.\n        \"\"\"\n        while True:\n            self._purge_children()  # Deal with subprocess failures\n\n            try:\n                task_id, status, expl, missing, new_requirements = (\n                    self._task_result_queue.get(\n                        timeout=self._config.wait_interval))\n            except Queue.Empty:\n                return\n\n            task = self._scheduled_tasks[task_id]\n            if not task or task_id not in self._running_tasks:\n                continue\n                # Not a running task. Probably already removed.\n                # Maybe it yielded something?\n            new_deps = []\n            if new_requirements:\n                new_req = [load_task(module, name, params)\n                           for module, name, params in new_requirements]\n                for t in new_req:\n                    self.add(t)\n                new_deps = [t.task_id for t in new_req]\n\n            self._add_task(worker=self._id,\n                           task_id=task_id,\n                           status=status,\n                           expl=expl,\n                           resources=task.process_resources(),\n                           runnable=None,\n                           params=task.to_str_params(),\n                           family=task.task_family,\n                           module=task.task_module,\n                           new_deps=new_deps,\n                           assistant=self._assistant)\n\n            self._running_tasks.pop(task_id)\n\n            # re-add task to reschedule missing dependencies\n            if missing:\n                reschedule = True\n\n                # keep out of infinite loops by not rescheduling too many times\n                for task_id in missing:\n                    self.unfulfilled_counts[task_id] += 1\n                    if (self.unfulfilled_counts[task_id] >\n                            self._config.max_reschedules):\n                        reschedule = False\n                if reschedule:\n                    self.add(task)\n\n            self.run_succeeded &= (status == DONE) or (len(new_deps) > 0)\n            return\n\n    def _sleeper(self):\n        # TODO is exponential backoff necessary?\n        while True:\n            jitter = self._config.wait_jitter\n            wait_interval = self._config.wait_interval + random.uniform(0, jitter)\n            logger.debug('Sleeping for %f seconds', wait_interval)\n            time.sleep(wait_interval)\n            yield\n\n    def _keep_alive(self, n_pending_tasks, n_unique_pending):\n        \"\"\"\n        Returns true if a worker should stay alive given.\n\n        If worker-keep-alive is not set, this will always return false.\n        For an assistant, it will always return the value of worker-keep-alive.\n        Otherwise, it will return true for nonzero n_pending_tasks.\n\n        If worker-count-uniques is true, it will also\n        require that one of the tasks is unique to this worker.\n        \"\"\"\n        if not self._config.keep_alive:\n            return False\n        elif self._assistant:\n            return True\n        else:\n            return n_pending_tasks and (n_unique_pending or not self._config.count_uniques)\n\n    def handle_interrupt(self, signum, _):\n        \"\"\"\n        Stops the assistant from asking for more work on SIGUSR1\n        \"\"\"\n        if signum == signal.SIGUSR1:\n            self._config.keep_alive = False\n            self._stop_requesting_work = True\n\n    def run(self):\n        \"\"\"\n        Returns True if all scheduled tasks were executed successfully.\n        \"\"\"\n        logger.info('Running Worker with %d processes', self.worker_processes)\n\n        sleeper = self._sleeper()\n        self.run_succeeded = True\n\n        self._add_worker()\n\n        while True:\n            while len(self._running_tasks) >= self.worker_processes:\n                logger.debug('%d running tasks, waiting for next task to finish', len(self._running_tasks))\n                self._handle_next_task()\n\n            task_id, running_tasks, n_pending_tasks, n_unique_pending = self._get_work()\n\n            if task_id is None:\n                if not self._stop_requesting_work:\n                    self._log_remote_tasks(running_tasks, n_pending_tasks, n_unique_pending)\n                if len(self._running_tasks) == 0:\n                    if self._keep_alive(n_pending_tasks, n_unique_pending):\n                        six.next(sleeper)\n                        continue\n                    else:\n                        break\n                else:\n                    self._handle_next_task()\n                    continue\n\n            # task_id is not None:\n            logger.debug(\"Pending tasks: %s\", n_pending_tasks)\n            self._run_task(task_id)\n\n        while len(self._running_tasks):\n            logger.debug('Shut down Worker, %d more tasks to go', len(self._running_tasks))\n            self._handle_next_task()\n\n        return self.run_succeeded\n", "idx": 1, "id": 13556, "msg": "Omit **kwargs, or propagate all the way? Better than including them to be silently dropped if passed in.", "proj": "spotify-luigi", "lang": "py"}
{"patch": "@@ -523,13 +523,13 @@ static void wait_cbfunc(struct pmix_peer_t *pr,\n                         (NULL == buf) ? -1 : (int)buf->bytes_used);\n \n     if (NULL == buf) {\n-        rc = PMIX_ERR_BAD_PARAM;\n+        ret = PMIX_ERR_BAD_PARAM;\n         goto report;\n     }\n     /* a zero-byte buffer indicates that this recv is being\n      * completed due to a lost connection */\n     if (PMIX_BUFFER_IS_EMPTY(buf)) {\n-        rc = PMIX_ERR_UNREACH;\n+        ret = PMIX_ERR_UNREACH;\n         goto report;\n     }\n ", "y": 0, "oldf": "/* -*- Mode: C; c-basic-offset:4 ; indent-tabs-mode:nil -*- */\n/*\n * Copyright (c) 2014-2018 Intel, Inc. All rights reserved.\n * Copyright (c) 2014-2015 Research Organization for Information Science\n *                         and Technology (RIST). All rights reserved.\n * Copyright (c) 2014      Artem Y. Polyakov <artpol84@gmail.com>.\n *                         All rights reserved.\n * Copyright (c) 2016      Mellanox Technologies, Inc.\n *                         All rights reserved.\n * Copyright (c) 2016      IBM Corporation.  All rights reserved.\n * $COPYRIGHT$\n *\n * Additional copyrights may follow\n *\n * $HEADER$\n */\n\n#include <src/include/pmix_config.h>\n\n#include <src/include/types.h>\n#include <src/include/pmix_stdint.h>\n\n#include <pmix.h>\n#include <pmix_rename.h>\n\n#include \"src/include/pmix_globals.h\"\n\n#ifdef HAVE_STRING_H\n#include <string.h>\n#endif\n#include <fcntl.h>\n#ifdef HAVE_UNISTD_H\n#include <unistd.h>\n#endif\n#ifdef HAVE_SYS_SOCKET_H\n#include <sys/socket.h>\n#endif\n#ifdef HAVE_SYS_UN_H\n#include <sys/un.h>\n#endif\n#ifdef HAVE_SYS_UIO_H\n#include <sys/uio.h>\n#endif\n#ifdef HAVE_SYS_TYPES_H\n#include <sys/types.h>\n#endif\n#include PMIX_EVENT_HEADER\n\n#include \"src/class/pmix_list.h\"\n#include \"src/threads/threads.h\"\n#include \"src/mca/bfrops/bfrops.h\"\n#include \"src/util/argv.h\"\n#include \"src/util/error.h\"\n#include \"src/util/output.h\"\n#include \"src/mca/ptl/ptl.h\"\n\n#include \"pmix_client_ops.h\"\n\nstatic void wait_cbfunc(struct pmix_peer_t *pr,\n                        pmix_ptl_hdr_t *hdr,\n                        pmix_buffer_t *buf, void *cbdata);\nstatic void op_cbfunc(pmix_status_t status, void *cbdata);\nstatic void wait_lookup_cbfunc(struct pmix_peer_t *pr,\n                               pmix_ptl_hdr_t *hdr,\n                               pmix_buffer_t *buf, void *cbdata);\nstatic void lookup_cbfunc(pmix_status_t status, pmix_pdata_t pdata[], size_t ndata,\n                          void *cbdata);\n\nPMIX_EXPORT pmix_status_t PMIx_Publish(const pmix_info_t info[],\n                                       size_t ninfo)\n{\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: publish called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* create a callback object to let us know when it is done */\n    cb = PMIX_NEW(pmix_cb_t);\n\n    if (PMIX_SUCCESS != (rc = PMIx_Publish_nb(info, ninfo, op_cbfunc, cb))) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(cb);\n        return rc;\n    }\n\n    /* wait for the server to ack our request */\n    PMIX_WAIT_THREAD(&cb->lock);\n    rc = (pmix_status_t)cb->status;\n    PMIX_RELEASE(cb);\n\n    return rc;\n}\n\nPMIX_EXPORT pmix_status_t PMIx_Publish_nb(const pmix_info_t info[], size_t ninfo,\n                                          pmix_op_cbfunc_t cbfunc, void *cbdata)\n{\n    pmix_buffer_t *msg;\n    pmix_cmd_t cmd = PMIX_PUBLISHNB_CMD;\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: publish called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* check for bozo cases */\n    if (NULL == info) {\n        /* nothing to publish */\n        PMIX_ERROR_LOG(PMIX_ERR_BAD_PARAM);\n        return PMIX_ERR_BAD_PARAM;\n    }\n\n    /* create the publish cmd */\n    msg = PMIX_NEW(pmix_buffer_t);\n    /* pack the cmd */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &cmd, 1, PMIX_COMMAND);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pack our effective userid - will be used to constrain lookup */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &pmix_globals.uid, 1, PMIX_UINT32);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pass the number of info structs - needed on remote end so\n     * space can be malloc'd for the values */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &ninfo, 1, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    if (0 < ninfo) {\n        /* pack the info structs */\n        PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                         msg, info, ninfo, PMIX_INFO);\n        if (PMIX_SUCCESS != rc) {\n            PMIX_ERROR_LOG(rc);\n            PMIX_RELEASE(msg);\n            return rc;\n        }\n    }\n\n    /* create a callback object as we need to pass it to the\n     * recv routine so we know which callback to use when\n     * the return message is recvd */\n    cb = PMIX_NEW(pmix_cb_t);\n    cb->cbfunc.opfn = cbfunc;\n    cb->cbdata = cbdata;\n\n    /* push the message into our event base to send to the server */\n    PMIX_PTL_SEND_RECV(rc, pmix_client_globals.myserver,\n                       msg, wait_cbfunc, (void*)cb);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_RELEASE(msg);\n        PMIX_RELEASE(cb);\n    }\n\n    return rc;\n}\n\nPMIX_EXPORT pmix_status_t PMIx_Lookup(pmix_pdata_t pdata[], size_t ndata,\n                                      const pmix_info_t info[], size_t ninfo)\n{\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n    char **keys = NULL;\n    size_t i;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: lookup called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* bozo protection */\n    if (NULL == pdata) {\n        return PMIX_ERR_BAD_PARAM;\n    }\n\n    /* transfer the pdata keys to the keys argv array */\n    for (i=0; i < ndata; i++) {\n        if ('\\0' != pdata[i].key[0]) {\n            pmix_argv_append_nosize(&keys, pdata[i].key);\n        }\n    }\n\n    /* create a callback object as we need to pass it to the\n     * recv routine so we know which callback to use when\n     * the return message is recvd */\n    cb = PMIX_NEW(pmix_cb_t);\n    cb->cbdata = (void*)pdata;\n    cb->nvals = ndata;\n\n    if (PMIX_SUCCESS != (rc = PMIx_Lookup_nb(keys, info, ninfo,\n                                             lookup_cbfunc, cb))) {\n        PMIX_RELEASE(cb);\n        pmix_argv_free(keys);\n        return rc;\n    }\n\n    /* wait for the server to ack our request */\n    PMIX_WAIT_THREAD(&cb->lock);\n\n    /* the data has been stored in the info array by lookup_cbfunc, so\n     * nothing more for us to do */\n    rc = cb->status;\n    PMIX_RELEASE(cb);\n    return rc;\n}\n\nPMIX_EXPORT pmix_status_t PMIx_Lookup_nb(char **keys,\n                                         const pmix_info_t info[], size_t ninfo,\n                                         pmix_lookup_cbfunc_t cbfunc, void *cbdata)\n{\n    pmix_buffer_t *msg;\n    pmix_cmd_t cmd = PMIX_LOOKUPNB_CMD;\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n    size_t nkeys, n;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: lookup_nb called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* check for bozo cases */\n    if (NULL == keys) {\n        return PMIX_ERR_BAD_PARAM;\n    }\n\n    /* create the lookup cmd */\n    msg = PMIX_NEW(pmix_buffer_t);\n    /* pack the cmd */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &cmd, 1, PMIX_COMMAND);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pack our effective userid - will be used to constrain lookup */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &pmix_globals.uid, 1, PMIX_UINT32);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pack the keys */\n    nkeys = pmix_argv_count(keys);\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &nkeys, 1, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    if (0 < nkeys) {\n        for (n=0; n < nkeys; n++) {\n            PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                             msg, &keys[n], 1, PMIX_STRING);\n            if (PMIX_SUCCESS != rc) {\n                PMIX_ERROR_LOG(rc);\n                PMIX_RELEASE(msg);\n                return rc;\n            }\n        }\n    }\n    /* pass the number of info structs - needed on remote end so\n     * space can be malloc'd for the values */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &ninfo, 1, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    if (0 < ninfo) {\n        /* pack the info structs */\n        PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                         msg, info, ninfo, PMIX_INFO);\n        if (PMIX_SUCCESS != rc) {\n            PMIX_ERROR_LOG(rc);\n            PMIX_RELEASE(msg);\n            return rc;\n        }\n    }\n\n    /* create a callback object as we need to pass it to the\n     * recv routine so we know which callback to use when\n     * the return message is recvd */\n    cb = PMIX_NEW(pmix_cb_t);\n    cb->cbfunc.lookupfn = cbfunc;\n    cb->cbdata = cbdata;\n\n    /* send to the server */\n    PMIX_PTL_SEND_RECV(rc, pmix_client_globals.myserver,\n                       msg, wait_lookup_cbfunc, (void*)cb);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_RELEASE(msg);\n        PMIX_RELEASE(cb);\n    }\n\n    return rc;\n}\n\nPMIX_EXPORT pmix_status_t PMIx_Unpublish(char **keys,\n                                         const pmix_info_t info[],\n                                         size_t ninfo)\n{\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: unpublish called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* create a callback object as we need to pass it to the\n     * recv routine so we know which callback to use when\n     * the return message is recvd */\n    cb = PMIX_NEW(pmix_cb_t);\n\n    /* push the message into our event base to send to the server */\n    if (PMIX_SUCCESS != (rc = PMIx_Unpublish_nb(keys, info, ninfo, op_cbfunc, cb))) {\n        PMIX_RELEASE(cb);\n        return rc;\n    }\n\n    /* wait for the server to ack our request */\n    PMIX_WAIT_THREAD(&cb->lock);\n    rc = cb->status;\n    PMIX_RELEASE(cb);\n\n    return rc;\n}\n\nPMIX_EXPORT pmix_status_t PMIx_Unpublish_nb(char **keys,\n                                            const pmix_info_t info[], size_t ninfo,\n                                            pmix_op_cbfunc_t cbfunc, void *cbdata)\n{\n    pmix_buffer_t *msg;\n    pmix_cmd_t cmd = PMIX_UNPUBLISHNB_CMD;\n    pmix_status_t rc;\n    pmix_cb_t *cb;\n    size_t i, j;\n\n    PMIX_ACQUIRE_THREAD(&pmix_global_lock);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix: unpublish called\");\n\n    if (pmix_globals.init_cntr <= 0) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_INIT;\n    }\n\n    /* if we aren't connected, don't attempt to send */\n    if (!pmix_globals.connected) {\n        PMIX_RELEASE_THREAD(&pmix_global_lock);\n        return PMIX_ERR_UNREACH;\n    }\n    PMIX_RELEASE_THREAD(&pmix_global_lock);\n\n    /* create the unpublish cmd */\n    msg = PMIX_NEW(pmix_buffer_t);\n    /* pack the cmd */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &cmd, 1, PMIX_COMMAND);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pack our effective userid - will be used to constrain lookup */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &pmix_globals.uid, 1, PMIX_UINT32);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    /* pack the number of keys */\n    i = pmix_argv_count(keys);\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &i, 1, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    if (0 < i) {\n        for (j=0; j < i; j++) {\n            PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                             msg, &keys[j], 1, PMIX_STRING);\n            if (PMIX_SUCCESS != rc) {\n                PMIX_ERROR_LOG(rc);\n                PMIX_RELEASE(msg);\n                return rc;\n            }\n        }\n    }\n    /* pass the number of info structs - needed on remote end so\n     * space can be malloc'd for the values */\n    PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                     msg, &ninfo, 1, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(msg);\n        return rc;\n    }\n    if (0 < ninfo) {\n        /* pack the info structs */\n        PMIX_BFROPS_PACK(rc, pmix_client_globals.myserver,\n                         msg, info, ninfo, PMIX_INFO);\n        if (PMIX_SUCCESS != rc) {\n            PMIX_ERROR_LOG(rc);\n            PMIX_RELEASE(msg);\n            return rc;\n        }\n    }\n\n    /* create a callback object */\n    cb = PMIX_NEW(pmix_cb_t);\n    cb->cbfunc.opfn = cbfunc;\n    cb->cbdata = cbdata;\n\n    /* send to the server */\n    PMIX_PTL_SEND_RECV(rc, pmix_client_globals.myserver,\n                       msg, wait_cbfunc, (void*)cb);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_RELEASE(msg);\n        PMIX_RELEASE(cb);\n    }\n\n    return rc;\n}\n\nstatic void wait_cbfunc(struct pmix_peer_t *pr,\n                        pmix_ptl_hdr_t *hdr,\n                        pmix_buffer_t *buf, void *cbdata)\n{\n    pmix_cb_t *cb = (pmix_cb_t*)cbdata;\n    pmix_status_t rc;\n    int ret;\n    int32_t cnt;\n\n    PMIX_ACQUIRE_OBJECT(cb);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix:client recv callback activated with %d bytes\",\n                        (NULL == buf) ? -1 : (int)buf->bytes_used);\n\n    if (NULL == buf) {\n        rc = PMIX_ERR_BAD_PARAM;\n        goto report;\n    }\n    /* a zero-byte buffer indicates that this recv is being\n     * completed due to a lost connection */\n    if (PMIX_BUFFER_IS_EMPTY(buf)) {\n        rc = PMIX_ERR_UNREACH;\n        goto report;\n    }\n\n    /* unpack the returned status */\n    cnt = 1;\n    PMIX_BFROPS_UNPACK(rc, pmix_client_globals.myserver,\n                       buf, &ret, &cnt, PMIX_STATUS);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n    }\n\n  report:\n    if (NULL != cb->cbfunc.opfn) {\n        cb->cbfunc.opfn(rc, cb->cbdata);\n    }\n    PMIX_RELEASE(cb);\n}\n\nstatic void op_cbfunc(pmix_status_t status, void *cbdata)\n{\n    pmix_cb_t *cb = (pmix_cb_t*)cbdata;\n\n    cb->status = status;\n    PMIX_POST_OBJECT(cb);\n    PMIX_WAKEUP_THREAD(&cb->lock);\n}\n\nstatic void wait_lookup_cbfunc(struct pmix_peer_t *pr,\n                               pmix_ptl_hdr_t *hdr,\n                               pmix_buffer_t *buf, void *cbdata)\n{\n    pmix_cb_t *cb = (pmix_cb_t*)cbdata;\n    pmix_status_t rc, ret;\n    int32_t cnt;\n    pmix_pdata_t *pdata;\n    size_t ndata;\n\n    PMIX_ACQUIRE_OBJECT(cb);\n\n    pmix_output_verbose(2, pmix_globals.debug_output,\n                        \"pmix:client recv callback activated with %d bytes\",\n                        (NULL == buf) ? -1 : (int)buf->bytes_used);\n\n    /* set the defaults */\n    pdata = NULL;\n    ndata = 0;\n\n    if (NULL == cb->cbfunc.lookupfn) {\n        /* nothing we can do with this */\n        PMIX_RELEASE(cb);\n        return;\n    }\n    if (NULL == buf) {\n        rc = PMIX_ERR_BAD_PARAM;\n        goto report;\n    }\n    /* a zero-byte buffer indicates that this recv is being\n     * completed due to a lost connection */\n    if (PMIX_BUFFER_IS_EMPTY(buf)) {\n        rc = PMIX_ERR_UNREACH;\n        goto report;\n    }\n\n    /* unpack the returned status */\n    cnt = 1;\n    PMIX_BFROPS_UNPACK(rc, pmix_client_globals.myserver,\n                       buf, &ret, &cnt, PMIX_STATUS);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        ret = rc;\n    }\n    if (PMIX_SUCCESS != ret) {\n        if (NULL != cb->cbfunc.lookupfn) {\n            cb->cbfunc.lookupfn(ret, NULL, 0, cb->cbdata);\n        }\n        PMIX_RELEASE(cb);\n        return;\n    }\n\n    /* unpack the number of returned values */\n    cnt = 1;\n    PMIX_BFROPS_UNPACK(rc, pmix_client_globals.myserver,\n                       buf, &ndata, &cnt, PMIX_SIZE);\n    if (PMIX_SUCCESS != rc) {\n        PMIX_ERROR_LOG(rc);\n        PMIX_RELEASE(cb);\n        return;\n    }\n    if (0 < ndata) {\n        /* create the array storage */\n        PMIX_PDATA_CREATE(pdata, ndata);\n        cnt = ndata;\n        /* unpack the returned values into the pdata array */\n        PMIX_BFROPS_UNPACK(rc, pmix_client_globals.myserver,\n                           buf, pdata, &cnt, PMIX_PDATA);\n        if (PMIX_SUCCESS != rc) {\n            PMIX_ERROR_LOG(rc);\n            goto cleanup;\n        }\n    }\n\n  report:\n    if (NULL != cb->cbfunc.lookupfn) {\n        cb->cbfunc.lookupfn(rc, pdata, ndata, cb->cbdata);\n    }\n\n cleanup:\n    /* cleanup */\n    if (NULL != pdata) {\n        PMIX_PDATA_FREE(pdata, ndata);\n    }\n\n    PMIX_RELEASE(cb);\n}\n\nstatic void lookup_cbfunc(pmix_status_t status, pmix_pdata_t pdata[], size_t ndata,\n                          void *cbdata)\n{\n    pmix_cb_t *cb = (pmix_cb_t*)cbdata;\n    pmix_pdata_t *tgt = (pmix_pdata_t*)cb->cbdata;\n    size_t i, j;\n\n    PMIX_ACQUIRE_OBJECT(cb);\n    cb->status = status;\n    if (PMIX_SUCCESS == status) {\n        /* find the matching key in the provided info array - error if not found */\n        for (i=0; i < ndata; i++) {\n            for (j=0; j < cb->nvals; j++) {\n                if (0 == strcmp(pdata[i].key, tgt[j].key)) {\n                    /* transfer the publishing proc id */\n                    pmix_strncpy(tgt[j].proc.nspace, pdata[i].proc.nspace, PMIX_MAX_NSLEN);\n                    tgt[j].proc.rank = pdata[i].proc.rank;\n                    /* transfer the value to the pmix_info_t */\n                    PMIX_BFROPS_VALUE_XFER(cb->status, pmix_client_globals.myserver, &tgt[j].value, &pdata[i].value);\n                    break;\n                }\n            }\n        }\n    }\n    PMIX_POST_OBJECT(cb);\n    PMIX_WAKEUP_THREAD(&cb->lock);\n}\n", "idx": 2, "id": 9189, "msg": "", "proj": "openpmix-openpmix", "lang": "c"}
{"patch": "@@ -244,38 +244,33 @@ static void roots_cursor_press_button(struct roots_cursor *cursor,\n \t\t\troots_seat_begin_rotate(seat, view);\n \t\t\tbreak;\n \t\t}\n-\t\treturn;\n-\t}\n+\t} else {\n \n-\tif (view && !surface) {\n-\t\tif (cursor->pointer_view) {\n-\t\t\tseat_view_deco_button(cursor->pointer_view, sx, sy, button, state);\n+\t\tif (view && !surface) {\n+\t\t\tif (cursor->pointer_view) {\n+\t\t\t\tseat_view_deco_button(cursor->pointer_view, sx, sy, button, state);\n+\t\t\t}\n \t\t}\n-\t}\n \n-\tif (state == WLR_BUTTON_RELEASED &&\n-\t\t\tcursor->mode != ROOTS_CURSOR_PASSTHROUGH) {\n-\t\tcursor->mode = ROOTS_CURSOR_PASSTHROUGH;\n-\t\tif (seat->seat->pointer_state.button_count == 0) {\n-\t\t\treturn;\n+\t\tif (state == WLR_BUTTON_RELEASED &&\n+\t\t\t\tcursor->mode != ROOTS_CURSOR_PASSTHROUGH) {\n+\t\t\tcursor->mode = ROOTS_CURSOR_PASSTHROUGH;\n \t\t}\n-\t}\n \n-\tif (view && surface) {\n-\t\tif (!is_touch) {\n-\t\t\twlr_seat_pointer_notify_button(seat->seat, time, button, state);\n+\t\tswitch (state) {\n+\t\tcase WLR_BUTTON_RELEASED:\n+\t\t\tif (!is_touch) {\n+\t\t\t\troots_cursor_update_position(cursor, time);\n+\t\t\t}\n+\t\t\tbreak;\n+\t\tcase WLR_BUTTON_PRESSED:\n+\t\t\troots_seat_set_focus(seat, view);\n+\t\t\tbreak;\n \t\t}\n \t}\n \n-\tswitch (state) {\n-\tcase WLR_BUTTON_RELEASED:\n-\t\tif (!is_touch) {\n-\t\t\troots_cursor_update_position(cursor, time);\n-\t\t}\n-\t\tbreak;\n-\tcase WLR_BUTTON_PRESSED:\n-\t\troots_seat_set_focus(seat, view);\n-\t\tbreak;\n+\tif (!is_touch) {\n+\t\twlr_seat_pointer_notify_button(seat->seat, time, button, state);\n \t}\n }\n ", "y": 1, "oldf": "#define _XOPEN_SOURCE 700\n#include <stdlib.h>\n#include <math.h>\n#ifdef __linux__\n#include <linux/input-event-codes.h>\n#elif __FreeBSD__\n#include <dev/evdev/input-event-codes.h>\n#endif\n#include <wlr/types/wlr_xcursor_manager.h>\n#include <wlr/util/log.h>\n#include <wlr/util/edges.h>\n#include \"rootston/xcursor.h\"\n#include \"rootston/cursor.h\"\n\nstruct roots_cursor *roots_cursor_create(struct roots_seat *seat) {\n\tstruct roots_cursor *cursor = calloc(1, sizeof(struct roots_cursor));\n\tif (!cursor) {\n\t\treturn NULL;\n\t}\n\tcursor->cursor = wlr_cursor_create();\n\tif (!cursor->cursor) {\n\t\tfree(cursor);\n\t\treturn NULL;\n\t}\n\tcursor->default_xcursor = ROOTS_XCURSOR_DEFAULT;\n\treturn cursor;\n}\n\nvoid roots_cursor_destroy(struct roots_cursor *cursor) {\n\t// TODO\n}\n\nstatic void seat_view_deco_motion(struct roots_seat_view *view, double deco_sx, double deco_sy) {\n\tstruct roots_cursor *cursor = view->seat->cursor;\n\n\tdouble sx = deco_sx;\n\tdouble sy = deco_sy;\n\tif (view->has_button_grab) {\n\t\tsx = view->grab_sx;\n\t\tsy = view->grab_sy;\n\t}\n\n\tenum roots_deco_part parts = view_get_deco_part(view->view, sx, sy);\n\n\tbool is_titlebar = (parts & ROOTS_DECO_PART_TITLEBAR);\n\tuint32_t edges = 0;\n\tif (parts & ROOTS_DECO_PART_LEFT_BORDER) {\n\t\tedges |= WLR_EDGE_LEFT;\n\t} else if (parts & ROOTS_DECO_PART_RIGHT_BORDER) {\n\t\tedges |= WLR_EDGE_RIGHT;\n\t} else if (parts & ROOTS_DECO_PART_BOTTOM_BORDER) {\n\t\tedges |= WLR_EDGE_BOTTOM;\n\t} else if (parts & ROOTS_DECO_PART_TOP_BORDER) {\n\t\tedges |= WLR_EDGE_TOP;\n\t}\n\n\tif (view->has_button_grab) {\n\t\tif (is_titlebar) {\n\t\t\troots_seat_begin_move(view->seat, view->view);\n\t\t} else if (edges) {\n\t\t\troots_seat_begin_resize(view->seat, view->view, edges);\n\t\t}\n\t\tview->has_button_grab = false;\n\t} else {\n\t\tif (is_titlebar) {\n\t\t\twlr_xcursor_manager_set_cursor_image(cursor->xcursor_manager,\n\t\t\t\tcursor->default_xcursor, cursor->cursor);\n\t\t} else if (edges) {\n\t\t\tconst char *resize_name = wlr_xcursor_get_resize_name(edges);\n\t\t\twlr_xcursor_manager_set_cursor_image(cursor->xcursor_manager,\n\t\t\t\tresize_name, cursor->cursor);\n\t\t}\n\t}\n}\n\nstatic void seat_view_deco_leave(struct roots_seat_view *view) {\n\tstruct roots_cursor *cursor = view->seat->cursor;\n\twlr_xcursor_manager_set_cursor_image(cursor->xcursor_manager,\n\t\tcursor->default_xcursor, cursor->cursor);\n\tview->has_button_grab = false;\n}\n\nstatic void seat_view_deco_button(struct roots_seat_view *view, double sx,\n\t\tdouble sy, uint32_t button, uint32_t state) {\n\tif (button == BTN_LEFT && state == WLR_BUTTON_PRESSED) {\n\t\tview->has_button_grab = true;\n\t\tview->grab_sx = sx;\n\t\tview->grab_sy = sy;\n\t} else {\n\t\tview->has_button_grab = false;\n\t}\n\n\tenum roots_deco_part parts = view_get_deco_part(view->view, sx, sy);\n\tif (state == WLR_BUTTON_RELEASED && (parts & ROOTS_DECO_PART_TITLEBAR)) {\n\t\tstruct roots_cursor *cursor = view->seat->cursor;\n\t\twlr_xcursor_manager_set_cursor_image(cursor->xcursor_manager,\n\t\t\t\tcursor->default_xcursor, cursor->cursor);\n\t}\n}\n\nstatic void roots_cursor_update_position(struct roots_cursor *cursor,\n\t\tuint32_t time) {\n\tstruct roots_desktop *desktop = cursor->seat->input->server->desktop;\n\tstruct roots_seat *seat = cursor->seat;\n\tstruct roots_view *view;\n\tstruct wlr_surface *surface = NULL;\n\tdouble sx, sy;\n\tswitch (cursor->mode) {\n\tcase ROOTS_CURSOR_PASSTHROUGH:\n\t\tview = desktop_view_at(desktop, cursor->cursor->x, cursor->cursor->y,\n\t\t\t&surface, &sx, &sy);\n\t\tstruct roots_seat_view *seat_view =\n\t\t\troots_seat_view_from_view(seat, view);\n\t\tif (cursor->pointer_view && (surface || seat_view != cursor->pointer_view)) {\n\t\t\tseat_view_deco_leave(cursor->pointer_view);\n\t\t\tcursor->pointer_view = NULL;\n\t\t}\n\t\tbool set_compositor_cursor = !view && !surface && cursor->cursor_client;\n\t\tif (view && surface) {\n\t\t\tstruct wl_client *view_client =\n\t\t\t\twl_resource_get_client(view->wlr_surface->resource);\n\t\t\tset_compositor_cursor = view_client != cursor->cursor_client;\n\t\t}\n\t\tif (set_compositor_cursor) {\n\t\t\twlr_xcursor_manager_set_cursor_image(cursor->xcursor_manager,\n\t\t\t\tcursor->default_xcursor, cursor->cursor);\n\t\t\tcursor->cursor_client = NULL;\n\t\t}\n\t\tif (view && !surface) {\n\t\t\tif (seat_view) {\n\t\t\t\tcursor->pointer_view = seat_view;\n\t\t\t\tseat_view_deco_motion(seat_view, sx, sy);\n\t\t\t}\n\t\t} if (view && surface) {\n\t\t\t// motion over a view surface\n\t\t\twlr_seat_pointer_notify_enter(seat->seat, surface, sx, sy);\n\t\t\twlr_seat_pointer_notify_motion(seat->seat, time, sx, sy);\n\t\t} else {\n\t\t\twlr_seat_pointer_clear_focus(seat->seat);\n\t\t}\n\t\tbreak;\n\tcase ROOTS_CURSOR_MOVE:\n\t\tview = roots_seat_get_focus(seat);\n\t\tif (view != NULL) {\n\t\t\tdouble dx = cursor->cursor->x - cursor->offs_x;\n\t\t\tdouble dy = cursor->cursor->y - cursor->offs_y;\n\t\t\tview_move(view, cursor->view_x + dx,\n\t\t\t\tcursor->view_y + dy);\n\t\t}\n\t\tbreak;\n\tcase ROOTS_CURSOR_RESIZE:\n\t\tview = roots_seat_get_focus(seat);\n\t\tif (view != NULL) {\n\t\t\tdouble dx = cursor->cursor->x - cursor->offs_x;\n\t\t\tdouble dy = cursor->cursor->y - cursor->offs_y;\n\t\t\tdouble x = view->x;\n\t\t\tdouble y = view->y;\n\t\t\tint width = cursor->view_width;\n\t\t\tint height = cursor->view_height;\n\t\t\tif (cursor->resize_edges & WLR_EDGE_TOP) {\n\t\t\t\ty = cursor->view_y + dy;\n\t\t\t\theight -= dy;\n\t\t\t\tif (height < 1) {\n\t\t\t\t\ty += height;\n\t\t\t\t}\n\t\t\t} else if (cursor->resize_edges & WLR_EDGE_BOTTOM) {\n\t\t\t\theight += dy;\n\t\t\t}\n\t\t\tif (cursor->resize_edges & WLR_EDGE_LEFT) {\n\t\t\t\tx = cursor->view_x + dx;\n\t\t\t\twidth -= dx;\n\t\t\t\tif (width < 1) {\n\t\t\t\t\tx += width;\n\t\t\t\t}\n\t\t\t} else if (cursor->resize_edges & WLR_EDGE_RIGHT) {\n\t\t\t\twidth += dx;\n\t\t\t}\n\n\t\t\tif (width < 1) {\n\t\t\t\twidth = 1;\n\t\t\t}\n\t\t\tif (height < 1) {\n\t\t\t\theight = 1;\n\t\t\t}\n\n\t\t\tview_move_resize(view, x, y, width, height);\n\t\t}\n\t\tbreak;\n\tcase ROOTS_CURSOR_ROTATE:\n\t\tview = roots_seat_get_focus(seat);\n\t\tif (view != NULL) {\n\t\t\tint ox = view->x + view->wlr_surface->current->width/2,\n\t\t\t\toy = view->y + view->wlr_surface->current->height/2;\n\t\t\tint ux = cursor->offs_x - ox,\n\t\t\t\tuy = cursor->offs_y - oy;\n\t\t\tint vx = cursor->cursor->x - ox,\n\t\t\t\tvy = cursor->cursor->y - oy;\n\t\t\tfloat angle = atan2(vx*uy - vy*ux, vx*ux + vy*uy);\n\t\t\tint steps = 12;\n\t\t\tangle = round(angle/M_PI*steps) / (steps/M_PI);\n\t\t\tview->rotation = cursor->view_rotation + angle;\n\t\t}\n\t\tbreak;\n\t}\n}\n\nstatic void roots_cursor_press_button(struct roots_cursor *cursor,\n\t\tstruct wlr_input_device *device, uint32_t time, uint32_t button,\n\t\tuint32_t state, double lx, double ly) {\n\tstruct roots_seat *seat = cursor->seat;\n\tstruct roots_desktop *desktop = seat->input->server->desktop;\n\tbool is_touch = device->type == WLR_INPUT_DEVICE_TOUCH;\n\n\tstruct wlr_surface *surface = NULL;\n\tdouble sx, sy;\n\tstruct roots_view *view =\n\t\tdesktop_view_at(desktop, lx, ly, &surface, &sx, &sy);\n\n\tif (state == WLR_BUTTON_PRESSED &&\n\t\t\tview &&\n\t\t\troots_seat_has_meta_pressed(seat)) {\n\t\troots_seat_set_focus(seat, view);\n\n\t\tuint32_t edges;\n\t\tswitch (button) {\n\t\tcase BTN_LEFT:\n\t\t\troots_seat_begin_move(seat, view);\n\t\t\tbreak;\n\t\tcase BTN_RIGHT:\n\t\t\tedges = 0;\n\t\t\tif (sx < view->wlr_surface->current->width/2) {\n\t\t\t\tedges |= WLR_EDGE_LEFT;\n\t\t\t} else {\n\t\t\t\tedges |= WLR_EDGE_RIGHT;\n\t\t\t}\n\t\t\tif (sy < view->wlr_surface->current->height/2) {\n\t\t\t\tedges |= WLR_EDGE_TOP;\n\t\t\t} else {\n\t\t\t\tedges |= WLR_EDGE_BOTTOM;\n\t\t\t}\n\t\t\troots_seat_begin_resize(seat, view, edges);\n\t\t\tbreak;\n\t\tcase BTN_MIDDLE:\n\t\t\troots_seat_begin_rotate(seat, view);\n\t\t\tbreak;\n\t\t}\n\t\treturn;\n\t}\n\n\tif (view && !surface) {\n\t\tif (cursor->pointer_view) {\n\t\t\tseat_view_deco_button(cursor->pointer_view, sx, sy, button, state);\n\t\t}\n\t}\n\n\tif (state == WLR_BUTTON_RELEASED &&\n\t\t\tcursor->mode != ROOTS_CURSOR_PASSTHROUGH) {\n\t\tcursor->mode = ROOTS_CURSOR_PASSTHROUGH;\n\t\tif (seat->seat->pointer_state.button_count == 0) {\n\t\t\treturn;\n\t\t}\n\t}\n\n\tif (view && surface) {\n\t\tif (!is_touch) {\n\t\t\twlr_seat_pointer_notify_button(seat->seat, time, button, state);\n\t\t}\n\t}\n\n\tswitch (state) {\n\tcase WLR_BUTTON_RELEASED:\n\t\tif (!is_touch) {\n\t\t\troots_cursor_update_position(cursor, time);\n\t\t}\n\t\tbreak;\n\tcase WLR_BUTTON_PRESSED:\n\t\troots_seat_set_focus(seat, view);\n\t\tbreak;\n\t}\n}\n\nvoid roots_cursor_handle_motion(struct roots_cursor *cursor,\n\t\tstruct wlr_event_pointer_motion *event) {\n\twlr_cursor_move(cursor->cursor, event->device,\n\t\t\tevent->delta_x, event->delta_y);\n\troots_cursor_update_position(cursor, event->time_msec);\n}\n\nvoid roots_cursor_handle_motion_absolute(struct roots_cursor *cursor,\n\t\tstruct wlr_event_pointer_motion_absolute *event) {\n\twlr_cursor_warp_absolute(cursor->cursor, event->device,\n\t\tevent->x_mm / event->width_mm, event->y_mm / event->height_mm);\n\troots_cursor_update_position(cursor, event->time_msec);\n}\n\nvoid roots_cursor_handle_button(struct roots_cursor *cursor,\n\t\tstruct wlr_event_pointer_button *event) {\n\troots_cursor_press_button(cursor, event->device, event->time_msec,\n\t\tevent->button, event->state, cursor->cursor->x, cursor->cursor->y);\n}\n\nvoid roots_cursor_handle_axis(struct roots_cursor *cursor,\n\t\tstruct wlr_event_pointer_axis *event) {\n\twlr_seat_pointer_notify_axis(cursor->seat->seat, event->time_msec,\n\t\tevent->orientation, event->delta);\n}\n\nvoid roots_cursor_handle_touch_down(struct roots_cursor *cursor,\n\t\tstruct wlr_event_touch_down *event) {\n\tstruct roots_desktop *desktop = cursor->seat->input->server->desktop;\n\tstruct wlr_surface *surface = NULL;\n\tdouble lx, ly;\n\tbool result =\n\t\twlr_cursor_absolute_to_layout_coords(cursor->cursor,\n\t\t\tevent->device, event->x_mm, event->y_mm, event->width_mm,\n\t\t\tevent->height_mm, &lx, &ly);\n\tif (!result) {\n\t\treturn;\n\t}\n\tdouble sx, sy;\n\tdesktop_view_at(desktop, lx, ly, &surface, &sx, &sy);\n\n\tuint32_t serial = 0;\n\tif (surface) {\n\t\tserial = wlr_seat_touch_notify_down(cursor->seat->seat, surface,\n\t\t\tevent->time_msec, event->touch_id, sx, sy);\n\t}\n\n\tif (serial && wlr_seat_touch_num_points(cursor->seat->seat) == 1) {\n\t\tcursor->seat->touch_id = event->touch_id;\n\t\tcursor->seat->touch_x = lx;\n\t\tcursor->seat->touch_y = ly;\n\t\troots_cursor_press_button(cursor, event->device, event->time_msec,\n\t\t\tBTN_LEFT, 1, lx, ly);\n\t}\n}\n\nvoid roots_cursor_handle_touch_up(struct roots_cursor *cursor,\n\t\tstruct wlr_event_touch_up *event) {\n\tstruct wlr_touch_point *point =\n\t\twlr_seat_touch_get_point(cursor->seat->seat, event->touch_id);\n\tif (!point) {\n\t\treturn;\n\t}\n\n\tif (wlr_seat_touch_num_points(cursor->seat->seat) == 1) {\n\t\troots_cursor_press_button(cursor, event->device, event->time_msec,\n\t\t\tBTN_LEFT, 0, cursor->seat->touch_x, cursor->seat->touch_y);\n\t}\n\n\twlr_seat_touch_notify_up(cursor->seat->seat, event->time_msec,\n\t\tevent->touch_id);\n}\n\nvoid roots_cursor_handle_touch_motion(struct roots_cursor *cursor,\n\t\tstruct wlr_event_touch_motion *event) {\n\tstruct roots_desktop *desktop = cursor->seat->input->server->desktop;\n\tstruct wlr_touch_point *point =\n\t\twlr_seat_touch_get_point(cursor->seat->seat, event->touch_id);\n\tif (!point) {\n\t\treturn;\n\t}\n\n\tstruct wlr_surface *surface = NULL;\n\tdouble lx, ly;\n\tbool result =\n\t\twlr_cursor_absolute_to_layout_coords(cursor->cursor,\n\t\t\tevent->device, event->x_mm, event->y_mm, event->width_mm,\n\t\t\tevent->height_mm, &lx, &ly);\n\tif (!result) {\n\t\treturn;\n\t}\n\n\tdouble sx, sy;\n\tdesktop_view_at(desktop, lx, ly, &surface, &sx, &sy);\n\n\tif (surface) {\n\t\twlr_seat_touch_point_focus(cursor->seat->seat, surface,\n\t\t\tevent->time_msec, event->touch_id, sx, sy);\n\t\twlr_seat_touch_notify_motion(cursor->seat->seat, event->time_msec,\n\t\t\tevent->touch_id, sx, sy);\n\t} else {\n\t\twlr_seat_touch_point_clear_focus(cursor->seat->seat, event->time_msec,\n\t\t\tevent->touch_id);\n\t}\n\n\tif (event->touch_id == cursor->seat->touch_id) {\n\t\tcursor->seat->touch_x = lx;\n\t\tcursor->seat->touch_y = ly;\n\t}\n}\n\nvoid roots_cursor_handle_tool_axis(struct roots_cursor *cursor,\n\t\tstruct wlr_event_tablet_tool_axis *event) {\n\tif ((event->updated_axes & WLR_TABLET_TOOL_AXIS_X) &&\n\t\t\t(event->updated_axes & WLR_TABLET_TOOL_AXIS_Y)) {\n\t\twlr_cursor_warp_absolute(cursor->cursor, event->device,\n\t\t\tevent->x_mm / event->width_mm, event->y_mm / event->height_mm);\n\t\troots_cursor_update_position(cursor, event->time_msec);\n\t} else if ((event->updated_axes & WLR_TABLET_TOOL_AXIS_X)) {\n\t\twlr_cursor_warp_absolute(cursor->cursor, event->device,\n\t\t\tevent->x_mm / event->width_mm, -1);\n\t\troots_cursor_update_position(cursor, event->time_msec);\n\t} else if ((event->updated_axes & WLR_TABLET_TOOL_AXIS_Y)) {\n\t\twlr_cursor_warp_absolute(cursor->cursor, event->device,\n\t\t\t-1, event->y_mm / event->height_mm);\n\t\troots_cursor_update_position(cursor, event->time_msec);\n\t}\n}\n\nvoid roots_cursor_handle_tool_tip(struct roots_cursor *cursor,\n\t\tstruct wlr_event_tablet_tool_tip *event) {\n\troots_cursor_press_button(cursor, event->device,\n\t\tevent->time_msec, BTN_LEFT, event->state, cursor->cursor->x,\n\t\tcursor->cursor->y);\n}\n\nvoid roots_cursor_handle_request_set_cursor(struct roots_cursor *cursor,\n\t\tstruct wlr_seat_pointer_request_set_cursor_event *event) {\n\tstruct wlr_surface *focused_surface =\n\t\tevent->seat_client->seat->pointer_state.focused_surface;\n\tbool has_focused =\n\t\tfocused_surface != NULL && focused_surface->resource != NULL;\n\tstruct wl_client *focused_client = NULL;\n\tif (has_focused) {\n\t\tfocused_client = wl_resource_get_client(focused_surface->resource);\n\t}\n\tif (event->seat_client->client != focused_client ||\n\t\t\tcursor->mode != ROOTS_CURSOR_PASSTHROUGH) {\n\t\twlr_log(L_DEBUG, \"Denying request to set cursor from unfocused client\");\n\t\treturn;\n\t}\n\n\twlr_log(L_DEBUG, \"Setting client cursor\");\n\twlr_cursor_set_surface(cursor->cursor, event->surface, event->hotspot_x,\n\t\tevent->hotspot_y);\n\tcursor->cursor_client = event->seat_client->client;\n}\n", "idx": 1, "id": 9767, "msg": "Now when meta is pressed, this will fall through and send the button to the window which is wrong. Our implicit grab should eat the event.", "proj": "swaywm-wlroots", "lang": "c"}
{"patch": "@@ -73,6 +73,8 @@ class EasyAdminExtension extends AbstractTypeExtension\n      * To be replaced by the usage of the request stack when 2.3 support is dropped.\n      *\n      * @param Request|null $request\n+     *\n+     * @return null\n      */\n     public function setRequest(Request $request = null)\n     {", "y": 1, "oldf": "<?php\n\n/*\n * This file is part of the EasyAdminBundle.\n *\n * (c) Javier Eguiluz <javier.eguiluz@gmail.com>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n */\n\nnamespace JavierEguiluz\\Bundle\\EasyAdminBundle\\Form\\Extension;\n\nuse JavierEguiluz\\Bundle\\EasyAdminBundle\\Form\\Util\\LegacyFormHelper;\nuse Symfony\\Component\\Form\\AbstractTypeExtension;\nuse Symfony\\Component\\Form\\FormInterface;\nuse Symfony\\Component\\Form\\FormView;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\RequestStack;\n\n/**\n * Extension that injects EasyAdmin related information in the view used to\n * render the form.\n *\n * @author Maxime Steinhausser <maxime.steinhausser@gmail.com>\n */\nclass EasyAdminExtension extends AbstractTypeExtension\n{\n    /** @var Request|null */\n    private $request;\n\n    /** @var RequestStack|null */\n    private $requestStack;\n\n    /**\n     * @param RequestStack|null $requestStack\n     */\n    public function __construct(RequestStack $requestStack = null)\n    {\n        $this->requestStack = $requestStack;\n    }\n\n    /**\n     * {@inheritdoc}\n     */\n    public function finishView(FormView $view, FormInterface $form, array $options)\n    {\n        if (null !== $this->requestStack) {\n            $this->request = $this->requestStack->getCurrentRequest();\n        }\n\n        if (null === $this->request) {\n            return;\n        }\n\n        if ($this->request->attributes->has('easyadmin')) {\n            $easyadmin = $this->request->attributes->get('easyadmin');\n            $entity = $easyadmin['entity'];\n            $action = $easyadmin['view'];\n            $fields = isset($entity[$action]['fields']) ? $entity[$action]['fields'] : array();\n            $view->vars['easyadmin'] = array(\n                'entity' => $entity,\n                'view' => $action,\n                'item' => $easyadmin['item'],\n                'field' => isset($fields[$view->vars['name']]) ? $fields[$view->vars['name']] : null,\n                'form_group' => $form->getConfig()->getAttribute('easyadmin_form_group'),\n            );\n        }\n    }\n\n    /**\n     * BC for SF < 2.4.\n     * To be replaced by the usage of the request stack when 2.3 support is dropped.\n     *\n     * @param Request|null $request\n     */\n    public function setRequest(Request $request = null)\n    {\n        $this->request = $request;\n    }\n\n    /**\n     * {@inheritdoc}\n     */\n    public function getExtendedType()\n    {\n        return LegacyFormHelper::getType('form');\n    }\n}\n", "idx": 1, "id": 10858, "msg": "Maybe this one can be ignored, gives no value.", "proj": "EasyCorp-EasyAdminBundle", "lang": "php"}
{"patch": "@@ -252,18 +252,41 @@ var (\n \t}\n \n \tgetChainMetaTests = []struct {\n+\t\t// Arguments\n+\t\temptyChain       bool\n+\t\tpollProtocolType string\n+\t\t// Expected values\n \t\theight     uint64\n \t\tnumActions int64\n \t\ttps        int64\n \t\tepoch      iotextypes.EpochData\n \t}{\n \t\t{\n+\t\t\temptyChain: true,\n+\t\t},\n+\n+\t\t{\n+\t\t\tfalse,\n+\t\t\t\"lifeLongDelegates\",\n \t\t\t4,\n \t\t\t15,\n \t\t\t15,\n \t\t\tiotextypes.EpochData{\n-\t\t\t\tNum:    1,\n-\t\t\t\tHeight: 1,\n+\t\t\t\tNum:           1,\n+\t\t\t\tHeight:        1,\n+\t\t\t\tGravityChainStartHeight: 1,\n+\t\t\t},\n+\t\t},\n+\t\t{\n+\t\t\tfalse,\n+\t\t\t\"governanceChainCommittee\",\n+\t\t\t4,\n+\t\t\t15,\n+\t\t\t15,\n+\t\t\tiotextypes.EpochData{\n+\t\t\t\tNum:           1,\n+\t\t\t\tHeight:        1,\n+\t\t\t\tGravityChainStartHeight: 100,\n \t\t\t},\n \t\t},\n \t}", "y": 1, "oldf": "// Copyright (c) 2019 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage api\n\nimport (\n\t\"context\"\n\t\"encoding/hex\"\n\t\"io/ioutil\"\n\t\"math/big\"\n\t\"os\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/golang/protobuf/proto\"\n\t\"github.com/iotexproject/iotex-election/test/mock/mock_committee\"\n\t\"github.com/pkg/errors\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n\n\t\"github.com/iotexproject/iotex-core/action\"\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/account\"\n\taccountutil \"github.com/iotexproject/iotex-core/action/protocol/account/util\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/execution\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/poll\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/poll/pollpb\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rewarding\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rolldpos\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/vote\"\n\t\"github.com/iotexproject/iotex-core/actpool\"\n\t\"github.com/iotexproject/iotex-core/blockchain\"\n\t\"github.com/iotexproject/iotex-core/blockchain/genesis\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/gasstation\"\n\t\"github.com/iotexproject/iotex-core/pkg/unit\"\n\t\"github.com/iotexproject/iotex-core/pkg/util/byteutil\"\n\t\"github.com/iotexproject/iotex-core/protogen/iotexapi\"\n\t\"github.com/iotexproject/iotex-core/protogen/iotextypes\"\n\t\"github.com/iotexproject/iotex-core/state\"\n\t\"github.com/iotexproject/iotex-core/state/factory\"\n\t\"github.com/iotexproject/iotex-core/test/identityset\"\n\t\"github.com/iotexproject/iotex-core/test/mock/mock_blockchain\"\n\t\"github.com/iotexproject/iotex-core/test/mock/mock_dispatcher\"\n\tta \"github.com/iotexproject/iotex-core/test/testaddress\"\n\t\"github.com/iotexproject/iotex-core/testutil\"\n)\n\nvar (\n\ttestTransfer, _ = testutil.SignedTransfer(ta.Addrinfo[\"alfa\"].String(),\n\t\tta.Keyinfo[\"alfa\"].PriKey, 3, big.NewInt(10), []byte{}, testutil.TestGasLimit,\n\t\tbig.NewInt(testutil.TestGasPriceInt64))\n\n\ttestTransferPb = testTransfer.Proto()\n\n\ttestExecution, _ = testutil.SignedExecution(ta.Addrinfo[\"bravo\"].String(),\n\t\tta.Keyinfo[\"bravo\"].PriKey, 1, big.NewInt(0), testutil.TestGasLimit,\n\t\tbig.NewInt(testutil.TestGasPriceInt64), []byte{})\n\n\ttestExecutionPb = testExecution.Proto()\n\n\ttestTransfer1, _ = testutil.SignedTransfer(ta.Addrinfo[\"charlie\"].String(), ta.Keyinfo[\"producer\"].PriKey, 1,\n\t\tbig.NewInt(10), []byte{}, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\ttransferHash1 = testTransfer1.Hash()\n\ttestVote1, _  = testutil.SignedVote(ta.Addrinfo[\"charlie\"].String(), ta.Keyinfo[\"charlie\"].PriKey, 5,\n\t\ttestutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tvoteHash1         = testVote1.Hash()\n\ttestExecution1, _ = testutil.SignedExecution(ta.Addrinfo[\"delta\"].String(), ta.Keyinfo[\"producer\"].PriKey, 5,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(10), []byte{1})\n\texecutionHash1    = testExecution1.Hash()\n\ttestExecution2, _ = testutil.SignedExecution(ta.Addrinfo[\"delta\"].String(), ta.Keyinfo[\"charlie\"].PriKey, 6,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64), []byte{1})\n\texecutionHash2    = testExecution2.Hash()\n\ttestExecution3, _ = testutil.SignedExecution(ta.Addrinfo[\"delta\"].String(), ta.Keyinfo[\"alfa\"].PriKey, 2,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64), []byte{1})\n\texecutionHash3 = testExecution3.Hash()\n)\n\nvar (\n\tdelegates = []genesis.Delegate{\n\t\t{\n\t\t\tOperatorAddrStr: identityset.Address(0).String(),\n\t\t\tVotesStr:        \"10\",\n\t\t},\n\t\t{\n\t\t\tOperatorAddrStr: identityset.Address(1).String(),\n\t\t\tVotesStr:        \"10\",\n\t\t},\n\t\t{\n\t\t\tOperatorAddrStr: identityset.Address(2).String(),\n\t\t\tVotesStr:        \"10\",\n\t\t},\n\t}\n)\n\nvar (\n\tgetAccountTests = []struct {\n\t\tin           string\n\t\taddress      string\n\t\tbalance      string\n\t\tnonce        uint64\n\t\tpendingNonce uint64\n\t}{\n\t\t{ta.Addrinfo[\"charlie\"].String(),\n\t\t\t\"io1d4c5lp4ea4754wy439g2t99ue7wryu5r2lslh2\",\n\t\t\t\"3\",\n\t\t\t8,\n\t\t\t9,\n\t\t},\n\t\t{\n\t\t\tta.Addrinfo[\"producer\"].String(),\n\t\t\t\"io1mflp9m6hcgm2qcghchsdqj3z3eccrnekx9p0ms\",\n\t\t\t\"9999999999999999999999999991\",\n\t\t\t1,\n\t\t\t6,\n\t\t},\n\t}\n\n\tgetActionsTests = []struct {\n\t\tstart      uint64\n\t\tcount      uint64\n\t\tnumActions int\n\t}{\n\t\t{\n\t\t\t1,\n\t\t\t11,\n\t\t\t11,\n\t\t},\n\t\t{\n\t\t\t11,\n\t\t\t5,\n\t\t\t4,\n\t\t},\n\t}\n\n\tgetActionTests = []struct {\n\t\tcheckPending bool\n\t\tin           string\n\t\tnonce        uint64\n\t\tsenderPubKey string\n\t}{\n\t\t{\n\t\t\tfalse,\n\t\t\thex.EncodeToString(transferHash1[:]),\n\t\t\t1,\n\t\t\ttestTransfer1.SrcPubkey().HexString(),\n\t\t},\n\t\t{\n\t\t\tfalse,\n\t\t\thex.EncodeToString(voteHash1[:]),\n\t\t\t5,\n\t\t\ttestVote1.SrcPubkey().HexString(),\n\t\t},\n\t\t{\n\t\t\ttrue,\n\t\t\thex.EncodeToString(executionHash1[:]),\n\t\t\t5,\n\t\t\ttestExecution1.SrcPubkey().HexString(),\n\t\t},\n\t}\n\n\tgetActionsByAddressTests = []struct {\n\t\taddress    string\n\t\tstart      uint64\n\t\tcount      uint64\n\t\tnumActions int\n\t}{\n\t\t{\n\t\t\tta.Addrinfo[\"producer\"].String(),\n\t\t\t0,\n\t\t\t3,\n\t\t\t2,\n\t\t},\n\t\t{\n\t\t\tta.Addrinfo[\"charlie\"].String(),\n\t\t\t1,\n\t\t\t8,\n\t\t\t8,\n\t\t},\n\t}\n\n\tgetUnconfirmedActionsByAddressTests = []struct {\n\t\taddress    string\n\t\tstart      uint64\n\t\tcount      uint64\n\t\tnumActions int\n\t}{\n\t\t{\n\t\t\tta.Addrinfo[\"producer\"].String(),\n\t\t\t0,\n\t\t\t4,\n\t\t\t4,\n\t\t},\n\t}\n\n\tgetActionsByBlockTests = []struct {\n\t\tblkHeight  uint64\n\t\tstart      uint64\n\t\tcount      uint64\n\t\tnumActions int\n\t}{\n\t\t{\n\t\t\t2,\n\t\t\t0,\n\t\t\t7,\n\t\t\t7,\n\t\t},\n\t\t{\n\t\t\t4,\n\t\t\t0,\n\t\t\t5,\n\t\t\t5,\n\t\t},\n\t}\n\n\tgetBlockMetasTests = []struct {\n\t\tstart   uint64\n\t\tcount   uint64\n\t\tnumBlks int\n\t}{\n\t\t{\n\t\t\t1,\n\t\t\t4,\n\t\t\t4,\n\t\t},\n\t\t{\n\t\t\t2,\n\t\t\t5,\n\t\t\t3,\n\t\t},\n\t}\n\n\tgetBlockMetaTests = []struct {\n\t\tblkHeight      uint64\n\t\tnumActions     int64\n\t\ttransferAmount string\n\t}{\n\t\t{\n\t\t\t2,\n\t\t\t7,\n\t\t\t\"4\",\n\t\t},\n\t\t{\n\t\t\t4,\n\t\t\t5,\n\t\t\t\"0\",\n\t\t},\n\t}\n\n\tgetChainMetaTests = []struct {\n\t\theight     uint64\n\t\tnumActions int64\n\t\ttps        int64\n\t\tepoch      iotextypes.EpochData\n\t}{\n\t\t{\n\t\t\t4,\n\t\t\t15,\n\t\t\t15,\n\t\t\tiotextypes.EpochData{\n\t\t\t\tNum:    1,\n\t\t\t\tHeight: 1,\n\t\t\t},\n\t\t},\n\t}\n\n\tsendActionTests = []struct {\n\t\tactionPb *iotextypes.Action\n\t}{\n\t\t{\n\t\t\ttestTransferPb,\n\t\t},\n\t\t{\n\t\t\ttestExecutionPb,\n\t\t},\n\t}\n\n\tgetReceiptByActionTests = []struct {\n\t\tin     string\n\t\tstatus uint64\n\t}{\n\t\t{\n\t\t\thex.EncodeToString(transferHash1[:]),\n\t\t\taction.SuccessReceiptStatus,\n\t\t},\n\t\t{\n\t\t\thex.EncodeToString(voteHash1[:]),\n\t\t\taction.SuccessReceiptStatus,\n\t\t},\n\t\t{\n\t\t\thex.EncodeToString(executionHash2[:]),\n\t\t\taction.SuccessReceiptStatus,\n\t\t},\n\t\t{\n\t\t\thex.EncodeToString(executionHash3[:]),\n\t\t\taction.SuccessReceiptStatus,\n\t\t},\n\t}\n\n\treadContractTests = []struct {\n\t\texecHash string\n\t\tretValue string\n\t}{\n\t\t{\n\t\t\thex.EncodeToString(executionHash2[:]),\n\t\t\t\"\",\n\t\t},\n\t}\n\n\tsuggestGasPriceTests = []struct {\n\t\tdefaultGasPrice   uint64\n\t\tsuggestedGasPrice uint64\n\t}{\n\t\t{\n\t\t\t1,\n\t\t\t1,\n\t\t},\n\t}\n\n\testimateGasForActionTests = []struct {\n\t\tactionHash   string\n\t\testimatedGas uint64\n\t}{\n\t\t{\n\t\t\thex.EncodeToString(transferHash1[:]),\n\t\t\t10000,\n\t\t},\n\t\t{\n\t\t\thex.EncodeToString(voteHash1[:]),\n\t\t\t10000,\n\t\t},\n\t}\n\n\treadUnclaimedBalanceTests = []struct {\n\t\t// Arguments\n\t\tprotocolID string\n\t\tmethodName string\n\t\taddr       string\n\t\t// Expected values\n\t\treturnErr bool\n\t\tbalance   *big.Int\n\t}{\n\t\t{\n\t\t\tprotocolID: rewarding.ProtocolID,\n\t\t\tmethodName: \"UnclaimedBalance\",\n\t\t\taddr:       identityset.Address(0).String(),\n\t\t\treturnErr:  false,\n\t\t\tbalance:    unit.ConvertIotxToRau(64), // 4 block * 36 IOTX reward by default = 144 IOTX\n\t\t},\n\t\t{\n\t\t\tprotocolID: rewarding.ProtocolID,\n\t\t\tmethodName: \"UnclaimedBalance\",\n\t\t\taddr:       identityset.Address(1).String(),\n\t\t\treturnErr:  false,\n\t\t\tbalance:    unit.ConvertIotxToRau(0), // 4 block * 36 IOTX reward by default = 144 IOTX\n\t\t},\n\t\t{\n\t\t\tprotocolID: \"Wrong ID\",\n\t\t\tmethodName: \"UnclaimedBalance\",\n\t\t\taddr:       ta.Addrinfo[\"producer\"].String(),\n\t\t\treturnErr:  true,\n\t\t},\n\t\t{\n\t\t\tprotocolID: rewarding.ProtocolID,\n\t\t\tmethodName: \"Wrong Method\",\n\t\t\taddr:       ta.Addrinfo[\"producer\"].String(),\n\t\t\treturnErr:  true,\n\t\t},\n\t}\n\n\treadActiveBlockProducersByHeightTests = []struct {\n\t\t// Arguments\n\t\tprotocolID            string\n\t\tprotocolType          string\n\t\tmethodName            string\n\t\theight                uint64\n\t\tnumCandidateDelegates uint64\n\t\t// Expected Values\n\t\tnumActiveBlockProducers int\n\t}{\n\t\t{\n\t\t\tprotocolID:              \"poll\",\n\t\t\tprotocolType:            \"lifeLongDelegates\",\n\t\t\tmethodName:              \"ActiveBlockProducersByHeight\",\n\t\t\theight:                  1,\n\t\t\tnumActiveBlockProducers: 3,\n\t\t},\n\t\t{\n\t\t\tprotocolID:              \"poll\",\n\t\t\tprotocolType:            \"lifeLongDelegates\",\n\t\t\tmethodName:              \"ActiveBlockProducersByHeight\",\n\t\t\theight:                  4,\n\t\t\tnumActiveBlockProducers: 3,\n\t\t},\n\t\t{\n\t\t\tprotocolID:              \"poll\",\n\t\t\tprotocolType:            \"governanceChainCommittee\",\n\t\t\tmethodName:              \"ActiveBlockProducersByHeight\",\n\t\t\theight:                  1,\n\t\t\tnumCandidateDelegates:   2,\n\t\t\tnumActiveBlockProducers: 2,\n\t\t},\n\t\t{\n\t\t\tprotocolID:              \"poll\",\n\t\t\tprotocolType:            \"governanceChainCommittee\",\n\t\t\tmethodName:              \"ActiveBlockProducersByHeight\",\n\t\t\theight:                  4,\n\t\t\tnumCandidateDelegates:   1,\n\t\t\tnumActiveBlockProducers: 1,\n\t\t},\n\t}\n\n\treadCommitteeProducersByHeightTests = []struct {\n\t\t// Arguments\n\t\tprotocolID   string\n\t\tprotocolType string\n\t\tmethodName   string\n\t\theight       uint64\n\t\tnumDelegates uint64\n\t\t// Expected Values\n\t\tnumCommitteeBlockProducers int\n\t}{\n\t\t{\n\t\t\tprotocolID:                 \"poll\",\n\t\t\tprotocolType:               \"lifeLongDelegates\",\n\t\t\tmethodName:                 \"CommitteeBlockProducersByHeight\",\n\t\t\theight:                     1,\n\t\t\tnumCommitteeBlockProducers: 3,\n\t\t},\n\t\t{\n\t\t\tprotocolID:                 \"poll\",\n\t\t\tprotocolType:               \"lifeLongDelegates\",\n\t\t\tmethodName:                 \"CommitteeBlockProducersByHeight\",\n\t\t\theight:                     4,\n\t\t\tnumCommitteeBlockProducers: 3,\n\t\t},\n\t\t{\n\t\t\tprotocolID:                 \"poll\",\n\t\t\tprotocolType:               \"governanceChainCommittee\",\n\t\t\tmethodName:                 \"CommitteeBlockProducersByHeight\",\n\t\t\theight:                     1,\n\t\t\tnumDelegates:               2,\n\t\t\tnumCommitteeBlockProducers: 2,\n\t\t},\n\t\t{\n\t\t\tprotocolID:                 \"poll\",\n\t\t\tprotocolType:               \"governanceChainCommittee\",\n\t\t\tmethodName:                 \"CommitteeBlockProducersByHeight\",\n\t\t\theight:                     4,\n\t\t\tnumDelegates:               1,\n\t\t\tnumCommitteeBlockProducers: 1,\n\t\t},\n\t}\n)\n\nfunc TestServer_GetAccount(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, true)\n\trequire.NoError(err)\n\n\t// success\n\tfor _, test := range getAccountTests {\n\t\trequest := &iotexapi.GetAccountRequest{Address: test.in}\n\t\tres, err := svr.GetAccount(context.Background(), request)\n\t\trequire.NoError(err)\n\t\taccountMeta := res.AccountMeta\n\t\trequire.Equal(test.address, accountMeta.Address)\n\t\trequire.Equal(test.balance, accountMeta.Balance)\n\t\trequire.Equal(test.nonce, accountMeta.Nonce)\n\t\trequire.Equal(test.pendingNonce, accountMeta.PendingNonce)\n\t}\n\t// failure\n\t_, err = svr.GetAccount(context.Background(), &iotexapi.GetAccountRequest{})\n\trequire.Error(err)\n}\n\nfunc TestServer_GetActions(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getActionsTests {\n\t\trequest := &iotexapi.GetActionsRequest{\n\t\t\tLookup: &iotexapi.GetActionsRequest_ByIndex{\n\t\t\t\tByIndex: &iotexapi.GetActionsByIndexRequest{\n\t\t\t\t\tStart: test.start,\n\t\t\t\t\tCount: test.count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetActions(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.numActions, len(res.Actions))\n\t}\n}\n\nfunc TestServer_GetAction(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, true)\n\trequire.NoError(err)\n\n\tfor _, test := range getActionTests {\n\t\trequest := &iotexapi.GetActionsRequest{\n\t\t\tLookup: &iotexapi.GetActionsRequest_ByHash{\n\t\t\t\tByHash: &iotexapi.GetActionByHashRequest{\n\t\t\t\t\tActionHash:   test.in,\n\t\t\t\t\tCheckPending: test.checkPending,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetActions(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(1, len(res.Actions))\n\t\tactPb := res.Actions[0]\n\t\trequire.Equal(test.nonce, actPb.GetCore().GetNonce())\n\t\trequire.Equal(test.senderPubKey, hex.EncodeToString(actPb.SenderPubKey))\n\t}\n}\n\nfunc TestServer_GetActionsByAddress(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getActionsByAddressTests {\n\t\trequest := &iotexapi.GetActionsRequest{\n\t\t\tLookup: &iotexapi.GetActionsRequest_ByAddr{\n\t\t\t\tByAddr: &iotexapi.GetActionsByAddressRequest{\n\t\t\t\t\tAddress: test.address,\n\t\t\t\t\tStart:   test.start,\n\t\t\t\t\tCount:   test.count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetActions(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.numActions, len(res.Actions))\n\t}\n}\n\nfunc TestServer_GetUnconfirmedActionsByAddress(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, true)\n\trequire.NoError(err)\n\n\tfor _, test := range getUnconfirmedActionsByAddressTests {\n\t\trequest := &iotexapi.GetActionsRequest{\n\t\t\tLookup: &iotexapi.GetActionsRequest_UnconfirmedByAddr{\n\t\t\t\tUnconfirmedByAddr: &iotexapi.GetUnconfirmedActionsByAddressRequest{\n\t\t\t\t\tAddress: test.address,\n\t\t\t\t\tStart:   test.start,\n\t\t\t\t\tCount:   test.count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetActions(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.numActions, len(res.Actions))\n\t}\n}\n\nfunc TestServer_GetActionsByBlock(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getActionsByBlockTests {\n\t\tblk, err := svr.bc.GetBlockByHeight(test.blkHeight)\n\t\trequire.NoError(err)\n\t\tblkHash := blk.HashBlock()\n\t\trequest := &iotexapi.GetActionsRequest{\n\t\t\tLookup: &iotexapi.GetActionsRequest_ByBlk{\n\t\t\t\tByBlk: &iotexapi.GetActionsByBlockRequest{\n\t\t\t\t\tBlkHash: hex.EncodeToString(blkHash[:]),\n\t\t\t\t\tStart:   test.start,\n\t\t\t\t\tCount:   test.count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetActions(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.numActions, len(res.Actions))\n\t}\n}\n\nfunc TestServer_GetBlockMetas(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getBlockMetasTests {\n\t\trequest := &iotexapi.GetBlockMetasRequest{\n\t\t\tLookup: &iotexapi.GetBlockMetasRequest_ByIndex{\n\t\t\t\tByIndex: &iotexapi.GetBlockMetasByIndexRequest{\n\t\t\t\t\tStart: test.start,\n\t\t\t\t\tCount: test.count,\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetBlockMetas(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.numBlks, len(res.BlkMetas))\n\t\tvar prevBlkPb *iotextypes.BlockMeta\n\t\tfor _, blkPb := range res.BlkMetas {\n\t\t\tif prevBlkPb != nil {\n\t\t\t\trequire.True(blkPb.Height > prevBlkPb.Height)\n\t\t\t}\n\t\t\tprevBlkPb = blkPb\n\t\t}\n\t}\n}\n\nfunc TestServer_GetBlockMeta(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getBlockMetaTests {\n\t\tblk, err := svr.bc.GetBlockByHeight(test.blkHeight)\n\t\trequire.NoError(err)\n\t\tblkHash := blk.HashBlock()\n\t\trequest := &iotexapi.GetBlockMetasRequest{\n\t\t\tLookup: &iotexapi.GetBlockMetasRequest_ByHash{\n\t\t\t\tByHash: &iotexapi.GetBlockMetaByHashRequest{\n\t\t\t\t\tBlkHash: hex.EncodeToString(blkHash[:]),\n\t\t\t\t},\n\t\t\t},\n\t\t}\n\t\tres, err := svr.GetBlockMetas(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(1, len(res.BlkMetas))\n\t\tblkPb := res.BlkMetas[0]\n\t\trequire.Equal(test.numActions, blkPb.NumActions)\n\t\trequire.Equal(test.transferAmount, blkPb.TransferAmount)\n\t}\n}\n\nfunc TestServer_GetChainMeta(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getChainMetaTests {\n\t\tres, err := svr.GetChainMeta(context.Background(), &iotexapi.GetChainMetaRequest{})\n\t\trequire.NoError(err)\n\t\tchainMetaPb := res.ChainMeta\n\t\trequire.Equal(test.height, chainMetaPb.Height)\n\t\trequire.Equal(test.numActions, chainMetaPb.NumActions)\n\t\trequire.Equal(test.tps, chainMetaPb.Tps)\n\t\trequire.Equal(test.epoch.Num, chainMetaPb.Epoch.Num)\n\t\trequire.Equal(test.epoch.Height, chainMetaPb.Epoch.Height)\n\t}\n}\n\nfunc TestServer_SendAction(t *testing.T) {\n\trequire := require.New(t)\n\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\n\tchain := mock_blockchain.NewMockBlockchain(ctrl)\n\tmDp := mock_dispatcher.NewMockDispatcher(ctrl)\n\tbroadcastHandlerCount := 0\n\tsvr := Server{bc: chain, dp: mDp, broadcastHandler: func(_ context.Context, _ uint32, _ proto.Message) error {\n\t\tbroadcastHandlerCount++\n\t\treturn nil\n\t}}\n\n\tchain.EXPECT().ChainID().Return(uint32(1)).Times(4)\n\tmDp.EXPECT().HandleBroadcast(gomock.Any(), gomock.Any(), gomock.Any()).Times(2)\n\n\tfor i, test := range sendActionTests {\n\t\trequest := &iotexapi.SendActionRequest{Action: test.actionPb}\n\t\t_, err := svr.SendAction(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(i+1, broadcastHandlerCount)\n\t}\n}\n\nfunc TestServer_GetReceiptByAction(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range getReceiptByActionTests {\n\t\trequest := &iotexapi.GetReceiptByActionRequest{ActionHash: test.in}\n\t\tres, err := svr.GetReceiptByAction(context.Background(), request)\n\t\trequire.NoError(err)\n\t\treceiptPb := res.Receipt\n\t\trequire.Equal(test.status, receiptPb.Status)\n\t}\n}\n\nfunc TestServer_ReadContract(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range readContractTests {\n\t\thash, err := toHash256(test.execHash)\n\t\trequire.NoError(err)\n\t\texec, err := svr.bc.GetActionByActionHash(hash)\n\t\trequire.NoError(err)\n\t\trequest := &iotexapi.ReadContractRequest{Action: exec.Proto()}\n\n\t\tres, err := svr.ReadContract(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.retValue, res.Data)\n\t}\n}\n\nfunc TestServer_SuggestGasPrice(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tfor _, test := range suggestGasPriceTests {\n\t\tcfg.API.GasStation.DefaultGas = test.defaultGasPrice\n\t\tsvr, err := createServer(cfg, false)\n\t\trequire.NoError(err)\n\t\tres, err := svr.SuggestGasPrice(context.Background(), &iotexapi.SuggestGasPriceRequest{})\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.suggestedGasPrice, res.GasPrice)\n\t}\n}\n\nfunc TestServer_EstimateGasForAction(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(err)\n\n\tfor _, test := range estimateGasForActionTests {\n\t\thash, err := toHash256(test.actionHash)\n\t\trequire.NoError(err)\n\t\tact, err := svr.bc.GetActionByActionHash(hash)\n\t\trequire.NoError(err)\n\t\trequest := &iotexapi.EstimateGasForActionRequest{Action: act.Proto()}\n\n\t\tres, err := svr.EstimateGasForAction(context.Background(), request)\n\t\trequire.NoError(err)\n\t\trequire.Equal(test.estimatedGas, res.Gas)\n\t}\n}\n\nfunc TestServer_ReadUnclaimedBalance(t *testing.T) {\n\tcfg := newConfig()\n\n\tsvr, err := createServer(cfg, false)\n\trequire.NoError(t, err)\n\n\tfor _, test := range readUnclaimedBalanceTests {\n\t\tout, err := svr.ReadState(context.Background(), &iotexapi.ReadStateRequest{\n\t\t\tProtocolID: []byte(test.protocolID),\n\t\t\tMethodName: []byte(test.methodName),\n\t\t\tArguments:  [][]byte{[]byte(test.addr)},\n\t\t})\n\t\tif test.returnErr {\n\t\t\trequire.Error(t, err)\n\t\t\tcontinue\n\t\t}\n\t\trequire.NoError(t, err)\n\t\tval, ok := big.NewInt(0).SetString(string(out.Data), 10)\n\t\trequire.True(t, ok)\n\t\tassert.Equal(t, test.balance, val)\n\t}\n}\n\nfunc TestServer_ReadActiveBlockProducersByHeight(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tmbc := mock_blockchain.NewMockBlockchain(ctrl)\n\tcommittee := mock_committee.NewMockCommittee(ctrl)\n\tcandidates := []*state.Candidate{\n\t\t{\n\t\t\tAddress: \"address1\",\n\t\t},\n\t\t{\n\t\t\tAddress: \"address2\",\n\t\t},\n\t}\n\tmbc.EXPECT().CandidatesByHeight(gomock.Any()).Return(candidates, nil).Times(2)\n\n\tfor _, test := range readActiveBlockProducersByHeightTests {\n\t\tvar pol poll.Protocol\n\t\tif test.protocolType == \"lifeLongDelegates\" {\n\t\t\tcfg.Genesis.Delegates = delegates\n\t\t\tpol = poll.NewLifeLongDelegatesProtocol(cfg.Genesis.Delegates)\n\t\t} else {\n\t\t\tpol, _ = poll.NewGovernanceChainCommitteeProtocol(\n\t\t\t\tmbc,\n\t\t\t\tcommittee,\n\t\t\t\tuint64(123456),\n\t\t\t\tfunc(uint64) (time.Time, error) { return time.Now(), nil },\n\t\t\t\tfunc(uint64) uint64 { return 1 },\n\t\t\t\tfunc(uint64) uint64 { return 1 },\n\t\t\t\ttest.numCandidateDelegates,\n\t\t\t\tcfg.Genesis.NumDelegates,\n\t\t\t)\n\t\t}\n\t\tsvr, err := createServer(cfg, false)\n\t\trequire.NoError(err)\n\t\trequire.NoError(svr.registry.ForceRegister(poll.ProtocolID, pol))\n\n\t\tres, err := svr.ReadState(context.Background(), &iotexapi.ReadStateRequest{\n\t\t\tProtocolID: []byte(test.protocolID),\n\t\t\tMethodName: []byte(test.methodName),\n\t\t\tArguments:  [][]byte{byteutil.Uint64ToBytes(test.height)},\n\t\t})\n\t\trequire.NoError(err)\n\t\tvar activeBlockProducers pollpb.BlockProducerList\n\t\trequire.NoError(proto.Unmarshal(res.Data, &activeBlockProducers))\n\t\trequire.Equal(test.numActiveBlockProducers, len(activeBlockProducers.BlockProducers))\n\t}\n}\n\nfunc TestServer_ReadCommitteeBlockProducersByHeight(t *testing.T) {\n\trequire := require.New(t)\n\tcfg := newConfig()\n\n\tctrl := gomock.NewController(t)\n\tdefer ctrl.Finish()\n\tmbc := mock_blockchain.NewMockBlockchain(ctrl)\n\tcommittee := mock_committee.NewMockCommittee(ctrl)\n\tcandidates := []*state.Candidate{\n\t\t{\n\t\t\tAddress: \"address1\",\n\t\t},\n\t\t{\n\t\t\tAddress: \"address2\",\n\t\t},\n\t}\n\tmbc.EXPECT().CandidatesByHeight(gomock.Any()).Return(candidates, nil).Times(2)\n\n\tfor _, test := range readCommitteeProducersByHeightTests {\n\t\tvar pol poll.Protocol\n\t\tif test.protocolType == \"lifeLongDelegates\" {\n\t\t\tcfg.Genesis.Delegates = delegates\n\t\t\tpol = poll.NewLifeLongDelegatesProtocol(cfg.Genesis.Delegates)\n\t\t} else {\n\t\t\tpol, _ = poll.NewGovernanceChainCommitteeProtocol(\n\t\t\t\tmbc,\n\t\t\t\tcommittee,\n\t\t\t\tuint64(123456),\n\t\t\t\tfunc(uint64) (time.Time, error) { return time.Now(), nil },\n\t\t\t\tfunc(uint64) uint64 { return 1 },\n\t\t\t\tfunc(uint64) uint64 { return 1 },\n\t\t\t\tcfg.Genesis.NumCandidateDelegates,\n\t\t\t\ttest.numDelegates,\n\t\t\t)\n\t\t}\n\t\tsvr, err := createServer(cfg, false)\n\t\trequire.NoError(err)\n\t\trequire.NoError(svr.registry.ForceRegister(poll.ProtocolID, pol))\n\n\t\tres, err := svr.ReadState(context.Background(), &iotexapi.ReadStateRequest{\n\t\t\tProtocolID: []byte(test.protocolID),\n\t\t\tMethodName: []byte(test.methodName),\n\t\t\tArguments:  [][]byte{byteutil.Uint64ToBytes(test.height)},\n\t\t})\n\t\trequire.NoError(err)\n\t\tvar committeeBlockProducers pollpb.BlockProducerList\n\t\trequire.NoError(proto.Unmarshal(res.Data, &committeeBlockProducers))\n\t\trequire.Equal(test.numCommitteeBlockProducers, len(committeeBlockProducers.BlockProducers))\n\t}\n}\n\nfunc addProducerToFactory(sf factory.Factory) error {\n\tws, err := sf.NewWorkingSet()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif _, err = accountutil.LoadOrCreateAccount(\n\t\tws,\n\t\tta.Addrinfo[\"producer\"].String(),\n\t\tunit.ConvertIotxToRau(10000000000),\n\t); err != nil {\n\t\treturn err\n\t}\n\tgasLimit := testutil.TestGasLimit\n\tctx := protocol.WithRunActionsCtx(context.Background(),\n\t\tprotocol.RunActionsCtx{\n\t\t\tProducer: ta.Addrinfo[\"producer\"],\n\t\t\tGasLimit: gasLimit,\n\t\t})\n\tif _, err = ws.RunActions(ctx, 0, nil); err != nil {\n\t\treturn err\n\t}\n\treturn sf.Commit(ws)\n}\n\nfunc addTestingBlocks(bc blockchain.Blockchain) error {\n\taddr0 := ta.Addrinfo[\"producer\"].String()\n\tpriKey0 := ta.Keyinfo[\"producer\"].PriKey\n\taddr1 := ta.Addrinfo[\"alfa\"].String()\n\tpriKey1 := ta.Keyinfo[\"alfa\"].PriKey\n\taddr2 := ta.Addrinfo[\"bravo\"].String()\n\taddr3 := ta.Addrinfo[\"charlie\"].String()\n\tpriKey3 := ta.Keyinfo[\"charlie\"].PriKey\n\taddr4 := ta.Addrinfo[\"delta\"].String()\n\t// Add block 1\n\t// Producer transfer--> C\n\ttsf, err := testutil.SignedTransfer(addr3, priKey0, 1, big.NewInt(10), []byte{}, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tactionMap := make(map[string][]action.SealedEnvelope)\n\tactionMap[addr0] = []action.SealedEnvelope{tsf}\n\tblk, err := bc.MintNewBlock(\n\t\tactionMap,\n\t\ttestutil.TimestampNow(),\n\t)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := bc.ValidateBlock(blk); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.CommitBlock(blk); err != nil {\n\t\treturn err\n\t}\n\n\t// Add block 2\n\t// Charlie transfer--> A, B, D, P\n\t// Charlie vote--> C\n\t// Charlie exec--> D\n\trecipients := []string{addr1, addr2, addr4, addr0}\n\tselps := make([]action.SealedEnvelope, 0)\n\tfor i, recipient := range recipients {\n\t\tselp, err := testutil.SignedTransfer(recipient, priKey3, uint64(i+1), big.NewInt(1), []byte{}, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tselps = append(selps, selp)\n\t}\n\tvote1, err := testutil.SignedVote(addr3, priKey3, 5, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\texecution1, err := testutil.SignedExecution(addr4, priKey3, 6,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64), []byte{1})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tselps = append(selps, vote1)\n\tselps = append(selps, execution1)\n\tactionMap = make(map[string][]action.SealedEnvelope)\n\tactionMap[addr3] = selps\n\tif blk, err = bc.MintNewBlock(\n\t\tactionMap,\n\t\ttestutil.TimestampNow(),\n\t); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.ValidateBlock(blk); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.CommitBlock(blk); err != nil {\n\t\treturn err\n\t}\n\n\t// Add block 3\n\t// Empty actions\n\tif blk, err = bc.MintNewBlock(\n\t\tnil,\n\t\ttestutil.TimestampNow(),\n\t); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.ValidateBlock(blk); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.CommitBlock(blk); err != nil {\n\t\treturn err\n\t}\n\n\t// Add block 4\n\t// Charlie vote--> C\n\t// Charlie exec--> D\n\t// Alfa vote--> A\n\t// Alfa exec--> D\n\tvote1, err = testutil.SignedVote(addr3, priKey3, 7, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\tvote2, err := testutil.SignedVote(addr1, priKey1, 1, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\texecution1, err = testutil.SignedExecution(addr4, priKey3, 8,\n\t\tbig.NewInt(2), testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64), []byte{1})\n\tif err != nil {\n\t\treturn err\n\t}\n\texecution2, err := testutil.SignedExecution(addr4, priKey1, 2,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64), []byte{1})\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tactionMap = make(map[string][]action.SealedEnvelope)\n\tactionMap[addr3] = []action.SealedEnvelope{vote1, execution1}\n\tactionMap[addr1] = []action.SealedEnvelope{vote2, execution2}\n\tif blk, err = bc.MintNewBlock(\n\t\tactionMap,\n\t\ttestutil.TimestampNow(),\n\t); err != nil {\n\t\treturn err\n\t}\n\tif err := bc.ValidateBlock(blk); err != nil {\n\t\treturn err\n\t}\n\treturn bc.CommitBlock(blk)\n}\n\nfunc addActsToActPool(ap actpool.ActPool) error {\n\t// Producer transfer--> A\n\ttsf1, err := testutil.SignedTransfer(ta.Addrinfo[\"alfa\"].String(), ta.Keyinfo[\"producer\"].PriKey, 2, big.NewInt(20), []byte{}, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Producer vote--> P\n\tvote1, err := testutil.SignedVote(ta.Addrinfo[\"producer\"].String(), ta.Keyinfo[\"producer\"].PriKey, 3, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Producer transfer--> B\n\ttsf2, err := testutil.SignedTransfer(ta.Addrinfo[\"bravo\"].String(), ta.Keyinfo[\"producer\"].PriKey, 4, big.NewInt(20), []byte{}, testutil.TestGasLimit, big.NewInt(testutil.TestGasPriceInt64))\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Producer exec--> D\n\texecution1, err := testutil.SignedExecution(ta.Addrinfo[\"delta\"].String(), ta.Keyinfo[\"producer\"].PriKey, 5,\n\t\tbig.NewInt(1), testutil.TestGasLimit, big.NewInt(10), []byte{1})\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := ap.Add(tsf1); err != nil {\n\t\treturn err\n\t}\n\tif err := ap.Add(vote1); err != nil {\n\t\treturn err\n\t}\n\tif err := ap.Add(tsf2); err != nil {\n\t\treturn err\n\t}\n\treturn ap.Add(execution1)\n}\n\nfunc setupChain(cfg config.Config) (blockchain.Blockchain, *protocol.Registry, error) {\n\tcfg.Chain.ProducerPrivKey = hex.EncodeToString(identityset.PrivateKey(0).Bytes())\n\tsf, err := factory.NewFactory(cfg, factory.InMemTrieOption())\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\t// create chain\n\tregistry := protocol.Registry{}\n\tbc := blockchain.NewBlockchain(\n\t\tcfg,\n\t\tblockchain.PrecreatedStateFactoryOption(sf),\n\t\tblockchain.InMemDaoOption(),\n\t\tblockchain.RegistryOption(&registry),\n\t)\n\tif bc == nil {\n\t\treturn nil, nil, errors.New(\"failed to create blockchain\")\n\t}\n\n\tacc := account.NewProtocol()\n\tv := vote.NewProtocol(bc)\n\tevm := execution.NewProtocol(bc)\n\tp := poll.NewLifeLongDelegatesProtocol(cfg.Genesis.Delegates)\n\trolldposProtocol := rolldpos.NewProtocol(\n\t\tgenesis.Default.NumCandidateDelegates,\n\t\tgenesis.Default.NumDelegates,\n\t\tgenesis.Default.NumSubEpochs,\n\t)\n\tr := rewarding.NewProtocol(bc, rolldposProtocol)\n\n\tif err := registry.Register(rolldpos.ProtocolID, rolldposProtocol); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := registry.Register(account.ProtocolID, acc); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := registry.Register(vote.ProtocolID, v); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := registry.Register(execution.ProtocolID, evm); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := registry.Register(rewarding.ProtocolID, r); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tif err := registry.Register(poll.ProtocolID, p); err != nil {\n\t\treturn nil, nil, err\n\t}\n\tsf.AddActionHandlers(acc, v, evm, r)\n\tbc.Validator().AddActionEnvelopeValidators(protocol.NewGenericValidator(bc, genesis.Default.ActionGasLimit))\n\tbc.Validator().AddActionValidators(acc, v, evm, r)\n\n\treturn bc, &registry, nil\n}\n\nfunc setupActPool(bc blockchain.Blockchain, cfg config.ActPool) (actpool.ActPool, error) {\n\tap, err := actpool.NewActPool(bc, cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tap.AddActionEnvelopeValidators(protocol.NewGenericValidator(bc, genesis.Default.ActionGasLimit))\n\tap.AddActionValidators(vote.NewProtocol(bc), execution.NewProtocol(bc))\n\n\treturn ap, nil\n}\n\nfunc newConfig() config.Config {\n\tcfg := config.Default\n\n\ttestTrieFile, _ := ioutil.TempFile(os.TempDir(), \"trie\")\n\ttestTriePath := testTrieFile.Name()\n\ttestDBFile, _ := ioutil.TempFile(os.TempDir(), \"db\")\n\ttestDBPath := testDBFile.Name()\n\n\tcfg.Plugins[config.GatewayPlugin] = true\n\tcfg.Chain.TrieDBPath = testTriePath\n\tcfg.Chain.ChainDBPath = testDBPath\n\tcfg.Chain.EnableAsyncIndexWrite = false\n\tcfg.Genesis.EnableGravityChainVoting = true\n\tcfg.ActPool.MinGasPriceStr = \"0\"\n\treturn cfg\n}\n\nfunc createServer(cfg config.Config, needActPool bool) (*Server, error) {\n\tbc, registry, err := setupChain(cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tctx := context.Background()\n\n\t// Start blockchain\n\tif err := bc.Start(ctx); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Create state for producer\n\tif err := addProducerToFactory(bc.GetFactory()); err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Add testing blocks\n\tif err := addTestingBlocks(bc); err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar ap actpool.ActPool\n\tif needActPool {\n\t\tap, err = setupActPool(bc, cfg.ActPool)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Add actions to actpool\n\t\tif err := addActsToActPool(ap); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tapiCfg := config.API{TpsWindow: 10, GasStation: cfg.API.GasStation}\n\n\tsvr := &Server{\n\t\tbc:       bc,\n\t\tap:       ap,\n\t\tcfg:      apiCfg,\n\t\tgs:       gasstation.NewGasStation(bc, apiCfg),\n\t\tregistry: registry,\n\t}\n\n\treturn svr, nil\n}\n", "idx": 1, "id": 16710, "msg": "File is not `goimports`-ed (from `goimports`)", "proj": "iotexproject-iotex-core", "lang": "go"}
{"patch": "@@ -134,10 +134,22 @@ type accountUpdates struct {\n \t// creatableDeltas stores creatable updates for every round after dbRound.\n \tcreatableDeltas []map[basics.CreatableIndex]modifiedCreatable\n \n+\t// storageDeltas holds storage updates for every round after dbRound.\n+\t// TODO(app refactor) make these storageDeltas and not *storageDeltas\n+\tstorageDeltas []map[basics.Address]map[storagePtr]*storageDelta\n+\n \t// creatables stores the most recent state for every creatable that\n \t// appears in creatableDeltas\n \tcreatables map[basics.CreatableIndex]modifiedCreatable\n \n+\t// storage stores the most recent allocation metadata for every\n+\t// storageDelta that appears in storageDeltas.\n+\tstorage map[addrApp]modifiedStorage\n+\n+\t// storageKeys stores the most recent value for each key that\n+\t// appears in storageDeltas.\n+\tstorageKeys map[appKeyPtr]modifiedAppKey\n+\n \t// protos stores consensus parameters dbRound and every\n \t// round after it; i.e., protos is one longer than deltas.\n \tprotos []config.ConsensusParams", "y": 0, "oldf": "// Copyright (C) 2019-2020 Algorand, Inc.\n// This file is part of go-algorand\n//\n// go-algorand is free software: you can redistribute it and/or modify\n// it under the terms of the GNU Affero General Public License as\n// published by the Free Software Foundation, either version 3 of the\n// License, or (at your option) any later version.\n//\n// go-algorand is distributed in the hope that it will be useful,\n// but WITHOUT ANY WARRANTY; without even the implied warranty of\n// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n// GNU Affero General Public License for more details.\n//\n// You should have received a copy of the GNU Affero General Public License\n// along with go-algorand.  If not, see <https://www.gnu.org/licenses/>.\n\npackage ledger\n\nimport (\n\t\"context\"\n\t\"database/sql\"\n\t\"encoding/hex\"\n\t\"fmt\"\n\t\"io\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"sort\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/algorand/go-deadlock\"\n\n\t\"github.com/algorand/go-algorand/config\"\n\t\"github.com/algorand/go-algorand/crypto\"\n\t\"github.com/algorand/go-algorand/crypto/merkletrie\"\n\t\"github.com/algorand/go-algorand/data/basics\"\n\t\"github.com/algorand/go-algorand/data/bookkeeping\"\n\t\"github.com/algorand/go-algorand/data/transactions\"\n\t\"github.com/algorand/go-algorand/logging\"\n\t\"github.com/algorand/go-algorand/logging/telemetryspec\"\n\t\"github.com/algorand/go-algorand/protocol\"\n\t\"github.com/algorand/go-algorand/util/db\"\n)\n\nconst (\n\t// balancesFlushInterval defines how frequently we want to flush our balances to disk.\n\tbalancesFlushInterval = 5 * time.Second\n\t// pendingDeltasFlushThreshold is the deltas count threshold above we flush the pending balances regardless of the flush interval.\n\tpendingDeltasFlushThreshold = 128\n\t// trieRebuildAccountChunkSize defines the number of accounts that would get read at a single chunk\n\t// before added to the trie during trie construction\n\ttrieRebuildAccountChunkSize = 512\n\t// trieAccumulatedChangesFlush defines the number of pending changes that would be applied to the merkle trie before\n\t// we attempt to commit them to disk while writing a batch of rounds balances to disk.\n\ttrieAccumulatedChangesFlush = 256\n)\n\n// trieCachedNodesCount defines how many balances trie nodes we would like to keep around in memory.\n// value was calibrated using BenchmarkCalibrateCacheNodeSize\nvar trieCachedNodesCount = 9000\n\n// A modifiedAccount represents an account that has been modified since\n// the persistent state stored in the account DB (i.e., in the range of\n// rounds covered by the accountUpdates tracker).\ntype modifiedAccount struct {\n\t// data stores the most recent AccountData for this modified\n\t// account.\n\tdata basics.AccountData\n\n\t// ndelta keeps track of how many times this account appears in\n\t// accountUpdates.deltas.  This is used to evict modifiedAccount\n\t// entries when all changes to an account have been reflected in\n\t// the account DB, and no outstanding modifications remain.\n\tndeltas int\n}\n\ntype modifiedCreatable struct {\n\t// Type of the creatable: app or asset\n\tctype basics.CreatableType\n\n\t// Created if true, deleted if false\n\tcreated bool\n\n\t// creator of the app/asset\n\tcreator basics.Address\n\n\t// Keeps track of how many times this app/asset appears in\n\t// accountUpdates.creatableDeltas\n\tndeltas int\n}\n\ntype accountUpdates struct {\n\t// constant variables ( initialized on initialize, and never changed afterward )\n\n\t// initAccounts specifies initial account values for database.\n\tinitAccounts map[basics.Address]basics.AccountData\n\n\t// initProto specifies the initial consensus parameters.\n\tinitProto config.ConsensusParams\n\n\t// dbDirectory is the directory where the ledger and block sql file resides as well as the parent directroy for the catchup files to be generated\n\tdbDirectory string\n\n\t// catchpointInterval is the configured interval at which the accountUpdates would generate catchpoint labels and catchpoint files.\n\tcatchpointInterval uint64\n\n\t// archivalLedger determines whether the associated ledger was configured as archival ledger or not.\n\tarchivalLedger bool\n\n\t// catchpointFileHistoryLength defines how many catchpoint files we want to store back.\n\t// 0 means don't store any, -1 mean unlimited and positive number suggest the number of most recent catchpoint files.\n\tcatchpointFileHistoryLength int\n\n\t// dynamic variables\n\n\t// Connection to the database.\n\tdbs dbPair\n\n\t// Prepared SQL statements for fast accounts DB lookups.\n\taccountsq *accountsDbQueries\n\n\t// dbRound is always exactly accountsRound(),\n\t// cached to avoid SQL queries.\n\tdbRound basics.Round\n\n\t// deltas stores updates for every round after dbRound.\n\tdeltas []map[basics.Address]accountDelta\n\n\t// accounts stores the most recent account state for every\n\t// address that appears in deltas.\n\taccounts map[basics.Address]modifiedAccount\n\n\t// creatableDeltas stores creatable updates for every round after dbRound.\n\tcreatableDeltas []map[basics.CreatableIndex]modifiedCreatable\n\n\t// creatables stores the most recent state for every creatable that\n\t// appears in creatableDeltas\n\tcreatables map[basics.CreatableIndex]modifiedCreatable\n\n\t// protos stores consensus parameters dbRound and every\n\t// round after it; i.e., protos is one longer than deltas.\n\tprotos []config.ConsensusParams\n\n\t// totals stores the totals for dbRound and every round after it;\n\t// i.e., totals is one longer than deltas.\n\troundTotals []AccountTotals\n\n\t// roundDigest stores the digest of the block for every round starting with dbRound and every round after it.\n\troundDigest []crypto.Digest\n\n\t// log copied from ledger\n\tlog logging.Logger\n\n\t// lastFlushTime is the time we last flushed updates to\n\t// the accounts DB (bumping dbRound).\n\tlastFlushTime time.Time\n\n\t// ledger is the source ledger, which is used to syncronize\n\t// the rounds at which we need to flush the balances to disk\n\t// in favor of the catchpoint to be generated.\n\tledger ledgerForTracker\n\n\t// The Trie tracking the current account balances. Always matches the balances that were\n\t// written to the database.\n\tbalancesTrie *merkletrie.Trie\n\n\t// The last catchpoint label that was writted to the database. Should always align with what's in the database.\n\t// note that this is the last catchpoint *label* and not the catchpoint file.\n\tlastCatchpointLabel string\n\n\t// catchpointWriting help to syncronize the catchpoint file writing. When this channel is closed, no writting is going on.\n\t// the channel is non-closed while writing the current accounts state to disk.\n\tcatchpointWriting chan struct{}\n\n\t// catchpointSlowWriting suggest to the accounts writer that it should finish writing up the catchpoint file ASAP.\n\t// when this channel is closed, the accounts writer would try and complete the writing as soon as possible.\n\t// otherwise, it would take it's time and perform periodic sleeps between chunks processing.\n\tcatchpointSlowWriting chan struct{}\n\n\t// ctx is the context for the committing go-routine. It's also used as the \"parent\" of the catchpoint generation operation.\n\tctx context.Context\n\n\t// ctxCancel is the canceling function for canceling the commiting go-routine ( i.e. signaling the commiting go-routine that it's time to abort )\n\tctxCancel context.CancelFunc\n\n\t// deltasAccum stores the accumulated deltas for every round starting dbRound-1.\n\tdeltasAccum []int\n\n\t// committedOffset is the offset at which we'd like to persist all the previous account information to disk.\n\tcommittedOffset chan deferedCommit\n\n\t// accountsMu is the syncronization mutex for accessing the various non-static varaibles.\n\taccountsMu deadlock.RWMutex\n\n\t// accountsWriting provides syncronization around the background writing of account balances.\n\taccountsWriting sync.WaitGroup\n\n\t// commitSyncerClosed is the blocking channel for syncronizing closing the commitSyncer goroutine. Once it's closed, the\n\t// commitSyncer can be assumed to have aborted.\n\tcommitSyncerClosed chan struct{}\n}\n\ntype deferedCommit struct {\n\toffset   uint64\n\tdbRound  basics.Round\n\tlookback basics.Round\n}\n\n// initialize initializes the accountUpdates structure\nfunc (au *accountUpdates) initialize(cfg config.Local, dbPathPrefix string, genesisProto config.ConsensusParams, genesisAccounts map[basics.Address]basics.AccountData) {\n\tau.initProto = genesisProto\n\tau.initAccounts = genesisAccounts\n\tau.dbDirectory = filepath.Dir(dbPathPrefix)\n\tau.archivalLedger = cfg.Archival\n\tau.catchpointInterval = cfg.CatchpointInterval\n\tau.catchpointFileHistoryLength = cfg.CatchpointFileHistoryLength\n\tif cfg.CatchpointFileHistoryLength < -1 {\n\t\tau.catchpointFileHistoryLength = -1\n\t}\n\t// initialize the commitSyncerClosed with a closed channel ( since the commitSyncer go-routine is not active )\n\tau.commitSyncerClosed = make(chan struct{})\n\tclose(au.commitSyncerClosed)\n}\n\n// loadFromDisk is the 2nd level initialization, and is required before the accountUpdates becomes functional\n// The close function is expected to be call in pair with loadFromDisk\nfunc (au *accountUpdates) loadFromDisk(l ledgerForTracker) error {\n\tau.accountsMu.Lock()\n\tdefer au.accountsMu.Unlock()\n\tvar writingCatchpointRound uint64\n\tlastBalancesRound, lastestBlockRound, err := au.initializeFromDisk(l)\n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tvar writingCatchpointDigest crypto.Digest\n\n\twritingCatchpointRound, _, err = au.accountsq.readCatchpointStateUint64(context.Background(), catchpointStateWritingCatchpoint)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\twritingCatchpointDigest, err = au.initializeCaches(lastBalancesRound, lastestBlockRound, basics.Round(writingCatchpointRound))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif writingCatchpointRound != 0 && au.catchpointInterval != 0 {\n\t\tau.generateCatchpoint(basics.Round(writingCatchpointRound), au.lastCatchpointLabel, writingCatchpointDigest, time.Duration(0))\n\t}\n\n\treturn nil\n}\n\n// waitAccountsWriting waits for all the pending ( or current ) account writing to be completed.\nfunc (au *accountUpdates) waitAccountsWriting() {\n\tau.accountsWriting.Wait()\n}\n\n// close closes the accountUpdates, waiting for all the child go-routine to complete\nfunc (au *accountUpdates) close() {\n\tif au.ctxCancel != nil {\n\t\tau.ctxCancel()\n\t}\n\tau.waitAccountsWriting()\n\t// this would block until the commitSyncerClosed channel get closed.\n\t<-au.commitSyncerClosed\n}\n\n// Lookup returns the accound data for a given address at a given round. The withRewards indicates whether the\n// rewards should be added to the AccountData before returning. Note that the function doesn't update the account with the rewards,\n// even while it could return the AccoutData which represent the \"rewarded\" account data.\nfunc (au *accountUpdates) Lookup(rnd basics.Round, addr basics.Address, withRewards bool) (data basics.AccountData, err error) {\n\tau.accountsMu.RLock()\n\tdefer au.accountsMu.RUnlock()\n\treturn au.lookupImpl(rnd, addr, withRewards)\n}\n\n// ListAssets lists the assets by their asset index, limiting to the first maxResults\nfunc (au *accountUpdates) ListAssets(maxAssetIdx basics.AssetIndex, maxResults uint64) ([]basics.CreatableLocator, error) {\n\treturn au.listCreatables(basics.CreatableIndex(maxAssetIdx), maxResults, basics.AssetCreatable)\n}\n\n// ListApplications lists the application by their app index, limiting to the first maxResults\nfunc (au *accountUpdates) ListApplications(maxAppIdx basics.AppIndex, maxResults uint64) ([]basics.CreatableLocator, error) {\n\treturn au.listCreatables(basics.CreatableIndex(maxAppIdx), maxResults, basics.AppCreatable)\n}\n\n// listCreatables lists the application/asset by their app/asset index, limiting to the first maxResults\nfunc (au *accountUpdates) listCreatables(maxCreatableIdx basics.CreatableIndex, maxResults uint64, ctype basics.CreatableType) ([]basics.CreatableLocator, error) {\n\tau.accountsMu.RLock()\n\tdefer au.accountsMu.RUnlock()\n\n\t// Sort indices for creatables that have been created/deleted. If this\n\t// turns out to be too inefficient, we could keep around a heap of\n\t// created/deleted asset indices in memory.\n\tkeys := make([]basics.CreatableIndex, 0, len(au.creatables))\n\tfor cidx, delta := range au.creatables {\n\t\tif delta.ctype != ctype {\n\t\t\tcontinue\n\t\t}\n\t\tif cidx <= maxCreatableIdx {\n\t\t\tkeys = append(keys, cidx)\n\t\t}\n\t}\n\tsort.Slice(keys, func(i, j int) bool { return keys[i] > keys[j] })\n\n\t// Check for creatables that haven't been synced to disk yet.\n\tvar unsyncedCreatables []basics.CreatableLocator\n\tdeletedCreatables := make(map[basics.CreatableIndex]bool)\n\tfor _, cidx := range keys {\n\t\tdelta := au.creatables[cidx]\n\t\tif delta.created {\n\t\t\t// Created but only exists in memory\n\t\t\tunsyncedCreatables = append(unsyncedCreatables, basics.CreatableLocator{\n\t\t\t\tType:    delta.ctype,\n\t\t\t\tIndex:   cidx,\n\t\t\t\tCreator: delta.creator,\n\t\t\t})\n\t\t} else {\n\t\t\t// Mark deleted creatables for exclusion from the results set\n\t\t\tdeletedCreatables[cidx] = true\n\t\t}\n\t}\n\n\t// Check in-memory created creatables, which will always be newer than anything\n\t// in the database\n\tvar res []basics.CreatableLocator\n\tfor _, loc := range unsyncedCreatables {\n\t\tif uint64(len(res)) == maxResults {\n\t\t\treturn res, nil\n\t\t}\n\t\tres = append(res, loc)\n\t}\n\n\t// Fetch up to maxResults - len(res) + len(deletedCreatables) from the database,\n\t// so we have enough extras in case creatables were deleted\n\tnumToFetch := maxResults - uint64(len(res)) + uint64(len(deletedCreatables))\n\tdbResults, err := au.accountsq.listCreatables(maxCreatableIdx, numToFetch, ctype)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Now we merge the database results with the in-memory results\n\tfor _, loc := range dbResults {\n\t\t// Check if we have enough results\n\t\tif uint64(len(res)) == maxResults {\n\t\t\treturn res, nil\n\t\t}\n\n\t\t// Creatable was deleted\n\t\tif _, ok := deletedCreatables[loc.Index]; ok {\n\t\t\tcontinue\n\t\t}\n\n\t\t// We're OK to include this result\n\t\tres = append(res, loc)\n\t}\n\n\treturn res, nil\n}\n\n// GetLastCatchpointLabel retrieves the last catchpoint label that was stored to the database.\nfunc (au *accountUpdates) GetLastCatchpointLabel() string {\n\tau.accountsMu.RLock()\n\tdefer au.accountsMu.RUnlock()\n\treturn au.lastCatchpointLabel\n}\n\n// GetCreatorForRound returns the creator for a given asset/app index at a given round\nfunc (au *accountUpdates) GetCreatorForRound(rnd basics.Round, cidx basics.CreatableIndex, ctype basics.CreatableType) (creator basics.Address, ok bool, err error) {\n\tau.accountsMu.RLock()\n\tdefer au.accountsMu.RUnlock()\n\treturn au.getCreatorForRoundImpl(rnd, cidx, ctype)\n}\n\n// committedUpTo enqueues commiting the balances for round committedRound-lookback.\n// The defered committing is done so that we could calculate the historical balances lookback rounds back.\n// Since we don't want to hold off the tracker's mutex for too long, we'll defer the database persistance of this\n// operation to a syncer goroutine. The one caviat is that when storing a catchpoint round, we would want to\n// wait until the catchpoint creation is done, so that the persistance of the catchpoint file would have an\n// uninterrupted view of the balances at a given point of time.\nfunc (au *accountUpdates) committedUpTo(committedRound basics.Round) (retRound basics.Round) {\n\tvar isCatchpointRound, hasMultipleIntermediateCatchpoint bool\n\tvar offset uint64\n\tvar dc deferedCommit\n\tau.accountsMu.RLock()\n\tdefer func() {\n\t\tau.accountsMu.RUnlock()\n\t\tif dc.offset != 0 {\n\t\t\tau.committedOffset <- dc\n\t\t}\n\t}()\n\n\tretRound = basics.Round(0)\n\tvar pendingDeltas int\n\n\tlookback := basics.Round(au.protos[len(au.protos)-1].MaxBalLookback)\n\tif committedRound < lookback {\n\t\treturn\n\t}\n\n\tretRound = au.dbRound\n\tnewBase := committedRound - lookback\n\tif newBase <= au.dbRound {\n\t\t// Already forgotten\n\t\treturn\n\t}\n\n\tif newBase > au.dbRound+basics.Round(len(au.deltas)) {\n\t\tau.log.Panicf(\"committedUpTo: block %d too far in the future, lookback %d, dbRound %d, deltas %d\", committedRound, lookback, au.dbRound, len(au.deltas))\n\t}\n\n\thasIntermediateCatchpoint := false\n\thasMultipleIntermediateCatchpoint = false\n\t// check if there was a catchpoint between au.dbRound+lookback and newBase+lookback\n\tif au.catchpointInterval > 0 {\n\t\tnextCatchpointRound := ((uint64(au.dbRound+lookback) + au.catchpointInterval) / au.catchpointInterval) * au.catchpointInterval\n\n\t\tif nextCatchpointRound < uint64(newBase+lookback) {\n\t\t\tmostRecentCatchpointRound := (uint64(committedRound) / au.catchpointInterval) * au.catchpointInterval\n\t\t\tnewBase = basics.Round(nextCatchpointRound) - lookback\n\t\t\tif mostRecentCatchpointRound > nextCatchpointRound {\n\t\t\t\thasMultipleIntermediateCatchpoint = true\n\t\t\t\t// skip if there is more than one catchpoint in queue\n\t\t\t\tnewBase = basics.Round(mostRecentCatchpointRound) - lookback\n\t\t\t}\n\t\t\thasIntermediateCatchpoint = true\n\t\t}\n\t}\n\n\t// if we're still writing the previous balances, we can't move forward yet.\n\tselect {\n\tcase <-au.catchpointWriting:\n\t\t// the channel catchpointWriting is currently closed, meaning that we're currently not writing any\n\t\t// catchpoint file. At this point, we should attempt to enqueue further tasks as usual.\n\tdefault:\n\t\t// if we hit this path, it means that the channel is currently non-closed, which means that we're still writing a catchpoint.\n\t\t// see if we're writing a catchpoint in that range.\n\t\tif hasIntermediateCatchpoint {\n\t\t\t// check if we're already attempting to perform fast-writing.\n\t\t\tselect {\n\t\t\tcase <-au.catchpointSlowWriting:\n\t\t\t\t// yes, we're already doing fast-writing.\n\t\t\tdefault:\n\t\t\t\t// no, we're not yet doing fast writing, make it so.\n\t\t\t\tclose(au.catchpointSlowWriting)\n\t\t\t}\n\t\t}\n\t\treturn\n\t}\n\n\toffset = uint64(newBase - au.dbRound)\n\n\t// check to see if this is a catchpoint round\n\tisCatchpointRound = ((offset + uint64(lookback+au.dbRound)) > 0) && (au.catchpointInterval != 0) && (0 == (uint64((offset + uint64(lookback+au.dbRound))) % au.catchpointInterval))\n\n\t// calculate the number of pending deltas\n\tpendingDeltas = au.deltasAccum[offset] - au.deltasAccum[0]\n\n\t// If we recently flushed, wait to aggregate some more blocks.\n\t// ( unless we're creating a catchpoint, in which case we want to flush it right away\n\t//   so that all the instances of the catchpoint would contain the exacy same data )\n\tflushTime := time.Now()\n\tif !flushTime.After(au.lastFlushTime.Add(balancesFlushInterval)) && !isCatchpointRound && pendingDeltas < pendingDeltasFlushThreshold {\n\t\treturn au.dbRound\n\t}\n\n\tif isCatchpointRound && au.archivalLedger {\n\t\tau.catchpointWriting = make(chan struct{}, 1)\n\t\tau.catchpointSlowWriting = make(chan struct{}, 1)\n\t\tif hasMultipleIntermediateCatchpoint {\n\t\t\tclose(au.catchpointSlowWriting)\n\t\t}\n\t}\n\n\tdc = deferedCommit{\n\t\toffset:   offset,\n\t\tdbRound:  au.dbRound,\n\t\tlookback: lookback,\n\t}\n\tau.accountsWriting.Add(1)\n\treturn\n}\n\n// newBlock is the accountUpdates implementation of the ledgerTracker interface. This is the \"external\" facing function\n// which invokes the internal implementation after taking the lock.\nfunc (au *accountUpdates) newBlock(blk bookkeeping.Block, delta StateDelta) {\n\tau.accountsMu.Lock()\n\tdefer au.accountsMu.Unlock()\n\tau.newBlockImpl(blk, delta)\n}\n\n// Totals returns the totals for a given round\nfunc (au *accountUpdates) Totals(rnd basics.Round) (totals AccountTotals, err error) {\n\tau.accountsMu.RLock()\n\tdefer au.accountsMu.RUnlock()\n\treturn au.totalsImpl(rnd)\n}\n\n// GetCatchpointStream returns an io.Reader to the catchpoint file associated with the provided round\nfunc (au *accountUpdates) GetCatchpointStream(round basics.Round) (io.ReadCloser, error) {\n\tdbFileName := \"\"\n\terr := au.dbs.rdb.Atomic(func(ctx context.Context, tx *sql.Tx) (err error) {\n\t\tdbFileName, _, _, err = getCatchpoint(tx, round)\n\t\treturn\n\t})\n\tif err != nil && err != sql.ErrNoRows {\n\t\t// we had some sql error.\n\t\treturn nil, fmt.Errorf(\"accountUpdates: getCatchpointStream: unable to lookup catchpoint %d: %v\", round, err)\n\t}\n\tif dbFileName != \"\" {\n\t\tcatchpointPath := filepath.Join(au.dbDirectory, dbFileName)\n\t\tfile, err := os.OpenFile(catchpointPath, os.O_RDONLY, 0666)\n\t\tif err == nil && file != nil {\n\t\t\treturn file, nil\n\t\t}\n\t\t// else, see if this is a file-not-found error\n\t\tif os.IsNotExist(err) {\n\t\t\t// the database told us that we have this file.. but we couldn't find it.\n\t\t\t// delete it from the database.\n\t\t\terr := au.saveCatchpointFile(round, \"\", 0, \"\")\n\t\t\tif err != nil {\n\t\t\t\tau.log.Warnf(\"accountUpdates: getCatchpointStream: unable to delete missing catchpoint entry: %v\", err)\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\treturn nil, ErrNoEntry{}\n\t\t}\n\t\t// it's some other error.\n\t\treturn nil, fmt.Errorf(\"accountUpdates: getCatchpointStream: unable to open catchpoint file '%s' %v\", catchpointPath, err)\n\t}\n\n\t// if the database doesn't know about that round, see if we have that file anyway:\n\tfileName := filepath.Join(\"catchpoints\", catchpointRoundToPath(round))\n\tcatchpointPath := filepath.Join(au.dbDirectory, fileName)\n\tfile, err := os.OpenFile(catchpointPath, os.O_RDONLY, 0666)\n\tif err == nil && file != nil {\n\t\t// great, if found that we should have had this in the database.. add this one now :\n\t\tfileInfo, err := file.Stat()\n\t\tif err != nil {\n\t\t\t// we couldn't get the stat, so just return with the file.\n\t\t\treturn file, nil\n\t\t}\n\n\t\terr = au.saveCatchpointFile(round, fileName, fileInfo.Size(), \"\")\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"accountUpdates: getCatchpointStream: unable to save missing catchpoint entry: %v\", err)\n\t\t}\n\t\treturn file, nil\n\t}\n\treturn nil, ErrNoEntry{}\n}\n\n// functions below this line are all internal functions\n\n// accountUpdatesLedgerEvaluator is a \"ledger emulator\" which is used *only* by initializeCaches, as a way to shortcut\n// the locks taken by the real ledger object when making requests that are being served by the accountUpdates.\n// Using this struct allow us to take the tracker lock *before* calling the loadFromDisk, and having the operation complete\n// without taking any locks. Note that it's not only the locks performance that is gained : by having the loadFrom disk\n// not requiring any external locks, we can safely take a trackers lock on the ledger during reloadLedger, which ensures\n// that even during catchpoint catchup mode switch, we're still correctly protected by a mutex.\ntype accountUpdatesLedgerEvaluator struct {\n\t// au is the associated accountUpdates structure which invoking the trackerEvalVerified function, passing this structure as input.\n\t// the accountUpdatesLedgerEvaluator would access the underlying accountUpdates function directly, bypassing the balances mutex lock.\n\tau *accountUpdates\n\t// prevHeader is the previous header to the current one. The usage of this is only in the context of initializeCaches where we iteratively\n\t// building the StateDelta, which requires a peek on the \"previous\" header information.\n\tprevHeader bookkeeping.BlockHeader\n}\n\n// GenesisHash returns the genesis hash\nfunc (aul *accountUpdatesLedgerEvaluator) GenesisHash() crypto.Digest {\n\treturn aul.au.ledger.GenesisHash()\n}\n\n// BlockHdr returns the header of the given round. When the evaluator is running, it's only referring to the previous header, which is what we\n// are providing here. Any attempt to access a different header would get denied.\nfunc (aul *accountUpdatesLedgerEvaluator) BlockHdr(r basics.Round) (bookkeeping.BlockHeader, error) {\n\tif r == aul.prevHeader.Round {\n\t\treturn aul.prevHeader, nil\n\t}\n\treturn bookkeeping.BlockHeader{}, ErrNoEntry{}\n}\n\n// Lookup returns the account balance for a given address at a given round\nfunc (aul *accountUpdatesLedgerEvaluator) Lookup(rnd basics.Round, addr basics.Address) (basics.AccountData, error) {\n\treturn aul.au.lookupImpl(rnd, addr, true)\n}\n\n// Totals returns the totals for a given round\nfunc (aul *accountUpdatesLedgerEvaluator) Totals(rnd basics.Round) (AccountTotals, error) {\n\treturn aul.au.totalsImpl(rnd)\n}\n\n// isDup return whether a transaction is a duplicate one. It's not needed by the accountUpdatesLedgerEvaluator and implemeted as a stub.\nfunc (aul *accountUpdatesLedgerEvaluator) isDup(config.ConsensusParams, basics.Round, basics.Round, basics.Round, transactions.Txid, txlease) (bool, error) {\n\t// this is a non-issue since this call will never be made on non-validating evaluation\n\treturn false, fmt.Errorf(\"accountUpdatesLedgerEvaluator: tried to check for dup during accountUpdates initilization \")\n}\n\n// LookupWithoutRewards returns the account balance for a given address at a given round, without the reward\nfunc (aul *accountUpdatesLedgerEvaluator) LookupWithoutRewards(rnd basics.Round, addr basics.Address) (basics.AccountData, error) {\n\treturn aul.au.lookupImpl(rnd, addr, false)\n}\n\n// GetCreatorForRound returns the asset/app creator for a given asset/app index at a given round\nfunc (aul *accountUpdatesLedgerEvaluator) GetCreatorForRound(rnd basics.Round, cidx basics.CreatableIndex, ctype basics.CreatableType) (creator basics.Address, ok bool, err error) {\n\treturn aul.au.getCreatorForRoundImpl(rnd, cidx, ctype)\n}\n\n// totalsImpl returns the totals for a given round\nfunc (au *accountUpdates) totalsImpl(rnd basics.Round) (totals AccountTotals, err error) {\n\toffset, err := au.roundOffset(rnd)\n\tif err != nil {\n\t\treturn\n\t}\n\n\ttotals = au.roundTotals[offset]\n\treturn\n}\n\n// initializeCaches fills up the accountUpdates cache with the most recent ~320 blocks\nfunc (au *accountUpdates) initializeCaches(lastBalancesRound, lastestBlockRound, writingCatchpointRound basics.Round) (catchpointBlockDigest crypto.Digest, err error) {\n\tvar blk bookkeeping.Block\n\tvar delta StateDelta\n\n\taccLedgerEval := accountUpdatesLedgerEvaluator{\n\t\tau: au,\n\t}\n\tif lastBalancesRound < lastestBlockRound {\n\t\taccLedgerEval.prevHeader, err = au.ledger.BlockHdr(lastBalancesRound)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\t}\n\n\tfor lastBalancesRound < lastestBlockRound {\n\t\tnext := lastBalancesRound + 1\n\n\t\tblk, err = au.ledger.Block(next)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tdelta, err = au.ledger.trackerEvalVerified(blk, &accLedgerEval)\n\t\tif err != nil {\n\t\t\treturn\n\t\t}\n\n\t\tau.newBlockImpl(blk, delta)\n\t\tlastBalancesRound = next\n\n\t\tif next == basics.Round(writingCatchpointRound) {\n\t\t\tcatchpointBlockDigest = blk.Digest()\n\t\t}\n\n\t\taccLedgerEval.prevHeader = *delta.hdr\n\t}\n\treturn\n}\n\n// initializeFromDisk performs the atomic operation of loading the accounts data information from disk\n// and preparing the accountUpdates for operation, including initlizating the commitSyncer goroutine.\nfunc (au *accountUpdates) initializeFromDisk(l ledgerForTracker) (lastBalancesRound, lastestBlockRound basics.Round, err error) {\n\tau.dbs = l.trackerDB()\n\tau.log = l.trackerLog()\n\tau.ledger = l\n\n\tif au.initAccounts == nil {\n\t\terr = fmt.Errorf(\"accountUpdates.initializeFromDisk: initAccounts not set\")\n\t\treturn\n\t}\n\n\tlastestBlockRound = l.Latest()\n\terr = au.dbs.wdb.Atomic(func(ctx context.Context, tx *sql.Tx) error {\n\t\tvar err0 error\n\t\tau.dbRound, err0 = au.accountsInitialize(ctx, tx)\n\t\tif err0 != nil {\n\t\t\treturn err0\n\t\t}\n\t\t// Check for blocks DB and tracker DB un-sync\n\t\tif au.dbRound > lastestBlockRound {\n\t\t\tau.log.Warnf(\"accountUpdates.initializeFromDisk: resetting accounts DB (on round %v, but blocks DB's latest is %v)\", au.dbRound, lastestBlockRound)\n\t\t\terr0 = accountsReset(tx)\n\t\t\tif err0 != nil {\n\t\t\t\treturn err0\n\t\t\t}\n\t\t\tau.dbRound, err0 = au.accountsInitialize(ctx, tx)\n\t\t\tif err0 != nil {\n\t\t\t\treturn err0\n\t\t\t}\n\t\t}\n\n\t\ttotals, err0 := accountsTotals(tx, false)\n\t\tif err0 != nil {\n\t\t\treturn err0\n\t\t}\n\n\t\tau.roundTotals = []AccountTotals{totals}\n\t\treturn nil\n\t})\n\tif err != nil {\n\t\treturn\n\t}\n\n\tau.accountsq, err = accountsDbInit(au.dbs.rdb.Handle, au.dbs.wdb.Handle)\n\tif err != nil {\n\t\treturn\n\t}\n\n\tau.lastCatchpointLabel, _, err = au.accountsq.readCatchpointStateString(context.Background(), catchpointStateLastCatchpoint)\n\tif err != nil {\n\t\treturn\n\t}\n\n\thdr, err := l.BlockHdr(au.dbRound)\n\tif err != nil {\n\t\treturn\n\t}\n\tau.protos = []config.ConsensusParams{config.Consensus[hdr.CurrentProtocol]}\n\tau.deltas = nil\n\tau.creatableDeltas = nil\n\tau.accounts = make(map[basics.Address]modifiedAccount)\n\tau.creatables = make(map[basics.CreatableIndex]modifiedCreatable)\n\tau.deltasAccum = []int{0}\n\n\t// keep these channel closed if we're not generating catchpoint\n\tau.catchpointWriting = make(chan struct{}, 1)\n\tau.catchpointSlowWriting = make(chan struct{}, 1)\n\tclose(au.catchpointSlowWriting)\n\tclose(au.catchpointWriting)\n\tau.ctx, au.ctxCancel = context.WithCancel(context.Background())\n\tau.committedOffset = make(chan deferedCommit, 1)\n\tau.commitSyncerClosed = make(chan struct{})\n\tgo au.commitSyncer(au.committedOffset)\n\n\tlastBalancesRound = au.dbRound\n\n\treturn\n}\n\n// accountHashBuilder calculates the hash key used for the trie by combining the account address and the account data\nfunc accountHashBuilder(addr basics.Address, accountData basics.AccountData, encodedAccountData []byte) []byte {\n\thash := make([]byte, 4+crypto.DigestSize)\n\t// write out the lowest 32 bits of the reward base. This should improve the caching of the trie by allowing\n\t// recent updated to be in-cache, and \"older\" nodes will be left alone.\n\tfor i, rewards := 3, accountData.RewardsBase; i >= 0; i, rewards = i-1, rewards>>8 {\n\t\t// the following takes the rewards & 255 -> hash[i]\n\t\thash[i] = byte(rewards)\n\t}\n\tentryHash := crypto.Hash(append(addr[:], encodedAccountData[:]...))\n\tcopy(hash[4:], entryHash[:])\n\treturn hash[:]\n}\n\n// accountsInitialize initializes the accounts DB if needed and return currrent account round.\n// as part of the initialization, it tests the current database schema version, and perform upgrade\n// procedures to bring it up to the database schema supported by the binary.\nfunc (au *accountUpdates) accountsInitialize(ctx context.Context, tx *sql.Tx) (basics.Round, error) {\n\t// check current database version.\n\tdbVersion, err := db.GetUserVersion(ctx, tx)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to read database schema version : %v\", err)\n\t}\n\n\t// if database version is greater than supported by current binary, write a warning. This would keep the existing\n\t// fallback behaviour where we could use an older binary iff the schema happen to be backward compatible.\n\tif dbVersion > accountDBVersion {\n\t\tau.log.Warnf(\"accountsInitialize database schema version is %d, but algod supports only %d\", dbVersion, accountDBVersion)\n\t}\n\n\tif dbVersion < accountDBVersion {\n\t\tau.log.Infof(\"accountsInitialize upgrading database schema from version %d to version %d\", dbVersion, accountDBVersion)\n\n\t\tfor dbVersion < accountDBVersion {\n\t\t\tau.log.Infof(\"accountsInitialize performing upgrade from version %d\", dbVersion)\n\t\t\t// perform the initialization/upgrade\n\t\t\tswitch dbVersion {\n\t\t\tcase 0:\n\t\t\t\tdbVersion, err = au.upgradeDatabaseSchema0(ctx, tx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tau.log.Warnf(\"accountsInitialize failed to upgrade accounts database (ledger.tracker.sqlite) from schema 0 : %v\", err)\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\tcase 1:\n\t\t\t\tdbVersion, err = au.upgradeDatabaseSchema1(ctx, tx)\n\t\t\t\tif err != nil {\n\t\t\t\t\tau.log.Warnf(\"accountsInitialize failed to upgrade accounts database (ledger.tracker.sqlite) from schema 1 : %v\", err)\n\t\t\t\t\treturn 0, err\n\t\t\t\t}\n\t\t\tdefault:\n\t\t\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to upgrade database from schema version %d\", dbVersion)\n\t\t\t}\n\t\t}\n\n\t\tau.log.Infof(\"accountsInitialize database schema upgrade complete\")\n\t}\n\n\trnd, hashRound, err := accountsRound(tx)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif hashRound != rnd {\n\t\t// if the hashed round is different then the base round, something was modified, and the accounts aren't in sync\n\t\t// with the hashes.\n\t\terr = resetAccountHashes(tx)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\t// if catchpoint is disabled on this node, we could complete the initialization right here.\n\t\tif au.catchpointInterval == 0 {\n\t\t\treturn rnd, nil\n\t\t}\n\t}\n\n\t// create the merkle trie for the balances\n\tcommitter, err := makeMerkleCommitter(tx, false)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize was unable to makeMerkleCommitter: %v\", err)\n\t}\n\ttrie, err := merkletrie.MakeTrie(committer, trieCachedNodesCount)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize was unable to MakeTrie: %v\", err)\n\t}\n\n\t// we might have a database that was previously initialized, and now we're adding the balances trie. In that case, we need to add all the existing balances to this trie.\n\t// we can figure this out by examinine the hash of the root:\n\trootHash, err := trie.RootHash()\n\tif err != nil {\n\t\treturn rnd, fmt.Errorf(\"accountsInitialize was unable to retrieve trie root hash: %v\", err)\n\t}\n\tif rootHash.IsZero() {\n\t\taccountIdx := 0\n\t\tfor {\n\t\t\tbal, err := encodedAccountsRange(ctx, tx, accountIdx, trieRebuildAccountChunkSize)\n\t\t\tif err != nil {\n\t\t\t\treturn rnd, err\n\t\t\t}\n\t\t\tif len(bal) == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor _, balance := range bal {\n\t\t\t\tvar accountData basics.AccountData\n\t\t\t\terr = protocol.Decode(balance.AccountData, &accountData)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn rnd, err\n\t\t\t\t}\n\t\t\t\thash := accountHashBuilder(balance.Address, accountData, balance.AccountData)\n\t\t\t\tadded, err := trie.Add(hash)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn rnd, fmt.Errorf(\"accountsInitialize was unable to add changes to trie: %v\", err)\n\t\t\t\t}\n\t\t\t\tif !added {\n\t\t\t\t\tau.log.Warnf(\"accountsInitialize attempted to add duplicate hash '%s' to merkle trie for account %v\", hex.EncodeToString(hash), balance.Address)\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// this trie Evict will commit using the current transaction.\n\t\t\t// if anything goes wrong, it will still get rolled back.\n\t\t\t_, err = trie.Evict(true)\n\t\t\tif err != nil {\n\t\t\t\treturn 0, fmt.Errorf(\"accountsInitialize was unable to commit changes to trie: %v\", err)\n\t\t\t}\n\t\t\tif len(bal) < trieRebuildAccountChunkSize {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\taccountIdx += trieRebuildAccountChunkSize\n\t\t}\n\n\t\t// we've just updated the markle trie, update the hashRound to reflect that.\n\t\terr = updateAccountsRound(tx, rnd, rnd)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"accountsInitialize was unable to update the account round to %d: %v\", rnd, err)\n\t\t}\n\t}\n\tau.balancesTrie = trie\n\treturn rnd, nil\n}\n\n// upgradeDatabaseSchema0 upgrades the database schema from version 0 to version 1\n//\n// Schema of version 0 is expected to be aligned with the schema used on version 2.0.8 or before.\n// Any database of version 2.0.8 would be of version 0. At this point, the database might\n// have the following tables : ( i.e. a newly created database would not have these )\n// * acctrounds\n// * accounttotals\n// * accountbase\n// * assetcreators\n// * storedcatchpoints\n// * accounthashes\n// * catchpointstate\n//\n// As the first step of the upgrade, the above tables are being created if they do not already exists.\n// Following that, the assetcreators table is being altered by adding a new column to it (ctype).\n// Last, in case the database was just created, it would get initialized with the following:\n// The accountbase would get initialized with the au.initAccounts\n// The accounttotals would get initialized to align with the initialization account added to accountbase\n// The acctrounds would get updated to indicate that the balance matches round 0\n//\nfunc (au *accountUpdates) upgradeDatabaseSchema0(ctx context.Context, tx *sql.Tx) (updatedDBVersion int32, err error) {\n\tau.log.Infof(\"accountsInitialize initializing schema\")\n\terr = accountsInit(tx, au.initAccounts, au.initProto)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to initialize schema : %v\", err)\n\t}\n\t_, err = db.SetUserVersion(ctx, tx, 1)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to update database schema version from 0 to 1: %v\", err)\n\t}\n\treturn 1, nil\n}\n\n// upgradeDatabaseSchema1 upgrades the database schema from version 1 to version 2\n//\n// The schema updated to verison 2 intended to ensure that the encoding of all the accounts data is\n// both canonical and identical across the entire network. On release 2.0.5 we released an upgrade to the messagepack.\n// the upgraded messagepack was decoding the account data correctly, but would have different\n// encoding compared to it's predecessor. As a result, some of the account data that was previously stored\n// would have different encoded representation than the one on disk.\n// To address this, this startup proceduce would attempt to scan all the accounts data. for each account data, we would\n// see if it's encoding aligns with the current messagepack encoder. If it doesn't we would update it's encoding.\n// then, depending if we found any such account data, we would reset the merkle trie and stored catchpoints.\n// once the upgrade is complete, the accountsInitialize would (if needed) rebuild the merke trie using the new\n// encoded accounts.\n//\n// This upgrade doesn't change any of the actual database schema ( i.e. tables, indexes ) but rather just performing\n// a functional update to it's content.\n//\nfunc (au *accountUpdates) upgradeDatabaseSchema1(ctx context.Context, tx *sql.Tx) (updatedDBVersion int32, err error) {\n\t// update accounts encoding.\n\tau.log.Infof(\"accountsInitialize verifying accounts data encoding\")\n\tmodifiedAccounts, err := reencodeAccounts(ctx, tx)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif modifiedAccounts > 0 {\n\t\tau.log.Infof(\"accountsInitialize reencoded %d accounts\", modifiedAccounts)\n\n\t\tau.log.Infof(\"accountsInitialize resetting account hashes\")\n\t\t// reset the merkle trie\n\t\terr = resetAccountHashes(tx)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to reset account hashes : %v\", err)\n\t\t}\n\n\t\tau.log.Infof(\"accountsInitialize preparing queries\")\n\t\t// initialize a new accountsq with the incoming transaction.\n\t\taccountsq, err := accountsDbInit(tx, tx)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to prepare queries : %v\", err)\n\t\t}\n\n\t\t// close the prepared statements when we're done with them.\n\t\tdefer accountsq.close()\n\n\t\tau.log.Infof(\"accountsInitialize resetting prior catchpoints\")\n\t\t// delete the last catchpoint label if we have any.\n\t\t_, err = accountsq.writeCatchpointStateString(ctx, catchpointStateLastCatchpoint, \"\")\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to clear prior catchpoint : %v\", err)\n\t\t}\n\n\t\tau.log.Infof(\"accountsInitialize deleting stored catchpoints\")\n\t\t// delete catchpoints.\n\t\terr = au.deleteStoredCatchpoints(ctx, accountsq)\n\t\tif err != nil {\n\t\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to delete stored catchpoints : %v\", err)\n\t\t}\n\t} else {\n\t\tau.log.Infof(\"accountsInitialize found that no accounts needed to be reencoded\")\n\t}\n\n\t// update version\n\t_, err = db.SetUserVersion(ctx, tx, 2)\n\tif err != nil {\n\t\treturn 0, fmt.Errorf(\"accountsInitialize unable to update database schema version from 1 to 2: %v\", err)\n\t}\n\treturn 2, nil\n}\n\n// deleteStoredCatchpoints iterates over the storedcatchpoints table and deletes all the files stored on disk.\n// once all the files have been deleted, it would go ahead and remove the entries from the table.\nfunc (au *accountUpdates) deleteStoredCatchpoints(ctx context.Context, dbQueries *accountsDbQueries) (err error) {\n\tcatchpointsFilesChunkSize := 50\n\tfor {\n\t\tfileNames, err := dbQueries.getOldestCatchpointFiles(ctx, catchpointsFilesChunkSize, 0)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tif len(fileNames) == 0 {\n\t\t\tbreak\n\t\t}\n\n\t\tfor round, fileName := range fileNames {\n\t\t\tabsCatchpointFileName := filepath.Join(au.dbDirectory, fileName)\n\t\t\terr = os.Remove(absCatchpointFileName)\n\t\t\tif err == nil || os.IsNotExist(err) {\n\t\t\t\t// it's ok if the file doesn't exist. just remove it from the database and we'll be good to go.\n\t\t\t\terr = nil\n\t\t\t} else {\n\t\t\t\t// we can't delete the file, abort -\n\t\t\t\treturn fmt.Errorf(\"unable to delete old catchpoint file '%s' : %v\", absCatchpointFileName, err)\n\t\t\t}\n\t\t\t// clear the entry from the database\n\t\t\terr = dbQueries.storeCatchpoint(ctx, round, \"\", \"\", 0)\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// accountsUpdateBalances applies the given deltas array to the merkle trie\nfunc (au *accountUpdates) accountsUpdateBalances(accountsDeltasRound []map[basics.Address]accountDelta, offset uint64) (err error) {\n\tif au.catchpointInterval == 0 {\n\t\treturn nil\n\t}\n\tvar added, deleted bool\n\taccumulatedChanges := 0\n\tfor i := uint64(0); i < offset; i++ {\n\t\taccountsDeltas := accountsDeltasRound[i]\n\t\tfor addr, delta := range accountsDeltas {\n\t\t\tif !delta.old.IsZero() {\n\t\t\t\tdeleteHash := accountHashBuilder(addr, delta.old, protocol.Encode(&delta.old))\n\t\t\t\tdeleted, err = au.balancesTrie.Delete(deleteHash)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif !deleted {\n\t\t\t\t\tau.log.Warnf(\"failed to delete hash '%s' from merkle trie for account %v\", hex.EncodeToString(deleteHash), addr)\n\t\t\t\t} else {\n\t\t\t\t\taccumulatedChanges++\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !delta.new.IsZero() {\n\t\t\t\taddHash := accountHashBuilder(addr, delta.new, protocol.Encode(&delta.new))\n\t\t\t\tadded, err = au.balancesTrie.Add(addHash)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tif !added {\n\t\t\t\t\tau.log.Warnf(\"attempted to add duplicate hash '%s' to merkle trie for account %v\", hex.EncodeToString(addHash), addr)\n\t\t\t\t} else {\n\t\t\t\t\taccumulatedChanges++\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif accumulatedChanges >= trieAccumulatedChangesFlush {\n\t\t\taccumulatedChanges = 0\n\t\t\terr = au.balancesTrie.Commit()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// write it all to disk.\n\tif accumulatedChanges > 0 {\n\t\terr = au.balancesTrie.Commit()\n\t}\n\treturn\n}\n\n// newBlockImpl is the accountUpdates implementation of the ledgerTracker interface. This is the \"internal\" facing function\n// which assumes that no lock need to be taken.\nfunc (au *accountUpdates) newBlockImpl(blk bookkeeping.Block, delta StateDelta) {\n\tproto := config.Consensus[blk.CurrentProtocol]\n\trnd := blk.Round()\n\n\tif rnd <= au.latest() {\n\t\t// Duplicate, ignore.\n\t\treturn\n\t}\n\n\tif rnd != au.latest()+1 {\n\t\tau.log.Panicf(\"accountUpdates: newBlock %d too far in the future, dbRound %d, deltas %d\", rnd, au.dbRound, len(au.deltas))\n\t}\n\tau.deltas = append(au.deltas, delta.accts)\n\tau.protos = append(au.protos, proto)\n\tau.creatableDeltas = append(au.creatableDeltas, delta.creatables)\n\tau.roundDigest = append(au.roundDigest, blk.Digest())\n\tau.deltasAccum = append(au.deltasAccum, len(delta.accts)+au.deltasAccum[len(au.deltasAccum)-1])\n\n\tvar ot basics.OverflowTracker\n\tnewTotals := au.roundTotals[len(au.roundTotals)-1]\n\tallBefore := newTotals.All()\n\tnewTotals.applyRewards(delta.hdr.RewardsLevel, &ot)\n\n\tfor addr, data := range delta.accts {\n\t\tnewTotals.delAccount(proto, data.old, &ot)\n\t\tnewTotals.addAccount(proto, data.new, &ot)\n\n\t\tmacct := au.accounts[addr]\n\t\tmacct.ndeltas++\n\t\tmacct.data = data.new\n\t\tau.accounts[addr] = macct\n\t}\n\n\tfor cidx, cdelta := range delta.creatables {\n\t\tmcreat := au.creatables[cidx]\n\t\tmcreat.creator = cdelta.creator\n\t\tmcreat.created = cdelta.created\n\t\tmcreat.ctype = cdelta.ctype\n\t\tmcreat.ndeltas++\n\t\tau.creatables[cidx] = mcreat\n\t}\n\n\tif ot.Overflowed {\n\t\tau.log.Panicf(\"accountUpdates: newBlock %d overflowed totals\", rnd)\n\t}\n\tallAfter := newTotals.All()\n\tif allBefore != allAfter {\n\t\tau.log.Panicf(\"accountUpdates: sum of money changed from %d to %d\", allBefore.Raw, allAfter.Raw)\n\t}\n\n\tau.roundTotals = append(au.roundTotals, newTotals)\n}\n\n// lookupImpl returns the accound data for a given address at a given round. The withRewards indicates whether the\n// rewards should be added to the AccountData before returning. Note that the function doesn't update the account with the rewards,\n// even while it could return the AccoutData which represent the \"rewarded\" account data.\nfunc (au *accountUpdates) lookupImpl(rnd basics.Round, addr basics.Address, withRewards bool) (data basics.AccountData, err error) {\n\toffset, err := au.roundOffset(rnd)\n\tif err != nil {\n\t\treturn\n\t}\n\n\toffsetForRewards := offset\n\n\tdefer func() {\n\t\tif withRewards {\n\t\t\ttotals := au.roundTotals[offsetForRewards]\n\t\t\tproto := au.protos[offsetForRewards]\n\t\t\tdata = data.WithUpdatedRewards(proto, totals.RewardsLevel)\n\t\t}\n\t}()\n\n\t// Check if this is the most recent round, in which case, we can\n\t// use a cache of the most recent account state.\n\tif offset == uint64(len(au.deltas)) {\n\t\tmacct, ok := au.accounts[addr]\n\t\tif ok {\n\t\t\treturn macct.data, nil\n\t\t}\n\t} else {\n\t\t// Check if the account has been updated recently.  Traverse the deltas\n\t\t// backwards to ensure that later updates take priority if present.\n\t\tfor offset > 0 {\n\t\t\toffset--\n\t\t\td, ok := au.deltas[offset][addr]\n\t\t\tif ok {\n\t\t\t\treturn d.new, nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// No updates of this account in the in-memory deltas; use on-disk DB.\n\t// The check in roundOffset() made sure the round is exactly the one\n\t// present in the on-disk DB.  As an optimization, we avoid creating\n\t// a separate transaction here, and directly use a prepared SQL query\n\t// against the database.\n\treturn au.accountsq.lookup(addr)\n}\n\n// getCreatorForRoundImpl returns the asset/app creator for a given asset/app index at a given round\nfunc (au *accountUpdates) getCreatorForRoundImpl(rnd basics.Round, cidx basics.CreatableIndex, ctype basics.CreatableType) (creator basics.Address, ok bool, err error) {\n\toffset, err := au.roundOffset(rnd)\n\tif err != nil {\n\t\treturn basics.Address{}, false, err\n\t}\n\n\t// If this is the most recent round, au.creatables has will have the latest\n\t// state and we can skip scanning backwards over creatableDeltas\n\tif offset == uint64(len(au.deltas)) {\n\t\t// Check if we already have the asset/creator in cache\n\t\tcreatableDelta, ok := au.creatables[cidx]\n\t\tif ok {\n\t\t\tif creatableDelta.created && creatableDelta.ctype == ctype {\n\t\t\t\treturn creatableDelta.creator, true, nil\n\t\t\t}\n\t\t\treturn basics.Address{}, false, nil\n\t\t}\n\t} else {\n\t\tfor offset > 0 {\n\t\t\toffset--\n\t\t\tcreatableDelta, ok := au.creatableDeltas[offset][cidx]\n\t\t\tif ok {\n\t\t\t\tif creatableDelta.created && creatableDelta.ctype == ctype {\n\t\t\t\t\treturn creatableDelta.creator, true, nil\n\t\t\t\t}\n\t\t\t\treturn basics.Address{}, false, nil\n\t\t\t}\n\t\t}\n\t}\n\n\t// Check the database\n\treturn au.accountsq.lookupCreator(cidx, ctype)\n}\n\n// accountsCreateCatchpointLabel creates a catchpoint label and write it.\nfunc (au *accountUpdates) accountsCreateCatchpointLabel(committedRound basics.Round, totals AccountTotals, ledgerBlockDigest crypto.Digest, trieBalancesHash crypto.Digest) (label string, err error) {\n\tcpLabel := makeCatchpointLabel(committedRound, ledgerBlockDigest, trieBalancesHash, totals)\n\tlabel = cpLabel.String()\n\t_, err = au.accountsq.writeCatchpointStateString(context.Background(), catchpointStateLastCatchpoint, label)\n\treturn\n}\n\n// roundOffset calculates the offset of the given round compared to the current dbRound. Requires that the lock would be taken.\nfunc (au *accountUpdates) roundOffset(rnd basics.Round) (offset uint64, err error) {\n\tif rnd < au.dbRound {\n\t\terr = fmt.Errorf(\"round %d before dbRound %d\", rnd, au.dbRound)\n\t\treturn\n\t}\n\n\toff := uint64(rnd - au.dbRound)\n\tif off > uint64(len(au.deltas)) {\n\t\terr = fmt.Errorf(\"round %d too high: dbRound %d, deltas %d\", rnd, au.dbRound, len(au.deltas))\n\t\treturn\n\t}\n\n\treturn off, nil\n}\n\n// commitSyncer is the syncer go-routine function which perform the database updates. Internally, it dequeue deferedCommits and\n// send the tasks to commitRound for completing the operation.\nfunc (au *accountUpdates) commitSyncer(deferedCommits chan deferedCommit) {\n\tdefer close(au.commitSyncerClosed)\n\tfor {\n\t\tselect {\n\t\tcase committedOffset, ok := <-deferedCommits:\n\t\t\tif !ok {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tau.commitRound(committedOffset.offset, committedOffset.dbRound, committedOffset.lookback)\n\t\tcase <-au.ctx.Done():\n\t\t\t// drain the pending commits queue:\n\t\t\tdrained := false\n\t\t\tfor !drained {\n\t\t\t\tselect {\n\t\t\t\tcase <-deferedCommits:\n\t\t\t\t\tau.accountsWriting.Done()\n\t\t\t\tdefault:\n\t\t\t\t\tdrained = true\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// commitRound write to the database a \"chunk\" of rounds, and update the dbRound accordingly.\nfunc (au *accountUpdates) commitRound(offset uint64, dbRound basics.Round, lookback basics.Round) {\n\tdefer au.accountsWriting.Done()\n\tau.accountsMu.RLock()\n\n\t// we can exit right away, as this is the result of mis-ordered call to committedUpTo.\n\tif au.dbRound < dbRound || offset < uint64(au.dbRound-dbRound) {\n\t\t// if this is an archival ledger, we might need to close the catchpointWriting channel\n\t\tif au.archivalLedger {\n\t\t\t// determine if this was a catchpoint round\n\t\t\tisCatchpointRound := ((offset + uint64(lookback+dbRound)) > 0) && (au.catchpointInterval != 0) && (0 == (uint64((offset + uint64(lookback+dbRound))) % au.catchpointInterval))\n\t\t\tif isCatchpointRound {\n\t\t\t\t// it was a catchpoint round, so close the channel.\n\t\t\t\tclose(au.catchpointWriting)\n\t\t\t}\n\t\t}\n\t\tau.accountsMu.RUnlock()\n\t\treturn\n\t}\n\n\t// adjust the offset according to what happend meanwhile..\n\toffset -= uint64(au.dbRound - dbRound)\n\tdbRound = au.dbRound\n\n\tnewBase := basics.Round(offset) + dbRound\n\tflushTime := time.Now()\n\tisCatchpointRound := ((offset + uint64(lookback+dbRound)) > 0) && (au.catchpointInterval != 0) && (0 == (uint64((offset + uint64(lookback+dbRound))) % au.catchpointInterval))\n\n\t// create a copy of the deltas, round totals and protos for the range we're going to flush.\n\tdeltas := make([]map[basics.Address]accountDelta, offset, offset)\n\tcreatableDeltas := make([]map[basics.CreatableIndex]modifiedCreatable, offset, offset)\n\troundTotals := make([]AccountTotals, offset+1, offset+1)\n\tprotos := make([]config.ConsensusParams, offset+1, offset+1)\n\tcopy(deltas, au.deltas[:offset])\n\tcopy(creatableDeltas, au.creatableDeltas[:offset])\n\tcopy(roundTotals, au.roundTotals[:offset+1])\n\tcopy(protos, au.protos[:offset+1])\n\n\t// Keep track of how many changes to each account we flush to the\n\t// account DB, so that we can drop the corresponding refcounts in\n\t// au.accounts.\n\tflushcount := make(map[basics.Address]int)\n\tcreatableFlushcount := make(map[basics.CreatableIndex]int)\n\n\tvar committedRoundDigest crypto.Digest\n\n\tif isCatchpointRound {\n\t\tcommittedRoundDigest = au.roundDigest[offset+uint64(lookback)-1]\n\t}\n\n\tau.accountsMu.RUnlock()\n\n\t// in committedUpTo, we expect that this function we close the catchpointWriting when\n\t// it's on a catchpoint round and it's an archival ledger. Doing this in a defered function\n\t// here would prevent us from \"forgetting\" to close that channel later on.\n\tdefer func() {\n\t\tif isCatchpointRound && au.archivalLedger {\n\t\t\tclose(au.catchpointWriting)\n\t\t}\n\t}()\n\n\tfor i := uint64(0); i < offset; i++ {\n\t\tfor addr := range deltas[i] {\n\t\t\tflushcount[addr] = flushcount[addr] + 1\n\t\t}\n\t\tfor cidx := range creatableDeltas[i] {\n\t\t\tcreatableFlushcount[cidx] = creatableFlushcount[cidx] + 1\n\t\t}\n\t}\n\n\tvar catchpointLabel string\n\tbeforeUpdatingBalancesTime := time.Now()\n\tvar trieBalancesHash crypto.Digest\n\n\terr := au.dbs.wdb.AtomicCommitWriteLock(func(ctx context.Context, tx *sql.Tx) (err error) {\n\t\ttreeTargetRound := basics.Round(0)\n\t\tif au.catchpointInterval > 0 {\n\t\t\tmc, err0 := makeMerkleCommitter(tx, false)\n\t\t\tif err0 != nil {\n\t\t\t\treturn err0\n\t\t\t}\n\t\t\tif au.balancesTrie == nil {\n\t\t\t\ttrie, err := merkletrie.MakeTrie(mc, trieCachedNodesCount)\n\t\t\t\tif err != nil {\n\t\t\t\t\tau.log.Warnf(\"unable to create merkle trie during committedUpTo: %v\", err)\n\t\t\t\t\treturn err\n\t\t\t\t}\n\t\t\t\tau.balancesTrie = trie\n\t\t\t} else {\n\t\t\t\tau.balancesTrie.SetCommitter(mc)\n\t\t\t}\n\t\t\ttreeTargetRound = dbRound + basics.Round(offset)\n\t\t}\n\t\tfor i := uint64(0); i < offset; i++ {\n\t\t\terr = accountsNewRound(tx, deltas[i], creatableDeltas[i])\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\terr = totalsNewRounds(tx, deltas[:offset], roundTotals[1:offset+1], protos[1:offset+1])\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = au.accountsUpdateBalances(deltas, offset)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\terr = updateAccountsRound(tx, dbRound+basics.Round(offset), treeTargetRound)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tif isCatchpointRound {\n\t\t\ttrieBalancesHash, err = au.balancesTrie.RootHash()\n\t\t\tif err != nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t\treturn nil\n\t}, &au.accountsMu)\n\n\tif err != nil {\n\t\tau.balancesTrie = nil\n\t\tau.log.Warnf(\"unable to advance account snapshot: %v\", err)\n\t\treturn\n\t}\n\n\tif isCatchpointRound {\n\t\tcatchpointLabel, err = au.accountsCreateCatchpointLabel(dbRound+basics.Round(offset)+lookback, roundTotals[offset], committedRoundDigest, trieBalancesHash)\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"commitRound : unable to create a catchpoint label: %v\", err)\n\t\t}\n\t}\n\tif au.balancesTrie != nil {\n\t\t_, err = au.balancesTrie.Evict(false)\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"merkle trie failed to evict: %v\", err)\n\t\t}\n\t}\n\n\tif isCatchpointRound && catchpointLabel != \"\" {\n\t\tau.lastCatchpointLabel = catchpointLabel\n\t}\n\tupdatingBalancesDuration := time.Now().Sub(beforeUpdatingBalancesTime)\n\n\t// Drop reference counts to modified accounts, and evict them\n\t// from in-memory cache when no references remain.\n\tfor addr, cnt := range flushcount {\n\t\tmacct, ok := au.accounts[addr]\n\t\tif !ok {\n\t\t\tau.log.Panicf(\"inconsistency: flushed %d changes to %s, but not in au.accounts\", cnt, addr)\n\t\t}\n\n\t\tif cnt > macct.ndeltas {\n\t\t\tau.log.Panicf(\"inconsistency: flushed %d changes to %s, but au.accounts had %d\", cnt, addr, macct.ndeltas)\n\t\t}\n\n\t\tmacct.ndeltas -= cnt\n\t\tif macct.ndeltas == 0 {\n\t\t\tdelete(au.accounts, addr)\n\t\t} else {\n\t\t\tau.accounts[addr] = macct\n\t\t}\n\t}\n\n\tfor cidx, cnt := range creatableFlushcount {\n\t\tmcreat, ok := au.creatables[cidx]\n\t\tif !ok {\n\t\t\tau.log.Panicf(\"inconsistency: flushed %d changes to creatable %d, but not in au.creatables\", cnt, cidx)\n\t\t}\n\n\t\tif cnt > mcreat.ndeltas {\n\t\t\tau.log.Panicf(\"inconsistency: flushed %d changes to creatable %d, but au.creatables had %d\", cnt, cidx, mcreat.ndeltas)\n\t\t}\n\n\t\tmcreat.ndeltas -= cnt\n\t\tif mcreat.ndeltas == 0 {\n\t\t\tdelete(au.creatables, cidx)\n\t\t} else {\n\t\t\tau.creatables[cidx] = mcreat\n\t\t}\n\t}\n\n\tau.deltas = au.deltas[offset:]\n\tau.deltasAccum = au.deltasAccum[offset:]\n\tau.roundDigest = au.roundDigest[offset:]\n\tau.protos = au.protos[offset:]\n\tau.roundTotals = au.roundTotals[offset:]\n\tau.creatableDeltas = au.creatableDeltas[offset:]\n\tau.dbRound = newBase\n\tau.lastFlushTime = flushTime\n\n\tau.accountsMu.Unlock()\n\n\tif isCatchpointRound && au.archivalLedger && catchpointLabel != \"\" {\n\t\t// generate the catchpoint file. This need to be done inline so that it will block any new accounts that from being written.\n\t\t// the generateCatchpoint expects that the accounts data would not be modified in the background during it's execution.\n\t\tau.generateCatchpoint(basics.Round(offset)+dbRound+lookback, catchpointLabel, committedRoundDigest, updatingBalancesDuration)\n\t}\n\n}\n\n// latest returns the latest round\nfunc (au *accountUpdates) latest() basics.Round {\n\treturn au.dbRound + basics.Round(len(au.deltas))\n}\n\n// generateCatchpoint generates a single catchpoint file\nfunc (au *accountUpdates) generateCatchpoint(committedRound basics.Round, label string, committedRoundDigest crypto.Digest, updatingBalancesDuration time.Duration) {\n\tbeforeGeneratingCatchpointTime := time.Now()\n\tcatchpointGenerationStats := telemetryspec.CatchpointGenerationEventDetails{\n\t\tBalancesWriteTime: uint64(updatingBalancesDuration.Nanoseconds()),\n\t}\n\n\t// the retryCatchpointCreation is used to repeat the catchpoint file generation in case the node crashed / aborted during startup\n\t// before the catchpoint file generation could be completed.\n\tretryCatchpointCreation := false\n\tau.log.Debugf(\"accountUpdates: generateCatchpoint: generating catchpoint for round %d\", committedRound)\n\tdefer func() {\n\t\tif !retryCatchpointCreation {\n\t\t\t// clear the writingCatchpoint flag\n\t\t\t_, err := au.accountsq.writeCatchpointStateUint64(context.Background(), catchpointStateWritingCatchpoint, uint64(0))\n\t\t\tif err != nil {\n\t\t\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint unable to clear catchpoint state '%s' for round %d: %v\", catchpointStateWritingCatchpoint, committedRound, err)\n\t\t\t}\n\t\t}\n\t}()\n\n\t_, err := au.accountsq.writeCatchpointStateUint64(context.Background(), catchpointStateWritingCatchpoint, uint64(committedRound))\n\tif err != nil {\n\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint unable to write catchpoint state '%s' for round %d: %v\", catchpointStateWritingCatchpoint, committedRound, err)\n\t\treturn\n\t}\n\n\trelCatchpointFileName := filepath.Join(\"catchpoints\", catchpointRoundToPath(committedRound))\n\tabsCatchpointFileName := filepath.Join(au.dbDirectory, relCatchpointFileName)\n\n\tcatchpointWriter := makeCatchpointWriter(absCatchpointFileName, au.dbs.rdb, committedRound, committedRoundDigest, label)\n\n\tmore := true\n\tconst shortChunkExecutionDuration = 50 * time.Millisecond\n\tconst longChunkExecutionDuration = 1 * time.Second\n\tvar chunkExecutionDuration time.Duration\n\tselect {\n\tcase <-au.catchpointSlowWriting:\n\t\tchunkExecutionDuration = longChunkExecutionDuration\n\tdefault:\n\t\tchunkExecutionDuration = shortChunkExecutionDuration\n\t}\n\tfor more {\n\t\tstepCtx, stepCancelFunction := context.WithTimeout(au.ctx, chunkExecutionDuration)\n\t\twriteStepStartTime := time.Now()\n\t\tmore, err = catchpointWriter.WriteStep(stepCtx)\n\t\t// accumulate the actual time we've spent writing in this step.\n\t\tcatchpointGenerationStats.CPUTime += uint64(time.Now().Sub(writeStepStartTime).Nanoseconds())\n\t\tstepCancelFunction()\n\t\tif more && err == nil {\n\t\t\t// we just wrote some data, but there is more to be written.\n\t\t\t// go to sleep for while.\n\t\t\tselect {\n\t\t\tcase <-time.After(100 * time.Millisecond):\n\t\t\tcase <-au.ctx.Done():\n\t\t\t\tretryCatchpointCreation = true\n\t\t\t\terr2 := catchpointWriter.Abort()\n\t\t\t\tif err2 != nil {\n\t\t\t\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint: error removing catchpoint file : %v\", err2)\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\tcase <-au.catchpointSlowWriting:\n\t\t\t\tchunkExecutionDuration = longChunkExecutionDuration\n\t\t\t}\n\t\t}\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint: unable to create catchpoint : %v\", err)\n\t\t\terr2 := catchpointWriter.Abort()\n\t\t\tif err2 != nil {\n\t\t\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint: error removing catchpoint file : %v\", err2)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\terr = au.saveCatchpointFile(committedRound, relCatchpointFileName, catchpointWriter.GetSize(), catchpointWriter.GetCatchpoint())\n\tif err != nil {\n\t\tau.log.Warnf(\"accountUpdates: generateCatchpoint: unable to save catchpoint: %v\", err)\n\t\treturn\n\t}\n\tcatchpointGenerationStats.FileSize = uint64(catchpointWriter.GetSize())\n\tcatchpointGenerationStats.WritingDuration = uint64(time.Now().Sub(beforeGeneratingCatchpointTime).Nanoseconds())\n\tcatchpointGenerationStats.AccountsCount = catchpointWriter.GetTotalAccounts()\n\tcatchpointGenerationStats.CatchpointLabel = catchpointWriter.GetCatchpoint()\n\tau.log.EventWithDetails(telemetryspec.Accounts, telemetryspec.CatchpointGenerationEvent, catchpointGenerationStats)\n\tau.log.With(\"writingDuration\", catchpointGenerationStats.WritingDuration).\n\t\tWith(\"CPUTime\", catchpointGenerationStats.CPUTime).\n\t\tWith(\"balancesWriteTime\", catchpointGenerationStats.BalancesWriteTime).\n\t\tWith(\"accountsCount\", catchpointGenerationStats.AccountsCount).\n\t\tWith(\"fileSize\", catchpointGenerationStats.FileSize).\n\t\tWith(\"catchpointLabel\", catchpointGenerationStats.CatchpointLabel).\n\t\tInfof(\"Catchpoint file was generated\")\n}\n\n// catchpointRoundToPath calculate the catchpoint file path for a given round\nfunc catchpointRoundToPath(rnd basics.Round) string {\n\tirnd := int64(rnd) / 256\n\toutStr := \"\"\n\tfor irnd > 0 {\n\t\toutStr = filepath.Join(outStr, fmt.Sprintf(\"%02x\", irnd%256))\n\t\tirnd = irnd / 256\n\t}\n\toutStr = filepath.Join(outStr, strconv.FormatInt(int64(rnd), 10)+\".catchpoint\")\n\treturn outStr\n}\n\n// saveCatchpointFile stores the provided fileName as the stored catchpoint for the given round.\n// after a successfull insert operation to the database, it would delete up to 2 old entries, as needed.\n// deleting 2 entries while inserting single entry allow us to adjust the size of the backing storage and have the\n// database and storage realign.\nfunc (au *accountUpdates) saveCatchpointFile(round basics.Round, fileName string, fileSize int64, catchpoint string) (err error) {\n\tif au.catchpointFileHistoryLength != 0 {\n\t\terr = au.accountsq.storeCatchpoint(context.Background(), round, fileName, catchpoint, fileSize)\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"accountUpdates: saveCatchpoint: unable to save catchpoint: %v\", err)\n\t\t\treturn\n\t\t}\n\t} else {\n\t\terr = os.Remove(fileName)\n\t\tif err != nil {\n\t\t\tau.log.Warnf(\"accountUpdates: saveCatchpoint: unable to remove file (%s): %v\", fileName, err)\n\t\t\treturn\n\t\t}\n\t}\n\tif au.catchpointFileHistoryLength == -1 {\n\t\treturn\n\t}\n\tvar filesToDelete map[basics.Round]string\n\tfilesToDelete, err = au.accountsq.getOldestCatchpointFiles(context.Background(), 2, au.catchpointFileHistoryLength)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"unable to delete catchpoint file, getOldestCatchpointFiles failed : %v\", err)\n\t}\n\tfor round, fileToDelete := range filesToDelete {\n\t\tabsCatchpointFileName := filepath.Join(au.dbDirectory, fileToDelete)\n\t\terr = os.Remove(absCatchpointFileName)\n\t\tif err == nil || os.IsNotExist(err) {\n\t\t\t// it's ok if the file doesn't exist. just remove it from the database and we'll be good to go.\n\t\t\terr = nil\n\t\t} else {\n\t\t\t// we can't delete the file, abort -\n\t\t\treturn fmt.Errorf(\"unable to delete old catchpoint file '%s' : %v\", absCatchpointFileName, err)\n\t\t}\n\t\terr = au.accountsq.storeCatchpoint(context.Background(), round, \"\", \"\", 0)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"unable to delete old catchpoint entry '%s' : %v\", fileToDelete, err)\n\t\t}\n\t}\n\treturn\n}\n", "idx": 8, "id": 40625, "msg": "", "proj": "algorand-go-algorand", "lang": "go"}
{"patch": "@@ -32,7 +32,15 @@ module Faker\n     end\n \n     def self.clear\n-      ObjectSpace.each_object(self, &:clear)\n+      marked_unique.each(&:clear)\n+      marked_unique.clear\n+    end\n+\n+    def exclude(name, arguments, values)\n+      values ||= []\n+      values.each do |value|\n+        @previous_results[[name, arguments]] << value\n+      end\n     end\n   end\n end", "y": 0, "oldf": "module Faker\n  class UniqueGenerator\n    def initialize(generator, max_retries)\n      @generator = generator\n      @max_retries = max_retries\n      @previous_results = Hash.new { |hash, key| hash[key] = Set.new }\n    end\n\n    # rubocop:disable Style/MethodMissingSuper\n    def method_missing(name, *arguments)\n      @max_retries.times do\n        result = @generator.public_send(name, *arguments)\n\n        next if @previous_results[[name, arguments]].include?(result)\n\n        @previous_results[[name, arguments]] << result\n        return result\n      end\n\n      raise RetryLimitExceeded, \"Retry limit exceeded for #{name}\"\n    end\n    # rubocop:enable Style/MethodMissingSuper\n\n    def respond_to_missing?(method_name, include_private = false)\n      method_name.to_s.start_with?('faker_') || super\n    end\n\n    RetryLimitExceeded = Class.new(StandardError)\n\n    def clear\n      @previous_results.clear\n    end\n\n    def self.clear\n      ObjectSpace.each_object(self, &:clear)\n    end\n  end\nend\n", "idx": 2, "id": 8544, "msg": "", "proj": "faker-ruby-faker", "lang": "rb"}
{"patch": "@@ -1,3 +1,4 @@\n+import copy\n import json\n import sys\n import time", "y": 0, "oldf": "import json\nimport sys\nimport time\nimport traceback\nfrom optparse import OptionParser\n\nimport nose\nfrom nose.plugins import Plugin\n\n\ndef get_apiritif():\n    try:\n        import apiritif\n    except ImportError:\n        apiritif = None\n    return apiritif\n\n\nclass Sample(object):\n    def __init__(self, test_suite=None, test_case=None, status=None, start_time=None, duration=None,\n                 error_msg=None, error_trace=None):\n        self.test_suite = test_suite  # test label (test method name)\n        self.test_case = test_case  # test suite name (class name)\n        self.status = status  # test status (PASSED/FAILED/BROKEN/SKIPPED)\n        self.start_time = start_time  # test start time\n        self.duration = duration  # test duration\n        self.error_msg = error_msg  # short error message\n        self.error_trace = error_trace  # traceback of a failure\n        self.extras = {}  # extra info: ('file' - location, 'full_name' - full qualified name, 'decsription' - docstr)\n        self.subsamples = []  # subsamples list\n\n    def add_subsample(self, sample):\n        self.subsamples.append(sample)\n\n    def to_dict(self):\n        # type: () -> dict\n        return {\n            \"test_suite\": self.test_suite,\n            \"test_case\": self.test_case,\n            \"status\": self.status,\n            \"start_time\": self.start_time,\n            \"duration\": self.duration,\n            \"error_msg\": self.error_msg,\n            \"error_trace\": self.error_trace,\n            \"extras\": self.extras,\n            \"subsamples\": [sample.to_dict() for sample in self.subsamples],\n        }\n\n    def __repr__(self):\n        return \"Sample(%r)\" % self.to_dict()\n\n\nclass BZTPlugin(Plugin):\n    \"\"\"\n    Saves test results in a format suitable for Taurus.\n    \"\"\"\n\n    name = 'bzt_plugin'\n    enabled = True\n\n    def __init__(self, output_file):\n        super(BZTPlugin, self).__init__()\n        self.output_file = output_file\n        self.test_count = 0\n        self.success_count = 0\n        self.current_sample = None\n        self.out_stream = None\n\n    def __enter__(self):\n        self.out_stream = open(self.output_file, \"wt\", buffering=1)\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.out_stream.close()\n\n    def begin(self):\n        \"\"\"\n        Before any test runs\n        open descriptor here\n        :return:\n        \"\"\"\n        pass\n\n    def finalize(self, result):\n        \"\"\"\n        After all tests\n        :param result:\n        :return:\n        \"\"\"\n        del result\n        if not self.test_count:\n            raise RuntimeError(\"Nothing to test.\")\n\n    def startTest(self, test):  # pylint: disable=invalid-name\n        \"\"\"\n        before test run\n        :param test:\n        :return:\n        \"\"\"\n        test_file, _, _ = test.address()  # file path, module name, class.method\n        test_fqn = test.id()  # [package].module.class.method\n        class_name, method_name = test_fqn.split('.')[-2:]\n\n        self.current_sample = Sample(test_case=method_name, test_suite=class_name, start_time=time.time())\n        self.current_sample.extras.update({\n            \"file\": test_file,\n            \"full_name\": test_fqn,\n            \"description\": test.shortDescription()\n        })\n\n    def addError(self, test, error):  # pylint: disable=invalid-name\n        \"\"\"\n        when a test raises an uncaught exception\n        :param test:\n        :param error:\n        :return:\n        \"\"\"\n        # test_dict will be None if startTest wasn't called (i.e. exception in setUp/setUpClass)\n        if self.current_sample is not None:\n            self.current_sample.status = \"BROKEN\"\n            self.current_sample.error_msg = str(error[1]).split('\\n')[0]\n            self.current_sample.error_trace = self._get_trace(error)\n\n    @staticmethod\n    def _get_trace(error):\n        if sys.version > '3':\n            lines = traceback.format_exception(*error, chain=not isinstance(error[1], str))\n        else:\n            lines = traceback.format_exception(*error)\n        return ''.join(lines).rstrip()\n\n    def addFailure(self, test, error):  # pylint: disable=invalid-name\n        \"\"\"\n        when a test fails\n        :param test:\n        :param error:\n\n        :return:\n        \"\"\"\n        self.current_sample.status = \"FAILED\"\n        self.current_sample.error_msg = str(error[1]).split('\\n')[0]\n        self.current_sample.error_trace = self._get_trace(error)\n\n    def addSkip(self, test):  # pylint: disable=invalid-name\n        \"\"\"\n        when a test is skipped\n        :param test:\n        :return:\n        \"\"\"\n        self.current_sample.status = \"SKIPPED\"\n\n    def addSuccess(self, test):  # pylint: disable=invalid-name\n        \"\"\"\n        when a test passes\n        :param test:\n        :return:\n        \"\"\"\n        self.current_sample.status = \"PASSED\"\n        self.success_count += 1\n\n    def process_apiritif_samples(self, sample):\n        samples_processed = 0\n        apiritif = get_apiritif()\n        test_case = sample.test_case\n\n        recording = apiritif.recorder.get_recording(test_case)\n        if not recording:\n            return samples_processed\n\n        samples = ApiritifExtractor.parse_recording(recording, sample)\n        for sample in samples:\n            samples_processed += 1\n            self.test_count += 1\n            self.write_sample(sample)\n            self.write_stdout_report(sample.test_case)\n\n        return samples_processed\n\n    def process_sample(self, sample):\n        self.test_count += 1\n        self.write_sample(sample)\n        self.write_stdout_report(sample.test_case)\n\n    def write_sample(self, sample):\n        self.out_stream.write(\"%s\\n\" % json.dumps(sample.to_dict()))\n        self.out_stream.flush()\n\n    def write_stdout_report(self, label):\n        report_pattern = \"%s,Total:%d Passed:%d Failed:%d\\n\"\n        failed = self.test_count - self.success_count\n        sys.stdout.write(report_pattern % (label, self.test_count, self.success_count, failed))\n        sys.stdout.flush()\n\n    def stopTest(self, test):  # pylint: disable=invalid-name\n        \"\"\"\n        after the test has been run\n        :param test:\n        :return:\n        \"\"\"\n        self.current_sample.duration = time.time() - self.current_sample.start_time\n\n        if get_apiritif() is not None:\n            samples_processed = self.process_apiritif_samples(self.current_sample)\n            if samples_processed == 0:\n                self.process_sample(self.current_sample)\n        else:\n            self.process_sample(self.current_sample)\n\n        self.current_sample = None\n\n\nclass ApiritifExtractor(object):\n    @staticmethod\n    def parse_recording(recording, test_case_sample):\n        \"\"\"\n\n        :type recording: list[apiritif.Event]\n        :type test_case_sample: Sample\n        :rtype: list[Sample]\n        \"\"\"\n        apiritif = get_apiritif()\n        test_case_name = test_case_sample.test_case\n        active_transactions = [test_case_sample]\n        response_map = {}  # response -> sample\n        transactions_present = False\n        for item in recording:\n            if isinstance(item, apiritif.Request):\n                sample = Sample(\n                    test_suite=test_case_name,\n                    test_case=item.address,\n                    status=\"PASSED\",\n                    start_time=item.timestamp,\n                    duration=item.response.elapsed.total_seconds(),\n                )\n                extras = ApiritifExtractor._extract_extras(item)\n                if extras:\n                    sample.extras.update(extras)\n                response_map[item.response] = sample\n                active_transactions[-1].add_subsample(sample)\n            elif isinstance(item, apiritif.TransactionStarted):\n                transactions_present = True\n                tran = Sample(test_case=item.transaction_name, test_suite=test_case_name, start_time=item.timestamp)\n                active_transactions.append(tran)\n            elif isinstance(item, apiritif.TransactionEnded):\n                tran = active_transactions.pop()\n                assert tran.test_case == item.transaction_name\n                tran.duration = item.timestamp - tran.start_time\n                tran.status = \"PASSED\"\n                for sample in tran.subsamples:\n                    if sample.status in (\"FAILED\", \"BROKEN\"):\n                        tran.status = sample.status\n                        tran.error_msg = sample.error_msg\n                        tran.error_trace = sample.error_trace\n                active_transactions[-1].add_subsample(tran)\n            elif isinstance(item, apiritif.Assertion):\n                sample = response_map.get(item.response, None)\n                if sample is None:\n                    raise ValueError(\"Found assertion for unknown response\")\n                if \"assertions\" not in sample.extras:\n                    sample.extras[\"assertions\"] = []\n                sample.extras[\"assertions\"].append({\n                    \"name\": item.name,\n                    \"isFailed\": False,\n                    \"failureMessage\": \"\",\n                })\n            elif isinstance(item, apiritif.AssertionFailure):\n                sample = response_map.get(item.response, None)\n                if sample is None:\n                    raise ValueError(\"Found assertion failure for unknown response\")\n                for ass in sample.extras.get(\"assertions\", []):\n                    if ass[\"name\"] == item.name:\n                        ass[\"isFailed\"] = True\n                        ass[\"failureMessage\"] = item.failure_message\n                        sample.status = \"FAILED\"\n                        sample.error_msg = item.failure_message\n            else:\n                raise ValueError(\"Unknown kind of event in apiritif recording: %s\" % item)\n\n        if len(active_transactions) != 1:\n            # TODO: shouldn't we auto-balance them?\n            raise ValueError(\"Can't parse apiritif recordings: unbalanced transactions\")\n\n        toplevel_sample = active_transactions.pop()\n\n        # do not capture toplevel sample if transactions were used\n        if transactions_present:\n            return toplevel_sample.subsamples\n        else:\n            return [toplevel_sample]\n\n    @staticmethod\n    def _headers_from_dict(headers):\n        return \"\\n\".join(key + \": \" + value for key, value in headers.items())\n\n    @staticmethod\n    def _cookies_from_dict(cookies):\n        return \"; \".join(key + \"=\" + value for key, value in cookies.items())\n\n    @staticmethod\n    def _extract_extras(request_event):\n        response = request_event.response\n        baked_request = request_event.request\n\n        record = {\n            'responseCode': response.status_code,\n            'responseMessage': response.reason,\n            'responseTime': response.elapsed.total_seconds(),\n            'connectTime': 0,\n            'latency': 0,\n            'responseSize': len(response.content),\n            'requestSize': 0,\n            'requestMethod': baked_request.method,\n            'requestURI': baked_request.url,\n            'assertions': [],  # will be filled later\n            'responseBody': response.text,\n            'requestBody': baked_request.body or \"\",\n            'requestCookies': dict(request_event.session.cookies),\n            'requestHeaders': dict(response._request.headers),\n            'responseHeaders': dict(response.headers),\n        }\n\n        record[\"requestCookiesRaw\"] = ApiritifExtractor._cookies_from_dict(record[\"requestCookies\"])\n        record[\"responseBodySize\"] = len(record[\"responseBody\"])\n        record[\"requestBodySize\"] = len(record[\"requestBody\"])\n        record[\"requestCookiesSize\"] = len(record[\"requestCookiesRaw\"])\n        record[\"requestHeadersSize\"] = len(ApiritifExtractor._headers_from_dict(record[\"requestHeaders\"]))\n        record[\"responseHeadersSize\"] = len(ApiritifExtractor._headers_from_dict(record[\"responseHeaders\"]))\n\n        return record\n\n\ndef run_nose(report_file, files, iteration_limit, hold):\n    argv = [__file__, '-v']\n    argv.extend(files)\n    argv.extend(['--with-bzt_plugin', '--nocapture', '--exe', '--nologcapture'])\n\n    if iteration_limit == 0:\n        if hold > 0:\n            iteration_limit = sys.maxsize\n        else:\n            iteration_limit = 1\n\n    start_time = int(time.time())\n    with BZTPlugin(report_file) as plugin:\n        iteration = 0\n        while True:\n            nose.run(addplugins=[plugin], argv=argv)\n            iteration += 1\n            if 0 < hold < int(time.time()) - start_time:\n                break\n            if iteration >= iteration_limit:\n                break\n\n\nif __name__ == \"__main__\":\n    parser = OptionParser()\n    parser.add_option('-r', '--report-file', action='store', default='nose_report.ldjson')\n    parser.add_option('-i', '--iterations', action='store', default=0)\n    parser.add_option('-d', '--duration', action='store', default=0)\n    parser.add_option('-w', '--with-nose_plugin', action='store', default=0)\n    opts, args = parser.parse_args()\n\n    run_nose(opts.report_file, args, int(opts.iterations), float(opts.duration))\n", "idx": 1, "id": 14395, "msg": "", "proj": "Blazemeter-taurus", "lang": "py"}
{"patch": "@@ -288,7 +288,8 @@ public class FirefoxDriver extends RemoteWebDriver\n     return fullPageScreenshot.getFullPageScreenshotAs(outputType);\n   }\n \n-  @Override public void setContext(FirefoxCommandContext commandContext) {\n+  @Override\n+  public void setContext(FirefoxCommandContext commandContext) {\n     context.setContext(commandContext);\n   }\n ", "y": 0, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.selenium.firefox;\n\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.Maps;\nimport com.google.common.collect.Sets;\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.ImmutableCapabilities;\nimport org.openqa.selenium.MutableCapabilities;\nimport org.openqa.selenium.OutputType;\nimport org.openqa.selenium.PersistentCapabilities;\nimport org.openqa.selenium.Proxy;\nimport org.openqa.selenium.WebDriverException;\nimport org.openqa.selenium.devtools.CdpEndpointFinder;\nimport org.openqa.selenium.devtools.CdpInfo;\nimport org.openqa.selenium.devtools.CdpVersionFinder;\nimport org.openqa.selenium.devtools.Connection;\nimport org.openqa.selenium.devtools.DevTools;\nimport org.openqa.selenium.devtools.DevToolsException;\nimport org.openqa.selenium.devtools.HasDevTools;\nimport org.openqa.selenium.devtools.noop.NoOpCdpInfo;\nimport org.openqa.selenium.html5.LocalStorage;\nimport org.openqa.selenium.html5.SessionStorage;\nimport org.openqa.selenium.html5.WebStorage;\nimport org.openqa.selenium.internal.Require;\nimport org.openqa.selenium.remote.CommandInfo;\nimport org.openqa.selenium.remote.FileDetector;\nimport org.openqa.selenium.remote.RemoteWebDriver;\nimport org.openqa.selenium.remote.html5.RemoteWebStorage;\nimport org.openqa.selenium.remote.http.ClientConfig;\nimport org.openqa.selenium.remote.http.HttpClient;\nimport org.openqa.selenium.remote.service.DriverCommandExecutor;\nimport org.openqa.selenium.remote.service.DriverService;\n\nimport java.net.URI;\nimport java.nio.file.Path;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.ServiceLoader;\nimport java.util.Set;\nimport java.util.stream.StreamSupport;\n\nimport static org.openqa.selenium.remote.CapabilityType.PROXY;\n\n/**\n * An implementation of the {#link WebDriver} interface that drives Firefox.\n * <p>\n * The best way to construct a {@code FirefoxDriver} with various options is to make use of the\n * {@link FirefoxOptions}, like so:\n *\n * <pre>\n * FirefoxOptions options = new FirefoxOptions()\n *     .addPreference(\"browser.startup.page\", 1)\n *     .addPreference(\"browser.startup.homepage\", \"https://www.google.co.uk\")\n *     .setAcceptInsecureCerts(true)\n *     .setHeadless(true);\n * WebDriver driver = new FirefoxDriver(options);\n * </pre>\n */\npublic class FirefoxDriver extends RemoteWebDriver\n  implements WebStorage, HasExtensions, HasFullPageScreenshot, HasContext, HasDevTools {\n\n  public static final class SystemProperty {\n\n    /**\n     * System property that defines the location of the Firefox executable file.\n     */\n    public static final String BROWSER_BINARY = \"webdriver.firefox.bin\";\n\n    /**\n     * System property that defines the location of the file where Firefox log should be stored.\n     */\n    public static final String BROWSER_LOGFILE = \"webdriver.firefox.logfile\";\n\n    /**\n     * System property that defines the additional library path (Linux only).\n     */\n    public static final String BROWSER_LIBRARY_PATH = \"webdriver.firefox.library.path\";\n\n    /**\n     * System property that defines the profile that should be used as a template.\n     * When the driver starts, it will make a copy of the profile it is using,\n     * rather than using that profile directly.\n     */\n    public static final String BROWSER_PROFILE = \"webdriver.firefox.profile\";\n\n    /**\n     * System property that defines the location of the webdriver.xpi browser extension to install\n     * in the browser. If not set, the prebuilt extension bundled with this class will be used.\n     */\n    public static final String DRIVER_XPI_PROPERTY = \"webdriver.firefox.driver\";\n\n    /**\n     * Boolean system property that instructs FirefoxDriver to use Marionette backend,\n     * overrides any capabilities specified by the user\n     */\n    public static final String DRIVER_USE_MARIONETTE = \"webdriver.firefox.marionette\";\n  }\n\n  /**\n   * @deprecated Use {@link Capability#BINARY}\n   */\n  @Deprecated\n  public static final String BINARY = Capability.BINARY;\n\n  /**\n   * @deprecated Use {@link Capability#PROFILE}\n   */\n  @Deprecated\n  public static final String PROFILE = Capability.PROFILE;\n\n  /**\n   * @deprecated Use {@link Capability#MARIONETTE}\n   */\n  @Deprecated\n  public static final String MARIONETTE = Capability.MARIONETTE;\n\n  public static final class Capability {\n    public static final String BINARY = \"firefox_binary\";\n    public static final String PROFILE = \"firefox_profile\";\n    public static final String MARIONETTE = \"marionette\";\n  }\n\n  private static class FirefoxDriverCommandExecutor extends DriverCommandExecutor {\n    public FirefoxDriverCommandExecutor(DriverService service) {\n      super(service, getExtraCommands());\n    }\n\n    private static Map<String, CommandInfo> getExtraCommands() {\n      return ImmutableMap.<String, CommandInfo>builder()\n        .putAll(new AddHasContext().getAdditionalCommands())\n        .putAll(new AddHasExtensions().getAdditionalCommands())\n        .putAll(new AddHasFullPageScreenshot().getAdditionalCommands())\n        .build();\n    }\n  }\n\n  private final Capabilities capabilities;\n  protected FirefoxBinary binary;\n  private final RemoteWebStorage webStorage;\n  private final HasExtensions extensions;\n  private final HasFullPageScreenshot fullPageScreenshot;\n  private final HasContext context;\n  private final Optional<URI> cdpUri;\n  private DevTools devTools;\n\n  public FirefoxDriver() {\n    this(new FirefoxOptions());\n  }\n\n  /**\n   * @deprecated Use {@link #FirefoxDriver(FirefoxOptions)}.\n   */\n  @Deprecated\n  public FirefoxDriver(Capabilities desiredCapabilities) {\n    this(new FirefoxOptions(Require.nonNull(\"Capabilities\", desiredCapabilities)));\n  }\n\n  /**\n   * @deprecated Use {@link #FirefoxDriver(FirefoxDriverService, FirefoxOptions)}.\n   */\n  @Deprecated\n  public FirefoxDriver(FirefoxDriverService service, Capabilities desiredCapabilities) {\n    this(\n        Require.nonNull(\"Driver service\", service),\n        new FirefoxOptions(desiredCapabilities));\n  }\n\n  public FirefoxDriver(FirefoxOptions options) {\n    this(toExecutor(options), options);\n  }\n\n  public FirefoxDriver(FirefoxDriverService service) {\n    this(service, new FirefoxOptions());\n  }\n\n  public FirefoxDriver(FirefoxDriverService service, FirefoxOptions options) {\n    this(new FirefoxDriverCommandExecutor(service), options);\n  }\n\n  private FirefoxDriver(FirefoxDriverCommandExecutor executor, FirefoxOptions options) {\n    super(executor, dropCapabilities(options));\n    webStorage = new RemoteWebStorage(getExecuteMethod());\n    extensions = new AddHasExtensions().getImplementation(getCapabilities(), getExecuteMethod());\n    fullPageScreenshot = new AddHasFullPageScreenshot().getImplementation(getCapabilities(), getExecuteMethod());\n    context = new AddHasContext().getImplementation(getCapabilities(), getExecuteMethod());\n\n    Capabilities capabilities = super.getCapabilities();\n    HttpClient.Factory clientFactory = HttpClient.Factory.createDefault();\n    Optional<URI> cdpUri = CdpEndpointFinder.getReportedUri(\"moz:debuggerAddress\", capabilities)\n      .flatMap(reported -> CdpEndpointFinder.getCdpEndPoint(clientFactory, reported));\n\n    this.cdpUri = cdpUri;\n    this.capabilities = cdpUri.map(uri ->\n        new ImmutableCapabilities(\n            new PersistentCapabilities(capabilities)\n                .setCapability(\"se:cdp\", uri.toString())\n                .setCapability(\"se:cdpVersion\", \"85\")))\n        .orElse(new ImmutableCapabilities(capabilities));\n  }\n\n  private static FirefoxDriverCommandExecutor toExecutor(FirefoxOptions options) {\n    Require.nonNull(\"Options to construct executor from\", options);\n\n    String sysProperty = System.getProperty(SystemProperty.DRIVER_USE_MARIONETTE);\n    boolean isLegacy = (sysProperty != null && ! Boolean.parseBoolean(sysProperty))\n                       || options.isLegacy();\n\n    FirefoxDriverService.Builder<?, ?> builder =\n        StreamSupport.stream(ServiceLoader.load(DriverService.Builder.class).spliterator(), false)\n            .filter(b -> b instanceof FirefoxDriverService.Builder)\n            .map(FirefoxDriverService.Builder.class::cast)\n            .filter(b -> b.isLegacy() == isLegacy)\n            .findFirst().orElseThrow(WebDriverException::new);\n\n    return new FirefoxDriverCommandExecutor(builder.withOptions(options).build());\n  }\n\n  @Override\n  public Capabilities getCapabilities() {\n    return capabilities;\n  }\n\n  @Override\n  public void setFileDetector(FileDetector detector) {\n    throw new WebDriverException(\n        \"Setting the file detector only works on remote webdriver instances obtained \" +\n        \"via RemoteWebDriver\");\n  }\n\n  @Override\n  public LocalStorage getLocalStorage() {\n    return webStorage.getLocalStorage();\n  }\n\n  @Override\n  public SessionStorage getSessionStorage() {\n    return webStorage.getSessionStorage();\n  }\n\n  private static boolean isLegacy(Capabilities desiredCapabilities) {\n    Boolean forceMarionette = forceMarionetteFromSystemProperty();\n    if (forceMarionette != null) {\n      return !forceMarionette;\n    }\n    Object marionette = desiredCapabilities.getCapability(Capability.MARIONETTE);\n    return marionette instanceof Boolean && ! (Boolean) marionette;\n  }\n\n  @Override\n  public String installExtension(Path path) {\n    Require.nonNull(\"Path\", path);\n    return extensions.installExtension(path);\n  }\n\n  @Override\n  public void uninstallExtension(String extensionId) {\n    Require.nonNull(\"Extension ID\", extensionId);\n    extensions.uninstallExtension(extensionId);\n  }\n\n  /**\n   * Capture the full page screenshot and store it in the specified location.\n   *\n   * @param <X> Return type for getFullPageScreenshotAs.\n   * @param outputType target type, @see OutputType\n   * @return Object in which is stored information about the screenshot.\n   * @throws WebDriverException on failure.\n   */\n  @Override\n  public <X> X getFullPageScreenshotAs(OutputType<X> outputType) throws WebDriverException {\n    return fullPageScreenshot.getFullPageScreenshotAs(outputType);\n  }\n\n  @Override public void setContext(FirefoxCommandContext commandContext) {\n    context.setContext(commandContext);\n  }\n\n  private static Boolean forceMarionetteFromSystemProperty() {\n    String useMarionette = System.getProperty(SystemProperty.DRIVER_USE_MARIONETTE);\n    if (useMarionette == null) {\n      return null;\n    }\n    return Boolean.valueOf(useMarionette);\n  }\n\n  /**\n   * Drops capabilities that we shouldn't send over the wire.\n   *\n   * Used for capabilities which aren't BeanToJson-convertable, and are only used by the local\n   * launcher.\n   */\n  private static Capabilities dropCapabilities(Capabilities capabilities) {\n    if (capabilities == null) {\n      return new ImmutableCapabilities();\n    }\n\n    MutableCapabilities caps;\n\n    if (isLegacy(capabilities)) {\n      final Set<String> toRemove = Sets.newHashSet(Capability.BINARY, Capability.PROFILE);\n      caps = new MutableCapabilities(\n          Maps.filterKeys(capabilities.asMap(), key -> !toRemove.contains(key)));\n    } else {\n      caps = new MutableCapabilities(capabilities);\n    }\n\n    // Ensure that the proxy is in a state fit to be sent to the extension\n    Proxy proxy = Proxy.extractFrom(capabilities);\n    if (proxy != null) {\n      caps.setCapability(PROXY, proxy);\n    }\n\n    return caps;\n  }\n\n  @Override\n  public Optional<DevTools> maybeGetDevTools() {\n    if (devTools != null) {\n      return Optional.of(devTools);\n    }\n\n    if (!cdpUri.isPresent()) {\n      return Optional.empty();\n    }\n\n    URI wsUri = cdpUri.orElseThrow(() ->\n      new DevToolsException(\"This version of Firefox or geckodriver does not support CDP\"));\n    HttpClient.Factory clientFactory = HttpClient.Factory.createDefault();\n\n    ClientConfig wsConfig = ClientConfig.defaultConfig().baseUri(wsUri);\n    HttpClient wsClient = clientFactory.createClient(wsConfig);\n\n    Connection connection = new Connection(wsClient, wsUri.toString());\n    CdpInfo cdpInfo = new CdpVersionFinder().match(\"85.0\").orElseGet(NoOpCdpInfo::new);\n    devTools = new DevTools(cdpInfo::getDomains, connection);\n\n    return Optional.of(devTools);\n  }\n\n  @Override\n  public DevTools getDevTools() {\n    if (!cdpUri.isPresent()) {\n      throw new DevToolsException(\"This version of Firefox or geckodriver does not support CDP\");\n    }\n\n    return maybeGetDevTools().orElseThrow(() -> new DevToolsException(\"Unable to initialize CDP connection\"));\n  }\n}\n", "idx": 2, "id": 19125, "msg": "", "proj": "SeleniumHQ-selenium", "lang": "rb"}
{"patch": "@@ -1,6 +1,7 @@\n #include <cstdint>\n \n #include <iostream>\n+#include <numeric>\n #include <stdexcept>\n \n #include <adios2.h>", "y": 0, "oldf": "#include <cstdint>\n\n#include <iostream>\n#include <stdexcept>\n\n#include <adios2.h>\n\n#include <gtest/gtest.h>\n\n#ifdef ADIOS2_HAVE_MPI\n\nTEST(ADIOSInterface, MPICommRemoved)\n{\n    MPI_Comm myComm;\n    MPI_Comm_dup(MPI_COMM_WORLD, &myComm);\n    adios2::ADIOS adios(myComm);\n    adios2::IO io = adios.DeclareIO(\"TestIO\");\n    MPI_Comm_free(&myComm);\n\n    adios2::Engine engine = io.Open(\"test.bp\", adios2::Mode::Write);\n}\n\n#endif\n\nclass ADIOS2_CXX11_API : public ::testing::Test\n{\npublic:\n    ADIOS2_CXX11_API()\n#ifdef ADIOS2_HAVE_MPI\n    : ad(MPI_COMM_WORLD, adios2::DebugON)\n#else\n    : ad(adios2::DebugON)\n#endif\n    {\n#ifdef ADIOS2_HAVE_MPI\n        MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n        MPI_Comm_size(MPI_COMM_WORLD, &size);\n#endif\n    }\n\n    adios2::ADIOS ad;\n    int rank = 0;\n    int size = 1;\n};\n\nclass ADIOS2_CXX11_API_IO : public ADIOS2_CXX11_API\n{\npublic:\n    ADIOS2_CXX11_API_IO() : io(ad.DeclareIO(\"CXX11_API_TestIO\")) {}\n\n    adios2::IO io;\n};\n\nTEST_F(ADIOS2_CXX11_API_IO, Engine)\n{\n    io.SetEngine(\"bpfile\");\n    EXPECT_EQ(io.EngineType(), \"bpfile\");\n\n    adios2::Engine engine = io.Open(\"types.bp\", adios2::Mode::Write);\n    EXPECT_EQ(engine.Name(), \"types.bp\");\n    EXPECT_EQ(engine.Type(), \"BP3\");\n\n    EXPECT_EQ(io.EngineType(), \"bp\"); // FIXME? Is it expected that adios2_open\n                                      // changes the engine_type string?\n}\n\nTEST_F(ADIOS2_CXX11_API_IO, EngineDefault)\n{\n    io.SetEngine(\"\");\n    EXPECT_EQ(io.EngineType(), \"\");\n\n    adios2::Engine engine = io.Open(\"types.bp\", adios2::Mode::Write);\n    EXPECT_EQ(engine.Name(), \"types.bp\");\n    EXPECT_EQ(engine.Type(), \"BP3\");\n\n    EXPECT_EQ(io.EngineType(), \"bp\"); // FIXME? Is it expected that adios2_open\n                                      // changes the engine_type string?\n}\n\nint main(int argc, char **argv)\n{\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Init(nullptr, nullptr);\n#endif\n\n    int result;\n    ::testing::InitGoogleTest(&argc, argv);\n    result = RUN_ALL_TESTS();\n\n#ifdef ADIOS2_HAVE_MPI\n    MPI_Finalize();\n#endif\n\n    return result;\n}\n", "idx": 1, "id": 12618, "msg": "", "proj": "ornladios-ADIOS2", "lang": "cpp"}
{"patch": "@@ -285,7 +285,8 @@ class MisdesignChecker(BaseChecker):\n                 \"default\": 5,\n                 \"type\": \"int\",\n                 \"metavar\": \"<num>\",\n-                \"help\": \"Maximum number of boolean expressions in an if \" \"statement.\",\n+                \"help\": \"Maximum number of boolean expressions in an if \" \n+                        \"statement (see R0916).\",\n             },\n         ),\n     )", "y": 1, "oldf": "# -*- coding: utf-8 -*-\n# Copyright (c) 2006, 2009-2010, 2012-2015 LOGILAB S.A. (Paris, FRANCE) <contact@logilab.fr>\n# Copyright (c) 2012, 2014 Google, Inc.\n# Copyright (c) 2014-2018 Claudiu Popa <pcmanticore@gmail.com>\n# Copyright (c) 2014 Arun Persaud <arun@nubati.net>\n# Copyright (c) 2015 Ionel Cristian Maries <contact@ionelmc.ro>\n# Copyright (c) 2016 \u0141ukasz Rogalski <rogalski.91@gmail.com>\n# Copyright (c) 2017 ahirnish <ahirnish@gmail.com>\n# Copyright (c) 2018 Mike Frysinger <vapier@gmail.com>\n# Copyright (c) 2018 Mark Miller <725mrm@gmail.com>\n# Copyright (c) 2018 Ashley Whetter <ashley@awhetter.co.uk>\n# Copyright (c) 2018 Ville Skytt\u00e4 <ville.skytta@upcloud.com>\n# Copyright (c) 2018 Jakub Wilk <jwilk@jwilk.net>\n\n# Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n# For details: https://github.com/PyCQA/pylint/blob/master/COPYING\n\n\"\"\"check for signs of poor design\"\"\"\n\nfrom collections import defaultdict\nimport re\n\nimport astroid\nfrom astroid import If, BoolOp\nfrom astroid import decorators\n\nfrom pylint.interfaces import IAstroidChecker\nfrom pylint.checkers import BaseChecker\nfrom pylint.checkers.utils import check_messages\nfrom pylint import utils\n\n\nMSGS = {\n    \"R0901\": (\n        \"Too many ancestors (%s/%s)\",\n        \"too-many-ancestors\",\n        \"Used when class has too many parent classes, try to reduce \"\n        \"this to get a simpler (and so easier to use) class.\",\n    ),\n    \"R0902\": (\n        \"Too many instance attributes (%s/%s)\",\n        \"too-many-instance-attributes\",\n        \"Used when class has too many instance attributes, try to reduce \"\n        \"this to get a simpler (and so easier to use) class.\",\n    ),\n    \"R0903\": (\n        \"Too few public methods (%s/%s)\",\n        \"too-few-public-methods\",\n        \"Used when class has too few public methods, so be sure it's \"\n        \"really worth it.\",\n    ),\n    \"R0904\": (\n        \"Too many public methods (%s/%s)\",\n        \"too-many-public-methods\",\n        \"Used when class has too many public methods, try to reduce \"\n        \"this to get a simpler (and so easier to use) class.\",\n    ),\n    \"R0911\": (\n        \"Too many return statements (%s/%s)\",\n        \"too-many-return-statements\",\n        \"Used when a function or method has too many return statement, \"\n        \"making it hard to follow.\",\n    ),\n    \"R0912\": (\n        \"Too many branches (%s/%s)\",\n        \"too-many-branches\",\n        \"Used when a function or method has too many branches, \"\n        \"making it hard to follow.\",\n    ),\n    \"R0913\": (\n        \"Too many arguments (%s/%s)\",\n        \"too-many-arguments\",\n        \"Used when a function or method takes too many arguments.\",\n    ),\n    \"R0914\": (\n        \"Too many local variables (%s/%s)\",\n        \"too-many-locals\",\n        \"Used when a function or method has too many local variables.\",\n    ),\n    \"R0915\": (\n        \"Too many statements (%s/%s)\",\n        \"too-many-statements\",\n        \"Used when a function or method has too many statements. You \"\n        \"should then split it in smaller functions / methods.\",\n    ),\n    \"R0916\": (\n        \"Too many boolean expressions in if statement (%s/%s)\",\n        \"too-many-boolean-expressions\",\n        \"Used when an if statement contains too many boolean expressions.\",\n    ),\n}\nSPECIAL_OBJ = re.compile(\"^_{2}[a-z]+_{2}$\")\nDATACLASS_DECORATOR = \"dataclass\"\nDATACLASS_IMPORT = \"dataclasses\"\nTYPING_NAMEDTUPLE = \"typing.NamedTuple\"\n\n\ndef _is_typing_namedtuple(node: astroid.ClassDef) -> bool:\n    \"\"\"Check if a class node is a typing.NamedTuple class\"\"\"\n    for base in node.ancestors():\n        if base.qname() == TYPING_NAMEDTUPLE:\n            return True\n    return False\n\n\ndef _is_enum_class(node: astroid.ClassDef) -> bool:\n    \"\"\"Check if a class definition defines an Enum class.\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents an Enum class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    for base in node.bases:\n        try:\n            inferred_bases = base.inferred()\n        except astroid.InferenceError:\n            continue\n\n        for ancestor in inferred_bases:\n            if not isinstance(ancestor, astroid.ClassDef):\n                continue\n\n            if ancestor.name == \"Enum\" and ancestor.root().name == \"enum\":\n                return True\n\n    return False\n\n\ndef _is_dataclass(node: astroid.ClassDef) -> bool:\n    \"\"\"Check if a class definition defines a Python 3.7+ dataclass\n\n    :param node: The class node to check.\n    :type node: astroid.ClassDef\n\n    :returns: True if the given node represents a dataclass class. False otherwise.\n    :rtype: bool\n    \"\"\"\n    if not node.decorators:\n        return False\n\n    root_locals = node.root().locals\n    for decorator in node.decorators.nodes:\n        if isinstance(decorator, astroid.Call):\n            decorator = decorator.func\n        if not isinstance(decorator, (astroid.Name, astroid.Attribute)):\n            continue\n        if isinstance(decorator, astroid.Name):\n            name = decorator.name\n        else:\n            name = decorator.attrname\n        if name == DATACLASS_DECORATOR and DATACLASS_DECORATOR in root_locals:\n            return True\n    return False\n\n\ndef _count_boolean_expressions(bool_op):\n    \"\"\"Counts the number of boolean expressions in BoolOp `bool_op` (recursive)\n\n    example: a and (b or c or (d and e)) ==> 5 boolean expressions\n    \"\"\"\n    nb_bool_expr = 0\n    for bool_expr in bool_op.get_children():\n        if isinstance(bool_expr, BoolOp):\n            nb_bool_expr += _count_boolean_expressions(bool_expr)\n        else:\n            nb_bool_expr += 1\n    return nb_bool_expr\n\n\ndef _count_methods_in_class(node):\n    all_methods = sum(1 for method in node.methods() if not method.name.startswith(\"_\"))\n    # Special methods count towards the number of public methods,\n    # but don't count towards there being too many methods.\n    for method in node.mymethods():\n        if SPECIAL_OBJ.search(method.name) and method.name != \"__init__\":\n            all_methods += 1\n    return all_methods\n\n\nclass MisdesignChecker(BaseChecker):\n    \"\"\"checks for sign of poor/misdesign:\n    * number of methods, attributes, local variables...\n    * size, complexity of functions, methods\n    \"\"\"\n\n    __implements__ = (IAstroidChecker,)\n\n    # configuration section name\n    name = \"design\"\n    # messages\n    msgs = MSGS\n    priority = -2\n    # configuration options\n    options = (\n        (\n            \"max-args\",\n            {\n                \"default\": 5,\n                \"type\": \"int\",\n                \"metavar\": \"<int>\",\n                \"help\": \"Maximum number of arguments for function / method.\",\n            },\n        ),\n        (\n            \"max-locals\",\n            {\n                \"default\": 15,\n                \"type\": \"int\",\n                \"metavar\": \"<int>\",\n                \"help\": \"Maximum number of locals for function / method body.\",\n            },\n        ),\n        (\n            \"max-returns\",\n            {\n                \"default\": 6,\n                \"type\": \"int\",\n                \"metavar\": \"<int>\",\n                \"help\": \"Maximum number of return / yield for function / \"\n                \"method body.\",\n            },\n        ),\n        (\n            \"max-branches\",\n            {\n                \"default\": 12,\n                \"type\": \"int\",\n                \"metavar\": \"<int>\",\n                \"help\": \"Maximum number of branch for function / method body.\",\n            },\n        ),\n        (\n            \"max-statements\",\n            {\n                \"default\": 50,\n                \"type\": \"int\",\n                \"metavar\": \"<int>\",\n                \"help\": \"Maximum number of statements in function / method \" \"body.\",\n            },\n        ),\n        (\n            \"max-parents\",\n            {\n                \"default\": 7,\n                \"type\": \"int\",\n                \"metavar\": \"<num>\",\n                \"help\": \"Maximum number of parents for a class (see R0901).\",\n            },\n        ),\n        (\n            \"max-attributes\",\n            {\n                \"default\": 7,\n                \"type\": \"int\",\n                \"metavar\": \"<num>\",\n                \"help\": \"Maximum number of attributes for a class \\\n(see R0902).\",\n            },\n        ),\n        (\n            \"min-public-methods\",\n            {\n                \"default\": 2,\n                \"type\": \"int\",\n                \"metavar\": \"<num>\",\n                \"help\": \"Minimum number of public methods for a class \\\n(see R0903).\",\n            },\n        ),\n        (\n            \"max-public-methods\",\n            {\n                \"default\": 20,\n                \"type\": \"int\",\n                \"metavar\": \"<num>\",\n                \"help\": \"Maximum number of public methods for a class \\\n(see R0904).\",\n            },\n        ),\n        (\n            \"max-bool-expr\",\n            {\n                \"default\": 5,\n                \"type\": \"int\",\n                \"metavar\": \"<num>\",\n                \"help\": \"Maximum number of boolean expressions in an if \" \"statement.\",\n            },\n        ),\n    )\n\n    def __init__(self, linter=None):\n        BaseChecker.__init__(self, linter)\n        self.stats = None\n        self._returns = None\n        self._branches = None\n        self._stmts = None\n\n    def open(self):\n        \"\"\"initialize visit variables\"\"\"\n        self.stats = self.linter.add_stats()\n        self._returns = []\n        self._branches = defaultdict(int)\n        self._stmts = []\n\n    def _inc_all_stmts(self, amount):\n        for i in range(len(self._stmts)):\n            self._stmts[i] += amount\n\n    @decorators.cachedproperty\n    def _ignored_argument_names(self):\n        return utils.get_global_option(self, \"ignored-argument-names\", default=None)\n\n    @check_messages(\n        \"too-many-ancestors\",\n        \"too-many-instance-attributes\",\n        \"too-few-public-methods\",\n        \"too-many-public-methods\",\n    )\n    def visit_classdef(self, node):\n        \"\"\"check size of inheritance hierarchy and number of instance attributes\n        \"\"\"\n        nb_parents = len(list(node.ancestors()))\n        if nb_parents > self.config.max_parents:\n            self.add_message(\n                \"too-many-ancestors\",\n                node=node,\n                args=(nb_parents, self.config.max_parents),\n            )\n\n        if len(node.instance_attrs) > self.config.max_attributes:\n            self.add_message(\n                \"too-many-instance-attributes\",\n                node=node,\n                args=(len(node.instance_attrs), self.config.max_attributes),\n            )\n\n    @check_messages(\"too-few-public-methods\", \"too-many-public-methods\")\n    def leave_classdef(self, node):\n        \"\"\"check number of public methods\"\"\"\n        my_methods = sum(\n            1 for method in node.mymethods() if not method.name.startswith(\"_\")\n        )\n\n        # Does the class contain less than n public methods ?\n        # This checks only the methods defined in the current class,\n        # since the user might not have control over the classes\n        # from the ancestors. It avoids some false positives\n        # for classes such as unittest.TestCase, which provides\n        # a lot of assert methods. It doesn't make sense to warn\n        # when the user subclasses TestCase to add his own tests.\n        if my_methods > self.config.max_public_methods:\n            self.add_message(\n                \"too-many-public-methods\",\n                node=node,\n                args=(my_methods, self.config.max_public_methods),\n            )\n\n        # Stop here for exception, metaclass, interface classes and other\n        # classes for which we don't need to count the methods.\n        if (\n            node.type != \"class\"\n            or _is_enum_class(node)\n            or _is_dataclass(node)\n            or _is_typing_namedtuple(node)\n        ):\n            return\n\n        # Does the class contain more than n public methods ?\n        # This checks all the methods defined by ancestors and\n        # by the current class.\n        all_methods = _count_methods_in_class(node)\n        if all_methods < self.config.min_public_methods:\n            self.add_message(\n                \"too-few-public-methods\",\n                node=node,\n                args=(all_methods, self.config.min_public_methods),\n            )\n\n    @check_messages(\n        \"too-many-return-statements\",\n        \"too-many-branches\",\n        \"too-many-arguments\",\n        \"too-many-locals\",\n        \"too-many-statements\",\n        \"keyword-arg-before-vararg\",\n    )\n    def visit_functiondef(self, node):\n        \"\"\"check function name, docstring, arguments, redefinition,\n        variable names, max locals\n        \"\"\"\n        # init branch and returns counters\n        self._returns.append(0)\n        # check number of arguments\n        args = node.args.args\n        ignored_argument_names = self._ignored_argument_names\n        if args is not None:\n            ignored_args_num = 0\n            if ignored_argument_names:\n                ignored_args_num = sum(\n                    1 for arg in args if ignored_argument_names.match(arg.name)\n                )\n\n            argnum = len(args) - ignored_args_num\n            if argnum > self.config.max_args:\n                self.add_message(\n                    \"too-many-arguments\",\n                    node=node,\n                    args=(len(args), self.config.max_args),\n                )\n        else:\n            ignored_args_num = 0\n        # check number of local variables\n        locnum = len(node.locals) - ignored_args_num\n        if locnum > self.config.max_locals:\n            self.add_message(\n                \"too-many-locals\", node=node, args=(locnum, self.config.max_locals)\n            )\n        # init new statements counter\n        self._stmts.append(1)\n\n    visit_asyncfunctiondef = visit_functiondef\n\n    @check_messages(\n        \"too-many-return-statements\",\n        \"too-many-branches\",\n        \"too-many-arguments\",\n        \"too-many-locals\",\n        \"too-many-statements\",\n    )\n    def leave_functiondef(self, node):\n        \"\"\"most of the work is done here on close:\n        checks for max returns, branch, return in __init__\n        \"\"\"\n        returns = self._returns.pop()\n        if returns > self.config.max_returns:\n            self.add_message(\n                \"too-many-return-statements\",\n                node=node,\n                args=(returns, self.config.max_returns),\n            )\n        branches = self._branches[node]\n        if branches > self.config.max_branches:\n            self.add_message(\n                \"too-many-branches\",\n                node=node,\n                args=(branches, self.config.max_branches),\n            )\n        # check number of statements\n        stmts = self._stmts.pop()\n        if stmts > self.config.max_statements:\n            self.add_message(\n                \"too-many-statements\",\n                node=node,\n                args=(stmts, self.config.max_statements),\n            )\n\n    leave_asyncfunctiondef = leave_functiondef\n\n    def visit_return(self, _):\n        \"\"\"count number of returns\"\"\"\n        if not self._returns:\n            return  # return outside function, reported by the base checker\n        self._returns[-1] += 1\n\n    def visit_default(self, node):\n        \"\"\"default visit method -> increments the statements counter if\n        necessary\n        \"\"\"\n        if node.is_statement:\n            self._inc_all_stmts(1)\n\n    def visit_tryexcept(self, node):\n        \"\"\"increments the branches counter\"\"\"\n        branches = len(node.handlers)\n        if node.orelse:\n            branches += 1\n        self._inc_branch(node, branches)\n        self._inc_all_stmts(branches)\n\n    def visit_tryfinally(self, node):\n        \"\"\"increments the branches counter\"\"\"\n        self._inc_branch(node, 2)\n        self._inc_all_stmts(2)\n\n    @check_messages(\"too-many-boolean-expressions\")\n    def visit_if(self, node):\n        \"\"\"increments the branches counter and checks boolean expressions\"\"\"\n        self._check_boolean_expressions(node)\n        branches = 1\n        # don't double count If nodes coming from some 'elif'\n        if node.orelse and (len(node.orelse) > 1 or not isinstance(node.orelse[0], If)):\n            branches += 1\n        self._inc_branch(node, branches)\n        self._inc_all_stmts(branches)\n\n    def _check_boolean_expressions(self, node):\n        \"\"\"Go through \"if\" node `node` and counts its boolean expressions\n\n        if the \"if\" node test is a BoolOp node\n        \"\"\"\n        condition = node.test\n        if not isinstance(condition, BoolOp):\n            return\n        nb_bool_expr = _count_boolean_expressions(condition)\n        if nb_bool_expr > self.config.max_bool_expr:\n            self.add_message(\n                \"too-many-boolean-expressions\",\n                node=condition,\n                args=(nb_bool_expr, self.config.max_bool_expr),\n            )\n\n    def visit_while(self, node):\n        \"\"\"increments the branches counter\"\"\"\n        branches = 1\n        if node.orelse:\n            branches += 1\n        self._inc_branch(node, branches)\n\n    visit_for = visit_while\n\n    def _inc_branch(self, node, branchesnum=1):\n        \"\"\"increments the branches counter\"\"\"\n        self._branches[node.scope()] += branchesnum\n\n\ndef register(linter):\n    \"\"\"required method to auto register this checker \"\"\"\n    linter.register_checker(MisdesignChecker(linter))\n", "idx": 1, "id": 10906, "msg": "Please use the symbolic message instead of the numeric one.", "proj": "PyCQA-pylint", "lang": "py"}
{"patch": "@@ -607,7 +607,7 @@ namespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http\n                 application.DisposeContext(httpContext, _applicationException);\n \n                 // Even for non-keep-alive requests, try to consume the entire body to avoid RSTs.\n-                if (_ioCompleted == 0 && _requestRejectedException == null && !messageBody.IsEmpty)\n+                if (_requestRejectedException == null && !messageBody.IsEmpty)\n                 {\n                     await messageBody.ConsumeAsync();\n                 }", "y": 1, "oldf": "// Copyright (c) .NET Foundation. All rights reserved.\n// Licensed under the Apache License, Version 2.0. See License.txt in the project root for license information.\n\nusing System;\nusing System.Buffers;\nusing System.Collections.Generic;\nusing System.Diagnostics;\nusing System.IO;\nusing System.IO.Pipelines;\nusing System.Linq;\nusing System.Net;\nusing System.Runtime.CompilerServices;\nusing System.Text;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Microsoft.AspNetCore.Connections;\nusing Microsoft.AspNetCore.Hosting.Server;\nusing Microsoft.AspNetCore.Http;\nusing Microsoft.AspNetCore.Http.Features;\nusing Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Infrastructure;\nusing Microsoft.Extensions.Logging;\nusing Microsoft.Extensions.Primitives;\n\n// ReSharper disable AccessToModifiedClosure\n\nnamespace Microsoft.AspNetCore.Server.Kestrel.Core.Internal.Http\n{\n    public abstract partial class HttpProtocol : IHttpResponseControl\n    {\n        private static readonly byte[] _bytesConnectionClose = Encoding.ASCII.GetBytes(\"\\r\\nConnection: close\");\n        private static readonly byte[] _bytesConnectionKeepAlive = Encoding.ASCII.GetBytes(\"\\r\\nConnection: keep-alive\");\n        private static readonly byte[] _bytesTransferEncodingChunked = Encoding.ASCII.GetBytes(\"\\r\\nTransfer-Encoding: chunked\");\n        private static readonly byte[] _bytesServer = Encoding.ASCII.GetBytes(\"\\r\\nServer: \" + Constants.ServerName);\n        private static readonly Func<PipeWriter, ReadOnlyMemory<byte>, long> _writeChunk = WriteChunk;\n\n        private readonly object _onStartingSync = new Object();\n        private readonly object _onCompletedSync = new Object();\n\n        protected Streams _streams;\n\n        private Stack<KeyValuePair<Func<object, Task>, object>> _onStarting;\n        private Stack<KeyValuePair<Func<object, Task>, object>> _onCompleted;\n\n        private volatile int _ioCompleted;\n        private CancellationTokenSource _abortedCts;\n        private CancellationToken? _manuallySetRequestAbortToken;\n\n        protected RequestProcessingStatus _requestProcessingStatus;\n        protected volatile bool _keepAlive; // volatile, see: https://msdn.microsoft.com/en-us/library/x13ttww7.aspx\n        protected bool _upgradeAvailable;\n        private bool _canHaveBody;\n        private bool _autoChunk;\n        private Exception _applicationException;\n        private BadHttpRequestException _requestRejectedException;\n\n        protected HttpVersion _httpVersion;\n\n        private string _requestId;\n        private int _requestHeadersParsed;\n\n        private long _responseBytesWritten;\n\n        private readonly IHttpProtocolContext _context;\n\n        protected string _methodText = null;\n        private string _scheme = null;\n\n        public HttpProtocol(IHttpProtocolContext context)\n        {\n            _context = context;\n\n            ServerOptions = ServiceContext.ServerOptions;\n            HttpResponseControl = this;\n        }\n\n        public IHttpResponseControl HttpResponseControl { get; set; }\n\n        public Pipe RequestBodyPipe { get; protected set; }\n\n        public ServiceContext ServiceContext => _context.ServiceContext;\n        private IPEndPoint LocalEndPoint => _context.LocalEndPoint;\n        private IPEndPoint RemoteEndPoint => _context.RemoteEndPoint;\n\n        public IFeatureCollection ConnectionFeatures => _context.ConnectionFeatures;\n        public IHttpOutputProducer Output { get; protected set; }\n\n        protected IKestrelTrace Log => ServiceContext.Log;\n        private DateHeaderValueManager DateHeaderValueManager => ServiceContext.DateHeaderValueManager;\n        // Hold direct reference to ServerOptions since this is used very often in the request processing path\n        protected KestrelServerOptions ServerOptions { get; }\n        protected string ConnectionId => _context.ConnectionId;\n\n        public string ConnectionIdFeature { get; set; }\n        public bool HasStartedConsumingRequestBody { get; set; }\n        public long? MaxRequestBodySize { get; set; }\n        public bool AllowSynchronousIO { get; set; }\n\n        /// <summary>\n        /// The request id. <seealso cref=\"HttpContext.TraceIdentifier\"/>\n        /// </summary>\n        public string TraceIdentifier\n        {\n            set => _requestId = value;\n            get\n            {\n                // don't generate an ID until it is requested\n                if (_requestId == null)\n                {\n                    _requestId = CreateRequestId();\n                }\n                return _requestId;\n            }\n        }\n\n        public abstract bool IsUpgradableRequest { get; }\n        public bool IsUpgraded { get; set; }\n        public IPAddress RemoteIpAddress { get; set; }\n        public int RemotePort { get; set; }\n        public IPAddress LocalIpAddress { get; set; }\n        public int LocalPort { get; set; }\n        public string Scheme { get; set; }\n        public HttpMethod Method { get; set; }\n        public string PathBase { get; set; }\n        public string Path { get; set; }\n        public string QueryString { get; set; }\n        public string RawTarget { get; set; }\n\n        public string HttpVersion\n        {\n            get\n            {\n                if (_httpVersion == Http.HttpVersion.Http11)\n                {\n                    return HttpUtilities.Http11Version;\n                }\n                if (_httpVersion == Http.HttpVersion.Http10)\n                {\n                    return HttpUtilities.Http10Version;\n                }\n                if (_httpVersion == Http.HttpVersion.Http2)\n                {\n                    return HttpUtilities.Http2Version;\n                }\n\n                return string.Empty;\n            }\n\n            [MethodImpl(MethodImplOptions.AggressiveInlining)]\n            set\n            {\n                // GetKnownVersion returns versions which ReferenceEquals interned string\n                // As most common path, check for this only in fast-path and inline\n                if (ReferenceEquals(value, HttpUtilities.Http11Version))\n                {\n                    _httpVersion = Http.HttpVersion.Http11;\n                }\n                else if (ReferenceEquals(value, HttpUtilities.Http10Version))\n                {\n                    _httpVersion = Http.HttpVersion.Http10;\n                }\n                else if (ReferenceEquals(value, HttpUtilities.Http2Version))\n                {\n                    _httpVersion = Http.HttpVersion.Http2;\n                }\n                else\n                {\n                    HttpVersionSetSlow(value);\n                }\n            }\n        }\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        private void HttpVersionSetSlow(string value)\n        {\n            if (value == HttpUtilities.Http11Version)\n            {\n                _httpVersion = Http.HttpVersion.Http11;\n            }\n            else if (value == HttpUtilities.Http10Version)\n            {\n                _httpVersion = Http.HttpVersion.Http10;\n            }\n            else if (value == HttpUtilities.Http2Version)\n            {\n                _httpVersion = Http.HttpVersion.Http2;\n            }\n            else\n            {\n                _httpVersion = Http.HttpVersion.Unknown;\n            }\n        }\n\n        public IHeaderDictionary RequestHeaders { get; set; }\n        public Stream RequestBody { get; set; }\n\n        private int _statusCode;\n        public int StatusCode\n        {\n            get => _statusCode;\n            set\n            {\n                if (HasResponseStarted)\n                {\n                    ThrowResponseAlreadyStartedException(nameof(StatusCode));\n                }\n\n                _statusCode = value;\n            }\n        }\n\n        private string _reasonPhrase;\n\n        public string ReasonPhrase\n        {\n            get => _reasonPhrase;\n\n            set\n            {\n                if (HasResponseStarted)\n                {\n                    ThrowResponseAlreadyStartedException(nameof(ReasonPhrase));\n                }\n\n                _reasonPhrase = value;\n            }\n        }\n\n        public IHeaderDictionary ResponseHeaders { get; set; }\n        public Stream ResponseBody { get; set; }\n\n        public CancellationToken RequestAborted\n        {\n            get\n            {\n                // If a request abort token was previously explicitly set, return it.\n                if (_manuallySetRequestAbortToken.HasValue)\n                {\n                    return _manuallySetRequestAbortToken.Value;\n                }\n                // Otherwise, get the abort CTS.  If we have one, which would mean that someone previously\n                // asked for the RequestAborted token, simply return its token.  If we don't,\n                // check to see whether we've already aborted, in which case just return an\n                // already canceled token.  Finally, force a source into existence if we still\n                // don't have one, and return its token.\n                var cts = _abortedCts;\n                return\n                    cts != null ? cts.Token :\n                    (_ioCompleted == 1) ? new CancellationToken(true) :\n                    RequestAbortedSource.Token;\n            }\n            set\n            {\n                // Set an abort token, overriding one we create internally.  This setter and associated\n                // field exist purely to support IHttpRequestLifetimeFeature.set_RequestAborted.\n                _manuallySetRequestAbortToken = value;\n            }\n        }\n\n        private CancellationTokenSource RequestAbortedSource\n        {\n            get\n            {\n                // Get the abort token, lazily-initializing it if necessary.\n                // Make sure it's canceled if an abort request already came in.\n\n                // EnsureInitialized can return null since _abortedCts is reset to null\n                // after it's already been initialized to a non-null value.\n                // If EnsureInitialized does return null, this property was accessed between\n                // requests so it's safe to return an ephemeral CancellationTokenSource.\n                var cts = LazyInitializer.EnsureInitialized(ref _abortedCts, () => new CancellationTokenSource())\n                            ?? new CancellationTokenSource();\n\n                if (_ioCompleted == 1)\n                {\n                    cts.Cancel();\n                }\n                return cts;\n            }\n        }\n\n        public bool HasResponseStarted => _requestProcessingStatus == RequestProcessingStatus.ResponseStarted;\n\n        protected HttpRequestHeaders HttpRequestHeaders { get; } = new HttpRequestHeaders();\n\n        protected HttpResponseHeaders HttpResponseHeaders { get; } = new HttpResponseHeaders();\n\n        public MinDataRate MinRequestBodyDataRate { get; set; }\n\n        public MinDataRate MinResponseDataRate { get; set; }\n\n        public void InitializeStreams(MessageBody messageBody)\n        {\n            if (_streams == null)\n            {\n                _streams = new Streams(bodyControl: this, httpResponseControl: this);\n            }\n\n            (RequestBody, ResponseBody) = _streams.Start(messageBody);\n        }\n\n        public void StopStreams() => _streams.Stop();\n\n        // For testing\n        internal void ResetState()\n        {\n            _requestProcessingStatus = RequestProcessingStatus.RequestPending;\n        }\n\n        public void Reset()\n        {\n            _onStarting = null;\n            _onCompleted = null;\n\n            _requestProcessingStatus = RequestProcessingStatus.RequestPending;\n            _autoChunk = false;\n            _applicationException = null;\n            _requestRejectedException = null;\n\n            ResetFeatureCollection();\n\n            HasStartedConsumingRequestBody = false;\n            MaxRequestBodySize = ServerOptions.Limits.MaxRequestBodySize;\n            AllowSynchronousIO = ServerOptions.AllowSynchronousIO;\n            TraceIdentifier = null;\n            Method = HttpMethod.None;\n            _methodText = null;\n            PathBase = null;\n            Path = null;\n            RawTarget = null;\n            QueryString = null;\n            _httpVersion = Http.HttpVersion.Unknown;\n            _statusCode = StatusCodes.Status200OK;\n            _reasonPhrase = null;\n\n            var remoteEndPoint = RemoteEndPoint;\n            RemoteIpAddress = remoteEndPoint?.Address;\n            RemotePort = remoteEndPoint?.Port ?? 0;\n\n            var localEndPoint = LocalEndPoint;\n            LocalIpAddress = localEndPoint?.Address;\n            LocalPort = localEndPoint?.Port ?? 0;\n\n            ConnectionIdFeature = ConnectionId;\n\n            HttpRequestHeaders.Reset();\n            HttpResponseHeaders.Reset();\n            RequestHeaders = HttpRequestHeaders;\n            ResponseHeaders = HttpResponseHeaders;\n\n            if (_scheme == null)\n            {\n                var tlsFeature = ConnectionFeatures?[typeof(ITlsConnectionFeature)];\n                _scheme = tlsFeature != null ? \"https\" : \"http\";\n            }\n\n            Scheme = _scheme;\n\n            _manuallySetRequestAbortToken = null;\n            _abortedCts = null;\n\n            // Allow two bytes for \\r\\n after headers\n            _requestHeadersParsed = 0;\n\n            _responseBytesWritten = 0;\n\n            MinRequestBodyDataRate = ServerOptions.Limits.MinRequestBodyDataRate;\n            MinResponseDataRate = ServerOptions.Limits.MinResponseDataRate;\n\n            OnReset();\n        }\n\n        protected abstract void OnReset();\n\n        protected virtual void OnRequestProcessingEnding()\n        {\n        }\n\n        protected virtual void OnRequestProcessingEnded()\n        {\n        }\n\n        protected virtual void BeginRequestProcessing()\n        {\n        }\n\n        protected virtual void OnErrorAfterResponseStarted()\n        {\n        }\n\n        protected virtual bool BeginRead(out ValueTask<ReadResult> awaitable)\n        {\n            awaitable = default;\n            return false;\n        }\n\n        protected abstract string CreateRequestId();\n\n        protected abstract MessageBody CreateMessageBody();\n\n        protected abstract bool TryParseRequest(ReadResult result, out bool endConnection);\n\n        private void CancelRequestAbortedToken()\n        {\n            try\n            {\n                RequestAbortedSource.Cancel();\n                _abortedCts = null;\n            }\n            catch (Exception ex)\n            {\n                Log.ApplicationError(ConnectionId, TraceIdentifier, ex);\n            }\n        }\n\n        public void OnInputOrOutputCompleted()\n        {\n            if (Interlocked.Exchange(ref _ioCompleted, 1) != 0)\n            {\n                return;\n            }\n\n            _keepAlive = false;\n\n            Output.Dispose();\n\n            // Potentially calling user code. CancelRequestAbortedToken logs any exceptions.\n            ServiceContext.Scheduler.Schedule(state => ((HttpProtocol)state).CancelRequestAbortedToken(), this);\n        }\n\n        protected void PoisonRequestBodyStream(Exception abortReason)\n        {\n            _streams?.Abort(abortReason);\n        }\n\n        public void OnHeader(Span<byte> name, Span<byte> value)\n        {\n            _requestHeadersParsed++;\n            if (_requestHeadersParsed > ServerOptions.Limits.MaxRequestHeaderCount)\n            {\n                BadHttpRequestException.Throw(RequestRejectionReason.TooManyHeaders);\n            }\n            var valueString = value.GetAsciiStringNonNullCharacters();\n\n            HttpRequestHeaders.Append(name, valueString);\n        }\n\n        public async Task ProcessRequestsAsync<TContext>(IHttpApplication<TContext> application)\n        {\n            try\n            {\n                await ProcessRequests(application);\n            }\n            catch (BadHttpRequestException ex)\n            {\n                // Handle BadHttpRequestException thrown during request line or header parsing.\n                // SetBadRequestState logs the error.\n                SetBadRequestState(ex);\n            }\n            catch (ConnectionResetException ex)\n            {\n                // Don't log ECONNRESET errors made between requests. Browsers like IE will reset connections regularly.\n                if (_requestProcessingStatus != RequestProcessingStatus.RequestPending)\n                {\n                    Log.RequestProcessingError(ConnectionId, ex);\n                }\n            }\n            catch (IOException ex)\n            {\n                Log.RequestProcessingError(ConnectionId, ex);\n            }\n            catch (Exception ex)\n            {\n                Log.LogWarning(0, ex, CoreStrings.RequestProcessingEndError);\n            }\n            finally\n            {\n                try\n                {\n                    OnRequestProcessingEnding();\n                    await TryProduceInvalidRequestResponse();\n\n                    // Prevent RequestAborted from firing.\n                    Reset();\n\n                    Output.Dispose();\n                }\n                catch (Exception ex)\n                {\n                    Log.LogWarning(0, ex, CoreStrings.ConnectionShutdownError);\n                }\n                finally\n                {\n                    OnRequestProcessingEnded();\n                }\n            }\n        }\n\n        private async Task ProcessRequests<TContext>(IHttpApplication<TContext> application)\n        {\n            // Keep-alive is default for HTTP/1.1 and HTTP/2; parsing and errors will change its value\n            _keepAlive = true;\n\n            while (_keepAlive)\n            {\n                BeginRequestProcessing();\n\n                var result = default(ReadResult);\n                var endConnection = false;\n                do\n                {\n                    if (BeginRead(out var awaitable))\n                    {\n                        result = await awaitable;\n                    }\n                } while (!TryParseRequest(result, out endConnection));\n\n                if (endConnection)\n                {\n                    // Connection finished, stop processing requests\n                    return;\n                }\n\n                var messageBody = CreateMessageBody();\n                if (!messageBody.RequestKeepAlive)\n                {\n                    _keepAlive = false;\n                }\n\n                _upgradeAvailable = messageBody.RequestUpgrade;\n\n                InitializeStreams(messageBody);\n\n                var httpContext = application.CreateContext(this);\n\n                try\n                {\n                    KestrelEventSource.Log.RequestStart(this);\n\n                    // Run the application code for this request\n                    await application.ProcessRequestAsync(httpContext);\n\n                    if (_ioCompleted == 0)\n                    {\n                        VerifyResponseContentLength();\n                    }\n                }\n                catch (BadHttpRequestException ex)\n                {\n                    // Capture BadHttpRequestException for further processing\n                    // This has to be caught here so StatusCode is set properly before disposing the HttpContext\n                    // (DisposeContext logs StatusCode).\n                    SetBadRequestState(ex);\n                    ReportApplicationError(ex);\n                }\n                catch (Exception ex)\n                {\n                    ReportApplicationError(ex);\n                }\n\n                KestrelEventSource.Log.RequestStop(this);\n\n                // Trigger OnStarting if it hasn't been called yet and the app hasn't\n                // already failed. If an OnStarting callback throws we can go through\n                // our normal error handling in ProduceEnd.\n                // https://github.com/aspnet/KestrelHttpServer/issues/43\n                if (!HasResponseStarted && _applicationException == null && _onStarting != null)\n                {\n                    await FireOnStarting();\n                }\n\n                // At this point all user code that needs use to the request or response streams has completed.\n                // Using these streams in the OnCompleted callback is not allowed.\n                StopStreams();\n\n                // 4XX responses are written by TryProduceInvalidRequestResponse during connection tear down.\n                if (_requestRejectedException == null)\n                {\n                    if (_ioCompleted == 0)\n                    {\n                        // Call ProduceEnd() before consuming the rest of the request body to prevent\n                        // delaying clients waiting for the chunk terminator:\n                        //\n                        // https://github.com/dotnet/corefx/issues/17330#issuecomment-288248663\n                        //\n                        // This also prevents the 100 Continue response from being sent if the app\n                        // never tried to read the body.\n                        // https://github.com/aspnet/KestrelHttpServer/issues/2102\n                        //\n                        // ProduceEnd() must be called before _application.DisposeContext(), to ensure\n                        // HttpContext.Response.StatusCode is correctly set when\n                        // IHttpContextFactory.Dispose(HttpContext) is called.\n                        await ProduceEnd();\n                    }\n                    else if (!HasResponseStarted)\n                    {\n                        // If the request was aborted and no response was sent, there's no\n                        // meaningful status code to log.\n                        StatusCode = 0;\n                    }\n                }\n\n                if (_onCompleted != null)\n                {\n                    await FireOnCompleted();\n                }\n\n                application.DisposeContext(httpContext, _applicationException);\n\n                // Even for non-keep-alive requests, try to consume the entire body to avoid RSTs.\n                if (_ioCompleted == 0 && _requestRejectedException == null && !messageBody.IsEmpty)\n                {\n                    await messageBody.ConsumeAsync();\n                }\n\n                if (HasStartedConsumingRequestBody)\n                {\n                    RequestBodyPipe.Reader.Complete();\n\n                    // Wait for MessageBody.PumpAsync() to call RequestBodyPipe.Writer.Complete().\n                    await messageBody.StopAsync();\n\n                    // At this point both the request body pipe reader and writer should be completed.\n                    RequestBodyPipe.Reset();\n                }\n            }\n        }\n\n        public void OnStarting(Func<object, Task> callback, object state)\n        {\n            lock (_onStartingSync)\n            {\n                if (HasResponseStarted)\n                {\n                    ThrowResponseAlreadyStartedException(nameof(OnStarting));\n                }\n\n                if (_onStarting == null)\n                {\n                    _onStarting = new Stack<KeyValuePair<Func<object, Task>, object>>();\n                }\n                _onStarting.Push(new KeyValuePair<Func<object, Task>, object>(callback, state));\n            }\n        }\n\n        public void OnCompleted(Func<object, Task> callback, object state)\n        {\n            lock (_onCompletedSync)\n            {\n                if (_onCompleted == null)\n                {\n                    _onCompleted = new Stack<KeyValuePair<Func<object, Task>, object>>();\n                }\n                _onCompleted.Push(new KeyValuePair<Func<object, Task>, object>(callback, state));\n            }\n        }\n\n        protected Task FireOnStarting()\n        {\n            Stack<KeyValuePair<Func<object, Task>, object>> onStarting;\n            lock (_onStartingSync)\n            {\n                onStarting = _onStarting;\n                _onStarting = null;\n            }\n\n            if (onStarting == null)\n            {\n                return Task.CompletedTask;\n            }\n            else\n            {\n                return FireOnStartingMayAwait(onStarting);\n            }\n\n        }\n\n        private Task FireOnStartingMayAwait(Stack<KeyValuePair<Func<object, Task>, object>> onStarting)\n        {\n            try\n            {\n                var count = onStarting.Count;\n                for (var i = 0; i < count; i++)\n                {\n                    var entry = onStarting.Pop();\n                    var task = entry.Key.Invoke(entry.Value);\n                    if (!ReferenceEquals(task, Task.CompletedTask))\n                    {\n                        return FireOnStartingAwaited(task, onStarting);\n                    }\n                }\n            }\n            catch (Exception ex)\n            {\n                ReportApplicationError(ex);\n            }\n\n            return Task.CompletedTask;\n        }\n\n        private async Task FireOnStartingAwaited(Task currentTask, Stack<KeyValuePair<Func<object, Task>, object>> onStarting)\n        {\n            try\n            {\n                await currentTask;\n\n                var count = onStarting.Count;\n                for (var i = 0; i < count; i++)\n                {\n                    var entry = onStarting.Pop();\n                    await entry.Key.Invoke(entry.Value);\n                }\n            }\n            catch (Exception ex)\n            {\n                ReportApplicationError(ex);\n            }\n        }\n\n        protected Task FireOnCompleted()\n        {\n            Stack<KeyValuePair<Func<object, Task>, object>> onCompleted;\n            lock (_onCompletedSync)\n            {\n                onCompleted = _onCompleted;\n                _onCompleted = null;\n            }\n\n            if (onCompleted == null)\n            {\n                return Task.CompletedTask;\n            }\n\n            return FireOnCompletedAwaited(onCompleted);\n        }\n\n        private async Task FireOnCompletedAwaited(Stack<KeyValuePair<Func<object, Task>, object>> onCompleted)\n        {\n            foreach (var entry in onCompleted)\n            {\n                try\n                {\n                    await entry.Key.Invoke(entry.Value);\n                }\n                catch (Exception ex)\n                {\n                    Log.ApplicationError(ConnectionId, TraceIdentifier, ex);\n                }\n            }\n        }\n\n        public Task FlushAsync(CancellationToken cancellationToken = default(CancellationToken))\n        {\n            if (!HasResponseStarted)\n            {\n                var initializeTask = InitializeResponseAsync(0);\n                // If return is Task.CompletedTask no awaiting is required\n                if (!ReferenceEquals(initializeTask, Task.CompletedTask))\n                {\n                    return FlushAsyncAwaited(initializeTask, cancellationToken);\n                }\n            }\n\n            return Output.FlushAsync(cancellationToken);\n        }\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        private async Task FlushAsyncAwaited(Task initializeTask, CancellationToken cancellationToken)\n        {\n            await initializeTask;\n            await Output.FlushAsync(cancellationToken);\n        }\n\n        public Task WriteAsync(ReadOnlyMemory<byte> data, CancellationToken cancellationToken = default(CancellationToken))\n        {\n            // For the first write, ensure headers are flushed if WriteDataAsync isn't called.\n            var firstWrite = !HasResponseStarted;\n\n            if (firstWrite)\n            {\n                var initializeTask = InitializeResponseAsync(data.Length);\n                // If return is Task.CompletedTask no awaiting is required\n                if (!ReferenceEquals(initializeTask, Task.CompletedTask))\n                {\n                    return WriteAsyncAwaited(initializeTask, data, cancellationToken);\n                }\n            }\n            else\n            {\n                VerifyAndUpdateWrite(data.Length);\n            }\n\n            if (_canHaveBody)\n            {\n                if (_autoChunk)\n                {\n                    if (data.Length == 0)\n                    {\n                        return !firstWrite ? Task.CompletedTask : FlushAsync(cancellationToken);\n                    }\n                    return WriteChunkedAsync(data, cancellationToken);\n                }\n                else\n                {\n                    CheckLastWrite();\n                    return Output.WriteDataAsync(data.Span, cancellationToken: cancellationToken);\n                }\n            }\n            else\n            {\n                HandleNonBodyResponseWrite();\n                return !firstWrite ? Task.CompletedTask : FlushAsync(cancellationToken);\n            }\n        }\n\n        public async Task WriteAsyncAwaited(Task initializeTask, ReadOnlyMemory<byte> data, CancellationToken cancellationToken)\n        {\n            await initializeTask;\n\n            // WriteAsyncAwaited is only called for the first write to the body.\n            // Ensure headers are flushed if Write(Chunked)Async isn't called.\n            if (_canHaveBody)\n            {\n                if (_autoChunk)\n                {\n                    if (data.Length == 0)\n                    {\n                        await FlushAsync(cancellationToken);\n                        return;\n                    }\n\n                    await WriteChunkedAsync(data, cancellationToken);\n                }\n                else\n                {\n                    CheckLastWrite();\n                    await Output.WriteDataAsync(data.Span, cancellationToken: cancellationToken);\n                }\n            }\n            else\n            {\n                HandleNonBodyResponseWrite();\n                await FlushAsync(cancellationToken);\n            }\n        }\n\n        private void VerifyAndUpdateWrite(int count)\n        {\n            var responseHeaders = HttpResponseHeaders;\n\n            if (responseHeaders != null &&\n                !responseHeaders.HasTransferEncoding &&\n                responseHeaders.ContentLength.HasValue &&\n                _responseBytesWritten + count > responseHeaders.ContentLength.Value)\n            {\n                _keepAlive = false;\n                ThrowTooManyBytesWritten(count);\n            }\n\n            _responseBytesWritten += count;\n        }\n\n        [StackTraceHidden]\n        private void ThrowTooManyBytesWritten(int count)\n        {\n            throw GetTooManyBytesWrittenException(count);\n        }\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        private InvalidOperationException GetTooManyBytesWrittenException(int count)\n        {\n            var responseHeaders = HttpResponseHeaders;\n            return new InvalidOperationException(\n                CoreStrings.FormatTooManyBytesWritten(_responseBytesWritten + count, responseHeaders.ContentLength.Value));\n        }\n\n        private void CheckLastWrite()\n        {\n            var responseHeaders = HttpResponseHeaders;\n\n            // Prevent firing request aborted token if this is the last write, to avoid\n            // aborting the request if the app is still running when the client receives\n            // the final bytes of the response and gracefully closes the connection.\n            //\n            // Called after VerifyAndUpdateWrite(), so _responseBytesWritten has already been updated.\n            if (responseHeaders != null &&\n                !responseHeaders.HasTransferEncoding &&\n                responseHeaders.ContentLength.HasValue &&\n                _responseBytesWritten == responseHeaders.ContentLength.Value)\n            {\n                _abortedCts = null;\n            }\n        }\n\n        protected void VerifyResponseContentLength()\n        {\n            var responseHeaders = HttpResponseHeaders;\n\n            if (Method != HttpMethod.Head &&\n                StatusCode != StatusCodes.Status304NotModified &&\n                !responseHeaders.HasTransferEncoding &&\n                responseHeaders.ContentLength.HasValue &&\n                _responseBytesWritten < responseHeaders.ContentLength.Value)\n            {\n                // We need to close the connection if any bytes were written since the client\n                // cannot be certain of how many bytes it will receive.\n                if (_responseBytesWritten > 0)\n                {\n                    _keepAlive = false;\n                }\n\n                ReportApplicationError(new InvalidOperationException(\n                    CoreStrings.FormatTooFewBytesWritten(_responseBytesWritten, responseHeaders.ContentLength.Value)));\n            }\n        }\n\n        private Task WriteChunkedAsync(ReadOnlyMemory<byte> data, CancellationToken cancellationToken)\n        {\n            return Output.WriteAsync(_writeChunk, data);\n        }\n\n        private static long WriteChunk(PipeWriter writableBuffer, ReadOnlyMemory<byte> buffer)\n        {\n            var bytesWritten = 0L;\n            if (buffer.Length > 0)\n            {\n                var writer = new BufferWriter<PipeWriter>(writableBuffer);\n\n                ChunkWriter.WriteBeginChunkBytes(ref writer, buffer.Length);\n                writer.Write(buffer.Span);\n                ChunkWriter.WriteEndChunkBytes(ref writer);\n                writer.Commit();\n\n                bytesWritten = writer.BytesCommitted;\n            }\n\n            return bytesWritten;\n        }\n\n        private static ArraySegment<byte> CreateAsciiByteArraySegment(string text)\n        {\n            var bytes = Encoding.ASCII.GetBytes(text);\n            return new ArraySegment<byte>(bytes);\n        }\n\n        public void ProduceContinue()\n        {\n            if (HasResponseStarted)\n            {\n                return;\n            }\n\n            if (_httpVersion != Http.HttpVersion.Http10 &&\n                RequestHeaders.TryGetValue(\"Expect\", out var expect) &&\n                (expect.FirstOrDefault() ?? \"\").Equals(\"100-continue\", StringComparison.OrdinalIgnoreCase))\n            {\n                Output.Write100ContinueAsync().GetAwaiter().GetResult();\n            }\n        }\n\n        public Task InitializeResponseAsync(int firstWriteByteCount)\n        {\n            var startingTask = FireOnStarting();\n            // If return is Task.CompletedTask no awaiting is required\n            if (!ReferenceEquals(startingTask, Task.CompletedTask))\n            {\n                return InitializeResponseAwaited(startingTask, firstWriteByteCount);\n            }\n\n            if (_applicationException != null)\n            {\n                ThrowResponseAbortedException();\n            }\n\n            VerifyAndUpdateWrite(firstWriteByteCount);\n            ProduceStart(appCompleted: false);\n\n            return Task.CompletedTask;\n        }\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        public async Task InitializeResponseAwaited(Task startingTask, int firstWriteByteCount)\n        {\n            await startingTask;\n\n            if (_applicationException != null)\n            {\n                ThrowResponseAbortedException();\n            }\n\n            VerifyAndUpdateWrite(firstWriteByteCount);\n            ProduceStart(appCompleted: false);\n        }\n\n        private void ProduceStart(bool appCompleted)\n        {\n            if (HasResponseStarted)\n            {\n                return;\n            }\n\n            _requestProcessingStatus = RequestProcessingStatus.ResponseStarted;\n\n            CreateResponseHeader(appCompleted);\n        }\n\n        protected Task TryProduceInvalidRequestResponse()\n        {\n            // If _ioCompleted is set, the connection has already been closed.\n            if (_requestRejectedException != null && _ioCompleted == 0)\n            {\n                return ProduceEnd();\n            }\n\n            return Task.CompletedTask;\n        }\n\n        protected Task ProduceEnd()\n        {\n            if (_requestRejectedException != null || _applicationException != null)\n            {\n                if (HasResponseStarted)\n                {\n                    // We can no longer change the response, so we simply close the connection.\n                    _keepAlive = false;\n                    OnErrorAfterResponseStarted();\n                    return Task.CompletedTask;\n                }\n\n                // If the request was rejected, the error state has already been set by SetBadRequestState and\n                // that should take precedence.\n                if (_requestRejectedException != null)\n                {\n                    SetErrorResponseException(_requestRejectedException);\n                }\n                else\n                {\n                    // 500 Internal Server Error\n                    SetErrorResponseHeaders(statusCode: StatusCodes.Status500InternalServerError);\n                }\n            }\n\n            if (!HasResponseStarted)\n            {\n                return ProduceEndAwaited();\n            }\n\n            return WriteSuffix();\n        }\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        private async Task ProduceEndAwaited()\n        {\n            ProduceStart(appCompleted: true);\n\n            // Force flush\n            await Output.FlushAsync(default(CancellationToken));\n\n            await WriteSuffix();\n        }\n\n        private Task WriteSuffix()\n        {\n            // _autoChunk should be checked after we are sure ProduceStart() has been called\n            // since ProduceStart() may set _autoChunk to true.\n            if (_autoChunk || _httpVersion == Http.HttpVersion.Http2)\n            {\n                return WriteSuffixAwaited();\n            }\n\n            if (_keepAlive)\n            {\n                Log.ConnectionKeepAlive(ConnectionId);\n            }\n\n            if (Method == HttpMethod.Head && _responseBytesWritten > 0)\n            {\n                Log.ConnectionHeadResponseBodyWrite(ConnectionId, _responseBytesWritten);\n            }\n\n            return Task.CompletedTask;\n        }\n\n        private async Task WriteSuffixAwaited()\n        {\n            // For the same reason we call CheckLastWrite() in Content-Length responses.\n            _abortedCts = null;\n\n            await Output.WriteStreamSuffixAsync();\n\n            if (_keepAlive)\n            {\n                Log.ConnectionKeepAlive(ConnectionId);\n            }\n\n            if (Method == HttpMethod.Head && _responseBytesWritten > 0)\n            {\n                Log.ConnectionHeadResponseBodyWrite(ConnectionId, _responseBytesWritten);\n            }\n        }\n\n        private void CreateResponseHeader(bool appCompleted)\n        {\n            var responseHeaders = HttpResponseHeaders;\n            var hasConnection = responseHeaders.HasConnection;\n            var connectionOptions = HttpHeaders.ParseConnection(responseHeaders.HeaderConnection);\n            var hasTransferEncoding = responseHeaders.HasTransferEncoding;\n\n            if (_keepAlive && hasConnection && (connectionOptions & ConnectionOptions.KeepAlive) != ConnectionOptions.KeepAlive)\n            {\n                _keepAlive = false;\n            }\n\n            // https://tools.ietf.org/html/rfc7230#section-3.3.1\n            // If any transfer coding other than\n            // chunked is applied to a response payload body, the sender MUST either\n            // apply chunked as the final transfer coding or terminate the message\n            // by closing the connection.\n            if (hasTransferEncoding &&\n                HttpHeaders.GetFinalTransferCoding(responseHeaders.HeaderTransferEncoding) != TransferCoding.Chunked)\n            {\n                _keepAlive = false;\n            }\n\n            // Set whether response can have body\n            _canHaveBody = StatusCanHaveBody(StatusCode) && Method != HttpMethod.Head;\n\n            // Don't set the Content-Length or Transfer-Encoding headers\n            // automatically for HEAD requests or 204, 205, 304 responses.\n            if (_canHaveBody)\n            {\n                if (!hasTransferEncoding && !responseHeaders.ContentLength.HasValue)\n                {\n                    if (appCompleted && StatusCode != StatusCodes.Status101SwitchingProtocols)\n                    {\n                        // Since the app has completed and we are only now generating\n                        // the headers we can safely set the Content-Length to 0.\n                        responseHeaders.ContentLength = 0;\n                    }\n                    else\n                    {\n                        // Note for future reference: never change this to set _autoChunk to true on HTTP/1.0\n                        // connections, even if we were to infer the client supports it because an HTTP/1.0 request\n                        // was received that used chunked encoding. Sending a chunked response to an HTTP/1.0\n                        // client would break compliance with RFC 7230 (section 3.3.1):\n                        //\n                        // A server MUST NOT send a response containing Transfer-Encoding unless the corresponding\n                        // request indicates HTTP/1.1 (or later).\n                        //\n                        // This also covers HTTP/2, which forbids chunked encoding in RFC 7540 (section 8.1:\n                        //\n                        // The chunked transfer encoding defined in Section 4.1 of [RFC7230] MUST NOT be used in HTTP/2.\n                        if (_httpVersion == Http.HttpVersion.Http11 && StatusCode != StatusCodes.Status101SwitchingProtocols)\n                        {\n                            _autoChunk = true;\n                            responseHeaders.SetRawTransferEncoding(\"chunked\", _bytesTransferEncodingChunked);\n                        }\n                        else\n                        {\n                            _keepAlive = false;\n                        }\n                    }\n                }\n            }\n            else if (hasTransferEncoding)\n            {\n                RejectNonBodyTransferEncodingResponse(appCompleted);\n            }\n\n            responseHeaders.SetReadOnly();\n\n            if (!hasConnection && _httpVersion != Http.HttpVersion.Http2)\n            {\n                if (!_keepAlive)\n                {\n                    responseHeaders.SetRawConnection(\"close\", _bytesConnectionClose);\n                }\n                else if (_httpVersion == Http.HttpVersion.Http10)\n                {\n                    responseHeaders.SetRawConnection(\"keep-alive\", _bytesConnectionKeepAlive);\n                }\n            }\n\n            if (ServerOptions.AddServerHeader && !responseHeaders.HasServer)\n            {\n                responseHeaders.SetRawServer(Constants.ServerName, _bytesServer);\n            }\n\n            if (!responseHeaders.HasDate)\n            {\n                var dateHeaderValues = DateHeaderValueManager.GetDateHeaderValues();\n                responseHeaders.SetRawDate(dateHeaderValues.String, dateHeaderValues.Bytes);\n            }\n\n            Output.WriteResponseHeaders(StatusCode, ReasonPhrase, responseHeaders);\n        }\n\n        public bool StatusCanHaveBody(int statusCode)\n        {\n            // List of status codes taken from Microsoft.Net.Http.Server.Response\n            return statusCode != StatusCodes.Status204NoContent &&\n                   statusCode != StatusCodes.Status205ResetContent &&\n                   statusCode != StatusCodes.Status304NotModified;\n        }\n\n        private void ThrowResponseAlreadyStartedException(string value)\n        {\n            throw new InvalidOperationException(CoreStrings.FormatParameterReadOnlyAfterResponseStarted(value));\n        }\n\n        private void RejectNonBodyTransferEncodingResponse(bool appCompleted)\n        {\n            var ex = new InvalidOperationException(CoreStrings.FormatHeaderNotAllowedOnResponse(\"Transfer-Encoding\", StatusCode));\n            if (!appCompleted)\n            {\n                // Back out of header creation surface exeception in user code\n                _requestProcessingStatus = RequestProcessingStatus.AppStarted;\n                throw ex;\n            }\n            else\n            {\n                ReportApplicationError(ex);\n\n                // 500 Internal Server Error\n                SetErrorResponseHeaders(statusCode: StatusCodes.Status500InternalServerError);\n            }\n        }\n\n        private void SetErrorResponseException(BadHttpRequestException ex)\n        {\n            SetErrorResponseHeaders(ex.StatusCode);\n\n            if (!StringValues.IsNullOrEmpty(ex.AllowedHeader))\n            {\n                HttpResponseHeaders.HeaderAllow = ex.AllowedHeader;\n            }\n        }\n\n        private void SetErrorResponseHeaders(int statusCode)\n        {\n            Debug.Assert(!HasResponseStarted, $\"{nameof(SetErrorResponseHeaders)} called after response had already started.\");\n\n            StatusCode = statusCode;\n            ReasonPhrase = null;\n\n            var responseHeaders = HttpResponseHeaders;\n            responseHeaders.Reset();\n            var dateHeaderValues = DateHeaderValueManager.GetDateHeaderValues();\n\n            responseHeaders.SetRawDate(dateHeaderValues.String, dateHeaderValues.Bytes);\n\n            responseHeaders.ContentLength = 0;\n\n            if (ServerOptions.AddServerHeader)\n            {\n                responseHeaders.SetRawServer(Constants.ServerName, _bytesServer);\n            }\n        }\n\n        public void HandleNonBodyResponseWrite()\n        {\n            // Writes to HEAD response are ignored and logged at the end of the request\n            if (Method != HttpMethod.Head)\n            {\n                ThrowWritingToResponseBodyNotSupported();\n            }\n        }\n\n        [StackTraceHidden]\n        private void ThrowWritingToResponseBodyNotSupported()\n        {\n            // Throw Exception for 204, 205, 304 responses.\n            throw new InvalidOperationException(CoreStrings.FormatWritingToResponseBodyNotSupported(StatusCode));\n        }\n\n        [StackTraceHidden]\n        private void ThrowResponseAbortedException()\n        {\n            throw new ObjectDisposedException(CoreStrings.UnhandledApplicationException, _applicationException);\n        }\n\n        [StackTraceHidden]\n        public void ThrowRequestTargetRejected(Span<byte> target)\n            => throw GetInvalidRequestTargetException(target);\n\n        [MethodImpl(MethodImplOptions.NoInlining)]\n        private BadHttpRequestException GetInvalidRequestTargetException(Span<byte> target)\n            => BadHttpRequestException.GetException(\n                RequestRejectionReason.InvalidRequestTarget,\n                Log.IsEnabled(LogLevel.Information)\n                    ? target.GetAsciiStringEscaped(Constants.MaxExceptionDetailSize)\n                    : string.Empty);\n\n        public void SetBadRequestState(BadHttpRequestException ex)\n        {\n            Log.ConnectionBadRequest(ConnectionId, ex);\n\n            if (!HasResponseStarted)\n            {\n                SetErrorResponseException(ex);\n            }\n\n            _keepAlive = false;\n            _requestRejectedException = ex;\n        }\n\n        protected void ReportApplicationError(Exception ex)\n        {\n            if (_applicationException == null)\n            {\n                _applicationException = ex;\n            }\n            else if (_applicationException is AggregateException)\n            {\n                _applicationException = new AggregateException(_applicationException, ex).Flatten();\n            }\n            else\n            {\n                _applicationException = new AggregateException(_applicationException, ex);\n            }\n\n            Log.ApplicationError(ConnectionId, TraceIdentifier, ex);\n        }\n    }\n}\n", "idx": 1, "id": 16354, "msg": "I think this is trying to avoid a secondary exception if the connection has already closed. Don't merge this until @halter73 can take a look.", "proj": "aspnet-KestrelHttpServer", "lang": ".cs"}
{"patch": "@@ -244,7 +244,7 @@ public class NodeTest {\n     Node remote = new RemoteNode(\n         tracer,\n         new PassthroughHttpClient.Factory(local),\n-        UUID.randomUUID(),\n+        new NodeId(UUID.randomUUID()),\n         uri,\n         ImmutableSet.of(caps));\n ", "y": 0, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.selenium.grid.node;\n\nimport com.google.common.collect.ImmutableMap;\nimport com.google.common.collect.ImmutableSet;\nimport org.junit.Before;\nimport org.junit.Test;\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.ImmutableCapabilities;\nimport org.openqa.selenium.NoSuchSessionException;\nimport org.openqa.selenium.SessionNotCreatedException;\nimport org.openqa.selenium.events.EventBus;\nimport org.openqa.selenium.events.local.GuavaEventBus;\nimport org.openqa.selenium.grid.data.CreateSessionRequest;\nimport org.openqa.selenium.grid.data.CreateSessionResponse;\nimport org.openqa.selenium.grid.data.Session;\nimport org.openqa.selenium.grid.node.local.LocalNode;\nimport org.openqa.selenium.grid.node.remote.RemoteNode;\nimport org.openqa.selenium.grid.testing.PassthroughHttpClient;\nimport org.openqa.selenium.grid.testing.TestSessionFactory;\nimport org.openqa.selenium.io.TemporaryFilesystem;\nimport org.openqa.selenium.io.Zip;\nimport org.openqa.selenium.json.Json;\nimport org.openqa.selenium.remote.Dialect;\nimport org.openqa.selenium.remote.SessionId;\nimport org.openqa.selenium.remote.http.HttpClient;\nimport org.openqa.selenium.remote.http.HttpHandler;\nimport org.openqa.selenium.remote.http.HttpRequest;\nimport org.openqa.selenium.remote.http.HttpResponse;\nimport org.openqa.selenium.remote.tracing.DefaultTestTracer;\nimport org.openqa.selenium.remote.tracing.Tracer;\nimport org.openqa.selenium.support.ui.FluentWait;\nimport org.openqa.selenium.support.ui.Wait;\n\nimport java.io.ByteArrayInputStream;\nimport java.io.File;\nimport java.io.IOException;\nimport java.io.UncheckedIOException;\nimport java.net.URI;\nimport java.net.URISyntaxException;\nimport java.nio.charset.StandardCharsets;\nimport java.nio.file.Files;\nimport java.time.Clock;\nimport java.time.Duration;\nimport java.time.Instant;\nimport java.time.ZoneId;\nimport java.util.Collections;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.UUID;\nimport java.util.concurrent.atomic.AtomicBoolean;\nimport java.util.concurrent.atomic.AtomicReference;\n\nimport static java.time.Duration.ofSeconds;\nimport static org.assertj.core.api.Assertions.assertThat;\nimport static org.assertj.core.api.Assertions.assertThatExceptionOfType;\nimport static org.assertj.core.api.InstanceOfAssertFactories.LIST;\nimport static org.assertj.core.api.InstanceOfAssertFactories.MAP;\nimport static org.openqa.selenium.grid.data.SessionClosedEvent.SESSION_CLOSED;\nimport static org.openqa.selenium.json.Json.MAP_TYPE;\nimport static org.openqa.selenium.remote.http.Contents.string;\nimport static org.openqa.selenium.remote.http.HttpMethod.GET;\nimport static org.openqa.selenium.remote.http.HttpMethod.POST;\n\npublic class NodeTest {\n\n  private Tracer tracer;\n  private EventBus bus;\n  private LocalNode local;\n  private Node node;\n  private ImmutableCapabilities caps;\n  private URI uri;\n\n  @Before\n  public void setUp() throws URISyntaxException {\n    tracer = DefaultTestTracer.createTracer();\n    bus = new GuavaEventBus();\n\n    caps = new ImmutableCapabilities(\"browserName\", \"cheese\");\n\n    uri = new URI(\"http://localhost:1234\");\n\n    class Handler extends Session implements HttpHandler {\n\n      private Handler(Capabilities capabilities) {\n        super(new SessionId(UUID.randomUUID()), uri, capabilities);\n      }\n\n      @Override\n      public HttpResponse execute(HttpRequest req) throws UncheckedIOException {\n        return new HttpResponse();\n      }\n    }\n\n    local = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> new Handler(c)))\n        .add(caps, new TestSessionFactory((id, c) -> new Handler(c)))\n        .add(caps, new TestSessionFactory((id, c) -> new Handler(c)))\n        .maximumConcurrentSessions(2)\n        .build();\n\n    node = new RemoteNode(\n        tracer,\n        new PassthroughHttpClient.Factory(local),\n        UUID.randomUUID(),\n        uri,\n        ImmutableSet.of(caps));\n  }\n\n  @Test\n  public void shouldRefuseToCreateASessionIfNoFactoriesAttached() {\n    Node local = LocalNode.builder(tracer, bus, uri, uri, null).build();\n    HttpClient.Factory clientFactory = new PassthroughHttpClient.Factory(local);\n    Node node = new RemoteNode(tracer, clientFactory, UUID.randomUUID(), uri, ImmutableSet.of());\n\n    Optional<Session> session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n\n    assertThat(session).isNotPresent();\n  }\n\n  @Test\n  public void shouldCreateASessionIfTheCorrectCapabilitiesArePassedToIt() {\n    Optional<Session> session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n\n    assertThat(session).isPresent();\n  }\n\n  @Test\n  public void shouldOnlyCreateAsManySessionsAsFactories() {\n    Node node = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> new Session(id, uri, c)))\n        .build();\n\n    Optional<Session> session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isPresent();\n\n    session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isNotPresent();\n  }\n\n  @Test\n  public void willRefuseToCreateMoreSessionsThanTheMaxSessionCount() {\n    Optional<Session> session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isPresent();\n\n    session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isPresent();\n\n    session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isNotPresent();\n  }\n\n  @Test\n  public void stoppingASessionReducesTheNumberOfCurrentlyActiveSessions() {\n    assertThat(local.getCurrentSessionCount()).isEqualTo(0);\n\n    Session session = local.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new RuntimeException(\"Session not created\"));\n\n    assertThat(local.getCurrentSessionCount()).isEqualTo(1);\n\n    local.stop(session.getId());\n\n    assertThat(local.getCurrentSessionCount()).isEqualTo(0);\n  }\n\n  @Test\n  public void sessionsThatAreStoppedWillNotBeReturned() {\n    Session expected = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new RuntimeException(\"Session not created\"));\n\n    node.stop(expected.getId());\n\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> local.getSession(expected.getId()));\n\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> node.getSession(expected.getId()));\n  }\n\n  @Test\n  public void stoppingASessionThatDoesNotExistWillThrowAnException() {\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> local.stop(new SessionId(UUID.randomUUID())));\n\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> node.stop(new SessionId(UUID.randomUUID())));\n  }\n\n  @Test\n  public void attemptingToGetASessionThatDoesNotExistWillCauseAnExceptionToBeThrown() {\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> local.getSession(new SessionId(UUID.randomUUID())));\n\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> node.getSession(new SessionId(UUID.randomUUID())));\n  }\n\n  @Test\n  public void willRespondToWebDriverCommandsSentToOwnedSessions() {\n    AtomicBoolean called = new AtomicBoolean(false);\n\n    class Recording extends Session implements HttpHandler {\n\n      private Recording() {\n        super(new SessionId(UUID.randomUUID()), uri, caps);\n      }\n\n      @Override\n      public HttpResponse execute(HttpRequest req) throws UncheckedIOException {\n        called.set(true);\n        return new HttpResponse();\n      }\n    }\n\n    Node local = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> new Recording()))\n        .build();\n    Node remote = new RemoteNode(\n        tracer,\n        new PassthroughHttpClient.Factory(local),\n        UUID.randomUUID(),\n        uri,\n        ImmutableSet.of(caps));\n\n    Session session = remote.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new RuntimeException(\"Session not created\"));\n\n    HttpRequest req = new HttpRequest(POST, String.format(\"/session/%s/url\", session.getId()));\n    remote.execute(req);\n\n    assertThat(called.get()).isTrue();\n  }\n\n  @Test\n  public void shouldOnlyRespondToWebDriverCommandsForSessionsTheNodeOwns() {\n    Session session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new RuntimeException(\"Session not created\"));\n\n    HttpRequest req = new HttpRequest(POST, String.format(\"/session/%s/url\", session.getId()));\n    assertThat(local.matches(req)).isTrue();\n    assertThat(node.matches(req)).isTrue();\n\n    req = new HttpRequest(POST, String.format(\"/session/%s/url\", UUID.randomUUID()));\n    assertThat(local.matches(req)).isFalse();\n    assertThat(node.matches(req)).isFalse();\n  }\n\n  @Test\n  public void aSessionThatTimesOutWillBeStoppedAndRemovedFromTheSessionMap() {\n    AtomicReference<Instant> now = new AtomicReference<>(Instant.now());\n\n    Clock clock = new MyClock(now);\n    Node node = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> new Session(id, uri, c)))\n        .sessionTimeout(Duration.ofMinutes(3))\n        .advanced()\n        .clock(clock)\n        .build();\n    Session session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new RuntimeException(\"Session not created\"));\n\n    now.set(now.get().plus(Duration.ofMinutes(5)));\n\n    assertThatExceptionOfType(NoSuchSessionException.class)\n        .isThrownBy(() -> node.getSession(session.getId()));\n  }\n\n  @Test\n  public void shouldNotPropagateExceptionsWhenSessionCreationFails() {\n    Node local = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> {\n          throw new SessionNotCreatedException(\"eeek\");\n        }))\n        .build();\n\n    Optional<Session> session = local.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n\n    assertThat(session).isNotPresent();\n  }\n\n  @Test\n  public void eachSessionShouldReportTheNodesUrl() throws URISyntaxException {\n    URI sessionUri = new URI(\"http://cheese:42/peas\");\n    Node node = LocalNode.builder(tracer, bus, uri, uri, null)\n        .add(caps, new TestSessionFactory((id, c) -> new Session(id, sessionUri, c)))\n        .build();\n    Optional<Session> session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession);\n    assertThat(session).isPresent();\n    assertThat(session.get().getUri()).isEqualTo(uri);\n  }\n\n  @Test\n  public void quittingASessionShouldCauseASessionClosedEventToBeFired() {\n    AtomicReference<Object> obj = new AtomicReference<>();\n    bus.addListener(SESSION_CLOSED, event -> obj.set(event.getData(Object.class)));\n\n    Session session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new AssertionError(\"Cannot create session\"));\n    node.stop(session.getId());\n\n    // Because we're using the event bus, we can't expect the event to fire instantly. We're using\n    // an inproc bus, so in reality it's reasonable to expect the event to fire synchronously, but\n    // let's play it safe.\n    Wait<AtomicReference<Object>> wait = new FluentWait<>(obj).withTimeout(ofSeconds(2));\n    wait.until(ref -> ref.get() != null);\n  }\n\n  @Test\n  public void canReturnStatus() {\n    node.newSession(createSessionRequest(caps))\n      .map(CreateSessionResponse::getSession)\n      .orElseThrow(() -> new AssertionError(\"Cannot create session\"));\n\n    HttpRequest req = new HttpRequest(GET, \"/status\");\n    HttpResponse res = node.execute(req);\n    assertThat(res.getStatus()).isEqualTo(200);\n    Map<String, Object> status = new Json().toType(string(res), MAP_TYPE);\n    assertThat(status).containsOnlyKeys(\"value\");\n    assertThat(status).extracting(\"value\").asInstanceOf(MAP)\n        .containsEntry(\"ready\", true)\n        .containsEntry(\"message\", \"Ready\")\n        .containsKey(\"node\");\n    assertThat(status).extracting(\"value.node\").asInstanceOf(MAP)\n        .containsKey(\"id\")\n        .containsEntry(\"uri\", \"http://localhost:1234\")\n        .containsEntry(\"maxSessions\", (long) 2)\n        .containsKey(\"stereotypes\")\n        .containsKey(\"sessions\");\n    assertThat(status).extracting(\"value.node.stereotypes\").asInstanceOf(LIST)\n        .hasSize(1)\n        .element(0).asInstanceOf(MAP)\n        .containsEntry(\"capabilities\", Collections.singletonMap(\"browserName\", \"cheese\"))\n        .containsEntry(\"count\", (long) 3);\n    assertThat(status).extracting(\"value.node.sessions\").asInstanceOf(LIST)\n        .hasSize(1)\n        .element(0).asInstanceOf(MAP)\n        .containsEntry(\"currentCapabilities\", Collections.singletonMap(\"browserName\", \"cheese\"))\n        .containsEntry(\"stereotype\", Collections.singletonMap(\"browserName\", \"cheese\"))\n        .containsKey(\"sessionId\");\n  }\n\n  @Test\n  public void returns404ForAnUnknownCommand() {\n    HttpRequest req = new HttpRequest(GET, \"/foo\");\n    HttpResponse res = node.execute(req);\n    assertThat(res.getStatus()).isEqualTo(404);\n    Map<String, Object> content = new Json().toType(string(res), MAP_TYPE);\n    assertThat(content).containsOnlyKeys(\"value\")\n        .extracting(\"value\").asInstanceOf(MAP)\n        .containsEntry(\"error\", \"unknown command\")\n        .containsEntry(\"message\", \"Unable to find handler for (GET) /foo\");\n  }\n\n  @Test\n  public void canUploadAFile() throws IOException {\n    Session session = node.newSession(createSessionRequest(caps))\n        .map(CreateSessionResponse::getSession)\n        .orElseThrow(() -> new AssertionError(\"Cannot create session\"));\n\n    HttpRequest req = new HttpRequest(POST, String.format(\"/session/%s/file\", session.getId()));\n    String hello = \"Hello, world!\";\n    String zip = Zip.zip(createTmpFile(hello));\n    String payload = new Json().toJson(Collections.singletonMap(\"file\", zip));\n    req.setContent(() -> new ByteArrayInputStream(payload.getBytes()));\n    node.execute(req);\n\n    File baseDir = getTemporaryFilesystemBaseDir(local.getTemporaryFilesystem(session.getId()));\n    assertThat(baseDir.listFiles()).hasSize(1);\n    File uploadDir = baseDir.listFiles()[0];\n    assertThat(uploadDir.listFiles()).hasSize(1);\n    assertThat(new String(Files.readAllBytes(uploadDir.listFiles()[0].toPath()))).isEqualTo(hello);\n\n    node.stop(session.getId());\n    assertThat(baseDir).doesNotExist();\n  }\n\n  private File createTmpFile(String content) {\n    try {\n      File f = File.createTempFile(\"webdriver\", \"tmp\");\n      f.deleteOnExit();\n      Files.write(f.toPath(), content.getBytes(StandardCharsets.UTF_8));\n      return f;\n    } catch (IOException e) {\n      throw new UncheckedIOException(e);\n    }\n  }\n\n  private File getTemporaryFilesystemBaseDir(TemporaryFilesystem tempFS) {\n    File tmp = tempFS.createTempDir(\"tmp\", \"\");\n    File baseDir = tmp.getParentFile();\n    tempFS.deleteTempDir(tmp);\n    return baseDir;\n  }\n\n  private CreateSessionRequest createSessionRequest(Capabilities caps) {\n    return new CreateSessionRequest(\n            ImmutableSet.copyOf(Dialect.values()),\n            caps,\n            ImmutableMap.of());\n  }\n\n  private static class MyClock extends Clock {\n\n    private final AtomicReference<Instant> now;\n\n    public MyClock(AtomicReference<Instant> now) {\n      this.now = now;\n    }\n\n    @Override\n    public ZoneId getZone() {\n      return ZoneId.systemDefault();\n    }\n\n    @Override\n    public Clock withZone(ZoneId zone) {\n      return this;\n    }\n\n    @Override\n    public Instant instant() {\n      return now.get();\n    }\n  }\n}\n", "idx": 4, "id": 18121, "msg": "", "proj": "SeleniumHQ-selenium", "lang": "java"}
{"patch": "@@ -615,5 +615,50 @@ module Bolt\n     rescue Bolt::Error => e\n       Bolt::PlanResult.new(e, 'failure')\n     end\n+\n+    def lookup(key, targets, inventory, executor, _concurrency)\n+      # Install the puppet-agent package and collect facts. Facts are\n+      # automatically added to the targets.\n+      in_plan_compiler(executor, inventory, nil) do |compiler|\n+        compiler.call_function('apply_prep', targets)\n+      end\n+\n+      overrides = {\n+        bolt_inventory: inventory,\n+        bolt_project:   @project\n+      }\n+\n+      # Do a lookup with a catalog compiler, which uses the 'hierarchy' key in\n+      # Hiera config.\n+      results = targets.map do |target|\n+        node = Puppet::Node.from_data_hash(\n+          'name'       => target.name,\n+          'parameters' => { 'clientcert' => target.name }\n+        )\n+\n+        trusted = Puppet::Context::TrustedInformation.local(node).to_h\n+\n+        env_conf = {\n+          modulepath: @modulepath.full_modulepath,\n+          facts:      target.facts,\n+          variables:  target.vars\n+        }\n+\n+        with_puppet_settings do\n+          Puppet::Pal.in_tmp_environment(target.name, **env_conf) do |pal|\n+            Puppet.override(overrides) do\n+              Puppet.lookup(:pal_current_node).trusted_data = trusted\n+              pal.with_catalog_compiler do |compiler|\n+                Bolt::Result.for_lookup(target, key, compiler.call_function('lookup', key))\n+              rescue StandardError => e\n+                Bolt::Result.from_exception(target, e)\n+              end\n+            end\n+          end\n+        end\n+      end\n+\n+      Bolt::ResultSet.new(results)\n+    end\n   end\n end", "y": 1, "oldf": "# frozen_string_literal: true\n\nrequire 'bolt/applicator'\nrequire 'bolt/executor'\nrequire 'bolt/error'\nrequire 'bolt/plan_result'\nrequire 'bolt/util'\nrequire 'bolt/config/modulepath'\nrequire 'etc'\n\nmodule Bolt\n  class PAL\n    # PALError is used to convert errors from executing puppet code into\n    # Bolt::Errors\n    class PALError < Bolt::Error\n      def self.from_preformatted_error(err)\n        error = if err.cause.is_a? Bolt::Error\n                  err.cause\n                else\n                  from_error(err)\n                end\n\n        # Provide the location of an error if it came from a plan\n        details = {}\n        details[:file]   = err.file if defined?(err.file)\n        details[:line]   = err.line if defined?(err.line)\n        details[:column] = err.pos if defined?(err.pos)\n\n        error.add_filelineno(details.compact)\n        error\n      end\n\n      # Generate a Bolt::Pal::PALError for non-bolt errors\n      def self.from_error(err)\n        # Use the original error message if available\n        message = err.cause ? err.cause.message : err.message\n        e = new(message)\n        e.set_backtrace(err.backtrace)\n        e\n      end\n\n      def initialize(msg, details = {})\n        super(msg, 'bolt/pal-error', details)\n      end\n    end\n\n    def initialize(modulepath, hiera_config, resource_types, max_compiles = Etc.nprocessors,\n                   trusted_external = nil, apply_settings = {}, project = nil)\n      unless modulepath.is_a?(Bolt::Config::Modulepath)\n        msg = \"Type error in PAL: modulepath must be a Bolt::Config::Modulepath\"\n        raise Bolt::Error.new(msg, \"bolt/execution-error\")\n      end\n      # Nothing works without initialized this global state. Reinitializing\n      # is safe and in practice only happens in tests\n      self.class.load_puppet\n      @modulepath = modulepath\n      @hiera_config = hiera_config\n      @trusted_external = trusted_external\n      @apply_settings = apply_settings\n      @max_compiles = max_compiles\n      @resource_types = resource_types\n      @project = project\n\n      @logger = Bolt::Logger.logger(self)\n      unless user_modulepath.empty?\n        @logger.debug(\"Loading modules from #{full_modulepath.join(File::PATH_SEPARATOR)}\")\n      end\n\n      @loaded = false\n    end\n\n    def full_modulepath\n      @modulepath.full_modulepath\n    end\n\n    def user_modulepath\n      @modulepath.user_modulepath\n    end\n\n    # Puppet logging is global so this is class method to avoid confusion\n    def self.configure_logging\n      Puppet::Util::Log.destinations.clear\n      Puppet::Util::Log.newdestination(Bolt::Logger.logger('Puppet'))\n      # Defer all log level decisions to the Logging library by telling Puppet\n      # to log everything\n      Puppet.settings[:log_level] = 'debug'\n    end\n\n    def self.load_puppet\n      if Bolt::Util.windows?\n        # Windows 'fix' for openssl behaving strangely. Prevents very slow operation\n        # of random_bytes later when establishing winrm connections from a Windows host.\n        # See https://github.com/rails/rails/issues/25805 for background.\n        require 'openssl'\n        OpenSSL::Random.random_bytes(1)\n      end\n\n      begin\n        require 'puppet_pal'\n      rescue LoadError\n        raise Bolt::Error.new(\"Puppet must be installed to execute tasks\", \"bolt/puppet-missing\")\n      end\n\n      require 'bolt/pal/logging'\n      require 'bolt/pal/issues'\n      require 'bolt/pal/yaml_plan/loader'\n      require 'bolt/pal/yaml_plan/transpiler'\n\n      # Now that puppet is loaded we can include puppet mixins in data types\n      Bolt::ResultSet.include_iterable\n    end\n\n    def setup\n      unless @loaded\n        # This is slow so don't do it until we have to\n        Bolt::PAL.load_puppet\n\n        # Make sure we don't create the puppet directories\n        with_puppet_settings { |_| nil }\n        @loaded = true\n      end\n    end\n\n    # Create a top-level alias for TargetSpec and PlanResult so that users don't have to\n    # namespace it with Boltlib, which is just an implementation detail. This\n    # allows them to feel like a built-in type in bolt, rather than\n    # something has been, no pun intended, \"bolted on\".\n    def alias_types(compiler)\n      compiler.evaluate_string('type TargetSpec = Boltlib::TargetSpec')\n      compiler.evaluate_string('type PlanResult = Boltlib::PlanResult')\n    end\n\n    # Register all resource types defined in $Project/.resource_types as well as\n    # the built in types registered with the runtime_3_init method.\n    def register_resource_types(loaders)\n      static_loader = loaders.static_loader\n      static_loader.runtime_3_init\n      if File.directory?(@resource_types)\n        Dir.children(@resource_types).each do |resource_pp|\n          type_name_from_file = File.basename(resource_pp, '.pp').capitalize\n          typed_name = Puppet::Pops::Loader::TypedName.new(:type, type_name_from_file)\n          resource_type = Puppet::Pops::Types::TypeFactory.resource(type_name_from_file)\n          loaders.static_loader.set_entry(typed_name, resource_type)\n        end\n      end\n    end\n\n    def detect_project_conflict(project, environment)\n      return unless project && project.load_as_module?\n      # The environment modulepath has stripped out non-existent directories,\n      # so we don't need to check for them\n      modules = environment.modulepath.flat_map do |path|\n        Dir.children(path).select { |name| Puppet::Module.is_module_directory?(name, path) }\n      end\n      if modules.include?(project.name)\n        Bolt::Logger.warn_once(\n          \"project_shadows_module\",\n          \"The project '#{project.name}' shadows an existing module of the same name\"\n        )\n      end\n    end\n\n    # Runs a block in a PAL script compiler configured for Bolt.  Catches\n    # exceptions thrown by the block and re-raises them ensuring they are\n    # Bolt::Errors since the script compiler block will squash all exceptions.\n    def in_bolt_compiler\n      # TODO: If we always call this inside a bolt_executor we can remove this here\n      setup\n      r = Puppet::Pal.in_tmp_environment('bolt', modulepath: full_modulepath, facts: {}) do |pal|\n        # Only load the project if it a) exists, b) has a name it can be loaded with\n        Puppet.override(bolt_project: @project,\n                        yaml_plan_instantiator: Bolt::PAL::YamlPlan::Loader) do\n          # Because this has the side effect of loading and caching the list\n          # of modules, it must happen *after* we have overridden\n          # bolt_project or the project will be ignored\n          detect_project_conflict(@project, Puppet.lookup(:environments).get('bolt'))\n          pal.with_script_compiler(set_local_facts: false) do |compiler|\n            alias_types(compiler)\n            register_resource_types(Puppet.lookup(:loaders)) if @resource_types\n            begin\n              yield compiler\n            rescue Bolt::Error => e\n              e\n            rescue Puppet::DataBinding::LookupError => e\n              if e.issue_code == :HIERA_UNDEFINED_VARIABLE\n                message = \"Interpolations are not supported in lookups outside of an apply block: #{e.message}\"\n                PALError.new(message)\n              else\n                PALError.from_preformatted_error(e)\n              end\n            rescue Puppet::PreformattedError => e\n              if e.issue_code == :UNKNOWN_VARIABLE &&\n                 %w[facts trusted server_facts settings].include?(e.arguments[:name])\n                message = \"Evaluation Error: Variable '#{e.arguments[:name]}' is not available in the current scope \"\\\n                          \"unless explicitly defined.\"\n                details = { file: e.file, line: e.line, column: e.pos }\n                PALError.new(message, details)\n              else\n                PALError.from_preformatted_error(e)\n              end\n            rescue StandardError => e\n              PALError.from_preformatted_error(e)\n            end\n          end\n        end\n      end\n\n      # Plans may return PuppetError but nothing should be throwing them\n      if r.is_a?(StandardError) && !r.is_a?(Bolt::PuppetError)\n        raise r\n      end\n      r\n    end\n\n    def with_bolt_executor(executor, inventory, pdb_client = nil, applicator = nil, &block)\n      setup\n      opts = {\n        bolt_project: @project,\n        bolt_executor: executor,\n        bolt_inventory: inventory,\n        bolt_pdb_client: pdb_client,\n        apply_executor: applicator || Applicator.new(\n          inventory,\n          executor,\n          full_modulepath,\n          # Skip syncing built-in plugins, since we vendor some Puppet 6\n          # versions of \"core\" types, which are already present on the agent,\n          # but may cause issues on Puppet 5 agents.\n          user_modulepath,\n          @project,\n          pdb_client,\n          @hiera_config,\n          @max_compiles,\n          @apply_settings\n        )\n      }\n      Puppet.override(opts, &block)\n    end\n\n    def in_plan_compiler(executor, inventory, pdb_client, applicator = nil)\n      with_bolt_executor(executor, inventory, pdb_client, applicator) do\n        # TODO: remove this call and see if anything breaks when\n        # settings dirs don't actually exist. Plans shouldn't\n        # actually be using them.\n        with_puppet_settings do\n          in_bolt_compiler do |compiler|\n            yield compiler\n          end\n        end\n      end\n    end\n\n    def in_task_compiler(executor, inventory)\n      with_bolt_executor(executor, inventory) do\n        in_bolt_compiler do |compiler|\n          yield compiler\n        end\n      end\n    end\n\n    # TODO: PUP-8553 should replace this\n    def with_puppet_settings\n      dir = Dir.mktmpdir('bolt')\n\n      cli = []\n      Puppet::Settings::REQUIRED_APP_SETTINGS.each do |setting|\n        cli << \"--#{setting}\" << dir\n      end\n      Puppet.settings.send(:clear_everything_for_tests)\n      Puppet.initialize_settings(cli)\n      Puppet::GettextConfig.create_default_text_domain\n      Puppet[:trusted_external_command] = @trusted_external\n      Puppet.settings[:hiera_config] = @hiera_config\n      self.class.configure_logging\n      yield\n    ensure\n      # Delete the tmpdir if it still exists. This check is needed to\n      # prevent Bolt from erroring if the tmpdir is somehow deleted\n      # before reaching this point.\n      FileUtils.remove_entry_secure(dir) if File.exist?(dir)\n    end\n\n    # Parses a snippet of Puppet manifest code and returns the AST represented\n    # in JSON.\n    def parse_manifest(code, filename)\n      setup\n      Puppet::Pops::Parser::EvaluatingParser.new.parse_string(code, filename)\n    rescue Puppet::Error => e\n      raise Bolt::PAL::PALError, \"Failed to parse manifest: #{e}\"\n    end\n\n    # Filters content by a list of names and glob patterns specified in project\n    # configuration.\n    def filter_content(content, patterns)\n      return content unless content && patterns\n\n      content.select do |name,|\n        patterns.any? { |pattern| File.fnmatch?(pattern, name, File::FNM_EXTGLOB) }\n      end\n    end\n\n    def list_tasks(filter_content: false)\n      in_bolt_compiler do |compiler|\n        tasks = compiler.list_tasks.map(&:name).sort.each_with_object([]) do |task_name, data|\n          task_sig = compiler.task_signature(task_name)\n          unless task_sig.task_hash['metadata']['private']\n            data << [task_name, task_sig.task_hash['metadata']['description']]\n          end\n        end\n\n        filter_content ? filter_content(tasks, @project&.tasks) : tasks\n      end\n    end\n\n    def parse_params(type, object_name, params)\n      in_bolt_compiler do |compiler|\n        case type\n        when 'task'\n          param_spec = compiler.task_signature(object_name)&.task_hash&.dig('parameters')\n        when 'plan'\n          plan = compiler.plan_signature(object_name)\n          param_spec = plan.params_type.elements&.each_with_object({}) { |t, h| h[t.name] = t.value_type } if plan\n        end\n        param_spec ||= {}\n\n        params.each_with_object({}) do |(name, str), acc|\n          type = param_spec[name]\n          begin\n            parsed = JSON.parse(str, quirks_mode: true)\n            # The type may not exist if the module is remote on orch or if a task\n            # defines no parameters. Since we treat no parameters as Any we\n            # should parse everything in this case\n            acc[name] = if type && !type.instance?(parsed)\n                          str\n                        else\n                          parsed\n                        end\n          rescue JSON::ParserError\n            # This value may not be assignable in which case run_* will error\n            acc[name] = str\n          end\n          acc\n        end\n      end\n    end\n\n    def task_signature(task_name)\n      in_bolt_compiler do |compiler|\n        compiler.task_signature(task_name)\n      end\n    end\n\n    def get_task(task_name)\n      task = task_signature(task_name)\n\n      if task.nil?\n        raise Bolt::Error.unknown_task(task_name)\n      end\n\n      Bolt::Task.from_task_signature(task)\n    end\n\n    def list_plans_with_cache(filter_content: false)\n      # Don't filter content yet, so that if users update their plan filters\n      # we don't need to refresh the cache\n      plan_names = list_plans(filter_content: false).map(&:first)\n      plan_cache = if @project\n                     Bolt::Util.read_optional_json_file(@project.plan_cache_file, 'Plan cache file')\n                   else\n                     {}\n                   end\n      updated = false\n\n      plan_list = plan_names.each_with_object([]) do |plan_name, list|\n        info = plan_cache[plan_name] || get_plan_info(plan_name, with_mtime: true)\n\n        # If the plan is a 'local' plan (in the project itself, or the\n        # modules/ directory) then verify it hasn't been updated since we\n        # cached it. If it has been updated, refresh the cache and use the\n        # new data.\n        if info['file'] &&\n           (File.mtime(info.dig('file', 'path')) <=> info.dig('file', 'mtime')) != 0\n          info = get_plan_info(plan_name, with_mtime: true)\n          updated = true\n          plan_cache[plan_name] = info\n        end\n\n        list << [plan_name, info['description']] unless info['private']\n      end\n\n      File.write(@project.plan_cache_file, plan_cache.to_json) if updated\n\n      filter_content ? filter_content(plan_list, @project&.plans) : plan_list\n    end\n\n    def list_plans(filter_content: false)\n      in_bolt_compiler do |compiler|\n        errors = []\n        plans = compiler.list_plans(nil, errors).map { |plan| [plan.name] }.sort\n        errors.each do |error|\n          Bolt::Logger.warn(\"plan_load_error\", error.details['original_error'])\n        end\n\n        filter_content ? filter_content(plans, @project&.plans) : plans\n      end\n    end\n\n    def get_plan_info(plan_name, with_mtime: false)\n      plan_sig = in_bolt_compiler do |compiler|\n        compiler.plan_signature(plan_name)\n      end\n\n      if plan_sig.nil?\n        raise Bolt::Error.unknown_plan(plan_name)\n      end\n\n      # path may be a Pathname object, so make sure to stringify it\n      mod = plan_sig.instance_variable_get(:@plan_func).loader.parent.path.to_s\n\n      # If it's a Puppet language plan, use strings to extract data. The only\n      # way to tell is to check which filename exists in the module.\n      plan_subpath = File.join(plan_name.split('::').drop(1))\n      plan_subpath = 'init' if plan_subpath.empty?\n\n      pp_path = File.join(mod, 'plans', \"#{plan_subpath}.pp\")\n      if File.exist?(pp_path)\n        require 'puppet-strings'\n        require 'puppet-strings/yard'\n        PuppetStrings::Yard.setup!\n        YARD::Logger.instance.level = :error\n        YARD.parse(pp_path)\n\n        plan = YARD::Registry.at(\"puppet_plans::#{plan_name}\")\n\n        description = if plan.tag(:summary)\n                        plan.tag(:summary).text\n                      elsif !plan.docstring.empty?\n                        plan.docstring\n                      end\n\n        defaults = plan.parameters.to_h.compact\n        signature_params = Set.new(plan.parameters.map(&:first))\n        parameters = plan.tags(:param).each_with_object({}) do |param, params|\n          name = param.name\n          if signature_params.include?(name)\n            params[name] = { 'type' => param.types.first }\n            params[name]['sensitive'] = param.types.first =~ /\\ASensitive(\\[.*\\])?\\z/ ? true : false\n            params[name]['default_value'] = defaults[name] if defaults.key?(name)\n            params[name]['description'] = param.text if param.text && !param.text.empty?\n          else\n            Bolt::Logger.warn(\n              \"missing_plan_parameter\",\n              \"The documented parameter '#{name}' does not exist in signature for plan '#{plan.name}'\"\n            )\n          end\n        end\n\n        privie = plan.tag(:private)&.text\n        unless privie.nil? || %w[true false].include?(privie.downcase)\n          msg = \"Plan #{plan_name} key 'private' must be a boolean, received: #{privie}\"\n          raise Bolt::Error.new(msg, 'bolt/invalid-plan')\n        end\n\n        pp_info = {\n          'name'        => plan_name,\n          'description' => description,\n          'parameters'  => parameters,\n          'module'      => mod\n        }\n        pp_info.merge!({ 'private' => privie&.downcase == 'true' }) unless privie.nil?\n        pp_info.merge!(get_plan_mtime(plan.file)) if with_mtime\n        pp_info\n\n      # If it's a YAML plan, fall back to limited data\n      else\n        yaml_path = File.join(mod, 'plans', \"#{plan_subpath}.yaml\")\n        plan_content = File.read(yaml_path)\n        plan = Bolt::PAL::YamlPlan::Loader.from_string(plan_name, plan_content, yaml_path)\n\n        parameters = plan.parameters.each_with_object({}) do |param, params|\n          name = param.name\n          type_str = case param.type_expr\n                     when Puppet::Pops::Types::PTypeReferenceType\n                       param.type_expr.type_string\n                     when nil\n                       'Any'\n                     else\n                       param.type_expr\n                     end\n          params[name] = { 'type' => type_str }\n          params[name]['sensitive'] = param.type_expr.instance_of?(Puppet::Pops::Types::PSensitiveType)\n          params[name]['default_value'] = param.value unless param.value.nil?\n          params[name]['description'] = param.description if param.description\n        end\n\n        yaml_info = {\n          'name'        => plan_name,\n          'description' => plan.description,\n          'parameters'  => parameters,\n          'module'      => mod\n        }\n        yaml_info.merge!({ 'private' => plan.private }) unless plan.private.nil?\n        yaml_info.merge!(get_plan_mtime(yaml_path)) if with_mtime\n        yaml_info\n      end\n    end\n\n    def get_plan_mtime(path)\n      # If the plan is from the project modules/ directory, or is in the\n      # project itself, include the last mtime of the file so we can compare\n      # if the plan has been updated since it was cached.\n      if @project &&\n         File.exist?(path) &&\n         (path.include?(File.join(@project.path, 'modules')) ||\n          path.include?(@project.plans_path.to_s))\n\n        { 'file' => { 'mtime' => File.mtime(path),\n                      'path' => path } }\n      else\n        {}\n      end\n    end\n\n    def convert_plan(plan)\n      path = File.expand_path(plan)\n\n      # If the path doesn't exist, check if it's a plan name\n      unless File.exist?(path)\n        in_bolt_compiler do |compiler|\n          sig = compiler.plan_signature(plan)\n\n          # If the plan was loaded, look for it on the module loader\n          # There has to be an easier way to do this...\n          if sig\n            type = compiler.list_plans.find { |p| p.name == plan }\n            path = sig.instance_variable_get(:@plan_func)\n                      .loader\n                      .find(type)\n                      .origin\n                      .first\n          end\n        end\n      end\n\n      Puppet[:tasks] = true\n      transpiler = YamlPlan::Transpiler.new\n      transpiler.transpile(path)\n    end\n\n    # Returns a mapping of all modules available to the Bolt compiler\n    #\n    # @return [Hash{String => Array<Hash{Symbol => String,nil}>}]\n    #   A hash that associates each directory on the modulepath with an array\n    #   containing a hash of information for each module in that directory.\n    #   The information hash provides the name, version, and a string\n    #   indicating whether the module belongs to an internal module group.\n    def list_modules\n      internal_module_groups = { Bolt::Config::Modulepath::BOLTLIB_PATH => 'Plan Language Modules',\n                                 Bolt::Config::Modulepath::MODULES_PATH => 'Packaged Modules',\n                                 @project.managed_moduledir.to_s => 'Project Dependencies' }\n\n      in_bolt_compiler do\n        # NOTE: Can replace map+to_h with transform_values when Ruby 2.4\n        #       is the minimum supported version.\n        Puppet.lookup(:current_environment).modules_by_path.map do |path, modules|\n          module_group = internal_module_groups[path]\n\n          values = modules.map do |mod|\n            mod_info = { name: (mod.forge_name || mod.name),\n                         version: mod.version }\n            mod_info[:internal_module_group] = module_group unless module_group.nil?\n\n            mod_info\n          end\n\n          [path, values]\n        end.to_h\n      end\n    end\n\n    def generate_types(cache: false)\n      require 'puppet/face/generate'\n      in_bolt_compiler do\n        generator = Puppet::Generate::Type\n        inputs = generator.find_inputs(:pcore)\n        FileUtils.mkdir_p(@resource_types)\n        cache_plan_info if @project && cache\n        generator.generate(inputs, @resource_types, true)\n      end\n    end\n\n    def cache_plan_info\n      # plan_name is an array here\n      plans_info = list_plans(filter_content: false).map do |plan_name,|\n        data = get_plan_info(plan_name, with_mtime: true)\n        { plan_name => data }\n      end.reduce({}, :merge)\n\n      FileUtils.touch(@project.plan_cache_file)\n      File.write(@project.plan_cache_file, plans_info.to_json)\n    end\n\n    def run_task(task_name, targets, params, executor, inventory, description = nil)\n      in_task_compiler(executor, inventory) do |compiler|\n        params = params.merge('_bolt_api_call' => true, '_catch_errors' => true)\n        compiler.call_function('run_task', task_name, targets, description, params)\n      end\n    end\n\n    def run_plan(plan_name, params, executor = nil, inventory = nil, pdb_client = nil, applicator = nil)\n      in_plan_compiler(executor, inventory, pdb_client, applicator) do |compiler|\n        r = compiler.call_function('run_plan', plan_name, params.merge('_bolt_api_call' => true))\n        Bolt::PlanResult.from_pcore(r, 'success')\n      end\n    rescue Bolt::Error => e\n      Bolt::PlanResult.new(e, 'failure')\n    end\n  end\nend\n", "idx": 1, "id": 18281, "msg": "This seems fine since the use case we're solving for is looking up a value on a limited set of targets for debugging, but we may want to parallelize this if people are using it to lookup lots of values.", "proj": "puppetlabs-bolt", "lang": "rb"}
{"patch": "@@ -80,11 +80,13 @@ class FileProvider extends BaseProvider implements FileProviderInterface\n \n     public function getReferenceImage(MediaInterface $media): string\n     {\n-        return sprintf(\n-            '%s/%s',\n-            $this->generatePath($media),\n-            $media->getProviderReference()\n-        );\n+        $providerReference = $media->getProviderReference();\n+\n+        if (null === $providerReference) {\n+            return '';\n+        }\n+\n+        return sprintf('%s/%s', $this->generatePath($media), $providerReference);\n     }\n \n     public function getReferenceFile(MediaInterface $media): GaufretteFile", "y": 1, "oldf": "<?php\n\ndeclare(strict_types=1);\n\n/*\n * This file is part of the Sonata Project package.\n *\n * (c) Thomas Rabaix <thomas.rabaix@sonata-project.org>\n *\n * For the full copyright and license information, please view the LICENSE\n * file that was distributed with this source code.\n */\n\nnamespace Sonata\\MediaBundle\\Provider;\n\nuse Gaufrette\\File as GaufretteFile;\nuse Gaufrette\\Filesystem;\nuse Sonata\\AdminBundle\\Form\\FormMapper;\nuse Sonata\\Form\\Validator\\ErrorElement;\nuse Sonata\\MediaBundle\\CDN\\CDNInterface;\nuse Sonata\\MediaBundle\\Extra\\ApiMediaFile;\nuse Sonata\\MediaBundle\\Filesystem\\Local;\nuse Sonata\\MediaBundle\\Generator\\GeneratorInterface;\nuse Sonata\\MediaBundle\\Metadata\\MetadataBuilderInterface;\nuse Sonata\\MediaBundle\\Model\\MediaInterface;\nuse Sonata\\MediaBundle\\Thumbnail\\ThumbnailInterface;\nuse Symfony\\Component\\Form\\Extension\\Core\\Type\\FileType;\nuse Symfony\\Component\\Form\\FormBuilderInterface;\nuse Symfony\\Component\\HttpFoundation\\BinaryFileResponse;\nuse Symfony\\Component\\HttpFoundation\\File\\Exception\\UploadException;\nuse Symfony\\Component\\HttpFoundation\\File\\File;\nuse Symfony\\Component\\HttpFoundation\\File\\UploadedFile;\nuse Symfony\\Component\\HttpFoundation\\Request;\nuse Symfony\\Component\\HttpFoundation\\Response;\nuse Symfony\\Component\\HttpFoundation\\StreamedResponse;\nuse Symfony\\Component\\Mime\\MimeTypes;\nuse Symfony\\Component\\Validator\\Constraints\\NotBlank;\nuse Symfony\\Component\\Validator\\Constraints\\NotNull;\n\nclass FileProvider extends BaseProvider implements FileProviderInterface\n{\n    /**\n     * @var string[]\n     */\n    protected $allowedExtensions;\n\n    /**\n     * @var string[]\n     */\n    protected $allowedMimeTypes;\n\n    /**\n     * @var MetadataBuilderInterface|null\n     */\n    protected $metadata;\n\n    /**\n     * @param string[] $allowedExtensions\n     * @param string[] $allowedMimeTypes\n     */\n    public function __construct(string $name, Filesystem $filesystem, CDNInterface $cdn, GeneratorInterface $pathGenerator, ThumbnailInterface $thumbnail, array $allowedExtensions = [], array $allowedMimeTypes = [], ?MetadataBuilderInterface $metadata = null)\n    {\n        parent::__construct($name, $filesystem, $cdn, $pathGenerator, $thumbnail);\n\n        $this->allowedExtensions = $allowedExtensions;\n        $this->allowedMimeTypes = $allowedMimeTypes;\n        $this->metadata = $metadata;\n    }\n\n    public function getProviderMetadata(): MetadataInterface\n    {\n        return new Metadata(\n            $this->getName(),\n            $this->getName().'.description',\n            null,\n            'SonataMediaBundle',\n            ['class' => 'fa fa-file-text-o']\n        );\n    }\n\n    public function getReferenceImage(MediaInterface $media): string\n    {\n        return sprintf(\n            '%s/%s',\n            $this->generatePath($media),\n            $media->getProviderReference()\n        );\n    }\n\n    public function getReferenceFile(MediaInterface $media): GaufretteFile\n    {\n        return $this->getFilesystem()->get($this->getReferenceImage($media), true);\n    }\n\n    public function getAllowedExtensions(): array\n    {\n        return $this->allowedExtensions;\n    }\n\n    public function getAllowedMimeTypes(): array\n    {\n        return $this->allowedMimeTypes;\n    }\n\n    public function buildEditForm(FormMapper $form): void\n    {\n        $form->add('name');\n        $form->add('enabled', null, ['required' => false]);\n        $form->add('authorName');\n        $form->add('cdnIsFlushable');\n        $form->add('description');\n        $form->add('copyright');\n        $form->add('binaryContent', FileType::class, ['required' => false]);\n    }\n\n    public function buildCreateForm(FormMapper $form): void\n    {\n        $form->add('binaryContent', FileType::class, [\n            'constraints' => [\n                new NotBlank(),\n                new NotNull(),\n            ],\n        ]);\n    }\n\n    public function buildMediaType(FormBuilderInterface $formBuilder): void\n    {\n        if ('api' === $formBuilder->getOption('context')) {\n            $formBuilder->add('binaryContent', FileType::class);\n            $formBuilder->add('contentType');\n        } else {\n            $formBuilder->add('binaryContent', FileType::class, [\n                'required' => false,\n                'label' => 'widget_label_binary_content',\n            ]);\n        }\n    }\n\n    public function postPersist(MediaInterface $media): void\n    {\n        if (null === $media->getBinaryContent()) {\n            return;\n        }\n\n        $this->setFileContents($media);\n\n        $this->generateThumbnails($media);\n\n        $media->resetBinaryContent();\n    }\n\n    public function postUpdate(MediaInterface $media): void\n    {\n        if (!$media->getBinaryContent() instanceof \\SplFileInfo) {\n            return;\n        }\n\n        // Delete the current file from the FS\n        $oldMedia = clone $media;\n        // if no previous reference is provided, it prevents\n        // Filesystem from trying to remove a directory\n        if (null !== $media->getPreviousProviderReference()) {\n            $oldMedia->setProviderReference($media->getPreviousProviderReference());\n\n            $path = $this->getReferenceImage($oldMedia);\n\n            if ($this->getFilesystem()->has($path)) {\n                $this->getFilesystem()->delete($path);\n            }\n        }\n\n        $this->fixBinaryContent($media);\n\n        $this->setFileContents($media);\n\n        $this->generateThumbnails($media);\n\n        $media->resetBinaryContent();\n    }\n\n    public function updateMetadata(MediaInterface $media, bool $force = true): void\n    {\n        if (!$media->getBinaryContent() instanceof \\SplFileInfo) {\n            // this is now optimized at all!!!\n            $path = tempnam(sys_get_temp_dir(), 'sonata_update_metadata_');\n\n            if (false === $path) {\n                throw new \\RuntimeException(sprintf('Unable to generate temporary file name for media %s.', $media->getId()));\n            }\n\n            $fileObject = new \\SplFileObject($path, 'w');\n            $fileObject->fwrite($this->getReferenceFile($media)->getContent());\n        } else {\n            $fileObject = $media->getBinaryContent();\n        }\n\n        $media->setSize($fileObject->getSize());\n    }\n\n    public function generatePublicUrl(MediaInterface $media, string $format): string\n    {\n        if (MediaProviderInterface::FORMAT_REFERENCE === $format) {\n            $path = $this->getReferenceImage($media);\n        } else {\n            // @todo: fix the asset path\n            $path = sprintf('sonatamedia/files/%s/file.png', $format);\n        }\n\n        return $this->getCdn()->getPath($path, $media->getCdnIsFlushable());\n    }\n\n    public function getHelperProperties(MediaInterface $media, string $format, array $options = []): array\n    {\n        return array_merge([\n            'title' => $media->getName(),\n            'thumbnail' => $this->getReferenceImage($media),\n            'file' => $this->getReferenceImage($media),\n        ], $options);\n    }\n\n    public function generatePrivateUrl(MediaInterface $media, string $format): string\n    {\n        if (MediaProviderInterface::FORMAT_REFERENCE === $format) {\n            return $this->getReferenceImage($media);\n        }\n\n        return '';\n    }\n\n    public function getDownloadResponse(MediaInterface $media, string $format, string $mode, array $headers = []): Response\n    {\n        // build the default headers\n        $headers = array_merge([\n            'Content-Type' => $media->getContentType(),\n            'Content-Disposition' => sprintf('attachment; filename=\"%s\"', $media->getMetadataValue('filename')),\n        ], $headers);\n\n        if (!\\in_array($mode, ['http', 'X-Sendfile', 'X-Accel-Redirect'], true)) {\n            throw new \\RuntimeException('Invalid mode provided');\n        }\n\n        if ('http' === $mode) {\n            if (MediaProviderInterface::FORMAT_REFERENCE === $format) {\n                $file = $this->getReferenceFile($media);\n            } else {\n                $file = $this->getFilesystem()->get($this->generatePrivateUrl($media, $format));\n            }\n\n            return new StreamedResponse(static function () use ($file): void {\n                echo $file->getContent();\n            }, 200, $headers);\n        }\n\n        $adapter = $this->getFilesystem()->getAdapter();\n\n        if (!$adapter instanceof Local) {\n            throw new \\RuntimeException(sprintf('Cannot use X-Sendfile or X-Accel-Redirect with non %s.', Local::class));\n        }\n\n        $filename = sprintf(\n            '%s/%s',\n            $adapter->getDirectory(),\n            $this->generatePrivateUrl($media, $format)\n        );\n\n        return new BinaryFileResponse($filename, 200, $headers);\n    }\n\n    public function validate(ErrorElement $errorElement, MediaInterface $media): void\n    {\n        if (!$media->getBinaryContent() instanceof \\SplFileInfo) {\n            return;\n        }\n\n        if ($media->getBinaryContent() instanceof UploadedFile) {\n            $fileName = $media->getBinaryContent()->getClientOriginalName();\n        } elseif ($media->getBinaryContent() instanceof File) {\n            $fileName = $media->getBinaryContent()->getFilename();\n        } else {\n            throw new \\RuntimeException(sprintf('Invalid binary content type: %s', \\get_class($media->getBinaryContent())));\n        }\n\n        if ($media->getBinaryContent() instanceof UploadedFile && 0 === ($media->getBinaryContent()->getSize() ?? 0)) {\n            $errorElement\n                ->with('binaryContent')\n                    ->addViolation(\n                        'The file is too big, max size: %maxFileSize%',\n                        ['%maxFileSize%' => ini_get('upload_max_filesize')]\n                    )\n                ->end();\n        }\n\n        if (!\\in_array(strtolower(pathinfo($fileName, \\PATHINFO_EXTENSION)), $this->allowedExtensions, true)) {\n            $errorElement\n                ->with('binaryContent')\n                ->addViolation('Invalid extensions')\n                ->end();\n        }\n\n        if ('' !== $media->getBinaryContent()->getFilename() && !\\in_array($media->getBinaryContent()->getMimeType(), $this->allowedMimeTypes, true)) {\n            $errorElement\n                ->with('binaryContent')\n                    ->addViolation('Invalid mime type : %type%', ['%type%' => $media->getBinaryContent()->getMimeType()])\n                ->end();\n        }\n    }\n\n    protected function fixBinaryContent(MediaInterface $media): void\n    {\n        if (null === $media->getBinaryContent() || $media->getBinaryContent() instanceof File) {\n            return;\n        }\n\n        if ($media->getBinaryContent() instanceof Request) {\n            $this->generateBinaryFromRequest($media);\n            $this->updateMetadata($media);\n\n            return;\n        }\n\n        // if the binary content is a filename => convert to a valid File\n        if (!is_file($media->getBinaryContent())) {\n            throw new \\RuntimeException('The file does not exist : '.$media->getBinaryContent());\n        }\n\n        $binaryContent = new File($media->getBinaryContent());\n        $media->setBinaryContent($binaryContent);\n    }\n\n    /**\n     * @throws \\RuntimeException\n     */\n    protected function fixFilename(MediaInterface $media): void\n    {\n        if ($media->getBinaryContent() instanceof UploadedFile) {\n            $media->setName($media->getName() ?? $media->getBinaryContent()->getClientOriginalName());\n            $media->setMetadataValue('filename', $media->getBinaryContent()->getClientOriginalName());\n        } elseif ($media->getBinaryContent() instanceof File) {\n            $media->setName($media->getName() ?? $media->getBinaryContent()->getBasename());\n            $media->setMetadataValue('filename', $media->getBinaryContent()->getBasename());\n        }\n\n        // This is the original name\n        if (null === $media->getName()) {\n            throw new \\RuntimeException('Please define a valid media\\'s name');\n        }\n    }\n\n    protected function doTransform(MediaInterface $media): void\n    {\n        $this->fixBinaryContent($media);\n        $this->fixFilename($media);\n\n        if ($media->getBinaryContent() instanceof UploadedFile && 0 === $media->getBinaryContent()->getSize()) {\n            $media->setProviderReference(uniqid($media->getName() ?? '', true));\n            $media->setProviderStatus(MediaInterface::STATUS_ERROR);\n\n            throw new UploadException('The uploaded file is not found');\n        }\n\n        // this is the name used to store the file\n        if (null === $media->getProviderReference() ||\n            MediaInterface::MISSING_BINARY_REFERENCE === $media->getProviderReference()\n        ) {\n            $media->setProviderReference($this->generateReferenceName($media));\n        }\n\n        if ($media->getBinaryContent() instanceof File) {\n            $media->setContentType($media->getBinaryContent()->getMimeType());\n            $media->setSize($media->getBinaryContent()->getSize());\n        }\n\n        $media->setProviderStatus(MediaInterface::STATUS_OK);\n    }\n\n    /**\n     * Set the file contents for an image.\n     */\n    protected function setFileContents(MediaInterface $media, ?string $contents = null): void\n    {\n        $file = $this->getFilesystem()->get(sprintf('%s/%s', $this->generatePath($media), $media->getProviderReference()), true);\n        $metadata = null !== $this->metadata ? $this->metadata->get($media, $file->getName()) : [];\n\n        if (null !== $contents) {\n            $file->setContent($contents, $metadata);\n\n            return;\n        }\n\n        $binaryContent = $media->getBinaryContent();\n        if ($binaryContent instanceof File) {\n            $path = false !== $binaryContent->getRealPath() ? $binaryContent->getRealPath() : $binaryContent->getPathname();\n            $fileContents = file_get_contents($path);\n\n            if (false === $fileContents) {\n                throw new \\RuntimeException(sprintf('Unable to get file contents for media %s', $media->getId()));\n            }\n\n            $file->setContent($fileContents, $metadata);\n\n            return;\n        }\n    }\n\n    protected function generateReferenceName(MediaInterface $media): string\n    {\n        return $this->generateMediaUniqId($media).'.'.$media->getBinaryContent()->guessExtension();\n    }\n\n    protected function generateMediaUniqId(MediaInterface $media): string\n    {\n        return sha1($media->getName().uniqid().random_int(11111, 99999));\n    }\n\n    /**\n     * Set media binary content according to request content.\n     */\n    protected function generateBinaryFromRequest(MediaInterface $media): void\n    {\n        $contentType = $media->getContentType();\n\n        if (null === $contentType) {\n            throw new \\RuntimeException(\n                'You must provide the content type value for your media before setting the binary content'\n            );\n        }\n\n        $request = $media->getBinaryContent();\n\n        if (!$request instanceof Request) {\n            throw new \\RuntimeException('Expected Request in binary content');\n        }\n\n        $content = $request->getContent();\n\n        // Create unique id for media reference\n        $guesser = MimeTypes::getDefault();\n        $extensions = $guesser->getExtensions($contentType);\n        $extension = $extensions[0] ?? null;\n\n        if (null === $extension) {\n            throw new \\RuntimeException(\n                sprintf('Unable to guess extension for content type %s', $media->getContentType())\n            );\n        }\n\n        $handle = tmpfile();\n\n        if (false === $handle) {\n            throw new \\RuntimeException('Unable to generate temporary file.');\n        }\n\n        fwrite($handle, $content);\n        $file = new ApiMediaFile($handle);\n        $file->setExtension($extension);\n        $file->setMimetype($contentType);\n\n        $media->setBinaryContent($file);\n    }\n}\n", "idx": 1, "id": 12407, "msg": "Same thing about reference to videos here.", "proj": "sonata-project-SonataMediaBundle", "lang": "php"}
{"patch": "@@ -97,12 +97,12 @@ class BaseTableScan implements TableScan {\n   }\n \n   @Override\n-  public TableScan useSnapshot(long snapshotId) {\n+  public TableScan useSnapshot(long scanSnapshotId) {\n     Preconditions.checkArgument(this.snapshotId == null,\n-        \"Cannot override snapshot, already set to id=%s\", snapshotId);\n-    Preconditions.checkArgument(ops.current().snapshot(snapshotId) != null,\n-        \"Cannot find snapshot with ID %s\", snapshotId);\n-    return new BaseTableScan(ops, table, snapshotId, schema, rowFilter, caseSensitive, selectedColumns);\n+        \"Cannot override snapshot, already set to id=%s\", scanSnapshotId);\n+    Preconditions.checkArgument(ops.current().snapshot(scanSnapshotId) != null,\n+        \"Cannot find snapshot with ID %s\", scanSnapshotId);\n+    return new BaseTableScan(ops, table, scanSnapshotId, schema, rowFilter, caseSensitive, selectedColumns);\n   }\n \n   @Override", "y": 0, "oldf": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *   http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing,\n * software distributed under the License is distributed on an\n * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n * KIND, either express or implied.  See the License for the\n * specific language governing permissions and limitations\n * under the License.\n */\n\npackage org.apache.iceberg;\n\nimport com.google.common.base.Objects;\nimport com.google.common.base.Preconditions;\nimport com.google.common.cache.CacheBuilder;\nimport com.google.common.cache.CacheLoader;\nimport com.google.common.cache.LoadingCache;\nimport com.google.common.collect.FluentIterable;\nimport com.google.common.collect.ImmutableList;\nimport com.google.common.collect.Iterables;\nimport com.google.common.collect.Sets;\nimport java.text.SimpleDateFormat;\nimport java.util.Collection;\nimport java.util.Collections;\nimport java.util.Date;\nimport java.util.List;\nimport java.util.Set;\nimport java.util.function.Function;\nimport org.apache.iceberg.TableMetadata.SnapshotLogEntry;\nimport org.apache.iceberg.events.Listeners;\nimport org.apache.iceberg.events.ScanEvent;\nimport org.apache.iceberg.expressions.Binder;\nimport org.apache.iceberg.expressions.Expression;\nimport org.apache.iceberg.expressions.Expressions;\nimport org.apache.iceberg.expressions.InclusiveManifestEvaluator;\nimport org.apache.iceberg.expressions.Projections;\nimport org.apache.iceberg.expressions.ResidualEvaluator;\nimport org.apache.iceberg.io.CloseableIterable;\nimport org.apache.iceberg.types.TypeUtil;\nimport org.apache.iceberg.util.BinPacking;\nimport org.apache.iceberg.util.ParallelIterable;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport static org.apache.iceberg.util.ThreadPools.getWorkerPool;\n\n/**\n * Base class for {@link TableScan} implementations.\n */\nclass BaseTableScan implements TableScan {\n  private static final Logger LOG = LoggerFactory.getLogger(TableScan.class);\n\n  private static final SimpleDateFormat DATE_FORMAT = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss.SSS\");\n  private static final List<String> SNAPSHOT_COLUMNS = ImmutableList.of(\n      \"snapshot_id\", \"file_path\", \"file_ordinal\", \"file_format\", \"block_size_in_bytes\",\n      \"file_size_in_bytes\", \"record_count\", \"partition\", \"value_counts\", \"null_value_counts\",\n      \"lower_bounds\", \"upper_bounds\"\n  );\n  private static final boolean PLAN_SCANS_WITH_WORKER_POOL =\n      SystemProperties.getBoolean(SystemProperties.SCAN_THREAD_POOL_ENABLED, true);\n\n  private final TableOperations ops;\n  private final Table table;\n  private final Long snapshotId;\n  private final Schema schema;\n  private final Expression rowFilter;\n  private final boolean caseSensitive;\n  private final Collection<String> selectedColumns;\n\n  BaseTableScan(TableOperations ops, Table table) {\n    this(ops, table, null, table.schema(), Expressions.alwaysTrue(), true, null);\n  }\n\n  private BaseTableScan(TableOperations ops, Table table, Long snapshotId, Schema schema,\n                        Expression rowFilter, boolean caseSensitive, Collection<String> selectedColumns) {\n    this.ops = ops;\n    this.table = table;\n    this.snapshotId = snapshotId;\n    this.schema = schema;\n    this.rowFilter = rowFilter;\n    this.caseSensitive = caseSensitive;\n    this.selectedColumns = selectedColumns;\n  }\n\n  @Override\n  public Table table() {\n    return table;\n  }\n\n  @Override\n  public TableScan useSnapshot(long snapshotId) {\n    Preconditions.checkArgument(this.snapshotId == null,\n        \"Cannot override snapshot, already set to id=%s\", snapshotId);\n    Preconditions.checkArgument(ops.current().snapshot(snapshotId) != null,\n        \"Cannot find snapshot with ID %s\", snapshotId);\n    return new BaseTableScan(ops, table, snapshotId, schema, rowFilter, caseSensitive, selectedColumns);\n  }\n\n  @Override\n  public TableScan asOfTime(long timestampMillis) {\n    Preconditions.checkArgument(this.snapshotId == null,\n        \"Cannot override snapshot, already set to id=%s\", snapshotId);\n\n    Long lastSnapshotId = null;\n    for (SnapshotLogEntry logEntry : ops.current().snapshotLog()) {\n      if (logEntry.timestampMillis() <= timestampMillis) {\n        lastSnapshotId = logEntry.snapshotId();\n      }\n    }\n\n    // the snapshot ID could be null if no entries were older than the requested time. in that case,\n    // there is no valid snapshot to read.\n    Preconditions.checkArgument(lastSnapshotId != null,\n        \"Cannot find a snapshot older than %s\", DATE_FORMAT.format(new Date(timestampMillis)));\n\n    return useSnapshot(lastSnapshotId);\n  }\n\n  @Override\n  public TableScan project(Schema schema) {\n    return new BaseTableScan(ops, table, snapshotId, schema, rowFilter, caseSensitive, selectedColumns);\n  }\n\n  @Override\n  public TableScan caseSensitive(boolean caseSensitive) {\n    return new BaseTableScan(ops, table, snapshotId, schema, rowFilter, caseSensitive, selectedColumns);\n  }\n\n  @Override\n  public TableScan select(Collection<String> columns) {\n    return new BaseTableScan(ops, table, snapshotId, schema, rowFilter, caseSensitive, columns);\n  }\n\n  @Override\n  public TableScan filter(Expression expr) {\n    return new BaseTableScan(ops, table, snapshotId, schema, Expressions.and(rowFilter, expr),\n                             caseSensitive, selectedColumns);\n  }\n\n  private final LoadingCache<Integer, InclusiveManifestEvaluator> evalCache = CacheBuilder\n      .newBuilder()\n      .build(new CacheLoader<Integer, InclusiveManifestEvaluator>() {\n        @Override\n        public InclusiveManifestEvaluator load(Integer specId) {\n          PartitionSpec spec = ops.current().spec(specId);\n          return new InclusiveManifestEvaluator(spec, rowFilter, caseSensitive);\n        }\n      });\n\n  @Override\n  public CloseableIterable<FileScanTask> planFiles() {\n    Snapshot snapshot = snapshotId != null ?\n        ops.current().snapshot(snapshotId) :\n        ops.current().currentSnapshot();\n\n    if (snapshot != null) {\n      LOG.info(\"Scanning table {} snapshot {} created at {} with filter {}\", table,\n          snapshot.snapshotId(), DATE_FORMAT.format(new Date(snapshot.timestampMillis())),\n          rowFilter);\n\n      Listeners.notifyAll(\n          new ScanEvent(table.toString(), snapshot.snapshotId(), rowFilter, schema()));\n\n      Iterable<ManifestFile> matchingManifests = Iterables.filter(snapshot.manifests(),\n          manifest -> evalCache.getUnchecked(manifest.partitionSpecId()).eval(manifest));\n\n      Iterable<CloseableIterable<FileScanTask>> readers = Iterables.transform(\n          matchingManifests,\n          manifest -> {\n              ManifestReader reader = ManifestReader\n                  .read(ops.io().newInputFile(manifest.path()), ops.current()::spec)\n                  .caseSensitive(caseSensitive);\n            PartitionSpec spec = ops.current().spec(manifest.partitionSpecId());\n            String schemaString = SchemaParser.toJson(spec.schema());\n            String specString = PartitionSpecParser.toJson(spec);\n            ResidualEvaluator residuals = new ResidualEvaluator(spec, rowFilter, caseSensitive);\n            return CloseableIterable.transform(\n                reader.filterRows(rowFilter).select(SNAPSHOT_COLUMNS),\n                file -> new BaseFileScanTask(file, schemaString, specString, residuals)\n            );\n          });\n\n      if (PLAN_SCANS_WITH_WORKER_POOL && snapshot.manifests().size() > 1) {\n        return new ParallelIterable<>(readers, getWorkerPool());\n      } else {\n        return CloseableIterable.concat(readers);\n      }\n\n    } else {\n      LOG.info(\"Scanning empty table {}\", table);\n      return CloseableIterable.empty();\n    }\n  }\n\n  @Override\n  public CloseableIterable<CombinedScanTask> planTasks() {\n    long splitSize = ops.current().propertyAsLong(\n        TableProperties.SPLIT_SIZE, TableProperties.SPLIT_SIZE_DEFAULT);\n    int lookback = ops.current().propertyAsInt(\n        TableProperties.SPLIT_LOOKBACK, TableProperties.SPLIT_LOOKBACK_DEFAULT);\n    long openFileCost = ops.current().propertyAsLong(\n      TableProperties.SPLIT_OPEN_FILE_COST, TableProperties.SPLIT_OPEN_FILE_COST_DEFAULT);\n\n    Function<FileScanTask, Long> weightFunc = file -> Math.max(file.length(), openFileCost);\n\n    CloseableIterable<FileScanTask> splitFiles = splitFiles(splitSize);\n    return CloseableIterable.transform(\n        CloseableIterable.combine(\n            new BinPacking.PackingIterable<>(splitFiles, splitSize, lookback, weightFunc, true),\n            splitFiles),\n        BaseCombinedScanTask::new);\n  }\n\n  @Override\n  public Schema schema() {\n    return lazyColumnProjection();\n  }\n\n  @Override\n  public Expression filter() {\n    return rowFilter;\n  }\n\n  @Override\n  public boolean isCaseSensitive() {\n    return caseSensitive;\n  }\n\n  @Override\n  public String toString() {\n    return Objects.toStringHelper(this)\n        .add(\"table\", table)\n        .add(\"projection\", schema().asStruct())\n        .add(\"filter\", rowFilter)\n        .add(\"caseSensitive\", caseSensitive)\n        .toString();\n  }\n\n  private CloseableIterable<FileScanTask> splitFiles(long splitSize) {\n    CloseableIterable<FileScanTask> fileScanTasks = planFiles();\n    Iterable<FileScanTask> splitTasks = FluentIterable\n        .from(fileScanTasks)\n        .transformAndConcat(input -> input.split(splitSize));\n    // Capture manifests which can be closed after scan planning\n    return CloseableIterable.combine(splitTasks, fileScanTasks);\n  }\n\n  /**\n   * To be able to make refinements {@link #select(Collection)} and {@link #caseSensitive(boolean)} in any order,\n   * we resolve the schema to be projected lazily here.\n   *\n   * @return the Schema to project\n   */\n  private Schema lazyColumnProjection() {\n    if (selectedColumns != null ) {\n      Set<Integer> requiredFieldIds = Sets.newHashSet();\n\n      // all of the filter columns are required\n      requiredFieldIds.addAll(\n          Binder.boundReferences(table.schema().asStruct(), Collections.singletonList(rowFilter), caseSensitive));\n\n      // all of the projection columns are required\n      Set<Integer> selectedIds;\n      if (caseSensitive) {\n        selectedIds = TypeUtil.getProjectedIds(table.schema().select(selectedColumns));\n      } else {\n        selectedIds = TypeUtil.getProjectedIds(table.schema().caseInsensitiveSelect(selectedColumns));\n      }\n      requiredFieldIds.addAll(selectedIds);\n\n      return TypeUtil.select(table.schema(), requiredFieldIds);\n    }\n\n    return schema;\n  }\n}\n", "idx": 5, "id": 13199, "msg": "", "proj": "apache-iceberg", "lang": "java"}
{"patch": "@@ -291,7 +291,9 @@ func getPageUsageByNUMA(cgroupPath string) (cgroups.PageUsageByNUMA, error) {\n \tstats := cgroups.PageUsageByNUMA{}\n \n \tfile, err := os.Open(path.Join(cgroupPath, cgroupMemoryPagesByNuma))\n-\tif err != nil {\n+\tif os.IsNotExist(err) {\n+\t\treturn stats, nil\n+\t} else if err != nil {\n \t\treturn stats, err\n \t}\n ", "y": 1, "oldf": "// +build linux\n\npackage fs\n\nimport (\n\t\"bufio\"\n\t\"fmt\"\n\t\"math\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"strconv\"\n\t\"strings\"\n\n\t\"github.com/opencontainers/runc/libcontainer/cgroups\"\n\t\"github.com/opencontainers/runc/libcontainer/cgroups/fscommon\"\n\t\"github.com/opencontainers/runc/libcontainer/configs\"\n)\n\nconst (\n\tnumaNodeSymbol            = \"N\"\n\tnumaStatColumnSeparator   = \" \"\n\tnumaStatKeyValueSeparator = \"=\"\n\tnumaStatMaxColumns        = math.MaxUint8 + 1\n\tnumaStatValueIndex        = 1\n\tnumaStatTypeIndex         = 0\n\tnumaStatColumnSliceLength = 2\n\tcgroupMemorySwapLimit     = \"memory.memsw.limit_in_bytes\"\n\tcgroupMemoryLimit         = \"memory.limit_in_bytes\"\n\tcgroupMemoryPagesByNuma   = \"memory.numa_stat\"\n)\n\ntype MemoryGroup struct {\n}\n\nfunc (s *MemoryGroup) Name() string {\n\treturn \"memory\"\n}\n\nfunc (s *MemoryGroup) Apply(d *cgroupData) (err error) {\n\tpath, err := d.path(\"memory\")\n\tif err != nil && !cgroups.IsNotFound(err) {\n\t\treturn err\n\t} else if path == \"\" {\n\t\treturn nil\n\t}\n\tif memoryAssigned(d.config) {\n\t\tif _, err := os.Stat(path); os.IsNotExist(err) {\n\t\t\tif err := os.MkdirAll(path, 0755); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\t// Only enable kernel memory accouting when this cgroup\n\t\t\t// is created by libcontainer, otherwise we might get\n\t\t\t// error when people use `cgroupsPath` to join an existed\n\t\t\t// cgroup whose kernel memory is not initialized.\n\t\t\tif err := EnableKernelMemoryAccounting(path); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\tdefer func() {\n\t\tif err != nil {\n\t\t\tos.RemoveAll(path)\n\t\t}\n\t}()\n\n\t// We need to join memory cgroup after set memory limits, because\n\t// kmem.limit_in_bytes can only be set when the cgroup is empty.\n\t_, err = d.join(\"memory\")\n\tif err != nil && !cgroups.IsNotFound(err) {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc setMemoryAndSwap(path string, cgroup *configs.Cgroup) error {\n\t// If the memory update is set to -1 and the swap is not explicitly\n\t// set, we should also set swap to -1, it means unlimited memory.\n\tif cgroup.Resources.Memory == -1 && cgroup.Resources.MemorySwap == 0 {\n\t\t// Only set swap if it's enabled in kernel\n\t\tif cgroups.PathExists(filepath.Join(path, cgroupMemorySwapLimit)) {\n\t\t\tcgroup.Resources.MemorySwap = -1\n\t\t}\n\t}\n\n\t// When memory and swap memory are both set, we need to handle the cases\n\t// for updating container.\n\tif cgroup.Resources.Memory != 0 && cgroup.Resources.MemorySwap != 0 {\n\t\tmemoryUsage, err := getMemoryData(path, \"\")\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// When update memory limit, we should adapt the write sequence\n\t\t// for memory and swap memory, so it won't fail because the new\n\t\t// value and the old value don't fit kernel's validation.\n\t\tif cgroup.Resources.MemorySwap == -1 || memoryUsage.Limit < uint64(cgroup.Resources.MemorySwap) {\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemorySwapLimit, strconv.FormatInt(cgroup.Resources.MemorySwap, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemoryLimit, strconv.FormatInt(cgroup.Resources.Memory, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t} else {\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemoryLimit, strconv.FormatInt(cgroup.Resources.Memory, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemorySwapLimit, strconv.FormatInt(cgroup.Resources.MemorySwap, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif cgroup.Resources.Memory != 0 {\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemoryLimit, strconv.FormatInt(cgroup.Resources.Memory, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\tif cgroup.Resources.MemorySwap != 0 {\n\t\t\tif err := fscommon.WriteFile(path, cgroupMemorySwapLimit, strconv.FormatInt(cgroup.Resources.MemorySwap, 10)); err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *MemoryGroup) Set(path string, cgroup *configs.Cgroup) error {\n\tif err := setMemoryAndSwap(path, cgroup); err != nil {\n\t\treturn err\n\t}\n\n\tif cgroup.Resources.KernelMemory != 0 {\n\t\tif err := setKernelMemory(path, cgroup.Resources.KernelMemory); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif cgroup.Resources.MemoryReservation != 0 {\n\t\tif err := fscommon.WriteFile(path, \"memory.soft_limit_in_bytes\", strconv.FormatInt(cgroup.Resources.MemoryReservation, 10)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tif cgroup.Resources.KernelMemoryTCP != 0 {\n\t\tif err := fscommon.WriteFile(path, \"memory.kmem.tcp.limit_in_bytes\", strconv.FormatInt(cgroup.Resources.KernelMemoryTCP, 10)); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif cgroup.Resources.OomKillDisable {\n\t\tif err := fscommon.WriteFile(path, \"memory.oom_control\", \"1\"); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\tif cgroup.Resources.MemorySwappiness == nil || int64(*cgroup.Resources.MemorySwappiness) == -1 {\n\t\treturn nil\n\t} else if *cgroup.Resources.MemorySwappiness <= 100 {\n\t\tif err := fscommon.WriteFile(path, \"memory.swappiness\", strconv.FormatUint(*cgroup.Resources.MemorySwappiness, 10)); err != nil {\n\t\t\treturn err\n\t\t}\n\t} else {\n\t\treturn fmt.Errorf(\"invalid value:%d. valid memory swappiness range is 0-100\", *cgroup.Resources.MemorySwappiness)\n\t}\n\n\treturn nil\n}\n\nfunc (s *MemoryGroup) Remove(d *cgroupData) error {\n\treturn removePath(d.path(\"memory\"))\n}\n\nfunc (s *MemoryGroup) GetStats(path string, stats *cgroups.Stats) error {\n\t// Set stats from memory.stat.\n\tstatsFile, err := os.Open(filepath.Join(path, \"memory.stat\"))\n\tif err != nil {\n\t\tif os.IsNotExist(err) {\n\t\t\treturn nil\n\t\t}\n\t\treturn err\n\t}\n\tdefer statsFile.Close()\n\n\tsc := bufio.NewScanner(statsFile)\n\tfor sc.Scan() {\n\t\tt, v, err := fscommon.GetCgroupParamKeyValue(sc.Text())\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to parse memory.stat (%q) - %v\", sc.Text(), err)\n\t\t}\n\t\tstats.MemoryStats.Stats[t] = v\n\t}\n\tstats.MemoryStats.Cache = stats.MemoryStats.Stats[\"cache\"]\n\n\tmemoryUsage, err := getMemoryData(path, \"\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.MemoryStats.Usage = memoryUsage\n\tswapUsage, err := getMemoryData(path, \"memsw\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.MemoryStats.SwapUsage = swapUsage\n\tkernelUsage, err := getMemoryData(path, \"kmem\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.MemoryStats.KernelUsage = kernelUsage\n\tkernelTCPUsage, err := getMemoryData(path, \"kmem.tcp\")\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.MemoryStats.KernelTCPUsage = kernelTCPUsage\n\n\tuseHierarchy := strings.Join([]string{\"memory\", \"use_hierarchy\"}, \".\")\n\tvalue, err := fscommon.GetCgroupParamUint(path, useHierarchy)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif value == 1 {\n\t\tstats.MemoryStats.UseHierarchy = true\n\t}\n\n\tpagesByNUMA, err := getPageUsageByNUMA(path)\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats.MemoryStats.PageUsageByNUMA = pagesByNUMA\n\n\treturn nil\n}\n\nfunc memoryAssigned(cgroup *configs.Cgroup) bool {\n\treturn cgroup.Resources.Memory != 0 ||\n\t\tcgroup.Resources.MemoryReservation != 0 ||\n\t\tcgroup.Resources.MemorySwap > 0 ||\n\t\tcgroup.Resources.KernelMemory > 0 ||\n\t\tcgroup.Resources.KernelMemoryTCP > 0 ||\n\t\tcgroup.Resources.OomKillDisable ||\n\t\t(cgroup.Resources.MemorySwappiness != nil && int64(*cgroup.Resources.MemorySwappiness) != -1)\n}\n\nfunc getMemoryData(path, name string) (cgroups.MemoryData, error) {\n\tmemoryData := cgroups.MemoryData{}\n\n\tmoduleName := \"memory\"\n\tif name != \"\" {\n\t\tmoduleName = strings.Join([]string{\"memory\", name}, \".\")\n\t}\n\tusage := strings.Join([]string{moduleName, \"usage_in_bytes\"}, \".\")\n\tmaxUsage := strings.Join([]string{moduleName, \"max_usage_in_bytes\"}, \".\")\n\tfailcnt := strings.Join([]string{moduleName, \"failcnt\"}, \".\")\n\tlimit := strings.Join([]string{moduleName, \"limit_in_bytes\"}, \".\")\n\n\tvalue, err := fscommon.GetCgroupParamUint(path, usage)\n\tif err != nil {\n\t\tif moduleName != \"memory\" && os.IsNotExist(err) {\n\t\t\treturn cgroups.MemoryData{}, nil\n\t\t}\n\t\treturn cgroups.MemoryData{}, fmt.Errorf(\"failed to parse %s - %v\", usage, err)\n\t}\n\tmemoryData.Usage = value\n\tvalue, err = fscommon.GetCgroupParamUint(path, maxUsage)\n\tif err != nil {\n\t\tif moduleName != \"memory\" && os.IsNotExist(err) {\n\t\t\treturn cgroups.MemoryData{}, nil\n\t\t}\n\t\treturn cgroups.MemoryData{}, fmt.Errorf(\"failed to parse %s - %v\", maxUsage, err)\n\t}\n\tmemoryData.MaxUsage = value\n\tvalue, err = fscommon.GetCgroupParamUint(path, failcnt)\n\tif err != nil {\n\t\tif moduleName != \"memory\" && os.IsNotExist(err) {\n\t\t\treturn cgroups.MemoryData{}, nil\n\t\t}\n\t\treturn cgroups.MemoryData{}, fmt.Errorf(\"failed to parse %s - %v\", failcnt, err)\n\t}\n\tmemoryData.Failcnt = value\n\tvalue, err = fscommon.GetCgroupParamUint(path, limit)\n\tif err != nil {\n\t\tif moduleName != \"memory\" && os.IsNotExist(err) {\n\t\t\treturn cgroups.MemoryData{}, nil\n\t\t}\n\t\treturn cgroups.MemoryData{}, fmt.Errorf(\"failed to parse %s - %v\", limit, err)\n\t}\n\tmemoryData.Limit = value\n\n\treturn memoryData, nil\n}\n\nfunc getPageUsageByNUMA(cgroupPath string) (cgroups.PageUsageByNUMA, error) {\n\tstats := cgroups.PageUsageByNUMA{}\n\n\tfile, err := os.Open(path.Join(cgroupPath, cgroupMemoryPagesByNuma))\n\tif err != nil {\n\t\treturn stats, err\n\t}\n\n\tscanner := bufio.NewScanner(file)\n\tfor scanner.Scan() {\n\t\tvar statsType string\n\t\tstatsByType := cgroups.PageStats{Nodes: map[uint8]uint64{}}\n\t\tcolumns := strings.SplitN(scanner.Text(), numaStatColumnSeparator, numaStatMaxColumns)\n\n\t\tfor _, column := range columns {\n\t\t\tpagesByNode := strings.SplitN(column, numaStatKeyValueSeparator, numaStatColumnSliceLength)\n\n\t\t\tif strings.HasPrefix(pagesByNode[numaStatTypeIndex], numaNodeSymbol) {\n\t\t\t\tnodeID, err := strconv.ParseUint(pagesByNode[numaStatTypeIndex][1:], 10, 8)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn cgroups.PageUsageByNUMA{}, err\n\t\t\t\t}\n\n\t\t\t\tstatsByType.Nodes[uint8(nodeID)], err = strconv.ParseUint(pagesByNode[numaStatValueIndex], 0, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn cgroups.PageUsageByNUMA{}, err\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tstatsByType.Total, err = strconv.ParseUint(pagesByNode[numaStatValueIndex], 0, 64)\n\t\t\t\tif err != nil {\n\t\t\t\t\treturn cgroups.PageUsageByNUMA{}, err\n\t\t\t\t}\n\n\t\t\t\tstatsType = pagesByNode[numaStatTypeIndex]\n\t\t\t}\n\n\t\t\terr := addNUMAStatsByType(&stats, statsByType, statsType)\n\t\t\tif err != nil {\n\t\t\t\treturn cgroups.PageUsageByNUMA{}, err\n\t\t\t}\n\t\t}\n\t}\n\terr = scanner.Err()\n\tif err != nil {\n\t\treturn cgroups.PageUsageByNUMA{}, err\n\t}\n\n\treturn stats, nil\n}\n\nfunc addNUMAStatsByType(stats *cgroups.PageUsageByNUMA, byTypeStats cgroups.PageStats, statsType string) error {\n\tswitch statsType {\n\tcase \"total\":\n\t\tstats.Total = byTypeStats\n\tcase \"file\":\n\t\tstats.File = byTypeStats\n\tcase \"anon\":\n\t\tstats.Anon = byTypeStats\n\tcase \"unevictable\":\n\t\tstats.Unevictable = byTypeStats\n\tcase \"hierarchical_total\":\n\t\tstats.Hierarchical.Total = byTypeStats\n\tcase \"hierarchical_file\":\n\t\tstats.Hierarchical.File = byTypeStats\n\tcase \"hierarchical_anon\":\n\t\tstats.Hierarchical.Anon = byTypeStats\n\tcase \"hierarchical_unevictable\":\n\t\tstats.Hierarchical.Unevictable = byTypeStats\n\tdefault:\n\t\treturn fmt.Errorf(\"unsupported NUMA page type found: %s\", statsType)\n\t}\n\treturn nil\n}\n", "idx": 1, "id": 20359, "msg": "`else` is redundant here because there's a `return` above it, but it's fine I guess.", "proj": "opencontainers-runc", "lang": "go"}
{"patch": "@@ -316,7 +316,10 @@ class Dataset(Element):\n         supplied, which will ensure the selection is only applied if the\n         specs match the selected object.\n         \"\"\"\n-        if selection_specs and not any(self.matches(sp) for sp in selection_specs):\n+        selection = {dim: sel for dim, sel in selection.items()\n+                     if dim in self.dimensions()+['selection_mask']}\n+        if (selection_specs and not any(self.matches(sp) for sp in selection_specs)\n+            or not selection):\n             return self\n \n         data = self.interface.select(self, **selection)", "y": 1, "oldf": "from __future__ import absolute_import\nfrom collections import OrderedDict\n\ntry:\n    import itertools.izip as zip\nexcept ImportError:\n    pass\n\nimport numpy as np\nimport param\n\nfrom ..dimension import redim\nfrom .interface import Interface\nfrom .array import ArrayInterface\nfrom .dictionary import DictInterface\nfrom .grid import GridInterface\nfrom .image import ImageInterface\nfrom .ndelement import NdElementInterface\n\ndatatypes = ['array', 'dictionary', 'grid', 'ndelement']\n\ntry:\n    import pandas as pd # noqa (Availability import)\n    from .pandas import PandasInterface\n    datatypes = ['array', 'dataframe', 'dictionary', 'grid', 'ndelement']\n    DFColumns = PandasInterface\nexcept ImportError:\n    pass\nexcept Exception as e:\n    param.main.warning('Pandas interface failed to import with '\n                       'following error: %s' % e)\n\ntry:\n    import iris # noqa (Availability import)\n    from .iris import CubeInterface # noqa (Conditional API import)\n    datatypes.append('cube')\nexcept ImportError:\n    pass\nexcept Exception as e:\n    param.main.warning('Iris interface failed to import with '\n                       'following error: %s' % e)\n\ntry:\n    import xarray # noqa (Availability import)\n    from .xarray import XArrayInterface # noqa (Conditional API import)\n    datatypes.append('xarray')\nexcept ImportError:\n    pass\n\ntry:\n    from .dask import DaskInterface\n    datatypes.append('dask')\nexcept ImportError:\n    pass\n\nfrom ..dimension import Dimension\nfrom ..element import Element\nfrom ..ndmapping import OrderedDict\nfrom ..spaces import HoloMap, DynamicMap\nfrom .. import util\n\n\nclass DataConversion(object):\n    \"\"\"\n    DataConversion is a very simple container object which can be\n    given an existing Dataset Element and provides methods to convert\n    the Dataset into most other Element types.\n    \"\"\"\n\n    def __init__(self, element):\n        self._element = element\n\n    def __call__(self, new_type, kdims=None, vdims=None, groupby=None,\n                 sort=False, **kwargs):\n        \"\"\"\n        Generic conversion method for Dataset based Element\n        types. Supply the Dataset Element type to convert to and\n        optionally the key dimensions (kdims), value dimensions\n        (vdims) and the dimensions.  to group over. Converted Columns\n        can be automatically sorted via the sort option and kwargs can\n        bepassed through.\n        \"\"\"\n        if 'mdims' in kwargs:\n            if groupby:\n                raise ValueError('Cannot supply both mdims and groupby')\n            else:\n                self._element.warning(\"'mdims' keyword has been renamed \"\n                                      \"to 'groupby'; the name mdims is \"\n                                      \"deprecated and will be removed \"\n                                      \"after version 1.7.\")\n                groupby = kwargs.pop('mdims')\n\n        if kdims is None:\n            kd_filter = groupby or []\n            if not isinstance(kd_filter, list):\n                kd_filter = [groupby]\n            kdims = [kd for kd in self._element.kdims if kd not in kd_filter]\n        elif kdims and not isinstance(kdims, list): kdims = [kdims]\n        if vdims is None:\n            vdims = self._element.vdims\n        if vdims and not isinstance(vdims, list): vdims = [vdims]\n\n        # Checks Element type supports dimensionality\n        type_name = new_type.__name__\n        for dim_type, dims in (('kdims', kdims), ('vdims', vdims)):\n            min_d, max_d = new_type.params(dim_type).bounds\n            if ((min_d is not None and len(dims) < min_d) or\n                (max_d is not None and len(dims) > max_d)):\n                raise ValueError(\"%s %s must be between length %s and %s.\" %\n                                 (type_name, dim_type, min_d, max_d))\n\n        if groupby is None:\n            groupby = [d for d in self._element.kdims if d not in kdims+vdims]\n        elif groupby and not isinstance(groupby, list):\n            groupby = [groupby]\n\n        if self._element.interface.gridded:\n            dropped_kdims = [kd for kd in self._element.kdims if kd not in groupby+kdims]\n            if dropped_kdims:\n                selected = self._element.reindex(groupby+kdims, vdims)\n            else:\n                selected = self._element\n        else:\n            selected = self._element.reindex(groupby+kdims, vdims)\n        params = {'kdims': [selected.get_dimension(kd, strict=True) for kd in kdims],\n                  'vdims': [selected.get_dimension(vd, strict=True) for vd in vdims],\n                  'label': selected.label}\n        if selected.group != selected.params()['group'].default:\n            params['group'] = selected.group\n        params.update(kwargs)\n        if len(kdims) == selected.ndims or not groupby:\n            element = new_type(selected, **params)\n            return element.sort() if sort else element\n        group = selected.groupby(groupby, container_type=HoloMap,\n                                 group_type=new_type, **params)\n        if sort:\n            return group.map(lambda x: x.sort(), [new_type])\n        else:\n            return group\n\n\n\nclass Dataset(Element):\n    \"\"\"\n    Dataset provides a general baseclass for Element types that\n    contain structured data and supports a range of data formats.\n\n    The Dataset class supports various methods offering a consistent way\n    of working with the stored data regardless of the storage format\n    used. These operations include indexing, selection and various ways\n    of aggregating or collapsing the data with a supplied function.\n    \"\"\"\n\n    datatype = param.List(datatypes,\n        doc=\"\"\" A priority list of the data types to be used for storage\n        on the .data attribute. If the input supplied to the element\n        constructor cannot be put into the requested format, the next\n        format listed will be used until a suitable format is found (or\n        the data fails to be understood).\"\"\")\n\n    # In the 1D case the interfaces should not automatically add x-values\n    # to supplied data\n    _auto_indexable_1d = True\n\n    # Define a class used to transform Datasets into other Element types\n    _conversion_interface = DataConversion\n\n    _vdim_reductions = {}\n    _kdim_reductions = {}\n\n    def __init__(self, data, **kwargs):\n        if isinstance(data, Element):\n            pvals = util.get_param_values(data)\n            kwargs.update([(l, pvals[l]) for l in ['group', 'label']\n                           if l in pvals and l not in kwargs])\n\n        kdims, vdims = None, None\n        if 'kdims' in kwargs:\n            kdims = [kd if isinstance(kd, Dimension) else Dimension(kd)\n                     for kd in kwargs['kdims']]\n        if 'vdims' in kwargs:\n            vdims = [kd if isinstance(kd, Dimension) else Dimension(kd)\n                     for kd in kwargs['vdims']]\n\n        initialized = Interface.initialize(type(self), data, kdims, vdims,\n                                           datatype=kwargs.get('datatype'))\n        (data, self.interface, dims, extra_kws) = initialized\n        super(Dataset, self).__init__(data, **dict(kwargs, **dict(dims, **extra_kws)))\n        self.interface.validate(self)\n\n        self.redim = redim(self, mode='dataset')\n\n\n    def __setstate__(self, state):\n        \"\"\"\n        Restores OrderedDict based Dataset objects, converting them to\n        the up-to-date NdElement format.\n        \"\"\"\n        self.__dict__ = state\n        if isinstance(self.data, OrderedDict):\n            self.data = Dataset(self.data, kdims=self.kdims,\n                                vdims=self.vdims, group=self.group,\n                                label=self.label)\n            self.interface = NdColumns\n        elif isinstance(self.data, np.ndarray):\n            self.interface = ArrayInterface\n        elif util.is_dataframe(self.data):\n            self.interface = PandasInterface\n\n        super(Dataset, self).__setstate__(state)\n\n\n    def closest(self, coords=[], **kwargs):\n        \"\"\"\n        Given a single coordinate or multiple coordinates as\n        a tuple or list of tuples or keyword arguments matching\n        the dimension closest will find the closest actual x/y\n        coordinates. Different Element types should implement this\n        appropriately depending on the space they represent, if the\n        Element does not support snapping raise NotImplementedError.\n        \"\"\"\n        if self.ndims > 1:\n            raise NotImplementedError(\"Closest method currently only \"\n                                      \"implemented for 1D Elements\")\n\n        if kwargs:\n            dim = self.get_dimension(list(kwargs.keys())[0], strict=True)\n            if len(kwargs) > 1:\n                raise NotImplementedError(\"Closest method currently only \"\n                                          \"supports 1D indexes\")\n            samples = list(kwargs.values())[0]\n            coords = samples if isinstance(samples, list) else [samples]\n\n        xs = self.dimension_values(0)\n        if xs.dtype.kind in 'SO':\n            raise NotImplementedError(\"Closest only supported for numeric types\")\n        idxs = [np.argmin(np.abs(xs-coord)) for coord in coords]\n        return [xs[idx] for idx in idxs]\n\n\n    def sort(self, by=[]):\n        \"\"\"\n        Sorts the data by the values along the supplied dimensions.\n        \"\"\"\n        if not by: by = self.kdims\n        if not isinstance(by, list): by = [by]\n\n        sorted_columns = self.interface.sort(self, by)\n        return self.clone(sorted_columns)\n\n\n    def range(self, dim, data_range=True):\n        \"\"\"\n        Computes the range of values along a supplied dimension, taking\n        into account the range and soft_range defined on the Dimension\n        object.\n        \"\"\"\n        dim = self.get_dimension(dim)\n        if dim is None:\n            return (None, None)\n        elif None not in dim.range:\n            return dim.range\n        elif dim in self.dimensions() and data_range:\n            if len(self):\n                drange = self.interface.range(self, dim)\n            else:\n                drange = (np.NaN, np.NaN)\n            soft_range = [r for r in dim.soft_range if r is not None]\n            if soft_range:\n                drange = util.max_range([drange, soft_range])\n        else:\n            drange = dim.soft_range\n        if dim.range[0] is not None:\n            return (dim.range[0], drange[1])\n        elif dim.range[1] is not None:\n            return (drange[0], dim.range[1])\n        else:\n            return drange\n\n\n\n    def add_dimension(self, dimension, dim_pos, dim_val, vdim=False, **kwargs):\n        \"\"\"\n        Create a new object with an additional key dimensions.  Requires\n        the dimension name or object, the desired position in the key\n        dimensions and a key value scalar or sequence of the same length\n        as the existing keys.\n        \"\"\"\n        if isinstance(dimension, (util.basestring, tuple)):\n            dimension = Dimension(dimension)\n\n        if dimension.name in self.kdims:\n            raise Exception('{dim} dimension already defined'.format(dim=dimension.name))\n\n        if vdim:\n            dims = self.vdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(vdims=dims)\n            dim_pos += self.ndims\n        else:\n            dims = self.kdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(kdims=dims)\n\n        data = self.interface.add_dimension(self, dimension, dim_pos, dim_val, vdim)\n        return self.clone(data, **dimensions)\n\n\n    def select(self, selection_specs=None, **selection):\n        \"\"\"\n        Allows selecting data by the slices, sets and scalar values\n        along a particular dimension. The indices should be supplied as\n        keywords mapping between the selected dimension and\n        value. Additionally selection_specs (taking the form of a list\n        of type.group.label strings, types or functions) may be\n        supplied, which will ensure the selection is only applied if the\n        specs match the selected object.\n        \"\"\"\n        if selection_specs and not any(self.matches(sp) for sp in selection_specs):\n            return self\n\n        data = self.interface.select(self, **selection)\n\n        if np.isscalar(data):\n            return data\n        else:\n            return self.clone(data)\n\n\n    def reindex(self, kdims=None, vdims=None):\n        \"\"\"\n        Create a new object with a re-ordered set of dimensions.  Allows\n        converting key dimensions to value dimensions and vice versa.\n        \"\"\"\n        if kdims is None:\n            key_dims = [d for d in self.kdims if not vdims or d not in vdims]\n        else:\n            key_dims = [self.get_dimension(k, strict=True) for k in kdims]\n\n        new_type = None\n        if vdims is None:\n            val_dims = [d for d in self.vdims if not kdims or d not in kdims]\n        else:\n            val_dims = [self.get_dimension(v, strict=True) for v in vdims]\n            new_type = self._vdim_reductions.get(len(val_dims), type(self))\n\n        data = self.interface.reindex(self, key_dims, val_dims)\n        return self.clone(data, kdims=key_dims, vdims=val_dims,\n                          new_type=new_type)\n\n\n    def __getitem__(self, slices):\n        \"\"\"\n        Allows slicing and selecting values in the Dataset object.\n        Supports multiple indexing modes:\n\n           (1) Slicing and indexing along the values of each dimension\n               in the columns object using either scalars, slices or\n               sets of values.\n           (2) Supplying the name of a dimension as the first argument\n               will return the values along that dimension as a numpy\n               array.\n           (3) Slicing of all key dimensions and selecting a single\n               value dimension by name.\n           (4) A boolean array index matching the length of the Dataset\n               object.\n        \"\"\"\n        slices = util.process_ellipses(self, slices, vdim_selection=True)\n        if isinstance(slices, np.ndarray) and slices.dtype.kind == 'b':\n            if not len(slices) == len(self):\n                raise IndexError(\"Boolean index must match length of sliced object\")\n            return self.clone(self.select(selection_mask=slices))\n        elif slices in [(), Ellipsis]:\n            return self\n        if not isinstance(slices, tuple): slices = (slices,)\n        value_select = None\n        if len(slices) == 1 and slices[0] in self.dimensions():\n            return self.dimension_values(slices[0])\n        elif len(slices) == self.ndims+1 and slices[self.ndims] in self.dimensions():\n            selection = dict(zip(self.dimensions('key', label=True), slices))\n            value_select = slices[self.ndims]\n        elif len(slices) == self.ndims+1 and isinstance(slices[self.ndims],\n                                                        (Dimension,str)):\n            raise Exception(\"%r is not an available value dimension\" % slices[self.ndims])\n        else:\n            selection = dict(zip(self.dimensions(label=True), slices))\n        data = self.select(**selection)\n        if value_select:\n            if data.shape[0] == 1:\n                return data[value_select][0]\n            else:\n                return data.reindex(vdims=[value_select])\n        return data\n\n\n    def sample(self, samples=[], closest=True, **kwargs):\n        \"\"\"\n        Allows sampling of Dataset as an iterator of coordinates\n        matching the key dimensions, returning a new object containing\n        just the selected samples. Alternatively may supply kwargs\n        to sample a coordinate on an object. By default it will attempt\n        to snap to the nearest coordinate if the Element supports it,\n        snapping may be disabled with the closest argument.\n        \"\"\"\n        if kwargs and samples:\n            raise Exception('Supply explicit list of samples or kwargs, not both.')\n        elif kwargs:\n            sample = [slice(None) for _ in range(self.ndims)]\n            for dim, val in kwargs.items():\n                sample[self.get_dimension_index(dim)] = val\n            samples = [tuple(sample)]\n\n        # Note: Special handling sampling of gridded 2D data as Curve\n        # may be replaced wih more general handling\n        # see https://github.com/ioam/holoviews/issues/1173\n        from ...element import Table, Curve\n        if len(samples) == 1:\n            sel = {kd.name: s for kd, s in zip(self.kdims, samples[0])}\n            dims = [kd for kd, v in sel.items() if not np.isscalar(v)]\n            selection = self.select(**sel)\n\n            # If a 1D cross-section of 2D space return Curve\n            if self.interface.gridded and self.ndims == 2 and len(dims) == 1:\n                new_type = Curve\n                kdims = [self.get_dimension(kd) for kd in dims]\n            else:\n                new_type = Table\n                kdims = self.kdims\n\n            if np.isscalar(selection):\n                selection = [samples[0]+(selection,)]\n            else:\n                selection = tuple(selection.columns(kdims+self.vdims).values())\n\n            return self.clone(selection, kdims=kdims, new_type=new_type)\n\n        lens = set(len(util.wrap_tuple(s)) for s in samples)\n        if len(lens) > 1:\n            raise IndexError('Sample coordinates must all be of the same length.')\n\n        if closest:\n            try:\n                samples = self.closest(samples)\n            except NotImplementedError:\n                pass\n        samples = [util.wrap_tuple(s) for s in samples]\n        return self.clone(self.interface.sample(self, samples), new_type=Table)\n\n\n    def reduce(self, dimensions=[], function=None, spreadfn=None, **reduce_map):\n        \"\"\"\n        Allows reducing the values along one or more key dimension with\n        the supplied function. The dimensions may be supplied as a list\n        and a function to apply or a mapping between the dimensions and\n        functions to apply along each dimension.\n        \"\"\"\n        if any(dim in self.vdims for dim in dimensions):\n            raise Exception(\"Reduce cannot be applied to value dimensions\")\n        function, dims = self._reduce_map(dimensions, function, reduce_map)\n        dims = [d for d in self.kdims if d not in dims]\n        return self.aggregate(dims, function, spreadfn)\n\n\n    def aggregate(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n        \"\"\"\n        Aggregates over the supplied key dimensions with the defined\n        function.\n        \"\"\"\n        if function is None:\n            raise ValueError(\"The aggregate method requires a function to be specified\")\n        if dimensions is None: dimensions = self.kdims\n        elif not isinstance(dimensions, list): dimensions = [dimensions]\n        kdims = [self.get_dimension(d, strict=True) for d in dimensions]\n        aggregated = self.interface.aggregate(self, kdims, function, **kwargs)\n        aggregated = self.interface.unpack_scalar(self, aggregated)\n\n        ndims = len(dimensions)\n        min_d, max_d = self.params('kdims').bounds\n        generic_type = (min_d is not None and ndims < min_d) or (max_d is not None and ndims > max_d)\n\n        vdims = self.vdims\n        if spreadfn:\n            error = self.interface.aggregate(self, dimensions, spreadfn)\n            spread_name = spreadfn.__name__\n            ndims = len(vdims)\n            error = self.clone(error, kdims=kdims, new_type=Dataset)\n            combined = self.clone(aggregated, kdims=kdims, new_type=Dataset)\n            for i, d in enumerate(vdims):\n                dim = d('_'.join([d.name, spread_name]))\n                dvals = error.dimension_values(d, False, False)\n                combined = combined.add_dimension(dim, ndims+i, dvals, True)\n            return combined.clone(new_type=Dataset if generic_type else type(self))\n\n        if np.isscalar(aggregated):\n            return aggregated\n        else:\n            try:\n                return self.clone(aggregated, kdims=kdims, vdims=vdims,\n                                  new_type=new_type)\n            except:\n                datatype = self.params('datatype').default\n                return self.clone(aggregated, kdims=kdims, vdims=vdims,\n                                  new_type=Dataset if generic_type else None,\n                                  datatype=datatype)\n\n\n    def groupby(self, dimensions=[], container_type=HoloMap, group_type=None,\n                dynamic=False, **kwargs):\n        \"\"\"Return the results of a groupby operation over the specified\n        dimensions as an object of type container_type (expected to be\n        dictionary-like).\n\n        Keys vary over the columns (dimensions) and the corresponding\n        values are collections of group_type (e.g an Element, list, tuple)\n        constructed with kwargs (if supplied).\n\n        If dynamic is requested container_type is automatically set to\n        a DynamicMap, allowing dynamic exploration of large\n        datasets. If the data does not represent a full cartesian grid\n        of the requested dimensions some Elements will be empty.\n        \"\"\"\n        if not isinstance(dimensions, list): dimensions = [dimensions]\n        if not len(dimensions): dimensions = self.dimensions('key', True)\n        if group_type is None: group_type = type(self)\n\n        dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        dim_names = [d.name for d in dimensions]\n\n        if dynamic:\n            group_dims = [d.name for d in self.kdims if d not in dimensions]\n            kdims = [self.get_dimension(d) for d in group_dims]\n            group_kwargs = dict(util.get_param_values(self), kdims=kdims)\n            group_kwargs.update(kwargs)\n            drop_dim = len(kdims) != len(group_kwargs['kdims'])\n            def load_subset(*args):\n                constraint = dict(zip(dim_names, args))\n                group = self.select(**constraint)\n                if np.isscalar(group):\n                    return group_type(([group],), group=self.group,\n                                      label=self.label, vdims=self.vdims)\n                data = group.reindex(group_dims)\n                if drop_dim and self.interface.gridded:\n                    data = data.columns()\n                return group_type(data, **group_kwargs)\n            dynamic_dims = [d(values=list(self.interface.values(self, d.name, False)))\n                            for d in dimensions]\n            return DynamicMap(load_subset, kdims=dynamic_dims)\n\n        return self.interface.groupby(self, dim_names, container_type,\n                                      group_type, **kwargs)\n\n    def __len__(self):\n        \"\"\"\n        Returns the number of rows in the Dataset object.\n        \"\"\"\n        return self.interface.length(self)\n\n    def __nonzero__(self):\n        return self.interface.nonzero(self)\n\n    __bool__ = __nonzero__\n\n    @property\n    def shape(self):\n        \"Returns the shape of the data.\"\n        return self.interface.shape(self)\n\n\n    def dimension_values(self, dim, expanded=True, flat=True):\n        \"\"\"\n        Returns the values along a particular dimension. If unique\n        values are requested will return only unique values.\n        \"\"\"\n        dim = self.get_dimension(dim, strict=True)\n        return self.interface.values(self, dim, expanded, flat)\n\n\n    def get_dimension_type(self, dim):\n        \"\"\"\n        Returns the specified Dimension type if specified or\n        if the dimension_values types are consistent otherwise\n        None is returned.\n        \"\"\"\n        dim_obj = self.get_dimension(dim)\n        if dim_obj and dim_obj.type is not None:\n            return dim_obj.type\n        return self.interface.dimension_type(self, dim_obj)\n\n\n    def dframe(self, dimensions=None):\n        \"\"\"\n        Returns the data in the form of a DataFrame. Supplying a list\n        of dimensions filters the dataframe. If the data is already\n        a DataFrame a copy is returned.\n        \"\"\"\n        if dimensions:\n            dimensions = [self.get_dimension(d, strict=True).name for d in dimensions]\n        return self.interface.dframe(self, dimensions)\n\n\n    def columns(self, dimensions=None):\n        if dimensions is None:\n            dimensions = self.dimensions()\n        else:\n            dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        return OrderedDict([(d.name, self.dimension_values(d)) for d in dimensions])\n\n\n    @property\n    def to(self):\n        \"\"\"\n        Property to create a conversion interface with methods to\n        convert to other Element types.\n        \"\"\"\n        return self._conversion_interface(self)\n\n\n\n# Aliases for pickle backward compatibility\nColumns      = Dataset\nArrayColumns = ArrayInterface\nDictColumns  = DictInterface\nNdColumns    = NdElementInterface\nGridColumns  = GridInterface\n", "idx": 1, "id": 17496, "msg": "What is the special string 'selection_mask'? I grepped the code base and it doesn't currently appear anywhere...", "proj": "holoviz-holoviews", "lang": "py"}
{"patch": "@@ -33,7 +33,7 @@ def assert_pyspark_version():\n         )\n     else:\n         pyspark_ver = getattr(pyspark, \"__version__\")\n-        if pyspark_ver is None or pyspark_ver < \"2.4\":\n+        if pyspark_ver is None or LooseVersion(pyspark_ver) < LooseVersion(\"2.4\"):\n             logging.warning(\n                 'Found pyspark version \"{}\" installed. pyspark>=2.4.0 is recommended.'.format(\n                     pyspark_ver if pyspark_ver is not None else \"<unknown version>\"", "y": 0, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nimport os\nimport sys\nfrom distutils.version import LooseVersion\n\nfrom databricks.koalas.version import __version__  # noqa: F401\n\n\ndef assert_pyspark_version():\n    import logging\n\n    pyspark_ver = None\n    try:\n        import pyspark\n    except ImportError:\n        raise ImportError(\n            \"Unable to import pyspark - consider doing a pip install with [spark] \"\n            \"extra to install pyspark with pip\"\n        )\n    else:\n        pyspark_ver = getattr(pyspark, \"__version__\")\n        if pyspark_ver is None or pyspark_ver < \"2.4\":\n            logging.warning(\n                'Found pyspark version \"{}\" installed. pyspark>=2.4.0 is recommended.'.format(\n                    pyspark_ver if pyspark_ver is not None else \"<unknown version>\"\n                )\n            )\n\n\nassert_pyspark_version()\n\nimport pyspark\nimport pyarrow\n\nif LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"):\n    if (\n        LooseVersion(pyarrow.__version__) >= LooseVersion(\"0.15\")\n        and \"ARROW_PRE_0_15_IPC_FORMAT\" not in os.environ\n    ):\n        import logging\n\n        logging.warning(\n            \"'ARROW_PRE_0_15_IPC_FORMAT' environment variable was not set. It is required to \"\n            \"set this environment variable to '1' in both driver and executor sides if you use \"\n            \"pyarrow>=0.15 and pyspark<3.0. \"\n            \"Koalas will set it for you but it does not work if there is a Spark context already \"\n            \"launched.\"\n        )\n        # This is required to support PyArrow 0.15 in PySpark versions lower than 3.0.\n        # See SPARK-29367.\n        os.environ[\"ARROW_PRE_0_15_IPC_FORMAT\"] = \"1\"\nelif \"ARROW_PRE_0_15_IPC_FORMAT\" in os.environ:\n    raise RuntimeError(\n        \"Please explicitly unset 'ARROW_PRE_0_15_IPC_FORMAT' environment variable in both \"\n        \"driver and executor sides. It is required to set this environment variable only \"\n        \"when you use pyarrow>=0.15 and pyspark<3.0.\"\n    )\n\nif (\n    LooseVersion(pyarrow.__version__) >= LooseVersion(\"2.0.0\")\n    and \"PYARROW_IGNORE_TIMEZONE\" not in os.environ\n):\n    import logging\n\n    logging.warning(\n        \"'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to \"\n        \"set this environment variable to '1' in both driver and executor sides if you use \"\n        \"pyarrow>=2.0.0. \"\n        \"Koalas will set it for you but it does not work if there is a Spark context already \"\n        \"launched.\"\n    )\n    os.environ[\"PYARROW_IGNORE_TIMEZONE\"] = \"1\"\n\nfrom databricks.koalas.frame import DataFrame\nfrom databricks.koalas.indexes.base import Index\nfrom databricks.koalas.indexes.datetimes import DatetimeIndex\nfrom databricks.koalas.indexes.multi import MultiIndex\nfrom databricks.koalas.indexes.numeric import Float64Index, Int64Index\nfrom databricks.koalas.series import Series\nfrom databricks.koalas.groupby import NamedAgg\n\n__all__ = [  # noqa: F405\n    \"read_csv\",\n    \"read_parquet\",\n    \"to_datetime\",\n    \"from_pandas\",\n    \"get_dummies\",\n    \"DataFrame\",\n    \"Series\",\n    \"Index\",\n    \"MultiIndex\",\n    \"Int64Index\",\n    \"Float64Index\",\n    \"DatetimeIndex\",\n    \"sql\",\n    \"range\",\n    \"concat\",\n    \"melt\",\n    \"get_option\",\n    \"set_option\",\n    \"reset_option\",\n    \"read_sql_table\",\n    \"read_sql_query\",\n    \"read_sql\",\n    \"options\",\n    \"option_context\",\n    \"NamedAgg\",\n]\n\n\ndef _auto_patch_spark():\n    import os\n    import logging\n\n    # Attach a usage logger.\n    logger_module = os.getenv(\"KOALAS_USAGE_LOGGER\", \"\")\n    if logger_module != \"\":\n        try:\n            from databricks.koalas import usage_logging\n\n            usage_logging.attach(logger_module)\n        except Exception as e:\n            logger = logging.getLogger(\"databricks.koalas.usage_logger\")\n            logger.warning(\n                \"Tried to attach usage logger `{}`, but an exception was raised: {}\".format(\n                    logger_module, str(e)\n                )\n            )\n\n    # Autopatching is on by default.\n    x = os.getenv(\"SPARK_KOALAS_AUTOPATCH\", \"true\")\n    if x.lower() in (\"true\", \"1\", \"enabled\"):\n        logger = logging.getLogger(\"spark\")\n        logger.info(\n            \"Patching spark automatically. You can disable it by setting \"\n            \"SPARK_KOALAS_AUTOPATCH=false in your environment\"\n        )\n\n        from pyspark.sql import dataframe as df\n\n        df.DataFrame.to_koalas = DataFrame.to_koalas\n\n\ndef _auto_patch_pandas():\n    import pandas as pd\n\n    # In order to use it in test cases.\n    global _frame_has_class_getitem\n    global _series_has_class_getitem\n\n    _frame_has_class_getitem = hasattr(pd.DataFrame, \"__class_getitem__\")\n    _series_has_class_getitem = hasattr(pd.Series, \"__class_getitem__\")\n\n    if sys.version_info >= (3, 7):\n        # Just in case pandas implements '__class_getitem__' later.\n        if not _frame_has_class_getitem:\n            pd.DataFrame.__class_getitem__ = lambda params: DataFrame.__class_getitem__(params)\n\n        if not _series_has_class_getitem:\n            pd.Series.__class_getitem__ = lambda params: Series.__class_getitem__(params)\n\n\n_auto_patch_spark()\n_auto_patch_pandas()\n\n# Import after the usage logger is attached.\nfrom databricks.koalas.config import get_option, options, option_context, reset_option, set_option\nfrom databricks.koalas.namespace import *  # F405\nfrom databricks.koalas.sql import sql\n", "idx": 2, "id": 17771, "msg": "", "proj": "databricks-koalas", "lang": "py"}
{"patch": "@@ -607,7 +607,7 @@ func (sm *Miner) submitPoSt(start, end *types.BlockHeight, inputs []generatePost\n \t\treturn\n \t}\n \tif len(faults) != 0 {\n-\t\tlog.Errorf(\"some faults when generating PoSt: %v\", faults)\n+\t\tlog.Warningf(\"some faults when generating PoSt: %v\", faults)\n \t\t// TODO: proper fault handling\n \t}\n ", "y": 1, "oldf": "package storage\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"fmt\"\n\t\"math/big\"\n\t\"math/rand\"\n\t\"strconv\"\n\t\"sync\"\n\t\"time\"\n\n\tinet \"gx/ipfs/QmNgLg1NTw37iWbYPKcyK85YJ9Whs1MkPtJwhfqbNYAyKg/go-libp2p-net\"\n\tunixfs \"gx/ipfs/QmQXze9tG878pa4Euya4rrDpyTNX3kQe4dhCaBzBozGgpe/go-unixfs\"\n\t\"gx/ipfs/QmR8BauakNcBa3RbE4nbQu76PDiJgoQgz8AJdhJuiU4TAw/go-cid\"\n\tcbor \"gx/ipfs/QmRoARq3nkUb13HSKZGepCZSWe5GrVPwx7xURJGZ7KWv9V/go-ipld-cbor\"\n\tdag \"gx/ipfs/QmTQdH4848iTVCJmKXYyRiK72HufWTLYQQ8iN3JaQ8K1Hq/go-merkledag\"\n\t\"gx/ipfs/QmVmDhyTTUcQXFD1rRQ64fGLMSAoaQvNH3hwuaCFAPq2hy/errors\"\n\tbserv \"gx/ipfs/QmYPZzd9VqmJDwxUnThfeSbV1Y5o53aVPDijTB7j7rS9Ep/go-blockservice\"\n\t\"gx/ipfs/QmZNkThpqfVXs9GNbexPrfBbXSLNYeKrE7jwFM2oqHbyqN/go-libp2p-protocol\"\n\t\"gx/ipfs/QmaoXrM4Z41PD48JY36YqQGKQpLGjyLA2cKcLsES7YddAq/go-libp2p-host\"\n\tipld \"gx/ipfs/QmcKKBwfz6FyQdHR2jsXrrF6XeSBXYL86anmWNewpFpoF5/go-ipld-format\"\n\tlogging \"gx/ipfs/QmcuXC5cxs79ro2cUuHs4HQ2bkDLJUYokwL8aivcX6HW3C/go-log\"\n\t\"gx/ipfs/Qmf4xQhNomPNhrtZc67qSnfJSjxjXs9LWvknJtSXwimPrM/go-datastore\"\n\t\"gx/ipfs/Qmf4xQhNomPNhrtZc67qSnfJSjxjXs9LWvknJtSXwimPrM/go-datastore/query\"\n\n\t\"github.com/filecoin-project/go-filecoin/abi\"\n\t\"github.com/filecoin-project/go-filecoin/actor/builtin/miner\"\n\t\"github.com/filecoin-project/go-filecoin/address\"\n\tcbu \"github.com/filecoin-project/go-filecoin/cborutil\"\n\t\"github.com/filecoin-project/go-filecoin/consensus\"\n\t\"github.com/filecoin-project/go-filecoin/exec\"\n\t\"github.com/filecoin-project/go-filecoin/porcelain\"\n\t\"github.com/filecoin-project/go-filecoin/proofs\"\n\t\"github.com/filecoin-project/go-filecoin/proofs/sectorbuilder\"\n\t\"github.com/filecoin-project/go-filecoin/repo\"\n\t\"github.com/filecoin-project/go-filecoin/types\"\n\t\"github.com/filecoin-project/go-filecoin/util/convert\"\n)\n\nvar log = logging.Logger(\"/fil/storage\")\n\nconst makeDealProtocol = protocol.ID(\"/fil/storage/mk/1.0.0\")\nconst queryDealProtocol = protocol.ID(\"/fil/storage/qry/1.0.0\")\n\n// TODO: replace this with a queries to pick reasonable gas price and limits.\nconst submitPostGasPrice = 0\nconst submitPostGasLimit = 100000000000\n\nconst dealsAwatingSeal = \"dealsAwaitingSeal\"\n\n// Miner represents a storage miner.\ntype Miner struct {\n\tminerAddr      address.Address\n\tminerOwnerAddr address.Address\n\n\t// deals is a list of deals we made. It is indexed by the CID of the proposal.\n\tdeals   map[cid.Cid]*storageDeal\n\tdealsDs repo.Datastore\n\tdealsLk sync.Mutex\n\n\tpostInProcessLk sync.Mutex\n\tpostInProcess   *types.BlockHeight\n\n\tdealsAwaitingSealDs repo.Datastore\n\tdealsAwaitingSeal   *dealsAwaitingSealStruct\n\n\tplumbingAPI plumbing\n\tnode        node\n\n\tproposalAcceptor func(ctx context.Context, m *Miner, p *DealProposal) (*DealResponse, error)\n\tproposalRejector func(ctx context.Context, m *Miner, p *DealProposal, reason string) (*DealResponse, error)\n}\n\ntype storageDeal struct {\n\tProposal *DealProposal\n\tResponse *DealResponse\n}\n\n// plumbing is the subset of the plumbing API that storage.Miner needs.\ntype plumbing interface {\n\tMessageQuery(ctx context.Context, optFrom, to address.Address, method string, params ...interface{}) ([][]byte, *exec.FunctionSignature, error)\n\tMessageSend(ctx context.Context, from, to address.Address, value *types.AttoFIL, gasPrice types.AttoFIL, gasLimit types.GasUnits, method string, params ...interface{}) (cid.Cid, error)\n\tMessageWait(ctx context.Context, msgCid cid.Cid, cb func(*types.Block, *types.SignedMessage, *types.MessageReceipt) error) error\n\tActorGetSignature(ctx context.Context, actorAddr address.Address, method string) (*exec.FunctionSignature, error)\n\tConfigGet(dottedPath string) (interface{}, error)\n}\n\n// node is subset of node on which this protocol depends. These deps\n// are moving off of node and into the plumbing api (see PlumbingAPI). Eventually this\n// dependency on node should go away, fully replaced by the dependency on the plumbing api.\ntype node interface {\n\tBlockHeight() (*types.BlockHeight, error)\n\tGetBlockTime() time.Duration\n\tBlockService() bserv.BlockService\n\tHost() host.Host\n\tSectorBuilder() sectorbuilder.SectorBuilder\n}\n\n// generatePostInput is a struct containing sector id and related commitments\n// used to generate a proof-of-spacetime\ntype generatePostInput struct {\n\tcommD     proofs.CommD\n\tcommR     proofs.CommR\n\tcommRStar proofs.CommRStar\n\tsectorID  uint64\n}\n\nfunc init() {\n\tcbor.RegisterCborType(storageDeal{})\n\tcbor.RegisterCborType(dealsAwaitingSealStruct{})\n}\n\n// NewMiner is\nfunc NewMiner(ctx context.Context, minerAddr, minerOwnerAddr address.Address, nd node, dealsDs repo.Datastore, dealsAwaitingSealDs repo.Datastore, plumbingAPI plumbing) (*Miner, error) {\n\tsm := &Miner{\n\t\tminerAddr:           minerAddr,\n\t\tminerOwnerAddr:      minerOwnerAddr,\n\t\tdeals:               make(map[cid.Cid]*storageDeal),\n\t\tplumbingAPI:         plumbingAPI,\n\t\tdealsDs:             dealsDs,\n\t\tdealsAwaitingSealDs: dealsAwaitingSealDs,\n\t\tnode:                nd,\n\t\tproposalAcceptor:    acceptProposal,\n\t\tproposalRejector:    rejectProposal,\n\t}\n\n\tif err := sm.loadDealsAwaitingSeal(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to load dealAwaitingSeal when creating miner\")\n\t}\n\tsm.dealsAwaitingSeal.onSuccess = sm.onCommitSuccess\n\tsm.dealsAwaitingSeal.onFail = sm.onCommitFail\n\n\tif err := sm.loadDeals(); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to load miner deals when creating miner\")\n\t}\n\n\tnd.Host().SetStreamHandler(makeDealProtocol, sm.handleMakeDeal)\n\tnd.Host().SetStreamHandler(queryDealProtocol, sm.handleQueryDeal)\n\n\treturn sm, nil\n}\n\nfunc (sm *Miner) handleMakeDeal(s inet.Stream) {\n\tdefer s.Close() // nolint: errcheck\n\n\tvar proposal DealProposal\n\tif err := cbu.NewMsgReader(s).ReadMsg(&proposal); err != nil {\n\t\tlog.Errorf(\"received invalid proposal: %s\", err)\n\t\treturn\n\t}\n\n\tctx := context.Background()\n\tresp, err := sm.receiveStorageProposal(ctx, &proposal)\n\tif err != nil {\n\t\tlog.Errorf(\"failed to process proposal: %s\", err)\n\t\treturn\n\t}\n\n\tif err := cbu.NewMsgWriter(s).WriteMsg(resp); err != nil {\n\t\tlog.Errorf(\"failed to write proposal response: %s\", err)\n\t}\n}\n\n// receiveStorageProposal is the entry point for the miner storage protocol\nfunc (sm *Miner) receiveStorageProposal(ctx context.Context, p *DealProposal) (*DealResponse, error) {\n\t// TODO: Check signature\n\n\t// TODO: check size, duration, totalprice match up with the payment info\n\t//       and also check that the payment info is valid.\n\t//       A valid payment info contains enough funds to *us* to cover the totalprice\n\n\tstoragePrice, err := sm.plumbingAPI.ConfigGet(\"mining.storagePrice\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tstoragePriceAF, ok := storagePrice.(*types.AttoFIL)\n\tif !ok {\n\t\treturn nil, errors.New(\"Could not retrieve storagePrice from config\")\n\t}\n\tif p.TotalPrice.LessThan(storagePriceAF) {\n\t\treturn sm.proposalRejector(ctx, sm, p,\n\t\t\tfmt.Sprintf(\"proposed price %s is less that miner's current asking price: %s\", p.TotalPrice, storagePriceAF))\n\t}\n\n\t// Payment is valid, everything else checks out, let's accept this proposal\n\treturn sm.proposalAcceptor(ctx, sm, p)\n}\n\nfunc acceptProposal(ctx context.Context, sm *Miner, p *DealProposal) (*DealResponse, error) {\n\tif sm.node.SectorBuilder() == nil {\n\t\treturn nil, errors.New(\"Mining disabled, can not process proposal\")\n\t}\n\n\tproposalCid, err := convert.ToCid(p)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get cid of proposal\")\n\t}\n\n\tresp := &DealResponse{\n\t\tState:       Accepted,\n\t\tProposalCid: proposalCid,\n\t\tSignature:   types.Signature(\"signaturrreee\"),\n\t}\n\n\tsm.dealsLk.Lock()\n\tdefer sm.dealsLk.Unlock()\n\n\tsm.deals[proposalCid] = &storageDeal{\n\t\tProposal: p,\n\t\tResponse: resp,\n\t}\n\tif err := sm.saveDeal(proposalCid); err != nil {\n\t\tsm.deals[proposalCid].Response.State = Failed\n\t\tsm.deals[proposalCid].Response.Message = \"Could not persist deal due to internal error\"\n\t\treturn nil, errors.Wrap(err, \"failed to save miner deal\")\n\t}\n\n\t// TODO: use some sort of nicer scheduler\n\tgo sm.processStorageDeal(proposalCid)\n\n\treturn resp, nil\n}\n\nfunc rejectProposal(ctx context.Context, sm *Miner, p *DealProposal, reason string) (*DealResponse, error) {\n\tproposalCid, err := convert.ToCid(p)\n\tif err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to get cid of proposal\")\n\t}\n\n\tresp := &DealResponse{\n\t\tState:       Rejected,\n\t\tProposalCid: proposalCid,\n\t\tMessage:     reason,\n\t\tSignature:   types.Signature(\"signaturrreee\"),\n\t}\n\n\tsm.dealsLk.Lock()\n\tdefer sm.dealsLk.Unlock()\n\n\tsm.deals[proposalCid] = &storageDeal{\n\t\tProposal: p,\n\t\tResponse: resp,\n\t}\n\tif err := sm.saveDeal(proposalCid); err != nil {\n\t\treturn nil, errors.Wrap(err, \"failed to save miner deal\")\n\t}\n\n\treturn resp, nil\n}\n\nfunc (sm *Miner) getStorageDeal(c cid.Cid) *storageDeal {\n\tsm.dealsLk.Lock()\n\tdefer sm.dealsLk.Unlock()\n\treturn sm.deals[c]\n}\n\nfunc (sm *Miner) updateDealResponse(proposalCid cid.Cid, f func(*DealResponse)) error {\n\tsm.dealsLk.Lock()\n\tdefer sm.dealsLk.Unlock()\n\tf(sm.deals[proposalCid].Response)\n\terr := sm.saveDeal(proposalCid)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to store updated deal response in datastore\")\n\t}\n\n\tlog.Debugf(\"Miner.updateDealResponse(%s) - %d\", proposalCid.String(), sm.deals[proposalCid].Response)\n\treturn nil\n}\n\nfunc (sm *Miner) processStorageDeal(c cid.Cid) {\n\tlog.Debugf(\"Miner.processStorageDeal(%s)\", c.String())\n\tctx, cancel := context.WithCancel(context.Background())\n\tdefer cancel()\n\n\td := sm.getStorageDeal(c)\n\tif d.Response.State != Accepted {\n\t\t// TODO: handle resumption of deal processing across miner restarts\n\t\tlog.Error(\"attempted to process an already started deal\")\n\t\treturn\n\t}\n\n\t// 'Receive' the data, this could also be a truck full of hard drives. (TODO: proper abstraction)\n\t// TODO: this is not a great way to do this. At least use a session\n\t// Also, this needs to be fetched into a staging area for miners to prepare and seal in data\n\tlog.Debug(\"Miner.processStorageDeal - FetchGraph\")\n\tif err := dag.FetchGraph(ctx, d.Proposal.PieceRef, dag.NewDAGService(sm.node.BlockService())); err != nil {\n\t\tlog.Errorf(\"failed to fetch data: %s\", err)\n\t\terr := sm.updateDealResponse(c, func(resp *DealResponse) {\n\t\t\tresp.Message = \"Transfer failed\"\n\t\t\tresp.State = Failed\n\t\t\t// TODO: signature?\n\t\t})\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"could not update to deal to 'Failed' state: %s\", err)\n\t\t}\n\t\treturn\n\t}\n\n\tfail := func(message, logerr string) {\n\t\tlog.Errorf(logerr)\n\t\terr := sm.updateDealResponse(c, func(resp *DealResponse) {\n\t\t\tresp.Message = message\n\t\t\tresp.State = Failed\n\t\t})\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"could not update to deal to 'Failed' state in fail callback: %s\", err)\n\t\t}\n\t}\n\n\tpi := &sectorbuilder.PieceInfo{\n\t\tRef:  d.Proposal.PieceRef,\n\t\tSize: d.Proposal.Size.Uint64(),\n\t}\n\n\t// There is a race here that requires us to use dealsAwaitingSeal below. If the\n\t// sector gets sealed and OnCommitmentAddedToChain is called right after\n\t// AddPiece returns but before we record the sector/deal mapping we might\n\t// miss it. Hence, dealsAwaitingSealStruct. I'm told that sealing in practice is\n\t// so slow that the race only exists in tests, but tests were flaky so\n\t// we fixed it with dealsAwaitingSealStruct.\n\t//\n\t// Also, this pattern of not being able to set up book-keeping ahead of\n\t// the call is inelegant.\n\tsectorID, err := sm.node.SectorBuilder().AddPiece(ctx, pi)\n\tif err != nil {\n\t\tfail(\"failed to submit seal proof\", fmt.Sprintf(\"failed to add piece: %s\", err))\n\t\treturn\n\t}\n\n\terr = sm.updateDealResponse(c, func(resp *DealResponse) {\n\t\tresp.State = Staged\n\t})\n\tif err != nil {\n\t\tlog.Errorf(\"could update to 'Staged': %s\", err)\n\t}\n\n\t// Careful: this might update state to success or failure so it should go after\n\t// updating state to Staged.\n\tsm.dealsAwaitingSeal.add(sectorID, c)\n\tif err := sm.saveDealsAwaitingSeal(); err != nil {\n\t\tlog.Errorf(\"could not save deal awaiting seal: %s\", err)\n\t}\n}\n\n// dealsAwaitingSealStruct is a container for keeping track of which sectors have\n// pieces from which deals. We need it to accommodate a race condition where\n// a sector commit message is added to chain before we can add the sector/deal\n// book-keeping. It effectively caches success and failure results for sectors\n// for tardy add() calls.\ntype dealsAwaitingSealStruct struct {\n\tl sync.Mutex\n\t// Maps from sector id to the deal cids with pieces in the sector.\n\tSectorsToDeals map[uint64][]cid.Cid\n\t// Maps from sector id to sector.\n\tSuccessfulSectors map[uint64]*sectorbuilder.SealedSectorMetadata\n\t// Maps from sector id to seal failure error string.\n\tFailedSectors map[uint64]string\n\n\tonSuccess func(dealCid cid.Cid, sector *sectorbuilder.SealedSectorMetadata)\n\tonFail    func(dealCid cid.Cid, message string)\n}\n\nfunc (sm *Miner) loadDealsAwaitingSeal() error {\n\tsm.dealsAwaitingSeal = &dealsAwaitingSealStruct{\n\t\tSectorsToDeals:    make(map[uint64][]cid.Cid),\n\t\tSuccessfulSectors: make(map[uint64]*sectorbuilder.SealedSectorMetadata),\n\t\tFailedSectors:     make(map[uint64]string),\n\t}\n\n\tresult, notFound := sm.dealsAwaitingSealDs.Get(datastore.NewKey(dealsAwatingSeal))\n\tif notFound == nil {\n\t\tif err := json.Unmarshal(result, &sm.dealsAwaitingSeal); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to unmarshal deals awaiting seal from datastore\")\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (sm *Miner) saveDealsAwaitingSeal() error {\n\tmarshalledDealsAwaitingSeal, err := json.Marshal(sm.dealsAwaitingSeal)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"Could not marshal dealsAwaitingSeal\")\n\t}\n\terr = sm.dealsAwaitingSealDs.Put(datastore.NewKey(dealsAwatingSeal), marshalledDealsAwaitingSeal)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"could not save deal awaiting seal record to disk, in-memory deals differ from persisted deals!\")\n\t}\n\n\treturn nil\n}\n\nfunc (dealsAwaitingSeal *dealsAwaitingSealStruct) add(sectorID uint64, dealCid cid.Cid) {\n\tdealsAwaitingSeal.l.Lock()\n\tdefer dealsAwaitingSeal.l.Unlock()\n\n\tif sector, ok := dealsAwaitingSeal.SuccessfulSectors[sectorID]; ok {\n\t\tdealsAwaitingSeal.onSuccess(dealCid, sector)\n\t\t// Don't keep references to sectors around forever. Assume that at most\n\t\t// one success-before-add call will happen (eg, in a test). Sector sealing\n\t\t// outside of tests is so slow that it shouldn't happen in practice.\n\t\t// So now that it has happened once, clean it up. If we wanted to keep\n\t\t// the state around for longer for some reason we need to limit how many\n\t\t// sectors we hang onto, eg keep a fixed-length slice of successes\n\t\t// and failures and shift the oldest off and the newest on.\n\t\tdelete(dealsAwaitingSeal.SuccessfulSectors, sectorID)\n\t} else if message, ok := dealsAwaitingSeal.FailedSectors[sectorID]; ok {\n\t\tdealsAwaitingSeal.onFail(dealCid, message)\n\t\t// Same as above.\n\t\tdelete(dealsAwaitingSeal.FailedSectors, sectorID)\n\t} else {\n\t\tdeals, ok := dealsAwaitingSeal.SectorsToDeals[sectorID]\n\t\tif ok {\n\t\t\tdealsAwaitingSeal.SectorsToDeals[sectorID] = append(deals, dealCid)\n\t\t} else {\n\t\t\tdealsAwaitingSeal.SectorsToDeals[sectorID] = []cid.Cid{dealCid}\n\t\t}\n\t}\n}\n\nfunc (dealsAwaitingSeal *dealsAwaitingSealStruct) success(sector *sectorbuilder.SealedSectorMetadata) {\n\tdealsAwaitingSeal.l.Lock()\n\tdefer dealsAwaitingSeal.l.Unlock()\n\n\tdealsAwaitingSeal.SuccessfulSectors[sector.SectorID] = sector\n\n\tfor _, dealCid := range dealsAwaitingSeal.SectorsToDeals[sector.SectorID] {\n\t\tdealsAwaitingSeal.onSuccess(dealCid, sector)\n\t}\n\tdelete(dealsAwaitingSeal.SectorsToDeals, sector.SectorID)\n}\n\nfunc (dealsAwaitingSeal *dealsAwaitingSealStruct) fail(sectorID uint64, message string) {\n\tdealsAwaitingSeal.l.Lock()\n\tdefer dealsAwaitingSeal.l.Unlock()\n\n\tdealsAwaitingSeal.FailedSectors[sectorID] = message\n\n\tfor _, dealCid := range dealsAwaitingSeal.SectorsToDeals[sectorID] {\n\t\tdealsAwaitingSeal.onFail(dealCid, message)\n\t}\n\tdelete(dealsAwaitingSeal.SectorsToDeals, sectorID)\n}\n\n// OnCommitmentAddedToChain is a callback, called when a sector seal message was posted to the chain.\nfunc (sm *Miner) OnCommitmentAddedToChain(sector *sectorbuilder.SealedSectorMetadata, err error) {\n\tsectorID := sector.SectorID\n\tlog.Debug(\"Miner.OnCommitmentAddedToChain\")\n\n\tif err != nil {\n\t\terrMsg := fmt.Sprintf(\"failed sealing sector: %v: %s:\", sectorID, err)\n\t\tlog.Error(errMsg)\n\t\tsm.dealsAwaitingSeal.fail(sector.SectorID, errMsg)\n\t} else {\n\t\tsm.dealsAwaitingSeal.success(sector)\n\t}\n\tif err := sm.saveDealsAwaitingSeal(); err != nil {\n\t\terrMsg := fmt.Sprintf(\"failed persisting deals awaiting seal: %s\", err)\n\t\tlog.Error(errMsg)\n\t\tsm.dealsAwaitingSeal.fail(sector.SectorID, errMsg)\n\t}\n}\n\nfunc (sm *Miner) onCommitSuccess(dealCid cid.Cid, sector *sectorbuilder.SealedSectorMetadata) {\n\terr := sm.updateDealResponse(dealCid, func(resp *DealResponse) {\n\t\tresp.State = Posted\n\t\tresp.ProofInfo = &ProofInfo{\n\t\t\tSectorID: sector.SectorID,\n\t\t\tCommR:    sector.CommR[:],\n\t\t\tCommD:    sector.CommD[:],\n\t\t}\n\t})\n\tif err != nil {\n\t\tlog.Errorf(\"commit succeeded but could not update to deal 'Posted' state: %s\", err)\n\t}\n}\n\nfunc (sm *Miner) onCommitFail(dealCid cid.Cid, message string) {\n\terr := sm.updateDealResponse(dealCid, func(resp *DealResponse) {\n\t\tresp.Message = message\n\t\tresp.State = Failed\n\t})\n\tlog.Errorf(\"commit failure but could not update to deal 'Failed' state: %s\", err)\n}\n\n// OnNewHeaviestTipSet is a callback called by node, everytime the the latest head is updated.\n// It is used to check if we are in a new proving period and need to trigger PoSt submission.\nfunc (sm *Miner) OnNewHeaviestTipSet(ts consensus.TipSet) {\n\tctx := context.Background()\n\n\trets, sig, err := sm.plumbingAPI.MessageQuery(ctx, (address.Address{}), sm.minerAddr, \"getSectorCommitments\")\n\tif err != nil {\n\t\tlog.Errorf(\"failed to call query method getSectorCommitments: %s\", err)\n\t\treturn\n\t}\n\n\tcommitmentsVal, err := abi.Deserialize(rets[0], sig.Return[0])\n\tif err != nil {\n\t\tlog.Errorf(\"failed to convert returned ABI value: %s\", err)\n\t\treturn\n\t}\n\n\tcommitments, ok := commitmentsVal.Val.(map[string]types.Commitments)\n\tif !ok {\n\t\tlog.Errorf(\"failed to convert returned ABI value to miner.Commitments\")\n\t\treturn\n\t}\n\n\tvar inputs []generatePostInput\n\tfor k, v := range commitments {\n\t\tn, err := strconv.ParseUint(k, 10, 64)\n\t\tif err != nil {\n\t\t\tlog.Errorf(\"failed to parse commitment sector id to uint64: %s\", err)\n\t\t\treturn\n\t\t}\n\n\t\tinputs = append(inputs, generatePostInput{\n\t\t\tcommD:     v.CommD,\n\t\t\tcommR:     v.CommR,\n\t\t\tcommRStar: v.CommRStar,\n\t\t\tsectorID:  n,\n\t\t})\n\t}\n\n\tif len(inputs) == 0 {\n\t\t// no sector sealed, nothing to do\n\t\treturn\n\t}\n\n\tprovingPeriodStart, err := sm.getProvingPeriodStart()\n\tif err != nil {\n\t\tlog.Errorf(\"failed to get provingPeriodStart: %s\", err)\n\t\treturn\n\t}\n\n\tsm.postInProcessLk.Lock()\n\tdefer sm.postInProcessLk.Unlock()\n\n\tif sm.postInProcess != nil && sm.postInProcess.Equal(provingPeriodStart) {\n\t\t// post is already being generated for this period, nothing to do\n\t\treturn\n\t}\n\n\theight, err := ts.Height()\n\tif err != nil {\n\t\tlog.Errorf(\"failed to get block height: %s\", err)\n\t\treturn\n\t}\n\n\th := types.NewBlockHeight(height)\n\tprovingPeriodEnd := provingPeriodStart.Add(miner.ProvingPeriodBlocks)\n\n\tif h.GreaterEqual(provingPeriodStart) {\n\t\tif h.LessThan(provingPeriodEnd) {\n\t\t\t// we are in a new proving period, lets get this post going\n\t\t\tsm.postInProcess = provingPeriodStart\n\t\t\tgo sm.submitPoSt(provingPeriodStart, provingPeriodEnd, inputs)\n\t\t} else {\n\t\t\t// we are too late\n\t\t\t// TODO: figure out faults and payments here\n\t\t\tlog.Errorf(\"too late start=%s  end=%s current=%s\", provingPeriodStart, provingPeriodEnd, h)\n\t\t}\n\t}\n}\n\nfunc (sm *Miner) getProvingPeriodStart() (*types.BlockHeight, error) {\n\tres, _, err := sm.plumbingAPI.MessageQuery(context.Background(), (address.Address{}), sm.minerAddr, \"getProvingPeriodStart\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn types.NewBlockHeightFromBytes(res[0]), nil\n}\n\n// generatePoSt creates the required PoSt, given a list of sector ids and\n// matching seeds. It returns the Snark Proof for the PoSt, and a list of\n// sectors that faulted, if there were any faults.\nfunc (sm *Miner) generatePoSt(commRs []proofs.CommR, challenge proofs.PoStChallengeSeed) (proofs.PoStProof, []uint64, error) {\n\treq := sectorbuilder.GeneratePoSTRequest{\n\t\tCommRs:        commRs,\n\t\tChallengeSeed: challenge,\n\t}\n\tres, err := sm.node.SectorBuilder().GeneratePoST(req)\n\tif err != nil {\n\t\treturn proofs.PoStProof{}, nil, errors.Wrap(err, \"failed to generate PoSt\")\n\t}\n\n\treturn res.Proof, res.Faults, nil\n}\n\nfunc (sm *Miner) submitPoSt(start, end *types.BlockHeight, inputs []generatePostInput) {\n\t// TODO: real seed generation\n\tseed := proofs.PoStChallengeSeed{}\n\tif _, err := rand.Read(seed[:]); err != nil {\n\t\tpanic(err)\n\t}\n\n\tcommRs := make([]proofs.CommR, len(inputs))\n\tfor i, input := range inputs {\n\t\tcommRs[i] = input.commR\n\t}\n\n\tproof, faults, err := sm.generatePoSt(commRs, seed)\n\tif err != nil {\n\t\tlog.Errorf(\"failed to generate PoSts: %s\", err)\n\t\treturn\n\t}\n\tif len(faults) != 0 {\n\t\tlog.Errorf(\"some faults when generating PoSt: %v\", faults)\n\t\t// TODO: proper fault handling\n\t}\n\n\theight, err := sm.node.BlockHeight()\n\tif err != nil {\n\t\tlog.Errorf(\"failed to submit PoSt, as the current block height can not be determined: %s\", err)\n\t\t// TODO: what should happen in this case?\n\t\treturn\n\t}\n\tif height.LessThan(start) {\n\t\t// TODO: what to do here? not sure this can happen, maybe through reordering?\n\t\tlog.Errorf(\"PoSt generation time took negative block time: %s < %s\", height, start)\n\t\treturn\n\t}\n\n\tif height.GreaterEqual(end) {\n\t\t// TODO: we are too late, figure out faults and decide if we want to still submit\n\t\tlog.Errorf(\"PoSt generation was too slow height=%s end=%s\", height, end)\n\t\treturn\n\t}\n\n\t// TODO: figure out a more sensible timeout\n\tctx, cancel := context.WithTimeout(context.Background(), 10*time.Minute)\n\tdefer cancel()\n\n\t// TODO: algorithmically determine appropriate values for these\n\tgasPrice := types.NewGasPrice(submitPostGasPrice)\n\tgasLimit := types.NewGasUnits(submitPostGasLimit)\n\n\terr = porcelain.MessageSendWithRetry(ctx, sm.plumbingAPI, 10 /*retries*/, sm.node.GetBlockTime() /*wait time*/, sm.minerOwnerAddr, sm.minerAddr, types.NewAttoFIL(big.NewInt(0)), \"submitPoSt\", gasPrice, gasLimit, proof[:])\n\tif err != nil {\n\t\tlog.Errorf(\"failed to submit PoSt: %s\", err)\n\t\treturn\n\t}\n\n\tlog.Debug(\"submitted PoSt\")\n}\n\n// Query responds to a query for the proposal referenced by the given cid\nfunc (sm *Miner) Query(ctx context.Context, c cid.Cid) *DealResponse {\n\tsm.dealsLk.Lock()\n\tdefer sm.dealsLk.Unlock()\n\td, ok := sm.deals[c]\n\tif !ok {\n\t\treturn &DealResponse{\n\t\t\tState:   Unknown,\n\t\t\tMessage: \"no such deal\",\n\t\t}\n\t}\n\n\treturn d.Response\n}\n\nfunc (sm *Miner) handleQueryDeal(s inet.Stream) {\n\tdefer s.Close() // nolint: errcheck\n\n\tvar q queryRequest\n\tif err := cbu.NewMsgReader(s).ReadMsg(&q); err != nil {\n\t\tlog.Errorf(\"received invalid query: %s\", err)\n\t\treturn\n\t}\n\n\tctx := context.Background()\n\tresp := sm.Query(ctx, q.Cid)\n\n\tif err := cbu.NewMsgWriter(s).WriteMsg(resp); err != nil {\n\t\tlog.Errorf(\"failed to write query response: %s\", err)\n\t}\n}\n\nfunc getFileSize(ctx context.Context, c cid.Cid, dserv ipld.DAGService) (uint64, error) {\n\tfnode, err := dserv.Get(ctx, c)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tswitch n := fnode.(type) {\n\tcase *dag.ProtoNode:\n\t\treturn unixfs.DataSize(n.Data())\n\tcase *dag.RawNode:\n\t\treturn n.Size()\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"unrecognized node type: %T\", fnode)\n\t}\n}\n\nfunc (sm *Miner) loadDeals() error {\n\tres, err := sm.dealsDs.Query(query.Query{})\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"failed to query deals from datastore\")\n\t}\n\n\tsm.deals = make(map[cid.Cid]*storageDeal)\n\n\tfor entry := range res.Next() {\n\t\tvar deal storageDeal\n\t\tif err := cbor.DecodeInto(entry.Value, &deal); err != nil {\n\t\t\treturn errors.Wrap(err, \"failed to unmarshal deals from datastore\")\n\t\t}\n\t\tsm.deals[deal.Response.ProposalCid] = &deal\n\t}\n\n\treturn nil\n}\n\nfunc (sm *Miner) saveDeal(proposalCid cid.Cid) error {\n\tmarshalledDeal, err := cbor.DumpObject(sm.deals[proposalCid])\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"Could not marshal storageDeal\")\n\t}\n\terr = sm.dealsDs.Put(datastore.NewKey(proposalCid.String()), marshalledDeal)\n\tif err != nil {\n\t\treturn errors.Wrap(err, \"could not save client storage deal\")\n\t}\n\treturn nil\n}\n", "idx": 1, "id": 16277, "msg": "As per the CONTRIBUTING.md doc: > Error: a truly unexpected condition that should not happen in Real Life and that a dev should go look at I don't think that a PoSt fault should cause a developer to go scrambling. I think you made the right choice to change this log entry to be a warning.", "proj": "filecoin-project-venus", "lang": "go"}
{"patch": "@@ -136,6 +136,15 @@ type Twin struct {\n \tReported TwinProperty `json:\"reported,omitempty\"`\n }\n \n+type DeviceState struct {\n+\tReported DeviceOnOffline `json:\"state,omitempty\"`\n+}\n+\n+type DeviceOnOffline struct {\n+\tLightState string `json:\"light_state, omitempty\"`\n+\tLastOnline string `json:\"last_online, omitempty\"`\n+}\n+\n // TwinProperty represents the device property for which an Expected/Actual state can be defined.\n type TwinProperty struct {\n \t// Required: The value for this property.", "y": 1, "oldf": "/*\nCopyright 2019 The KubeEdge Authors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage v1alpha1\n\nimport (\n\t\"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n)\n\n// DeviceSpec represents a single device instance. It is an instantation of a device model.\ntype DeviceSpec struct {\n\t// Required: DeviceModelRef is reference to the device model used as a template\n\t// to create the device instance.\n\tDeviceModelRef *v1.LocalObjectReference `json:\"deviceModelRef,omitempty\"`\n\t// Required: The protocol configuration used to connect to the device.\n\tProtocol ProtocolConfig `json:\"protocol,omitempty\"`\n\t// NodeSelector indicates the binding preferences between devices and nodes.\n\t// Refer to k8s.io/kubernetes/pkg/apis/core NodeSelector for more details\n\t// +optional\n\tNodeSelector *v1.NodeSelector `json:\"nodeSelector,omitempty\"`\n}\n\n// Only one of its members may be specified.\ntype ProtocolConfig struct {\n\t// Protocol configuration for opc-ua\n\t// +optional\n\tOpcUA *ProtocolConfigOpcUA `json:\"opcua,omitempty\"`\n\t// Protocol configuration for modbus\n\t// +optional\n\tModbus *ProtocolConfigModbus `json:\"modbus,omitempty\"`\n\t// Protocol configuration for bluetooth\n\t// +optional\n\tBluetooth *ProtocolConfigBluetooth `json:\"bluetooth,omitempty\"`\n}\n\ntype ProtocolConfigOpcUA struct {\n\t// Required: The URL for opc server endpoint.\n\tURL string `json:\"url,omitempty\"`\n\t// Username for access opc server.\n\t// +optional\n\tUserName string `json:\"userName,omitempty\"`\n\t// Password for access opc server.\n\t// +optional\n\tPassword string `json:\"password,omitempty\"`\n\t// Defaults to \"none\".\n\t// +optional\n\tSecurityPolicy string `json:\"securityPolicy,omitempty\"`\n\t// Defaults to \"none\".\n\t// +optional\n\tSecurityMode string `json:\"securityMode,omitempty\"`\n\t// Certificate for access opc server.\n\t// +optional\n\tCertificate string `json:\"certificate,omitempty\"`\n\t// PrivateKey for access opc server.\n\t// +optional\n\tPrivateKey string `json:\"privateKey,omitempty\"`\n\t// Timeout seconds for the opc server connection.???\n\t// +optional\n\tTimeout int64 `json:\"timeout,omitempty\"`\n}\n\n// Only one of its members may be specified.\ntype ProtocolConfigModbus struct {\n\t// +optional\n\tRTU *ProtocolConfigModbusRTU `json:\"rtu,omitempty\"`\n\t// +optional\n\tTCP *ProtocolConfigModbusTCP `json:\"tcp,omitempty\"`\n}\n\ntype ProtocolConfigModbusTCP struct {\n\t// Required.\n\tIP string `json:\"ip,omitempty\"`\n\t// Required.\n\tPort int64 `json:\"port,omitempty\"`\n\t// Required.\n\tSlaveID string `json:\"slaveID,omitempty\"`\n}\n\ntype ProtocolConfigModbusRTU struct {\n\t// Required.\n\tSerialPort string `json:\"serialPort,omitempty\"`\n\t// Required. BaudRate 115200|57600|38400|19200|9600|4800|2400|1800|1200|600|300|200|150|134|110|75|50\n\tBaudRate int64 `json:\"baudRate,omitempty\"`\n\t// Required. Valid values are 8, 7, 6, 5.\n\tDataBits int64 `json:\"dataBits,omitempty\"`\n\t// Required. Valid options are \"none\", \"even\", \"odd\". Defaults to \"none\".\n\tParity string `json:\"parity,omitempty\"`\n\t// Required. Bit that stops 1|2\n\tStopBits int64 `json:\"stopBits,omitempty\"`\n\t// Required. 0-255\n\tSlaveID int64 `json:\"slaveID,omitempty\"`\n}\n\ntype ProtocolConfigBluetooth struct {\n\t// Unique identifier assigned to the device.\n\t// +optional\n\tMACAddress string `json:\"macAddress,omitempty\"`\n}\n\n// DeviceStatus reports the device state and the desired/reported values of twin attributes.\ntype DeviceStatus struct {\n\t// A list of device twins containing desired/reported desired/reported values of twin properties..\n\t// Optional: A passive device won't have twin properties and this list could be empty.\n\t// +optional\n\tTwins []Twin `json:\"twins,omitempty\"`\n}\n\n// Twin provides a logical representation of control properties (writable properties in the\n// device model). The properties can have a Desired state and a Reported state. The cloud configures\n// the `Desired`state of a device property and this configuration update is pushed to the edge node.\n// The mapper sends a command to the device to change this property value as per the desired state .\n// It receives the `Reported` state of the property once the previous operation is complete and sends\n// the reported state to the cloud. Offline device interaction in the edge is possible via twin\n// properties for control/command operations.\ntype Twin struct {\n\t// Required: The property name for which the desired/reported values are specified.\n\t// This property should be present in the device model.\n\tPropertyName string `json:\"propertyName,omitempty\"`\n\t// Required: the desired property value.\n\tDesired TwinProperty `json:\"desired,omitempty\"`\n\t// Required: the reported property value.\n\tReported TwinProperty `json:\"reported,omitempty\"`\n}\n\n// TwinProperty represents the device property for which an Expected/Actual state can be defined.\ntype TwinProperty struct {\n\t// Required: The value for this property.\n\tValue string `json:\"value,omitempty\"`\n\t// Additional metadata like timestamp when the value was reported etc.\n\t// +optional\n\tMetadata map[string]string `json:\"metadata,omitempty\"`\n}\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// Device is the Schema for the devices API\n// +k8s:openapi-gen=true\ntype Device struct {\n\tmetav1.TypeMeta   `json:\",inline\"`\n\tmetav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n\tSpec   DeviceSpec   `json:\"spec,omitempty\"`\n\tStatus DeviceStatus `json:\"status,omitempty\"`\n}\n\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// DeviceList contains a list of Device\ntype DeviceList struct {\n\tmetav1.TypeMeta `json:\",inline\"`\n\tmetav1.ListMeta `json:\"metadata,omitempty\"`\n\tItems           []Device `json:\"items\"`\n}\n", "idx": 1, "id": 11907, "msg": "what do we mean by LightState ? all devices don't have such kind of state .", "proj": "kubeedge-kubeedge", "lang": "go"}
{"patch": "@@ -16,35 +16,19 @@ import (\n \t\"go.uber.org/zap\"\n \n \t\"github.com/iotexproject/iotex-core/address\"\n-\t\"github.com/iotexproject/iotex-core/pkg/keypair\"\n \t\"github.com/iotexproject/iotex-core/pkg/log\"\n \t\"github.com/iotexproject/iotex-core/pkg/unit\"\n+\t\"github.com/iotexproject/iotex-core/test/identityset\"\n )\n \n-// DefaultAdminPrivateKey is used to create the default genesis config. It could facilitate quick setup of the\n-// blockchain, but it MUST NOT be used in production.\n-const DefaultAdminPrivateKey = \"bace9b2435db45b119e1570b4ea9c57993b2311e0c408d743d87cd22838ae892\"\n-\n var (\n \t// Default contains the default genesis config\n-\tDefault Genesis\n-\n-\tgenesisPath      string\n-\tdefaultAdminAddr address.Address\n+\tDefault     Genesis\n+\tgenesisPath string\n )\n \n func init() {\n \tflag.StringVar(&genesisPath, \"genesis-path\", \"\", \"Genesis path\")\n-\tsk, err := keypair.DecodePrivateKey(DefaultAdminPrivateKey)\n-\tif err != nil {\n-\t\tlog.L().Panic(\"Error when decoding the default admin private key.\", zap.Error(err))\n-\t}\n-\tpkHash := keypair.HashPubKey(&sk.PublicKey)\n-\tdefaultAdminAddr, err = address.FromBytes(pkHash[:])\n-\tif err != nil {\n-\t\tlog.L().Panic(\"Error when constructing the default admin address.\", zap.Error(err))\n-\t}\n-\n \tinitDefaultConfig()\n }\n ", "y": 1, "oldf": "// Copyright (c) 2019 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage genesis\n\nimport (\n\t\"flag\"\n\t\"math/big\"\n\t\"time\"\n\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/config\"\n\t\"go.uber.org/zap\"\n\n\t\"github.com/iotexproject/iotex-core/address\"\n\t\"github.com/iotexproject/iotex-core/pkg/keypair\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/pkg/unit\"\n)\n\n// DefaultAdminPrivateKey is used to create the default genesis config. It could facilitate quick setup of the\n// blockchain, but it MUST NOT be used in production.\nconst DefaultAdminPrivateKey = \"bace9b2435db45b119e1570b4ea9c57993b2311e0c408d743d87cd22838ae892\"\n\nvar (\n\t// Default contains the default genesis config\n\tDefault Genesis\n\n\tgenesisPath      string\n\tdefaultAdminAddr address.Address\n)\n\nfunc init() {\n\tflag.StringVar(&genesisPath, \"genesis-path\", \"\", \"Genesis path\")\n\tsk, err := keypair.DecodePrivateKey(DefaultAdminPrivateKey)\n\tif err != nil {\n\t\tlog.L().Panic(\"Error when decoding the default admin private key.\", zap.Error(err))\n\t}\n\tpkHash := keypair.HashPubKey(&sk.PublicKey)\n\tdefaultAdminAddr, err = address.FromBytes(pkHash[:])\n\tif err != nil {\n\t\tlog.L().Panic(\"Error when constructing the default admin address.\", zap.Error(err))\n\t}\n\n\tinitDefaultConfig()\n}\n\nfunc initDefaultConfig() {\n\tDefault = Genesis{\n\t\tBlockchain: Blockchain{\n\t\t\tTimestamp:         1546329600,\n\t\t\tBlockGasLimit:     20000000,\n\t\t\tActionGasLimit:    5000000,\n\t\t\tBlockInterval:     10 * time.Second,\n\t\t\tNumSubEpochs:      1,\n\t\t\tNumDelegates:      21,\n\t\t\tTimeBasedRotation: false,\n\t\t},\n\t\tRewarding: Rewarding{\n\t\t\tInitAdminAddrStr:           defaultAdminAddr.String(),\n\t\t\tInitBalanceStr:             unit.ConvertIotxToRau(1200000000).String(),\n\t\t\tBlockRewardStr:             unit.ConvertIotxToRau(36).String(),\n\t\t\tEpochRewardStr:             unit.ConvertIotxToRau(400000).String(),\n\t\t\tNumDelegatesForEpochReward: 100,\n\t\t},\n\t}\n}\n\ntype (\n\t// Genesis is the root level of genesis config. Genesis config is the network-wide blockchain config. All the nodes\n\t// participating into the same network should use EXACTLY SAME genesis config.\n\tGenesis struct {\n\t\tBlockchain `yaml:\"blockchain\"`\n\t\tRewarding  `yaml:\"rewarding\"`\n\t}\n\t// Blockchain contains blockchain level configs\n\tBlockchain struct {\n\t\t// Timestamp is the timestamp of the genesis block\n\t\tTimestamp int64\n\t\t// BlockGasLimit is the total gas limit could be consumed in a block\n\t\tBlockGasLimit uint64 `yaml:\"blockGasLimit\"`\n\t\t// ActionGasLimit is the per action gas limit cap\n\t\tActionGasLimit uint64 `yaml:\"actionGasLimit\"`\n\t\t// BlockInterval is the interval between two blocks\n\t\tBlockInterval time.Duration `yaml:\"blockInterval\"`\n\t\t// NumSubEpochs is the number of sub epochs in one epoch of block production\n\t\tNumSubEpochs uint64 `yaml:\"numSubEpochs\"`\n\t\t// NumDelegates is the number of delegates that participate into one epoch of block production\n\t\tNumDelegates uint64 `yaml:\"numDelegates\"`\n\t\t// TimeBasedRotation is the flag to enable rotating delegates' time slots on a block height\n\t\tTimeBasedRotation bool `yaml:\"timeBasedRotation\"`\n\t}\n\t// Rewarding contains the configs for rewarding protocol\n\tRewarding struct {\n\t\t// InitAdminAddrStr is the address of the initial rewarding protocol admin in encoded string format\n\t\tInitAdminAddrStr string `yaml:\"initAdminAddr\"`\n\t\t// InitBalanceStr is the initial balance of the rewarding protocol in decimal string format\n\t\tInitBalanceStr string `yaml:\"initBalance\"`\n\t\t// BlockReward is the block reward amount in decimal string format\n\t\tBlockRewardStr string `yaml:\"blockReward\"`\n\t\t// EpochReward is the epoch reward amount in decimal string format\n\t\tEpochRewardStr string `yaml:\"epochReward\"`\n\t\t// NumDelegatesForEpochReward is the number of top candidates that will share a epoch reward\n\t\tNumDelegatesForEpochReward uint64 `yaml:\"numDelegatesForEpochReward\"`\n\t}\n)\n\n// New constructs a genesis config. It loads the default values, and could be overwritten by values defined in the yaml\n// config files\nfunc New() (Genesis, error) {\n\topts := make([]config.YAMLOption, 0)\n\topts = append(opts, config.Static(Default))\n\tif genesisPath != \"\" {\n\t\topts = append(opts, config.File(genesisPath))\n\t}\n\tyaml, err := config.NewYAML(opts...)\n\tif err != nil {\n\t\treturn Genesis{}, errors.Wrap(err, \"error when constructing a genesis in yaml\")\n\t}\n\n\tvar genesis Genesis\n\tif err := yaml.Get(config.Root).Populate(&genesis); err != nil {\n\t\treturn Genesis{}, errors.Wrap(err, \"failed to unmarshal yaml genesis to struct\")\n\t}\n\treturn genesis, nil\n}\n\n// InitAdminAddr returns the address of the initial rewarding protocol admin\nfunc (r *Rewarding) InitAdminAddr() address.Address {\n\taddr, err := address.FromString(r.InitAdminAddrStr)\n\tif err != nil {\n\t\tlog.L().Panic(\"Error when decoding the rewarding protocol init admin address from string.\", zap.Error(err))\n\t}\n\treturn addr\n}\n\n// InitBalance returns the init balance of the rewarding fund\nfunc (r *Rewarding) InitBalance() *big.Int {\n\tval, ok := big.NewInt(0).SetString(r.InitBalanceStr, 10)\n\tif !ok {\n\t\tlog.S().Panicf(\"Error when casting init balance string %s into big int\", r.InitBalanceStr)\n\t}\n\treturn val\n}\n\n// BlockReward returns the block reward amount\nfunc (r *Rewarding) BlockReward() *big.Int {\n\tval, ok := big.NewInt(0).SetString(r.BlockRewardStr, 10)\n\tif !ok {\n\t\tlog.S().Panicf(\"Error when casting block reward string %s into big int\", r.BlockRewardStr)\n\t}\n\treturn val\n}\n\n// EpochReward returns the epoch reward amount\nfunc (r *Rewarding) EpochReward() *big.Int {\n\tval, ok := big.NewInt(0).SetString(r.EpochRewardStr, 10)\n\tif !ok {\n\t\tlog.S().Panicf(\"Error when casting epoch reward string %s into big int\", r.EpochRewardStr)\n\t}\n\treturn val\n}\n", "idx": 1, "id": 15497, "msg": "`Default` is a global variable (from `gochecknoglobals`)", "proj": "iotexproject-iotex-core", "lang": "go"}
{"patch": "@@ -87,32 +87,24 @@ func NewRecordReplayClient(ctx context.Context, t *testing.T, rf func(r *httprep\n \t\t\tt.Fatal(err)\n \t\t}\n \t\trf(rec)\n-\t\tc, err = rec.Client(ctx, opts...)\n-\t\tif err != nil {\n-\t\t\tt.Fatal(err)\n-\t\t}\n \t\tcleanup = func() {\n \t\t\tif err := rec.Close(); err != nil {\n \t\t\t\tt.Fatal(err)\n \t\t\t}\n \t\t}\n \n-\t\treturn c, cleanup, state.UnixNano()\n+\t\treturn rec.Client(), cleanup, state.UnixNano()\n \t}\n \tt.Logf(\"Replaying from golden file %s\", path)\n \trep, err := httpreplay.NewReplayer(path)\n \tif err != nil {\n \t\tt.Fatal(err)\n \t}\n-\tc, err = rep.Client(ctx)\n-\tif err != nil {\n-\t\tt.Fatal(err)\n-\t}\n \trecState := new(time.Time)\n \tif err := recState.UnmarshalBinary(rep.Initial()); err != nil {\n \t\tt.Fatal(err)\n \t}\n-\treturn c, func() { rep.Close() }, recState.UnixNano()\n+\treturn rep.Client(), func() { rep.Close() }, recState.UnixNano()\n }\n \n // NewAWSSession creates a new session for testing against AWS.", "y": 0, "oldf": "// Copyright 2019 The Go Cloud Development Kit Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage setup // import \"gocloud.dev/internal/testing/setup\"\n\nimport (\n\t\"context\"\n\t\"flag\"\n\t\"io/ioutil\"\n\t\"net/http\"\n\t\"os\"\n\t\"path/filepath\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/aws/aws-sdk-go/aws\"\n\tawscreds \"github.com/aws/aws-sdk-go/aws/credentials\"\n\t\"github.com/aws/aws-sdk-go/aws/session\"\n\t\"gocloud.dev/gcp\"\n\t\"gocloud.dev/internal/useragent\"\n\n\t\"cloud.google.com/go/httpreplay\"\n\t\"github.com/google/go-replayers/grpcreplay\"\n\t\"golang.org/x/oauth2/google\"\n\t\"google.golang.org/api/option\"\n\t\"google.golang.org/grpc\"\n\tgrpccreds \"google.golang.org/grpc/credentials\"\n\t\"google.golang.org/grpc/credentials/oauth\"\n\n\t\"github.com/Azure/azure-pipeline-go/pipeline\"\n\t\"github.com/Azure/azure-storage-blob-go/azblob\"\n)\n\n// Record is true iff the tests are being run in \"record\" mode.\nvar Record = flag.Bool(\"record\", false, \"whether to run tests against cloud resources and record the interactions\")\n\n// FakeGCPCredentials gets fake GCP credentials.\nfunc FakeGCPCredentials(ctx context.Context) (*google.Credentials, error) {\n\treturn google.CredentialsFromJSON(ctx, []byte(`{\"type\": \"service_account\", \"project_id\": \"my-project-id\"}`))\n}\n\nfunc awsSession(region string, client *http.Client) (*session.Session, error) {\n\t// Provide fake creds if running in replay mode.\n\tvar creds *awscreds.Credentials\n\tif !*Record {\n\t\tcreds = awscreds.NewStaticCredentials(\"FAKE_ID\", \"FAKE_SECRET\", \"FAKE_TOKEN\")\n\t}\n\treturn session.NewSession(&aws.Config{\n\t\tHTTPClient:  client,\n\t\tRegion:      aws.String(region),\n\t\tCredentials: creds,\n\t\tMaxRetries:  aws.Int(0),\n\t})\n}\n\n// NewRecordReplayClient creates a new http.Client for tests. This client's\n// activity is being either recorded to files (when *Record is set) or replayed\n// from files. rf is a modifier function that will be invoked with the address\n// of the httpreplay.Recorder object used to obtain the client; this function\n// can mutate the recorder to add service-specific header filters, for example.\n// An initState is returned for tests that need a state to have deterministic\n// results, for example, a seed to generate random sequences.\nfunc NewRecordReplayClient(ctx context.Context, t *testing.T, rf func(r *httpreplay.Recorder),\n\topts ...option.ClientOption) (c *http.Client, cleanup func(), initState int64) {\n\thttpreplay.DebugHeaders()\n\tpath := filepath.Join(\"testdata\", t.Name()+\".replay\")\n\tif *Record {\n\t\tt.Logf(\"Recording into golden file %s\", path)\n\t\tif err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tstate := time.Now()\n\t\tb, _ := state.MarshalBinary()\n\t\trec, err := httpreplay.NewRecorder(path, b)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\trf(rec)\n\t\tc, err = rec.Client(ctx, opts...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\tcleanup = func() {\n\t\t\tif err := rec.Close(); err != nil {\n\t\t\t\tt.Fatal(err)\n\t\t\t}\n\t\t}\n\n\t\treturn c, cleanup, state.UnixNano()\n\t}\n\tt.Logf(\"Replaying from golden file %s\", path)\n\trep, err := httpreplay.NewReplayer(path)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tc, err = rep.Client(ctx)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\trecState := new(time.Time)\n\tif err := recState.UnmarshalBinary(rep.Initial()); err != nil {\n\t\tt.Fatal(err)\n\t}\n\treturn c, func() { rep.Close() }, recState.UnixNano()\n}\n\n// NewAWSSession creates a new session for testing against AWS.\n// If the test is in --record mode, the test will call out to AWS, and the\n// results are recorded in a replay file.\n// Otherwise, the session reads a replay file and runs the test as a replay,\n// which never makes an outgoing HTTP call and uses fake credentials.\n// An initState is returned for tests that need a state to have deterministic\n// results, for example, a seed to generate random sequences.\nfunc NewAWSSession(ctx context.Context, t *testing.T, region string) (sess *session.Session,\n\trt http.RoundTripper, cleanup func(), initState int64) {\n\tclient, cleanup, state := NewRecordReplayClient(ctx, t, func(r *httpreplay.Recorder) {\n\t\tr.RemoveQueryParams(\"X-Amz-Credential\", \"X-Amz-Signature\", \"X-Amz-Security-Token\")\n\t\tr.RemoveRequestHeaders(\"Authorization\", \"Duration\", \"X-Amz-Security-Token\")\n\t\tr.ClearHeaders(\"X-Amz-Date\")\n\t\tr.ClearQueryParams(\"X-Amz-Date\")\n\t\tr.ClearHeaders(\"User-Agent\") // AWS includes the Go version\n\t}, option.WithoutAuthentication())\n\tsess, err := awsSession(region, client)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\treturn sess, client.Transport, cleanup, state\n}\n\n// NewGCPClient creates a new HTTPClient for testing against GCP.\n// If the test is in --record mode, the client will call out to GCP, and the\n// results are recorded in a replay file.\n// Otherwise, the session reads a replay file and runs the test as a replay,\n// which never makes an outgoing HTTP call and uses fake credentials.\nfunc NewGCPClient(ctx context.Context, t *testing.T) (client *gcp.HTTPClient, rt http.RoundTripper, done func()) {\n\tvar co option.ClientOption\n\tif *Record {\n\t\tcreds, err := gcp.DefaultCredentials(ctx)\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"failed to get default credentials: %v\", err)\n\t\t}\n\t\tco = option.WithTokenSource(gcp.CredentialsTokenSource(creds))\n\t} else {\n\t\tco = option.WithoutAuthentication()\n\t}\n\tc, cleanup, _ := NewRecordReplayClient(ctx, t, func(r *httpreplay.Recorder) {\n\t\tr.ClearQueryParams(\"Expires\")\n\t\tr.ClearQueryParams(\"Signature\")\n\t\tr.ClearHeaders(\"Expires\")\n\t\tr.ClearHeaders(\"Signature\")\n\t}, co)\n\treturn &gcp.HTTPClient{Client: *c}, c.Transport, cleanup\n}\n\n// NewGCPgRPCConn creates a new connection for testing against GCP via gRPC.\n// If the test is in --record mode, the client will call out to GCP, and the\n// results are recorded in a replay file.\n// Otherwise, the session reads a replay file and runs the test as a replay,\n// which never makes an outgoing RPC and uses fake credentials.\nfunc NewGCPgRPCConn(ctx context.Context, t *testing.T, endPoint, api string) (*grpc.ClientConn, func()) {\n\tfilename := t.Name() + \".replay\"\n\tif *Record {\n\t\topts, done := newGCPRecordDialOptions(t, filename)\n\t\topts = append(opts, useragent.GRPCDialOption(api))\n\t\t// Add credentials for real RPCs.\n\t\tcreds, err := gcp.DefaultCredentials(ctx)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\topts = append(opts, grpc.WithTransportCredentials(grpccreds.NewClientTLSFromCert(nil, \"\")))\n\t\topts = append(opts, grpc.WithPerRPCCredentials(oauth.TokenSource{TokenSource: gcp.CredentialsTokenSource(creds)}))\n\t\tconn, err := grpc.DialContext(ctx, endPoint, opts...)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\t\treturn conn, done\n\t}\n\trep, done := newGCPReplayer(t, filename)\n\tconn, err := rep.Connection()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\treturn conn, done\n}\n\n// contentTypeInjectPolicy and contentTypeInjector are somewhat of a hack to\n// overcome an impedance mismatch between the Azure pipeline library and\n// httpreplay - the tool we use to record/replay HTTP traffic for tests.\n// azure-pipeline-go does not set the Content-Type header in its requests,\n// setting X-Ms-Blob-Content-Type instead; however, httpreplay expects\n// Content-Type to be non-empty in some cases. This injector makes sure that\n// the content type is copied into the right header when that is originally\n// empty. It's only used for testing.\ntype contentTypeInjectPolicy struct {\n\tnode pipeline.Policy\n}\n\nfunc (p *contentTypeInjectPolicy) Do(ctx context.Context, request pipeline.Request) (pipeline.Response, error) {\n\tif len(request.Header.Get(\"Content-Type\")) == 0 {\n\t\tcType := request.Header.Get(\"X-Ms-Blob-Content-Type\")\n\t\trequest.Header.Set(\"Content-Type\", cType)\n\t}\n\tresponse, err := p.node.Do(ctx, request)\n\treturn response, err\n}\n\ntype contentTypeInjector struct {\n}\n\nfunc (f contentTypeInjector) New(node pipeline.Policy, opts *pipeline.PolicyOptions) pipeline.Policy {\n\treturn &contentTypeInjectPolicy{node: node}\n}\n\n// NewAzureTestPipeline creates a new connection for testing against Azure Blob.\nfunc NewAzureTestPipeline(ctx context.Context, t *testing.T, api string, credential azblob.Credential, accountName string) (pipeline.Pipeline, func(), *http.Client) {\n\tclient, done, _ := NewRecordReplayClient(ctx, t, func(r *httpreplay.Recorder) {\n\t\tr.RemoveQueryParams(\"se\", \"sig\")\n\t\tr.RemoveQueryParams(\"X-Ms-Date\")\n\t\tr.ClearHeaders(\"X-Ms-Date\")\n\t\tr.ClearHeaders(\"User-Agent\") // includes the full Go version\n\t}, option.WithoutAuthentication())\n\tf := []pipeline.Factory{\n\t\t// Sets User-Agent for recorder.\n\t\tazblob.NewTelemetryPolicyFactory(azblob.TelemetryOptions{\n\t\t\tValue: useragent.AzureUserAgentPrefix(api),\n\t\t}),\n\t\tcontentTypeInjector{},\n\t\tcredential,\n\t\tpipeline.MethodFactoryMarker(),\n\t}\n\t// Create a pipeline that uses client to make requests.\n\tp := pipeline.NewPipeline(f, pipeline.Options{\n\t\tHTTPSender: pipeline.FactoryFunc(func(next pipeline.Policy, po *pipeline.PolicyOptions) pipeline.PolicyFunc {\n\t\t\treturn func(ctx context.Context, request pipeline.Request) (pipeline.Response, error) {\n\t\t\t\tr, err := client.Do(request.WithContext(ctx))\n\t\t\t\tif err != nil {\n\t\t\t\t\terr = pipeline.NewError(err, \"HTTP request failed\")\n\t\t\t\t}\n\t\t\t\treturn pipeline.NewHTTPResponse(r), err\n\t\t\t}\n\t\t}),\n\t})\n\n\treturn p, done, client\n}\n\n// NewAzureKeyVaultTestClient creates a *http.Client for Azure KeyVault test\n// recordings.\nfunc NewAzureKeyVaultTestClient(ctx context.Context, t *testing.T) (*http.Client, func()) {\n\tclient, cleanup, _ := NewRecordReplayClient(ctx, t, func(r *httpreplay.Recorder) {\n\t\tr.RemoveQueryParams(\"se\", \"sig\")\n\t\tr.RemoveQueryParams(\"X-Ms-Date\")\n\t\tr.ClearHeaders(\"X-Ms-Date\")\n\t\tr.ClearHeaders(\"User-Agent\") // includes the full Go version\n\t}, option.WithoutAuthentication())\n\treturn client, cleanup\n}\n\n// FakeGCPDefaultCredentials sets up the environment with fake GCP credentials.\n// It returns a cleanup function.\nfunc FakeGCPDefaultCredentials(t *testing.T) func() {\n\tconst envVar = \"GOOGLE_APPLICATION_CREDENTIALS\"\n\tjsonCred := []byte(`{\"client_id\": \"foo.apps.googleusercontent.com\", \"client_secret\": \"bar\", \"refresh_token\": \"baz\", \"type\": \"authorized_user\"}`)\n\tf, err := ioutil.TempFile(\"\", \"fake-gcp-creds\")\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tif err := ioutil.WriteFile(f.Name(), jsonCred, 0666); err != nil {\n\t\tt.Fatal(err)\n\t}\n\toldEnvVal := os.Getenv(envVar)\n\tos.Setenv(envVar, f.Name())\n\treturn func() {\n\t\tos.Remove(f.Name())\n\t\tos.Setenv(envVar, oldEnvVal)\n\t}\n}\n\n// newGCPRecordDialOptions return grpc.DialOptions that are to be appended to a\n// GRPC dial request. These options allow a recorder to intercept RPCs and save\n// RPCs to the file at filename, or read the RPCs from the file and return them.\nfunc newGCPRecordDialOptions(t *testing.T, filename string) (opts []grpc.DialOption, done func()) {\n\tpath := filepath.Join(\"testdata\", filename)\n\tt.Logf(\"Recording into golden file %s\", path)\n\tr, err := grpcreplay.NewRecorder(path, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\topts = r.DialOptions()\n\tdone = func() {\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Errorf(\"unable to close recorder: %v\", err)\n\t\t}\n\t}\n\treturn opts, done\n}\n\n// newGCPReplayer returns a Replayer for GCP gRPC connections, as well as a function\n// to call when done with the Replayer.\nfunc newGCPReplayer(t *testing.T, filename string) (*grpcreplay.Replayer, func()) {\n\tpath := filepath.Join(\"testdata\", filename)\n\tt.Logf(\"Replaying from golden file %s\", path)\n\tr, err := grpcreplay.NewReplayer(path, nil)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdone := func() {\n\t\tif err := r.Close(); err != nil {\n\t\t\tt.Errorf(\"unable to close recorder: %v\", err)\n\t\t}\n\t}\n\treturn r, done\n}\n\n// HasDockerTestEnvironment returns true when either:\n// 1) Not on Travis.\n// 2) On Travis Linux environment, where Docker is available.\nfunc HasDockerTestEnvironment() bool {\n\ts := os.Getenv(\"TRAVIS_OS_NAME\")\n\treturn s == \"\" || s == \"linux\"\n}\n", "idx": 3, "id": 19549, "msg": "", "proj": "google-go-cloud", "lang": "go"}
{"patch": "@@ -432,26 +432,6 @@ namespace NLog.Targets\n             destination.Append(']');\n         }\n \n-        private bool SerializeTypeCodeValue(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options, SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n-        {\n-            if (objTypeCode == TypeCode.Object)\n-            {\n-                if (value is DateTimeOffset)\n-                {\n-                    QuoteValue(destination, $\"{value:yyyy-MM-dd HH:mm:ss zzz}\");\n-                    return true;\n-                }\n-                else\n-                {\n-                    return SerializeObjectWithProperties(value, destination, options, ref objectsInPath, depth);\n-                }\n-            }\n-            else\n-            {\n-                return SerializeSimpleTypeCodeValue(value, objTypeCode, destination, options);\n-            }\n-        }\n-\n         private bool SerializeObjectWithProperties(object value, StringBuilder destination, JsonSerializeOptions options, ref SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n         {\n             int originalLength = destination.Length;", "y": 1, "oldf": "\ufeff// \n// Copyright (c) 2004-2018 Jaroslaw Kowalski <jaak@jkowalski.net>, Kim Christensen, Julian Verdurmen\n// \n// All rights reserved.\n// \n// Redistribution and use in source and binary forms, with or without \n// modification, are permitted provided that the following conditions \n// are met:\n// \n// * Redistributions of source code must retain the above copyright notice, \n//   this list of conditions and the following disclaimer. \n// \n// * Redistributions in binary form must reproduce the above copyright notice,\n//   this list of conditions and the following disclaimer in the documentation\n//   and/or other materials provided with the distribution. \n// \n// * Neither the name of Jaroslaw Kowalski nor the names of its \n//   contributors may be used to endorse or promote products derived from this\n//   software without specific prior written permission. \n// \n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n// AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE \n// ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE \n// LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR \n// CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n// SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS \n// INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN \n// CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) \n// ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF \n// THE POSSIBILITY OF SUCH DAMAGE.\n// \n\nusing System;\nusing System.Collections;\nusing System.Collections.Generic;\nusing System.Globalization;\nusing System.Text;\nusing NLog.Internal;\n\nnamespace NLog.Targets\n{\n    /// <summary>\n    /// Default class for serialization of values to JSON format.\n    /// </summary>\n#pragma warning disable 618\n    public class DefaultJsonSerializer : IJsonConverter, IJsonSerializer\n#pragma warning restore 618\n    {\n        private readonly ObjectReflectionCache _objectReflectionCache = new ObjectReflectionCache();\n        private readonly MruCache<Enum, string> _enumCache = new MruCache<Enum, string>(1500);\n        private readonly JsonSerializeOptions _serializeOptions = new JsonSerializeOptions();\n        private readonly JsonSerializeOptions _exceptionSerializeOptions = new JsonSerializeOptions() { SanitizeDictionaryKeys = true };\n        private readonly IFormatProvider _defaultFormatProvider = CreateFormatProvider();\n\n        private const int MaxJsonLength = 512 * 1024;\n\n        private static readonly DefaultJsonSerializer instance = new DefaultJsonSerializer();\n\n        private static readonly IEqualityComparer<object> _referenceEqualsComparer = SingleItemOptimizedHashSet<object>.ReferenceEqualityComparer.Default;\n\n        /// <summary>\n        /// Singleton instance of the serializer.\n        /// </summary>\n        public static DefaultJsonSerializer Instance => instance;\n\n        static DefaultJsonSerializer()\n        {\n        }\n\n        /// <summary>\n        /// Private. Use <see cref=\"Instance\"/>\n        /// </summary>\n        private DefaultJsonSerializer()\n        { }\n\n        /// <summary>\n        /// Returns a serialization of an object into JSON format.\n        /// </summary>\n        /// <param name=\"value\">The object to serialize to JSON.</param>\n        /// <returns>Serialized value.</returns>\n        public string SerializeObject(object value)\n        {\n            return SerializeObject(value, _serializeOptions);\n        }\n\n        /// <summary>\n        /// Returns a serialization of an object into JSON format.\n        /// </summary>\n        /// <param name=\"value\">The object to serialize to JSON.</param>\n        /// <param name=\"options\">serialisation options</param>\n        /// <returns>Serialized value.</returns>\n        public string SerializeObject(object value, JsonSerializeOptions options)\n        {\n            if (value == null)\n            {\n                return \"null\";\n            }\n            else if (value is string str)\n            {\n                for (int i = 0; i < str.Length; ++i)\n                {\n                    if (RequiresJsonEscape(str[i], options.EscapeUnicode))\n                    {\n                        StringBuilder sb = new StringBuilder(str.Length + 4);\n                        sb.Append('\"');\n                        AppendStringEscape(sb, str, options.EscapeUnicode);\n                        sb.Append('\"');\n                        return sb.ToString();\n                    }\n                }\n                return QuoteValue(str);\n            }\n            else\n            {\n                TypeCode objTypeCode = Convert.GetTypeCode(value);\n                if (objTypeCode != TypeCode.Object && objTypeCode != TypeCode.Char && StringHelpers.IsNullOrWhiteSpace(options.Format) && options.FormatProvider == null)\n                {\n                    Enum enumValue;\n                    if (!options.EnumAsInteger && IsNumericTypeCode(objTypeCode, false) && (enumValue = value as Enum) != null)\n                    {\n                        return QuoteValue(EnumAsString(enumValue));\n                    }\n                    else\n                    {\n                        string xmlStr = XmlHelper.XmlConvertToString(value, objTypeCode);\n                        if (SkipQuotes(value, objTypeCode))\n                        {\n                            return xmlStr;\n                        }\n                        else\n                        {\n                            return QuoteValue(xmlStr);\n                        }\n                    }\n                }\n                else\n                {\n                    StringBuilder sb = new StringBuilder();\n                    if (!SerializeObject(value, sb, options))\n                    {\n                        return null;\n                    }\n                    return sb.ToString();\n                }\n            }\n        }\n\n        /// <summary>\n        /// Serialization of the object in JSON format to the destination StringBuilder\n        /// </summary>\n        /// <param name=\"value\">The object to serialize to JSON.</param>\n        /// <param name=\"destination\">Write the resulting JSON to this destination.</param>\n        /// <returns>Object serialized succesfully (true/false).</returns>\n        public bool SerializeObject(object value, StringBuilder destination)\n        {\n            return SerializeObject(value, destination, _serializeOptions);\n        }\n\n        /// <summary>\n        /// Serialization of the object in JSON format to the destination StringBuilder\n        /// </summary>\n        /// <param name=\"value\">The object to serialize to JSON.</param>\n        /// <param name=\"destination\">Write the resulting JSON to this destination.</param>\n        /// <param name=\"options\">serialisation options</param>\n        /// <returns>Object serialized succesfully (true/false).</returns>\n        public bool SerializeObject(object value, StringBuilder destination, JsonSerializeOptions options)\n        {\n            return SerializeObject(value, Convert.GetTypeCode(value), destination, options, default(SingleItemOptimizedHashSet<object>), 0);\n        }\n\n        /// <summary>\n        /// Serialization of the object in JSON format to the destination StringBuilder\n        /// </summary>\n        /// <param name=\"value\">The object to serialize to JSON.</param>\n        /// <param name=\"objTypeCode\">The TypeCode for the object to serialize.</param>\n        /// <param name=\"destination\">Write the resulting JSON to this destination.</param>\n        /// <param name=\"options\">serialisation options</param>\n        /// <param name=\"objectsInPath\">The objects in path (Avoid cyclic reference loop).</param>\n        /// <param name=\"depth\">The current depth (level) of recursion.</param>\n        /// <returns>Object serialized succesfully (true/false).</returns>\n        private bool SerializeObject(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options,\n                SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            if (objTypeCode == TypeCode.Object && objectsInPath.Contains(value))\n            {\n                return false; // detected reference loop, skip serialization\n            }\n\n            if (value == null)\n            {\n                destination.Append(\"null\");\n            }\n            else if (objTypeCode == TypeCode.String)\n            {\n                destination.Append('\"');\n                AppendStringEscape(destination, value.ToString(), options.EscapeUnicode);\n                destination.Append('\"');\n            }\n            else if (objTypeCode != TypeCode.Object)\n            {\n                return SerializeWithTypeCode(value, objTypeCode, destination, options, ref objectsInPath, depth);\n            }\n            else if (value is IDictionary dict)\n            {\n                using (StartScope(ref objectsInPath, dict))\n                {\n                    SerializeDictionaryObject(dict, destination, options, objectsInPath, depth);\n                }\n            }\n            else if (value is IDictionary<string, object> expando)\n            {\n                // Special case for Expando-objects\n                using (StartScope(ref objectsInPath, expando))\n                {\n                    return SerializeObjectProperties(new ObjectReflectionCache.ObjectPropertyList(expando), destination, options, objectsInPath, depth);\n                }\n            }\n            else if (value is IEnumerable enumerable)\n            {\n                using (StartScope(ref objectsInPath, value))\n                {\n                    SerializeCollectionObject(enumerable, destination, options, objectsInPath, depth);\n                }\n            }\n            else\n            {\n                return SerializeWithTypeCode(value, objTypeCode, destination, options, ref objectsInPath, depth);\n            }\n            return true;\n        }\n\n        private bool SerializeWithTypeCode(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options, ref SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            var hasFormat = !StringHelpers.IsNullOrWhiteSpace(options.Format);\n            if ((options.FormatProvider != null || hasFormat) && (value is IFormattable formattable))\n            {\n                return SerializeWithFormatProvider(value, objTypeCode, destination, options, formattable, options.Format, hasFormat);\n            }\n            else\n            {\n                if (objTypeCode == TypeCode.Object)\n                {\n                    if (value is DateTimeOffset)\n                    {\n                        QuoteValue(destination, $\"{value:yyyy-MM-dd HH:mm:ss zzz}\");\n                        return true;\n                    }\n                    else\n                    {\n                        return SerializeObjectWithProperties(value, destination, options, ref objectsInPath, depth);\n                    }\n                }\n                else\n                {\n                    return SerializeSimpleTypeCodeValue(value, objTypeCode, destination, options);\n                }\n            }\n        }\n\n        private static SingleItemOptimizedHashSet<object>.SingleItemScopedInsert StartScope(ref SingleItemOptimizedHashSet<object> objectsInPath, object value)\n        {\n            return new SingleItemOptimizedHashSet<object>.SingleItemScopedInsert(value, ref objectsInPath, true, _referenceEqualsComparer);\n        }\n\n        private bool SerializeWithFormatProvider(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options, IFormattable formattable, string format, bool hasFormat)\n        {\n            int originalLength = destination.Length;\n\n            try\n            {\n                bool includeQuotes = !SkipQuotes(value, objTypeCode);\n                if (includeQuotes)\n                {\n                    destination.Append('\"');\n                }\n\n                if (hasFormat)\n                {\n                    var formatProvider = options.FormatProvider ?? _defaultFormatProvider;\n                    destination.AppendFormat(formatProvider, string.Concat(\"{0:\", format, \"}\"), value);\n                }\n                else\n                {\n                    //format provider passed without FormatProvider\n                    var str = formattable.ToString(\"\", options.FormatProvider);\n                    AppendStringEscape(destination, str, options.EscapeUnicode);\n                }\n\n                if (includeQuotes)\n                {\n                    destination.Append('\"');\n                }\n\n                return true;\n            }\n            catch\n            {\n                destination.Length = originalLength;\n                return false;\n            }\n        }\n\n        private void SerializeDictionaryObject(IDictionary value, StringBuilder destination, JsonSerializeOptions options, SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            bool first = true;\n\n            int nextDepth = objectsInPath.Count <= 1 ? depth : (depth + 1);\n            if (nextDepth > options.MaxRecursionLimit)\n            {\n                destination.Append(\"{}\");\n                return;\n            }\n\n            int originalLength;\n            destination.Append('{');\n            foreach (DictionaryEntry de in value)\n            {\n                originalLength = destination.Length;\n                if (originalLength > MaxJsonLength)\n                {\n                    break;\n                }\n\n                if (!first)\n                {\n                    destination.Append(',');\n                }\n\n                var itemKey = de.Key;\n                var itemKeyTypeCode = Convert.GetTypeCode(itemKey);\n                if (options.QuoteKeys)\n                {\n                    if (!SerializeObjectAsString(itemKey, itemKeyTypeCode, destination, options))\n                    {\n                        destination.Length = originalLength;\n                        continue;\n                    }\n                }\n                else\n                {\n                    if (!SerializeObject(itemKey, itemKeyTypeCode, destination, options, objectsInPath, nextDepth))\n                    {\n                        destination.Length = originalLength;\n                        continue;\n                    }\n                }\n\n                if (options.SanitizeDictionaryKeys)\n                {\n                    int quoteSkipCount = options.QuoteKeys ? 1 : 0;\n                    int keyEndIndex = destination.Length - quoteSkipCount;\n                    int keyStartIndex = originalLength + (first ? 0 : 1) + quoteSkipCount;\n                    if (!SanitizeDictionaryKey(destination, keyStartIndex, keyEndIndex - keyStartIndex))\n                    {\n                        destination.Length = originalLength;    // Empty keys are not allowed\n                        continue;\n                    }\n                }\n\n                destination.Append(':');\n\n                //only serialize, if key and value are serialized without error (e.g. due to reference loop)\n                var itemValue = de.Value;\n                var itemValueTypeCode = Convert.GetTypeCode(itemValue);\n                if (!SerializeObject(itemValue, itemValueTypeCode, destination, options, objectsInPath, nextDepth))\n                {\n                    destination.Length = originalLength;\n                }\n                else\n                {\n                    first = false;\n                }\n            }\n            destination.Append('}');\n        }\n\n        private static bool SanitizeDictionaryKey(StringBuilder destination, int keyStartIndex, int keyLength)\n        {\n            if (keyLength == 0)\n            {\n                return false;   // Empty keys are not allowed\n            }\n\n            int keyEndIndex = keyStartIndex + keyLength;\n            for (int i = keyStartIndex; i < keyEndIndex; ++i)\n            {\n                char keyChar = destination[i];\n                if (keyChar == '_' || char.IsLetterOrDigit(keyChar))\n                    continue;\n\n                destination[i] = '_';\n            }\n            return true;\n        }\n\n        private void SerializeCollectionObject(IEnumerable value, StringBuilder destination, JsonSerializeOptions options, SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            bool first = true;\n\n            int nextDepth = objectsInPath.Count <= 1 ? depth : (depth + 1); // Allow serialization of list-items \n            if (nextDepth > options.MaxRecursionLimit)\n            {\n                destination.Append(\"[]\");\n                return;\n            }\n\n            int originalLength;\n            destination.Append('[');\n            foreach (var val in value)\n            {\n                originalLength = destination.Length;\n                if (originalLength > MaxJsonLength)\n                {\n                    break;\n                }\n\n                if (!first)\n                {\n                    destination.Append(',');\n                }\n\n                if (!SerializeObject(val, Convert.GetTypeCode(val), destination, options, objectsInPath, nextDepth))\n                {\n                    destination.Length = originalLength;\n                }\n                else\n                {\n                    first = false;\n                }\n            }\n            destination.Append(']');\n        }\n\n        private bool SerializeTypeCodeValue(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options, SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            if (objTypeCode == TypeCode.Object)\n            {\n                if (value is DateTimeOffset)\n                {\n                    QuoteValue(destination, $\"{value:yyyy-MM-dd HH:mm:ss zzz}\");\n                    return true;\n                }\n                else\n                {\n                    return SerializeObjectWithProperties(value, destination, options, ref objectsInPath, depth);\n                }\n            }\n            else\n            {\n                return SerializeSimpleTypeCodeValue(value, objTypeCode, destination, options);\n            }\n        }\n\n        private bool SerializeObjectWithProperties(object value, StringBuilder destination, JsonSerializeOptions options, ref SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            int originalLength = destination.Length;\n            if (originalLength > MaxJsonLength)\n            {\n                return false;\n            }\n\n            if (depth < options.MaxRecursionLimit)\n            {\n                try\n                {\n                    if (ReferenceEquals(options, instance._serializeOptions) && value is Exception)\n                    {\n                        // Exceptions are seldom under control, and can include random Data-Dictionary-keys, so we sanitize by default\n                        options = instance._exceptionSerializeOptions;\n                    }\n\n                    var objectPropertyList = _objectReflectionCache.LookupObjectProperties(value);\n                    using (new SingleItemOptimizedHashSet<object>.SingleItemScopedInsert(value, ref objectsInPath, false, _referenceEqualsComparer))\n                    {\n                        return SerializeObjectProperties(objectPropertyList, destination, options, objectsInPath, depth);\n                    }\n                }\n                catch\n                {\n                    //nothing to add, so return is OK\n                    destination.Length = originalLength;\n                    return false;\n                }\n            }\n            else\n            {\n                return SerializeObjectAsString(value, TypeCode.Object, destination, options);\n            }\n        }\n\n        private bool SerializeSimpleTypeCodeValue(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options, bool forceQuotes = false)\n        {\n            if (objTypeCode == TypeCode.String || objTypeCode == TypeCode.Char)\n            {\n                destination.Append('\"');\n                AppendStringEscape(destination, value.ToString(), options.EscapeUnicode);\n                destination.Append('\"');\n            }\n            else if (IsNumericTypeCode(objTypeCode, false))\n            {\n                if (!options.EnumAsInteger && value is Enum enumValue)\n                {\n                    QuoteValue(destination, EnumAsString(enumValue));\n                }\n                else\n                {\n                    if (forceQuotes)\n                        destination.Append('\"');\n                    destination.AppendIntegerAsString(value, objTypeCode);\n                    if (forceQuotes)\n                        destination.Append('\"');\n                }\n            }\n            else\n            {\n                string str = XmlHelper.XmlConvertToString(value, objTypeCode);\n                if (str == null)\n                {\n                    return false;\n                }\n\n                if (!forceQuotes && SkipQuotes(value, objTypeCode))\n                {\n                    destination.Append(str);\n                }\n                else\n                {\n                    QuoteValue(destination, str);\n                }\n            }\n            return true;\n        }\n\n        private static CultureInfo CreateFormatProvider()\n        {\n#if SILVERLIGHT || NETSTANDARD1_0\n            var culture = new CultureInfo(\"en-US\");\n#else\n            var culture = new CultureInfo(\"en-US\", false);\n#endif\n            var numberFormat = culture.NumberFormat;\n            numberFormat.NumberGroupSeparator = string.Empty;\n            numberFormat.NumberDecimalSeparator = \".\";\n            numberFormat.NumberGroupSizes = new int[] { 0 };\n            return culture;\n        }\n\n        private static string QuoteValue(string value)\n        {\n            return string.Concat(\"\\\"\", value, \"\\\"\");\n        }\n\n        private static void QuoteValue(StringBuilder destination, string value)\n        {\n            destination.Append('\"');\n            destination.Append(value);\n            destination.Append('\"');\n        }\n\n        private string EnumAsString(Enum value)\n        {\n            string textValue;\n            if (!_enumCache.TryGetValue(value, out textValue))\n            {\n                textValue = Convert.ToString(value, CultureInfo.InvariantCulture);\n                _enumCache.TryAddValue(value, textValue);\n            }\n            return textValue;\n        }\n\n        /// <summary>\n        /// No quotes needed for this type?\n        /// </summary>\n        private static bool SkipQuotes(object value, TypeCode objTypeCode)\n        {\n            switch (objTypeCode)\n            {\n                case TypeCode.String: return false;\n                case TypeCode.Char: return false;\n                case TypeCode.DateTime: return false;\n                case TypeCode.Empty: return true;\n                case TypeCode.Boolean: return true;\n                case TypeCode.Decimal: return true;\n                case TypeCode.Double:\n                    {\n                        double dblValue = (double)value;\n                        return !double.IsNaN(dblValue) && !double.IsInfinity(dblValue);\n                    }\n                case TypeCode.Single:\n                    {\n                        float floatValue = (float)value;\n                        return !float.IsNaN(floatValue) && !float.IsInfinity(floatValue);\n                    }\n                default:\n                    return IsNumericTypeCode(objTypeCode, false);\n            }\n        }\n\n        /// <summary>\n        /// Checks the object <see cref=\"TypeCode\" /> if it is numeric\n        /// </summary>\n        /// <param name=\"objTypeCode\">TypeCode for the object</param>\n        /// <param name=\"includeDecimals\">Accept fractional types as numeric type.</param>\n        /// <returns></returns>\n        private static bool IsNumericTypeCode(TypeCode objTypeCode, bool includeDecimals)\n        {\n            switch (objTypeCode)\n            {\n                case TypeCode.Byte:\n                case TypeCode.SByte:\n                case TypeCode.Int16:\n                case TypeCode.Int32:\n                case TypeCode.Int64:\n                case TypeCode.UInt16:\n                case TypeCode.UInt32:\n                case TypeCode.UInt64:\n                    return true;\n                case TypeCode.Single:\n                case TypeCode.Double:\n                case TypeCode.Decimal:\n                    return includeDecimals;\n            }\n            return false;\n        }\n\n        /// <summary>\n        /// Checks input string if it needs JSON escaping, and makes necessary conversion\n        /// </summary>\n        /// <param name=\"destination\">Destination Builder</param>\n        /// <param name=\"text\">Input string</param>\n        /// <param name=\"escapeUnicode\">Should non-ascii characters be encoded</param>\n        /// <returns>JSON escaped string</returns>\n        internal static void AppendStringEscape(StringBuilder destination, string text, bool escapeUnicode)\n        {\n            if (string.IsNullOrEmpty(text))\n                return;\n\n            StringBuilder sb = null;\n\n            for (int i = 0; i < text.Length; ++i)\n            {\n                char ch = text[i];\n                if (!RequiresJsonEscape(ch, escapeUnicode))\n                {\n                    if (sb != null)\n                        sb.Append(ch);\n                    continue;\n                }\n                else if (sb == null)\n                {\n                    sb = destination;\n                    sb.Append(text, 0, i);\n                }\n\n                switch (ch)\n                {\n                    case '\"':\n                        sb.Append(\"\\\\\\\"\");\n                        break;\n\n                    case '\\\\':\n                        sb.Append(\"\\\\\\\\\");\n                        break;\n\n                    case '/':\n                        sb.Append(\"\\\\/\");\n                        break;\n\n                    case '\\b':\n                        sb.Append(\"\\\\b\");\n                        break;\n\n                    case '\\r':\n                        sb.Append(\"\\\\r\");\n                        break;\n\n                    case '\\n':\n                        sb.Append(\"\\\\n\");\n                        break;\n\n                    case '\\f':\n                        sb.Append(\"\\\\f\");\n                        break;\n\n                    case '\\t':\n                        sb.Append(\"\\\\t\");\n                        break;\n\n                    default:\n                        if (EscapeChar(ch, escapeUnicode))\n                        {\n                            sb.AppendFormat(CultureInfo.InvariantCulture, \"\\\\u{0:x4}\", (int)ch);\n                        }\n                        else\n                        {\n                            sb.Append(ch);\n                        }\n                        break;\n                }\n            }\n\n            if (sb == null)\n                destination.Append(text);   // Faster to make single Append\n        }\n\n        internal static bool RequiresJsonEscape(char ch, bool escapeUnicode)\n        {\n            if (!EscapeChar(ch, escapeUnicode))\n            {\n                switch (ch)\n                {\n                    case '\"':\n                    case '\\\\':\n                    case '/':\n                        return true;\n                    default:\n                        return false;\n                }\n            }\n            return true;\n        }\n\n        private static bool EscapeChar(char ch, bool escapeUnicode)\n        {\n            if (ch < 32)\n                return true;\n            else\n                return escapeUnicode && ch > 127;\n        }\n\n        private bool SerializeObjectProperties(ObjectReflectionCache.ObjectPropertyList objectPropertyList,StringBuilder destination, JsonSerializeOptions options,\n            SingleItemOptimizedHashSet<object> objectsInPath, int depth)\n        {\n            if (objectPropertyList.Count == 0)\n            { \n                //no props\n                return SerializeObjectAsString(objectPropertyList.ToString(), TypeCode.Object, destination, options);\n            }\n\n            destination.Append('{');\n\n            bool first = true;\n            foreach (var propertyValue in objectPropertyList)\n            {\n                var originalLength = destination.Length;\n\n                try\n                {\n                    if (propertyValue.Name != null && propertyValue.Value != null)\n                    {\n                        if (!first)\n                        {\n                            destination.Append(\", \");\n                        }\n\n                        if (options.QuoteKeys)\n                        {\n                            QuoteValue(destination, propertyValue.Name);\n                        }\n                        else\n                        {\n                            destination.Append(propertyValue.Name);\n                        }\n                        destination.Append(':');\n\n                        if (!SerializeObject(propertyValue.Value, propertyValue.TypeCode, destination, options, objectsInPath, depth + 1))\n                        {\n                            destination.Length = originalLength;\n                        }\n                        else\n                        {\n                            first = false;\n                        }\n                    }\n                }\n                catch\n                {\n                    //skip this property\n                    destination.Length = originalLength;\n                }\n            }\n\n            destination.Append('}');\n            return true;\n        }\n\n        private bool SerializeObjectAsString(object value, TypeCode objTypeCode, StringBuilder destination, JsonSerializeOptions options)\n        {\n            try\n            {\n                if (objTypeCode == TypeCode.Object)\n                {\n                    var str = Convert.ToString(value, CultureInfo.InvariantCulture);\n                    destination.Append('\"');\n                    AppendStringEscape(destination, str, options.EscapeUnicode);\n                    destination.Append('\"');\n                    return true;\n                }\n                else\n                {\n                    return SerializeSimpleTypeCodeValue(value, objTypeCode, destination, options, true);\n                }\n            }\n            catch\n            {\n                return false;\n            }\n        }\n    }\n}\n", "idx": 1, "id": 17920, "msg": "this is not needed anymore? Please do describe these changes in the PR title to make the review less of a surprise - thanks!", "proj": "NLog-NLog", "lang": ".cs"}
{"patch": "@@ -0,0 +1,47 @@\n+require 'spec_helper'\n+\n+describe Subscriber::CancellationsController do\n+  describe '#new without being signed in' do\n+    it 'redirects to sign in page' do\n+      get :new\n+\n+      expect(response).to redirect_to sign_in_path\n+    end\n+  end\n+\n+  describe '#create without being signed in' do\n+    it 'redirects to sign in page' do\n+      post :create\n+\n+      expect(response).to redirect_to sign_in_path\n+    end\n+  end\n+\n+  describe '#create with a subscription that has been charged' do\n+    it 'redirects to the refund page' do\n+      subscription = create(:subscription)\n+      sign_in_as(subscription.user.reload)\n+\n+      post :create\n+\n+      expect(response).to redirect_to new_subscriber_refund_path\n+    end\n+  end\n+\n+  describe '#create with a subscription that has never been charged' do\n+    it 'redirects to the account page' do\n+      user = build_stubbed(:user)\n+      subscription = stub(last_charge: nil)\n+      user.stubs(subscription: subscription)\n+      Cancellation.stubs(:new).returns(stub(schedule: nil))\n+      sign_in_as(user)\n+\n+      post :create\n+\n+      expect(Cancellation).to have_received(:new)\n+        .with(subscription)\n+      expect(subscription).to have_received(:last_charge)\n+      expect(response).to redirect_to my_account_path\n+    end\n+  end\n+end", "y": 1, "oldf": "", "idx": 1, "id": 9304, "msg": "Any chance we can use `build_stubbed` here?", "proj": "thoughtbot-upcase", "lang": "rb"}
{"patch": "@@ -2,6 +2,8 @@\n     requester_name: @proposal.requester.full_name,\n     step_type_noun: @step.noun) %>\n \n+<%= t(\"mailer.step_mailer.proposal_notification.step_status_html\") %>\n+\n <%= @step.action_name %>\n <%= generate_approve_url(@step) %>\n ", "y": 1, "oldf": "<%= t(\"mailer.step_mailer.proposal_notification.header\",\n    requester_name: @proposal.requester.full_name,\n    step_type_noun: @step.noun) %>\n\n<%= @step.action_name %>\n<%= generate_approve_url(@step) %>\n\n<%= t(\"mailer.step_mailer.proposal_notification.cta\") %>\n<%= proposal_url(@proposal, anchor: \"comments\") %>\n", "idx": 1, "id": 16829, "msg": "if this locale has `html` in it, we want to use something else for the text-only version of the email", "proj": "18F-C2", "lang": "rb"}
{"patch": "@@ -60,7 +60,7 @@ public class ProtocolHandshake {\n     parameters.append(\",\");\n     amendOssParamters(parameters, des, req);\n     parameters.append(\"}\");\n-    LOG.info(\"Attempting bi-dialect session, assuming Postel's Law holds true on the remote end\");\n+    LOG.fine(\"Attempting bi-dialect session, assuming Postel's Law holds true on the remote end\");\n     Optional<Result> result = createSession(client, parameters);\n \n     // Assume a fragile OSS webdriver implementation", "y": 0, "oldf": "// Licensed to the Software Freedom Conservancy (SFC) under one\n// or more contributor license agreements.  See the NOTICE file\n// distributed with this work for additional information\n// regarding copyright ownership.  The SFC licenses this file\n// to you under the Apache License, Version 2.0 (the\n// \"License\"); you may not use this file except in compliance\n// with the License.  You may obtain a copy of the License at\n//\n//   http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing,\n// software distributed under the License is distributed on an\n// \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n// KIND, either express or implied.  See the License for the\n// specific language governing permissions and limitations\n// under the License.\n\npackage org.openqa.selenium.remote;\n\nimport static com.google.common.base.Charsets.UTF_8;\nimport static com.google.common.net.HttpHeaders.CONTENT_LENGTH;\nimport static com.google.common.net.HttpHeaders.CONTENT_TYPE;\nimport static com.google.common.net.MediaType.JSON_UTF_8;\n\nimport com.google.common.base.Preconditions;\n\nimport org.openqa.selenium.Capabilities;\nimport org.openqa.selenium.SessionNotCreatedException;\nimport org.openqa.selenium.remote.http.HttpClient;\nimport org.openqa.selenium.remote.http.HttpMethod;\nimport org.openqa.selenium.remote.http.HttpRequest;\nimport org.openqa.selenium.remote.http.HttpResponse;\n\nimport java.io.IOException;\nimport java.net.HttpURLConnection;\nimport java.util.HashMap;\nimport java.util.Map;\nimport java.util.Optional;\nimport java.util.logging.Logger;\n\npublic class ProtocolHandshake {\n\n  private final static Logger LOG = Logger.getLogger(ProtocolHandshake.class.getName());\n\n  public Result createSession(HttpClient client, Command command)\n    throws IOException {\n    // Avoid serialising the capabilities too many times. Things like profiles are expensive.\n\n    Capabilities desired = (Capabilities) command.getParameters().get(\"desiredCapabilities\");\n    desired = desired == null ? new DesiredCapabilities() : desired;\n    Capabilities required = (Capabilities) command.getParameters().get(\"requiredCapabilities\");\n    required = required == null ? new DesiredCapabilities() : required;\n\n    String des = new BeanToJsonConverter().convert(desired);\n    String req = new BeanToJsonConverter().convert(required);\n\n    // Assume the remote end obeys the robustness principle.\n    StringBuilder parameters = new StringBuilder(\"{\");\n    amendW3CParameters(parameters, des, req);\n    parameters.append(\",\");\n    amendOssParamters(parameters, des, req);\n    parameters.append(\"}\");\n    LOG.info(\"Attempting bi-dialect session, assuming Postel's Law holds true on the remote end\");\n    Optional<Result> result = createSession(client, parameters);\n\n    // Assume a fragile OSS webdriver implementation\n    if (!result.isPresent()) {\n      parameters = new StringBuilder(\"{\");\n      amendOssParamters(parameters, des, req);\n      parameters.append(\"}\");\n      LOG.info(\"Falling back to original OSS JSON Wire Protocol.\");\n      result = createSession(client, parameters);\n    }\n\n    // Assume a fragile w3c implementation\n    if (!result.isPresent()) {\n      parameters = new StringBuilder(\"{\");\n      amendW3CParameters(parameters, des, req);\n      parameters.append(\"}\");\n      LOG.info(\"Falling back to straight W3C remote end connection\");\n      result = createSession(client, parameters);\n    }\n\n    if (result.isPresent()) {\n      Result toReturn = result.get();\n      LOG.info(String.format(\"Detected dialect: %s\", toReturn.dialect));\n      return toReturn;\n    }\n\n    throw new SessionNotCreatedException(\n      String.format(\n        \"Unable to create new remote session. \" +\n        \"desired capabilities = %s, required capabilities = %s\",\n        desired,\n        required));\n  }\n\n  private Optional<Result> createSession(HttpClient client, StringBuilder params)\n    throws IOException {\n    // Create the http request and send it\n    HttpRequest request = new HttpRequest(HttpMethod.POST, \"/session\");\n    String content = params.toString();\n    byte[] data = content.getBytes(UTF_8);\n\n    request.setHeader(CONTENT_LENGTH, String.valueOf(data.length));\n    request.setHeader(CONTENT_TYPE, JSON_UTF_8.toString());\n    request.setContent(data);\n    HttpResponse response = client.execute(request, true);\n\n    Map<?, ?> jsonBlob = null;\n    String resultString = response.getContentString();\n    try {\n      jsonBlob = new JsonToBeanConverter().convert(Map.class, resultString);\n    } catch (ClassCastException e) {\n      LOG.info(\"Unable to parse response from server: \" + resultString);\n      return Optional.empty();\n    } catch (JsonException e) {\n      // Fine. Handle that below\n    }\n\n    if (jsonBlob == null) {\n      jsonBlob = new HashMap<>();\n    }\n\n    // If the result looks positive, return the result.\n    Object sessionId = jsonBlob.get(\"sessionId\");\n    Object value = jsonBlob.get(\"value\");\n    Object w3cError = jsonBlob.get(\"error\");\n    Object ossStatus = jsonBlob.get(\"status\");\n    Map<String, ?> capabilities = null;\n    if (value != null && value instanceof Map) {\n      capabilities = (Map<String, ?>) value;\n    } else if (value != null && value instanceof Capabilities) {\n      capabilities = ((Capabilities) capabilities).asMap();\n    }\n\n    if (response.getStatus() == HttpURLConnection.HTTP_OK) {\n      if (sessionId != null && capabilities != null) {\n        Dialect dialect = ossStatus == null ? Dialect.W3C : Dialect.OSS;\n        return Optional.of(\n          new Result(dialect, String.valueOf(sessionId), capabilities));\n      }\n    }\n\n    // If the result was an error that we believe has to do with the remote end failing to start the\n    // session, create an exception and throw it.\n    Response tempResponse = null;\n    if (\"session not created\".equals(w3cError)) {\n      tempResponse = new Response(null);\n      tempResponse.setStatus(ErrorCodes.SESSION_NOT_CREATED);\n      tempResponse.setValue(jsonBlob);\n    } else if (\n      ossStatus instanceof Number &&\n      ((Number) ossStatus).intValue() == ErrorCodes.SESSION_NOT_CREATED) {\n      tempResponse = new Response(null);\n      tempResponse.setStatus(ErrorCodes.SESSION_NOT_CREATED);\n      tempResponse.setValue(jsonBlob);\n    }\n\n    if (tempResponse != null) {\n      new ErrorHandler(true).throwIfResponseFailed(tempResponse, 0);\n    }\n\n    // Otherwise, just return empty.\n    return Optional.empty();\n  }\n\n  private void amendW3CParameters(\n    StringBuilder params,\n    String desired,\n    String required) {\n    params.append(\"\\\"capabilities\\\": {\");\n    params.append(\"\\\"desiredCapabilities\\\": \").append(desired);\n    params.append(\",\");\n    params.append(\"\\\"requiredCapabilities\\\": \").append(required);\n    params.append(\"}\");\n  }\n\n  private void amendOssParamters(\n    StringBuilder params,\n    String desired,\n    String required) {\n    params.append(\"\\\"desiredCapabilities\\\": \").append(desired);\n    params.append(\",\");\n    params.append(\"\\\"requiredCapabilities\\\": \").append(required);\n  }\n\n\n  public class Result {\n    private final Dialect dialect;\n    private final Map<String, ?> capabilities;\n    private final SessionId sessionId;\n\n    private Result(Dialect dialect, String sessionId, Map<String, ?> capabilities) {\n      this.dialect = dialect;\n      this.sessionId = new SessionId(Preconditions.checkNotNull(sessionId));\n      this.capabilities = capabilities;\n    }\n\n    public Dialect getDialect() {\n      return dialect;\n    }\n\n    public Response createResponse() {\n      Response response = new Response(sessionId);\n      response.setValue(capabilities);\n      response.setStatus(ErrorCodes.SUCCESS);\n      return response;\n    }\n\n    @Override\n    public String toString() {\n      return String.format(\"%s: %s\", dialect, capabilities);\n    }\n  }\n}\n", "idx": 1, "id": 13862, "msg": "", "proj": "SeleniumHQ-selenium", "lang": "py"}
{"patch": "@@ -19,7 +19,7 @@ from databricks.koalas.testing.utils import ReusedSQLTestCase\n \n \n class ReprTests(ReusedSQLTestCase):\n-    max_display_count = 123\n+    max_display_count = 23\n \n     @classmethod\n     def setUpClass(cls):", "y": 1, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\nfrom databricks import koalas as ks\nfrom databricks.koalas.config import set_option, reset_option\nfrom databricks.koalas.testing.utils import ReusedSQLTestCase\n\n\nclass ReprTests(ReusedSQLTestCase):\n    max_display_count = 123\n\n    @classmethod\n    def setUpClass(cls):\n        super(ReprTests, cls).setUpClass()\n        set_option(\"display.max_rows\", ReprTests.max_display_count)\n\n    @classmethod\n    def tearDownClass(cls):\n        reset_option(\"display.max_rows\")\n        super(ReprTests, cls).tearDownClass()\n\n    def test_repr_dataframe(self):\n        kdf = ks.range(ReprTests.max_display_count)\n        self.assertTrue(\"Showing only the first\" not in repr(kdf))\n        self.assert_eq(repr(kdf), repr(kdf.to_pandas()))\n\n        kdf = ks.range(ReprTests.max_display_count + 1)\n        self.assertTrue(\"Showing only the first\" in repr(kdf))\n\n        set_option(\"display.max_rows\", None)\n        try:\n            kdf = ks.range(ReprTests.max_display_count + 1)\n            self.assert_eq(repr(kdf), repr(kdf.to_pandas()))\n        finally:\n            set_option(\"display.max_rows\", ReprTests.max_display_count)\n\n    def test_repr_series(self):\n        kser = ks.range(ReprTests.max_display_count).id\n        self.assertTrue(\"Showing only the first\" not in repr(kser))\n        self.assert_eq(repr(kser), repr(kser.to_pandas()))\n\n        kser = ks.range(ReprTests.max_display_count + 1).id\n        self.assertTrue(\"Showing only the first\" in repr(kser))\n\n        set_option(\"display.max_rows\", None)\n        try:\n            kser = ks.range(ReprTests.max_display_count + 1).id\n            self.assert_eq(repr(kser), repr(kser.to_pandas()))\n        finally:\n            set_option(\"display.max_rows\", ReprTests.max_display_count)\n\n    def test_repr_indexes(self):\n        kdf = ks.range(ReprTests.max_display_count)\n        kidx = kdf.index\n        self.assertTrue(\"Showing only the first\" not in repr(kidx))\n        self.assert_eq(repr(kidx), repr(kidx.to_pandas()))\n\n        kdf = ks.range(ReprTests.max_display_count + 1)\n        kidx = kdf.index\n        self.assertTrue(\"Showing only the first\" in repr(kidx))\n\n        set_option(\"display.max_rows\", None)\n        try:\n            kdf = ks.range(ReprTests.max_display_count + 1)\n            kidx = kdf.index\n            self.assert_eq(repr(kidx), repr(kidx.to_pandas()))\n        finally:\n            set_option(\"display.max_rows\", ReprTests.max_display_count)\n\n    def test_html_repr(self):\n        kdf = ks.range(ReprTests.max_display_count)\n        self.assertTrue(\"Showing only the first\" not in kdf._repr_html_())\n        self.assertEqual(kdf._repr_html_(), kdf.to_pandas()._repr_html_())\n\n        kdf = ks.range(ReprTests.max_display_count + 1)\n        self.assertTrue(\"Showing only the first\" in kdf._repr_html_())\n\n        set_option(\"display.max_rows\", None)\n        try:\n            kdf = ks.range(ReprTests.max_display_count + 1)\n            self.assertEqual(kdf._repr_html_(), kdf.to_pandas()._repr_html_())\n        finally:\n            set_option(\"display.max_rows\", ReprTests.max_display_count)\n", "idx": 1, "id": 11560, "msg": "pandas' `display.max_rows` value is 60 by default. So, lowering then that will test the changes correctly.", "proj": "databricks-koalas", "lang": "py"}
{"patch": "@@ -985,7 +985,7 @@ public interface Iterator<T> extends java.util.Iterator<T>, Traversable<T> {\n      */\n     static <T> Iterator<T> iterate(Supplier<? extends Option<? extends T>> supplier) {\n         Objects.requireNonNull(supplier, \"supplier is null\");\n-        return new AbstractIterator<T>() {\n+        return new Iterator<T>() {\n             Option<? extends T> nextOption;\n \n             @Override", "y": 0, "oldf": "/*  __    __  __  __    __  ___\n * \\  \\  /  /    \\  \\  /  /  __/\n *  \\  \\/  /  /\\  \\  \\/  /  /\n *   \\____/__/  \\__\\____/__/\n *\n * Copyright 2014-2019 Vavr, http://vavr.io\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage io.vavr.collection;\n\nimport io.vavr.*;\nimport io.vavr.collection.IteratorModule.*;\nimport io.vavr.control.Option;\n\nimport java.math.BigDecimal;\nimport java.util.*;\nimport java.util.function.*;\n\nimport static io.vavr.collection.IteratorModule.BigDecimalHelper.areEqual;\nimport static io.vavr.collection.IteratorModule.BigDecimalHelper.asDecimal;\nimport static java.lang.Double.NEGATIVE_INFINITY;\nimport static java.lang.Double.POSITIVE_INFINITY;\nimport static java.math.RoundingMode.HALF_UP;\n\n/**\n * {@code io.vavr.collection.Iterator} is a compositional replacement for {@code java.util.Iterator}\n * whose purpose is to iterate <em>once</em> over a sequence of elements.\n * <p>\n * <strong>Note:</strong> Iterators encapsulate mutable state.\n * They are not meant to be used concurrently by different threads. Do not reuse Iterators, e.g. after passing to\n * {@linkplain io.vavr.collection.List#ofAll(Iterable)}.\n * <p>\n * There are two abstract methods: {@code hasNext} for checking if there is a next element available,\n * and {@code next} which removes the next element from the iterator and returns it. They can be called\n * an arbitrary amount of times. If {@code hasNext} returns false, a call of {@code next} will throw\n * a {@code NoSuchElementException}.\n * <p>\n * <strong>Caution: Other methods than {@code hasNext} and {@code next} can be called only once (exclusively).\n * More specifically, after calling a method it cannot be guaranteed that the next call will succeed.</strong>\n * <p>\n * An Iterator that can be only used once because it is a traversal pointer into a collection, and not a collection\n * itself.\n *\n * @param <T> Component type\n * @author Daniel Dietrich\n */\n// DEV-NOTE: we prefer returning empty() over this if !hasNext() == true in order to free memory.\npublic interface Iterator<T> extends java.util.Iterator<T>, Traversable<T> {\n\n    /**\n     * Creates an Iterator which traverses along the concatenation of the given iterables.\n     *\n     * @param iterables The iterables\n     * @param <T>       Component type.\n     * @return A new {@code io.vavr.collection.Iterator}\n     */\n    @SuppressWarnings(\"varargs\")\n    @SafeVarargs\n    static <T> Iterator<T> concat(Iterable<? extends T>... iterables) {\n        Objects.requireNonNull(iterables, \"iterables is null\");\n        if (iterables.length == 0) {\n            return empty();\n        } else {\n            return new ConcatIterator<>(of(iterables).map(Iterable::iterator));\n        }\n    }\n\n    /**\n     * Creates an Iterator which traverses along the concatenation of the given iterables.\n     *\n     * @param iterables The iterable of iterables\n     * @param <T>       Component type.\n     * @return A new {@code io.vavr.collection.Iterator}\n     */\n    static <T> Iterator<T> concat(Iterable<? extends Iterable<? extends T>> iterables) {\n        Objects.requireNonNull(iterables, \"iterables is null\");\n        final Iterator<Iterator<T>> iterators = ofAll(iterables).map(Iterator::ofAll);\n        if (!iterators.hasNext()) {\n            return empty();\n        } else {\n            return new ConcatIterator<>(iterators);\n        }\n    }\n\n    /**\n     * Returns the empty Iterator.\n     *\n     * @param <T> Component type\n     * @return The empty Iterator\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> Iterator<T> empty() {\n        return (Iterator<T>) EmptyIterator.INSTANCE;\n    }\n\n    /**\n     * Narrows a widened {@code Iterator<? extends T>} to {@code Iterator<T>}\n     * by performing a type-safe cast. This is eligible because immutable/read-only\n     * collections are covariant.\n     *\n     * @param iterator An {@code Iterator}.\n     * @param <T>      Component type of the {@code Iterator}.\n     * @return the given {@code iterator} instance as narrowed type {@code Iterator<T>}.\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> Iterator<T> narrow(Iterator<? extends T> iterator) {\n        return (Iterator<T>) iterator;\n    }\n\n    /**\n     * Creates an Iterator which traverses one element.\n     *\n     * @param element An element\n     * @param <T>     Component type.\n     * @return A new Iterator\n     */\n    static <T> Iterator<T> of(T element) {\n        return new AbstractIterator<T>() {\n\n            boolean hasNext = true;\n\n            @Override\n            public boolean hasNext() {\n                return hasNext;\n            }\n\n            @Override\n            public T getNext() {\n                hasNext = false;\n                return element;\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator which traverses the given elements.\n     *\n     * @param elements Zero or more elements\n     * @param <T>      Component type\n     * @return A new Iterator\n     */\n    @SafeVarargs\n    static <T> Iterator<T> of(T... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        if (elements.length == 0) {\n            return empty();\n        } else {\n            return new AbstractIterator<T>() {\n\n                int index = 0;\n\n                @Override\n                public boolean hasNext() {\n                    return index < elements.length;\n                }\n\n                @Override\n                public T getNext() {\n                    return elements[index++];\n                }\n            };\n        }\n    }\n\n    /**\n     * Creates an Iterator based on the given Iterable. This is a convenience method for\n     * {@code Iterator.ofAll(iterable.iterator()}.\n     *\n     * @param iterable A {@link Iterable}\n     * @param <T>      Component type.\n     * @return A new {@code io.vavr.collection.Iterator}\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> Iterator<T> ofAll(Iterable<? extends T> iterable) {\n        Objects.requireNonNull(iterable, \"iterable is null\");\n        if (iterable instanceof Iterator) {\n            return (Iterator<T>) iterable;\n        } else {\n            return ofAll(iterable.iterator());\n        }\n    }\n\n    /**\n     * Creates an Iterator based on the given Iterator by\n     * delegating calls of {@code hasNext()} and {@code next()} to it.\n     *\n     * @param iterator A {@link java.util.Iterator}\n     * @param <T>      Component type.\n     * @return A new {@code io.vavr.collection.Iterator}\n     */\n    @SuppressWarnings(\"unchecked\")\n    static <T> Iterator<T> ofAll(java.util.Iterator<? extends T> iterator) {\n        Objects.requireNonNull(iterator, \"iterator is null\");\n        if (iterator instanceof Iterator) {\n            return (Iterator<T>) iterator;\n        } else {\n            return new AbstractIterator<T>() {\n\n                @Override\n                public boolean hasNext() {\n                    return iterator.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    return iterator.next();\n                }\n            };\n        }\n    }\n\n    /**\n     * Creates an Iterator from boolean values.\n     *\n     * @param elements boolean values\n     * @return A new Iterator of Boolean values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Boolean> ofAll(boolean... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Boolean>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Boolean getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from byte values.\n     *\n     * @param elements byte values\n     * @return A new Iterator of Byte values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Byte> ofAll(byte... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Byte>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Byte getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from char values.\n     *\n     * @param elements char values\n     * @return A new Iterator of Character values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Character> ofAll(char... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Character>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Character getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates ann Iterator from double values.\n     *\n     * @param elements double values\n     * @return A new Iterator of Double values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Double> ofAll(double... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Double>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Double getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from float values.\n     *\n     * @param elements float values\n     * @return A new Iterator of Float values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Float> ofAll(float... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Float>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Float getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from int values.\n     *\n     * @param elements int values\n     * @return A new Iterator of Integer values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Integer> ofAll(int... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Integer>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Integer getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from long values.\n     *\n     * @param elements long values\n     * @return A new Iterator of Long values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Long> ofAll(long... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Long>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Long getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Creates an Iterator from short values.\n     *\n     * @param elements short values\n     * @return A new Iterator of Short values\n     * @throws NullPointerException if elements is null\n     */\n    static Iterator<Short> ofAll(short... elements) {\n        Objects.requireNonNull(elements, \"elements is null\");\n        return new AbstractIterator<Short>() {\n            int i = 0;\n\n            @Override\n            public boolean hasNext() {\n                return i < elements.length;\n            }\n\n            @Override\n            public Short getNext() {\n                return elements[i++];\n            }\n        };\n    }\n\n    /**\n     * Returns an Iterator on a sequence of {@code n} values of a given Function {@code f}\n     * over a range of integer values from 0 to {@code n - 1}.\n     *\n     * @param <T> Component type of the Iterator\n     * @param n   The number of elements\n     * @param f   The Function computing element values\n     * @return An Iterator on a sequence of elements {@code f(0),f(1), ..., f(n - 1)}\n     * @throws NullPointerException if {@code f} is null\n     */\n    static <T> Iterator<T> tabulate(int n, Function<? super Integer, ? extends T> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return io.vavr.collection.Collections.tabulate(n, f);\n    }\n\n    /**\n     * Returns an Iterator on a sequence of {@code n} values supplied by a given Supplier {@code s}.\n     *\n     * @param <T> Component type of the Iterator\n     * @param n   The number of elements\n     * @param s   The Supplier computing element values\n     * @return An iterator on a sequence of {@code n} elements, where each element contains the result supplied by {@code s}.\n     * @throws NullPointerException if {@code s} is null\n     */\n    static <T> Iterator<T> fill(int n, Supplier<? extends T> s) {\n        Objects.requireNonNull(s, \"s is null\");\n        return io.vavr.collection.Collections.fill(n, s);\n    }\n\n    /**\n     * Returns a Iterator containing {@code n} times the given {@code element}\n     *\n     * @param <T>     Component type of the Iterator\n     * @param n       The number of elements\n     * @param element The element\n     * @return An iterator of {@code n} sequence elements, where each element is the given {@code element}.\n     */\n    static <T> Iterator<T> fill(int n, T element) {\n        return io.vavr.collection.Collections.fillObject(n, element);\n    }\n\n    /**\n     * Creates an Iterator of characters starting from {@code from}, extending to {@code toExclusive - 1}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.range('a', 'c')  // = ('a', 'b')\n     * Iterator.range('c', 'a')  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first character\n     * @param toExclusive the successor of the last character\n     * @return a range of characters as specified or the empty range if {@code from >= toExclusive}\n     */\n    static Iterator<Character> range(char from, char toExclusive) {\n        return rangeBy(from, toExclusive, 1);\n    }\n\n    /**\n     * Creates an Iterator of characters starting from {@code from}, extending to {@code toExclusive - 1},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeBy('a', 'c', 1)  // = ('a', 'b')\n     * Iterator.rangeBy('a', 'd', 2)  // = ('a', 'c')\n     * Iterator.rangeBy('d', 'a', -2) // = ('d', 'b')\n     * Iterator.rangeBy('d', 'a', 2)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first character\n     * @param toExclusive the successor of the last character if step &gt; 0, the predecessor of the last character if step &lt; 0\n     * @param step        the step\n     * @return a range of characters as specified or the empty range if {@code signum(step) == signum(from - toExclusive)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Character> rangeBy(char from, char toExclusive, int step) {\n        return rangeBy((int) from, (int) toExclusive, step).map(i -> (char) i.shortValue());\n    }\n\n    @GwtIncompatible(\"BigDecimalHelper is GwtIncompatible\")\n    static Iterator<Double> rangeBy(double from, double toExclusive, double step) {\n        final BigDecimal fromDecimal = asDecimal(from), toDecimal = asDecimal(toExclusive), stepDecimal = asDecimal(step);\n        return rangeBy(fromDecimal, toDecimal, stepDecimal).map(BigDecimal::doubleValue);\n    }\n\n    static Iterator<BigDecimal> rangeBy(BigDecimal from, BigDecimal toExclusive, BigDecimal step) {\n        if (step.signum() == 0) {\n            throw new IllegalArgumentException(\"step cannot be 0\");\n        } else if (areEqual(from, toExclusive) || step.signum() == from.subtract(toExclusive).signum()) {\n            return empty();\n        } else {\n            if (step.signum() > 0) {\n                return new AbstractIterator<BigDecimal>() {\n                    BigDecimal i = from;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i.compareTo(toExclusive) < 0;\n                    }\n\n                    @Override\n                    public BigDecimal getNext() {\n                        final BigDecimal next = this.i;\n                        this.i = next.add(step);\n                        return next;\n                    }\n                };\n            } else {\n                return new AbstractIterator<BigDecimal>() {\n                    BigDecimal i = from;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i.compareTo(toExclusive) > 0;\n                    }\n\n                    @Override\n                    public BigDecimal getNext() {\n                        final BigDecimal next = this.i;\n                        this.i = next.add(step);\n                        return next;\n                    }\n                };\n            }\n        }\n    }\n\n    /**\n     * Creates an Iterator of int numbers starting from {@code from}, extending to {@code toExclusive - 1}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.range(0, 0)  // = ()\n     * Iterator.range(2, 0)  // = ()\n     * Iterator.range(-2, 2) // = (-2, -1, 0, 1)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @return a range of int values as specified or the empty range if {@code from >= toExclusive}\n     */\n    static Iterator<Integer> range(int from, int toExclusive) {\n        return rangeBy(from, toExclusive, 1);\n    }\n\n    /**\n     * Creates an Iterator of int numbers starting from {@code from}, extending to {@code toExclusive - 1},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeBy(1, 3, 1)  // = (1, 2)\n     * Iterator.rangeBy(1, 4, 2)  // = (1, 3)\n     * Iterator.rangeBy(4, 1, -2) // = (4, 2)\n     * Iterator.rangeBy(4, 1, 2)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1 if step &gt; 0, the last number - 1 if step &lt; 0\n     * @param step        the step\n     * @return a range of long values as specified or the empty range if {@code (from == toExclusive) || (step * (from - toExclusive) > 0)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Integer> rangeBy(int from, int toExclusive, int step) {\n        final int toInclusive = toExclusive - (step > 0 ? 1 : -1);\n        return rangeClosedBy(from, toInclusive, step);\n    }\n\n    /**\n     * Creates an Iterator of long numbers starting from {@code from}, extending to {@code toExclusive - 1}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.range(0L, 0L)  // = ()\n     * Iterator.range(2L, 0L)  // = ()\n     * Iterator.range(-2L, 2L) // = (-2L, -1L, 0L, 1L)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1\n     * @return a range of long values as specified or the empty range if {@code from >= toExclusive}\n     */\n    static Iterator<Long> range(long from, long toExclusive) {\n        return rangeBy(from, toExclusive, 1);\n    }\n\n    /**\n     * Creates an Iterator of long numbers starting from {@code from}, extending to {@code toExclusive - 1},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeBy(1L, 3L, 1L)  // = (1L, 2L)\n     * Iterator.rangeBy(1L, 4L, 2L)  // = (1L, 3L)\n     * Iterator.rangeBy(4L, 1L, -2L) // = (4L, 2L)\n     * Iterator.rangeBy(4L, 1L, 2L)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toExclusive the last number + 1 if step &gt; 0, the last number - 1 if step &lt; 0\n     * @param step        the step\n     * @return a range of long values as specified or the empty range if {@code (from == toExclusive) || (step * (from - toExclusive) > 0)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Long> rangeBy(long from, long toExclusive, long step) {\n        final long toInclusive = toExclusive - (step > 0 ? 1 : -1);\n        return rangeClosedBy(from, toInclusive, step);\n    }\n\n    /**\n     * Creates an Iterator of characters starting from {@code from}, extending to {@code toInclusive}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosed('a', 'c')  // = ('a', 'b', 'c')\n     * Iterator.rangeClosed('c', 'a')  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first character\n     * @param toInclusive the last character\n     * @return a range of characters as specified or the empty range if {@code from > toInclusive}\n     */\n    static Iterator<Character> rangeClosed(char from, char toInclusive) {\n        return rangeClosedBy(from, toInclusive, 1);\n    }\n\n    /**\n     * Creates an Iterator of characters starting from {@code from}, extending to {@code toInclusive},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosedBy('a', 'c', 1)  // = ('a', 'b', 'c')\n     * Iterator.rangeClosedBy('a', 'd', 2)  // = ('a', 'c')\n     * Iterator.rangeClosedBy('d', 'a', -2) // = ('d', 'b')\n     * Iterator.rangeClosedBy('d', 'a', 2)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first character\n     * @param toInclusive the last character\n     * @param step        the step\n     * @return a range of characters as specified or the empty range if {@code signum(step) == signum(from - toInclusive)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Character> rangeClosedBy(char from, char toInclusive, int step) {\n        return rangeClosedBy((int) from, (int) toInclusive, step).map(i -> (char) i.shortValue());\n    }\n\n    @GwtIncompatible\n    static Iterator<Double> rangeClosedBy(double from, double toInclusive, double step) {\n        if (from == toInclusive) {\n            return of(from);\n        }\n\n        final double toExclusive = (step > 0) ? Math.nextUp(toInclusive) : Math.nextDown(toInclusive);\n        return rangeBy(from, toExclusive, step);\n    }\n\n    /**\n     * Creates an Iterator of int numbers starting from {@code from}, extending to {@code toInclusive}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosed(0, 0)  // = (0)\n     * Iterator.rangeClosed(2, 0)  // = ()\n     * Iterator.rangeClosed(-2, 2) // = (-2, -1, 0, 1, 2)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @return a range of int values as specified or the empty range if {@code from > toInclusive}\n     */\n    static Iterator<Integer> rangeClosed(int from, int toInclusive) {\n        return rangeClosedBy(from, toInclusive, 1);\n    }\n\n    /**\n     * Creates an Iterator of int numbers starting from {@code from}, extending to {@code toInclusive},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosedBy(1, 3, 1)  // = (1, 2, 3)\n     * Iterator.rangeClosedBy(1, 4, 2)  // = (1, 3)\n     * Iterator.rangeClosedBy(4, 1, -2) // = (4, 2)\n     * Iterator.rangeClosedBy(4, 1, 2)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @param step        the step\n     * @return a range of int values as specified or the empty range if {@code signum(step) == signum(from - toInclusive)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Integer> rangeClosedBy(int from, int toInclusive, int step) {\n        if (step == 0) {\n            throw new IllegalArgumentException(\"step cannot be 0\");\n        } else if (from == toInclusive) {\n            return of(from);\n        } else if (Integer.signum(step) == Integer.signum(from - toInclusive)) {\n            return empty();\n        } else {\n            final int end = toInclusive - step;\n            if (step > 0) {\n                return new AbstractIterator<Integer>() {\n                    int i = from - step;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i <= end;\n                    }\n\n                    @Override\n                    public Integer getNext() {\n                        return i += step;\n                    }\n                };\n            } else {\n                return new AbstractIterator<Integer>() {\n                    int i = from - step;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i >= end;\n                    }\n\n                    @Override\n                    public Integer getNext() {\n                        return i += step;\n                    }\n                };\n            }\n        }\n    }\n\n    /**\n     * Creates an Iterator of long numbers starting from {@code from}, extending to {@code toInclusive}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosed(0L, 0L)  // = (0L)\n     * Iterator.rangeClosed(2L, 0L)  // = ()\n     * Iterator.rangeClosed(-2L, 2L) // = (-2L, -1L, 0L, 1L, 2L)\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @return a range of long values as specified or the empty range if {@code from > toInclusive}\n     */\n    static Iterator<Long> rangeClosed(long from, long toInclusive) {\n        return rangeClosedBy(from, toInclusive, 1L);\n    }\n\n    /**\n     * Creates an Iterator of long numbers starting from {@code from}, extending to {@code toInclusive},\n     * with {@code step}.\n     * <p>\n     * Examples:\n     * <pre>\n     * <code>\n     * Iterator.rangeClosedBy(1L, 3L, 1L)  // = (1L, 2L, 3L)\n     * Iterator.rangeClosedBy(1L, 4L, 2L)  // = (1L, 3L)\n     * Iterator.rangeClosedBy(4L, 1L, -2L) // = (4L, 2L)\n     * Iterator.rangeClosedBy(4L, 1L, 2L)  // = ()\n     * </code>\n     * </pre>\n     *\n     * @param from        the first number\n     * @param toInclusive the last number\n     * @param step        the step\n     * @return a range of int values as specified or the empty range if {@code signum(step) == signum(from - toInclusive)}.\n     * @throws IllegalArgumentException if {@code step} is zero\n     */\n    static Iterator<Long> rangeClosedBy(long from, long toInclusive, long step) {\n        if (step == 0) {\n            throw new IllegalArgumentException(\"step cannot be 0\");\n        } else if (from == toInclusive) {\n            return of(from);\n        } else if (Long.signum(step) == Long.signum(from - toInclusive)) {\n            return empty();\n        } else {\n            final long end = toInclusive - step;\n            if (step > 0) {\n                return new AbstractIterator<Long>() {\n                    long i = from - step;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i <= end;\n                    }\n\n                    @Override\n                    public Long getNext() {\n                        return i += step;\n                    }\n                };\n            } else {\n                return new AbstractIterator<Long>() {\n                    long i = from - step;\n\n                    @Override\n                    public boolean hasNext() {\n                        return i >= end;\n                    }\n\n                    @Override\n                    public Long getNext() {\n                        return i += step;\n                    }\n                };\n\n            }\n        }\n    }\n\n    /**\n     * Returns an infinite iterator of int values starting from {@code value}.\n     * <p>\n     * The {@code Iterator} extends to {@code Integer.MIN_VALUE} when passing {@code Integer.MAX_VALUE}.\n     *\n     * @param value a start int value\n     * @return a new {@code Iterator} of int values starting from {@code from}\n     */\n    static Iterator<Integer> from(int value) {\n        return new AbstractIterator<Integer>() {\n            private int next = value;\n\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public Integer getNext() {\n                return next++;\n            }\n        };\n    }\n\n    /**\n     * Returns an infinite iterator of int values starting from {@code value} and spaced by {@code step}.\n     * <p>\n     * The {@code Iterator} extends to {@code Integer.MIN_VALUE} when passing {@code Integer.MAX_VALUE}.\n     *\n     * @param value a start int value\n     * @param step  the step by which to advance on each iteration\n     * @return a new {@code Iterator} of int values starting from {@code from}\n     */\n    static Iterator<Integer> from(int value, int step) {\n        return new AbstractIterator<Integer>() {\n            private int next = value;\n\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public Integer getNext() {\n                final int result = next;\n                next += step;\n                return result;\n            }\n        };\n    }\n\n    /**\n     * Returns an infinite iterator of long values starting from {@code value}.\n     * <p>\n     * The {@code Iterator} extends to {@code Long.MIN_VALUE} when passing {@code Long.MAX_VALUE}.\n     *\n     * @param value a start long value\n     * @return a new {@code Iterator} of long values starting from {@code from}\n     */\n    static Iterator<Long> from(long value) {\n        return new AbstractIterator<Long>() {\n            private long next = value;\n\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public Long getNext() {\n                return next++;\n            }\n        };\n    }\n\n    /**\n     * Returns an infinite iterator of long values starting from {@code value} and spaced by {@code step}.\n     * <p>\n     * The {@code Iterator} extends to {@code Long.MIN_VALUE} when passing {@code Long.MAX_VALUE}.\n     *\n     * @param value a start long value\n     * @param step  the step by which to advance on each iteration\n     * @return a new {@code Iterator} of long values starting from {@code from}\n     */\n    static Iterator<Long> from(long value, long step) {\n        return new AbstractIterator<Long>() {\n            private long next = value;\n\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public Long getNext() {\n                final long result = next;\n                next += step;\n                return result;\n            }\n        };\n    }\n\n    /**\n     * Generates an infinite iterator using a value Supplier.\n     *\n     * @param supplier A Supplier of iterator values\n     * @param <T>      value type\n     * @return A new {@code Iterator}\n     */\n    static <T> Iterator<T> continually(Supplier<? extends T> supplier) {\n        Objects.requireNonNull(supplier, \"supplier is null\");\n        return new AbstractIterator<T>() {\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public T getNext() {\n                return supplier.get();\n            }\n        };\n    }\n\n    /**\n     * Creates an iterator that repeatedly invokes the supplier\n     * while it's a {@code Some} and end on the first {@code None}\n     *\n     * @param supplier A Supplier of iterator values\n     * @param <T> value type\n     * @return A new {@code Iterator}\n     * @throws NullPointerException if supplier produces null value\n     */\n    static <T> Iterator<T> iterate(Supplier<? extends Option<? extends T>> supplier) {\n        Objects.requireNonNull(supplier, \"supplier is null\");\n        return new AbstractIterator<T>() {\n            Option<? extends T> nextOption;\n\n            @Override\n            public boolean hasNext() {\n                if (nextOption == null) {\n                    nextOption = supplier.get();\n                }\n                return nextOption.isDefined();\n            }\n\n            @Override\n            public T getNext() {\n                final T next =  nextOption.get();\n                nextOption = null;\n                return next;\n            }\n        };\n    }\n\n    /**\n     * Generates an infinite iterator using a function to calculate the next value\n     * based on the previous.\n     *\n     * @param seed The first value in the iterator\n     * @param f    A function to calculate the next value based on the previous\n     * @param <T>  value type\n     * @return A new {@code Iterator}\n     */\n    static <T> Iterator<T> iterate(T seed, Function<? super T, ? extends T> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return new AbstractIterator<T>() {\n            Function<? super T, ? extends T> nextFunc = s -> {\n                nextFunc = f;\n                return seed;\n            };\n            T current = null;\n\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public T getNext() {\n                current = nextFunc.apply(current);\n                return current;\n            }\n        };\n    }\n\n    /**\n     * Creates an infinite iterator returning the given element.\n     *\n     * @param t   An element\n     * @param <T> Element type\n     * @return A new Iterator containing infinite {@code t}'s.\n     */\n    static <T> Iterator<T> continually(T t) {\n        return new AbstractIterator<T>() {\n            @Override\n            public boolean hasNext() {\n                return true;\n            }\n\n            @Override\n            public T getNext() {\n                return t;\n            }\n        };\n    }\n\n    // -- Additional methods of Iterator\n\n    @Override\n    default <R> Iterator<R> collect(PartialFunction<? super T, ? extends R> partialFunction) {\n        Objects.requireNonNull(partialFunction, \"partialFunction is null\");\n        return filter(partialFunction::isDefinedAt).map(partialFunction::apply);\n    }\n\n    // DEV-NOTE: cannot use arg Iterable, it would be ambiguous\n    default Iterator<T> concat(java.util.Iterator<? extends T> that) {\n        Objects.requireNonNull(that, \"that is null\");\n        if (!that.hasNext()) {\n            return this;\n        } else if (!hasNext()) {\n            return ofAll(that);\n        } else {\n            return concat(this, ofAll(that));\n        }\n    }\n\n    /**\n     * Inserts an element between all elements of this Iterator.\n     *\n     * @param element An element.\n     * @return an interspersed version of this\n     */\n    default Iterator<T> intersperse(T element) {\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                boolean insertElement = false;\n\n                @Override\n                public boolean hasNext() {\n                    return that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    if (insertElement) {\n                        insertElement = false;\n                        return element;\n                    } else {\n                        insertElement = true;\n                        return that.next();\n                    }\n                }\n            };\n        }\n    }\n\n    /**\n     * Transforms this {@code Iterator}.\n     *\n     * @param f   A transformation\n     * @param <U> Type of transformation result\n     * @return An instance of type {@code U}\n     * @throws NullPointerException if {@code f} is null\n     */\n    default <U> U transform(Function<? super Iterator<T>, ? extends U> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return f.apply(this);\n    }\n\n    @Override\n    default <U> Iterator<Tuple2<T, U>> zip(Iterable<? extends U> that) {\n        return zipWith(that, Tuple::of);\n    }\n\n    @Override\n    default <U, R> Iterator<R> zipWith(Iterable<? extends U> that, BiFunction<? super T, ? super U, ? extends R> mapper) {\n        Objects.requireNonNull(that, \"that is null\");\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        if (isEmpty()) {\n            return empty();\n        } else {\n            final Iterator<T> it1 = this;\n            final java.util.Iterator<? extends U> it2 = that.iterator();\n            return new AbstractIterator<R>() {\n                @Override\n                public boolean hasNext() {\n                    return it1.hasNext() && it2.hasNext();\n                }\n\n                @Override\n                public R getNext() {\n                    return mapper.apply(it1.next(), it2.next());\n                }\n            };\n        }\n    }\n\n    @Override\n    default <U> Iterator<Tuple2<T, U>> zipAll(Iterable<? extends U> that, T thisElem, U thatElem) {\n        Objects.requireNonNull(that, \"that is null\");\n        final java.util.Iterator<? extends U> thatIt = that.iterator();\n        if (isEmpty() && !thatIt.hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> thisIt = this;\n            return new AbstractIterator<Tuple2<T, U>>() {\n                @Override\n                public boolean hasNext() {\n                    return thisIt.hasNext() || thatIt.hasNext();\n                }\n\n                @Override\n                public Tuple2<T, U> getNext() {\n                    final T v1 = thisIt.hasNext() ? thisIt.next() : thisElem;\n                    final U v2 = thatIt.hasNext() ? thatIt.next() : thatElem;\n                    return Tuple.of(v1, v2);\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<Tuple2<T, Integer>> zipWithIndex() {\n        return zipWithIndex(Tuple::of);\n    }\n\n    @Override\n    default <U> Iterator<U> zipWithIndex(BiFunction<? super T, ? super Integer, ? extends U> mapper) {\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        if (isEmpty()) {\n            return empty();\n        } else {\n            final Iterator<T> it1 = this;\n            return new AbstractIterator<U>() {\n                private int index = 0;\n\n                @Override\n                public boolean hasNext() {\n                    return it1.hasNext();\n                }\n\n                @Override\n                public U getNext() {\n                    return mapper.apply(it1.next(), index++);\n                }\n            };\n        }\n    }\n\n    @Override\n    default <T1, T2> Tuple2<Iterator<T1>, Iterator<T2>> unzip(\n            Function<? super T, Tuple2<? extends T1, ? extends T2>> unzipper) {\n        Objects.requireNonNull(unzipper, \"unzipper is null\");\n        if (!hasNext()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            final Stream<Tuple2<? extends T1, ? extends T2>> source = Stream.ofAll(this.map(unzipper));\n            return Tuple.of(source.map(t -> (T1) t._1).iterator(), source.map(t -> (T2) t._2).iterator());\n        }\n    }\n\n    @Override\n    default <T1, T2, T3> Tuple3<Iterator<T1>, Iterator<T2>, Iterator<T3>> unzip3(\n            Function<? super T, Tuple3<? extends T1, ? extends T2, ? extends T3>> unzipper) {\n        Objects.requireNonNull(unzipper, \"unzipper is null\");\n        if (!hasNext()) {\n            return Tuple.of(empty(), empty(), empty());\n        } else {\n            final Stream<Tuple3<? extends T1, ? extends T2, ? extends T3>> source = Stream.ofAll(this.map(unzipper));\n            return Tuple.of(source.map(t -> (T1) t._1).iterator(), source.map(t -> (T2) t._2).iterator(), source.map(t -> (T3) t._3).iterator());\n        }\n    }\n\n    /**\n     * Creates an iterator from a seed value and a function.\n     * The function takes the seed at first.\n     * The function should return {@code None} when it's\n     * done generating elements, otherwise {@code Some} {@code Tuple}\n     * of the value to add to the resulting iterator and\n     * the element for the next call.\n     * <p>\n     * Example:\n     * <pre>\n     * <code>\n     * Iterator.unfold(10, x -&gt; x == 0\n     *                 ? Option.none()\n     *                 : Option.of(new Tuple2&lt;&gt;(x-1, x)));\n     * // List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n     * </code>\n     * </pre>\n     *\n     * @param <T>  type of seeds and unfolded values\n     * @param seed the start value for the iteration\n     * @param f    the function to get the next step of the iteration\n     * @return a list with the values built up by the iteration\n     * @throws NullPointerException if {@code f} is null\n     */\n    static <T> Iterator<T> unfold(T seed, Function<? super T, Option<Tuple2<? extends T, ? extends T>>> f) {\n        return unfoldLeft(seed, f);\n    }\n\n    /**\n     * Creates an iterator from a seed value and a function.\n     * The function takes the seed at first.\n     * The function should return {@code None} when it's\n     * done generating elements, otherwise {@code Some} {@code Tuple}\n     * of the value to add to the resulting iterator and\n     * the element for the next call.\n     * <p>\n     * Example:\n     * <pre>\n     * <code>\n     * Iterator.unfoldLeft(10, x -&gt; x == 0\n     *                    ? Option.none()\n     *                    : Option.of(new Tuple2&lt;&gt;(x-1, x)));\n     * // List(1, 2, 3, 4, 5, 6, 7, 8, 9, 10))\n     * </code>\n     * </pre>\n     *\n     * @param <T>  type of seeds\n     * @param <U>  type of unfolded values\n     * @param seed the start value for the iteration\n     * @param f    the function to get the next step of the iteration\n     * @return a list with the values built up by the iteration\n     * @throws NullPointerException if {@code f} is null\n     */\n    static <T, U> Iterator<U> unfoldLeft(T seed, Function<? super T, Option<Tuple2<? extends T, ? extends U>>> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return Stream.<U> ofAll(\n                unfoldRight(seed, f.andThen(tupleOpt -> tupleOpt.map(t -> Tuple.of(t._2, t._1)))))\n                .reverse().iterator();\n    }\n\n    /**\n     * Creates an iterator from a seed value and a function.\n     * The function takes the seed at first.\n     * The function should return {@code None} when it's\n     * done generating elements, otherwise {@code Some} {@code Tuple}\n     * of the element for the next call and the value to add to the\n     * resulting iterator.\n     * <p>\n     * Example:\n     * <pre>\n     * <code>\n     * Iterator.unfoldRight(10, x -&gt; x == 0\n     *             ? Option.none()\n     *             : Option.of(new Tuple2&lt;&gt;(x, x-1)));\n     * // List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1))\n     * </code>\n     * </pre>\n     *\n     * @param <T>  type of seeds\n     * @param <U>  type of unfolded values\n     * @param seed the start value for the iteration\n     * @param f    the function to get the next step of the iteration\n     * @return a list with the values built up by the iteration\n     * @throws NullPointerException if {@code f} is null\n     */\n    static <T, U> Iterator<U> unfoldRight(T seed, Function<? super T, Option<Tuple2<? extends U, ? extends T>>> f) {\n        Objects.requireNonNull(f, \"the unfold iterating function is null\");\n        return new AbstractIterator<U>() {\n            private Option<Tuple2<? extends U, ? extends T>> nextVal = f.apply(seed);\n\n            @Override\n            public boolean hasNext() {\n                return nextVal.isDefined();\n            }\n\n            @Override\n            public U getNext() {\n                final U result = nextVal.get()._1;\n                nextVal = f.apply(nextVal.get()._2);\n                return result;\n            }\n        };\n    }\n\n    // -- Overridden methods of Traversable\n\n    @Override\n    default Iterator<T> distinct() {\n        if (!hasNext()) {\n            return empty();\n        } else {\n            return new DistinctIterator<>(this, io.vavr.collection.HashSet.empty(), Function.identity());\n        }\n    }\n\n    @Override\n    default Iterator<T> distinctBy(Comparator<? super T> comparator) {\n        Objects.requireNonNull(comparator, \"comparator is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            return new DistinctIterator<>(this, TreeSet.empty(comparator), Function.identity());\n        }\n    }\n\n    @Override\n    default <U> Iterator<T> distinctBy(Function<? super T, ? extends U> keyExtractor) {\n        Objects.requireNonNull(keyExtractor, \"keyExtractor is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            return new DistinctIterator<>(this, io.vavr.collection.HashSet.empty(), keyExtractor);\n        }\n    }\n\n    /**\n     * Removes up to n elements from this iterator.\n     *\n     * @param n A number\n     * @return The empty iterator, if {@code n <= 0} or this is empty, otherwise a new iterator without the first n elements.\n     */\n    @Override\n    default Iterator<T> drop(int n) {\n        if (n <= 0) {\n            return this;\n        } else if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                long count = n;\n\n                @Override\n                public boolean hasNext() {\n                    while (count > 0 && that.hasNext()) {\n                        that.next(); // discarded\n                        count--;\n                    }\n                    return that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    return that.next();\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> dropRight(int n) {\n        if (n <= 0) {\n            return this;\n        } else if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n                private io.vavr.collection.Queue<T> queue = io.vavr.collection.Queue.empty();\n\n                @Override\n                public boolean hasNext() {\n                    while (queue.length() < n && that.hasNext()) {\n                        queue = queue.append(that.next());\n                    }\n                    return queue.length() == n && that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    final Tuple2<T, io.vavr.collection.Queue<T>> t = queue.append(that.next()).dequeue();\n                    queue = t._2;\n                    return t._1;\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> dropUntil(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return dropWhile(predicate.negate());\n    }\n\n    @Override\n    default Iterator<T> dropWhile(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final CachedIterator<T> that = new CachedIterator<>(this);\n            while (that.hasNext() && predicate.test(that.touch())) {\n                that.next();\n            }\n            return that;\n        }\n    }\n\n    /**\n     * Returns an Iterator that contains elements that satisfy the given {@code predicate}.\n     *\n     * @param predicate A predicate\n     * @return A new Iterator\n     */\n    @Override\n    default Iterator<T> filter(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                Option<T> next = Option.none();\n\n                @Override\n                public boolean hasNext() {\n                    while (next.isEmpty() && that.hasNext()) {\n                        final T candidate = that.next();\n                        if (predicate.test(candidate)) {\n                            next = Option.some(candidate);\n                        }\n                    }\n                    return next.isDefined();\n                }\n\n                @Override\n                public T getNext() {\n                    final T result = next.get();\n                    next = Option.none();\n                    return result;\n                }\n            };\n        }\n    }\n\n    /**\n     * Returns an Iterator that contains elements that not satisfy the given {@code predicate}.\n     *\n     * @param predicate A predicate\n     * @return A new Iterator\n     */\n    @Override\n    default Iterator<T> filterNot(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return filter(predicate.negate());\n    }\n\n    @Deprecated\n    @Override\n    default Iterator<T> reject(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return filter(predicate.negate());\n    }\n\n    @Override\n    default Option<T> findLast(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        T last = null;\n        while (hasNext()) {\n            final T elem = next();\n            if (predicate.test(elem)) {\n                last = elem;\n            }\n        }\n        return Option.of(last);\n    }\n\n    /**\n     * FlatMaps the elements of this Iterator to Iterables, which are iterated in the order of occurrence.\n     *\n     * @param mapper A mapper\n     * @param <U>    Component type\n     * @return A new Iterable\n     */\n    @Override\n    default <U> Iterator<U> flatMap(Function<? super T, ? extends Iterable<? extends U>> mapper) {\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<U>() {\n\n                final Iterator<? extends T> inputs = that;\n                java.util.Iterator<? extends U> current = java.util.Collections.emptyIterator();\n\n                @Override\n                public boolean hasNext() {\n                    boolean currentHasNext;\n                    while (!(currentHasNext = current.hasNext()) && inputs.hasNext()) {\n                        current = mapper.apply(inputs.next()).iterator();\n                    }\n                    return currentHasNext;\n                }\n\n                @Override\n                public U getNext() {\n                    return current.next();\n                }\n            };\n        }\n    }\n\n    @Override\n    default <U> U foldRight(U zero, BiFunction<? super T, ? super U, ? extends U> f) {\n        Objects.requireNonNull(f, \"f is null\");\n        return Stream.ofAll(this).foldRight(zero, f);\n    }\n\n    @Override\n    default T get() {\n        return head();\n    }\n\n    @Override\n    default <C> Map<C, Iterator<T>> groupBy(Function<? super T, ? extends C> classifier) {\n        return io.vavr.collection.Collections.groupBy(this, classifier, Iterator::ofAll);\n    }\n\n    @Override\n    default Iterator<Seq<T>> grouped(int size) {\n        return new GroupedIterator<>(this, size, size);\n    }\n    \n    @Override\n    default boolean hasDefiniteSize() {\n        return false;\n    }\n\n    @Override\n    default T head() {\n        if (!hasNext()) {\n            throw new NoSuchElementException(\"head() on empty iterator\");\n        }\n        return next();\n    }\n\n    @Override\n    default Iterator<T> init() {\n        if (!hasNext()) {\n            throw new UnsupportedOperationException();\n        } else {\n            return dropRight(1);\n        }\n    }\n\n    @Override\n    default Option<Iterator<T>> initOption() {\n        return hasNext() ? Option.some(init()) : Option.none();\n    }\n\n    /**\n     * An {@code Iterator} is computed synchronously.\n     *\n     * @return false\n     */\n    @Override\n    default boolean isAsync() {\n        return false;\n    }\n\n    @Override\n    default boolean isEmpty() {\n        return !hasNext();\n    }\n\n    /**\n     * An {@code Iterator} is computed lazily.\n     *\n     * @return true\n     */\n    @Override\n    default boolean isLazy() {\n        return true;\n    }\n\n    @Override\n    default boolean isTraversableAgain() {\n        return false;\n    }\n\n    @Override\n    default boolean isSequential() {\n        return true;\n    }\n\n    @Override\n    default Iterator<T> iterator() {\n        return this;\n    }\n\n    @Override\n    default T last() {\n        return Collections.last(this);\n    }\n\n    @Override\n    default int length() {\n        return foldLeft(0, (n, ignored) -> n + 1);\n    }\n\n    /**\n     * Maps the elements of this Iterator lazily using the given {@code mapper}.\n     *\n     * @param mapper A mapper.\n     * @param <U>    Component type\n     * @return A new Iterator\n     */\n    @Override\n    default <U> Iterator<U> map(Function<? super T, ? extends U> mapper) {\n        Objects.requireNonNull(mapper, \"mapper is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<U>() {\n\n                @Override\n                public boolean hasNext() {\n                    return that.hasNext();\n                }\n\n                @Override\n                public U getNext() {\n                    return mapper.apply(that.next());\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> orElse(Iterable<? extends T> other) {\n        return isEmpty() ? ofAll(other) : this;\n    }\n\n    @Override\n    default Iterator<T> orElse(Supplier<? extends Iterable<? extends T>> supplier) {\n        return isEmpty() ? ofAll(supplier.get()) : this;\n    }\n\n    @Override\n    default Tuple2<Iterator<T>, Iterator<T>> partition(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        if (!hasNext()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            final Stream<T> that = Stream.ofAll(this);\n            final Iterator<T> first = that.iterator().filter(predicate);\n            final Iterator<T> second = that.iterator().filter(predicate.negate());\n            return Tuple.of(first, second);\n        }\n    }\n\n    @Override\n    default Iterator<T> peek(Consumer<? super T> action) {\n        Objects.requireNonNull(action, \"action is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n                @Override\n                public boolean hasNext() {\n                    return that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    final T next = that.next();\n                    action.accept(next);\n                    return next;\n                }\n            };\n        }\n    }\n\n    @Override\n    default T reduceLeft(BiFunction<? super T, ? super T, ? extends T> op) {\n        Objects.requireNonNull(op, \"op is null\");\n        if (isEmpty()) {\n            throw new NoSuchElementException(\"reduceLeft on Nil\");\n        } else {\n            T xs = next();\n            while (hasNext()) {\n                xs = op.apply(xs, next());\n            }\n            return xs;\n        }\n    }\n\n    @Override\n    default T reduceRight(BiFunction<? super T, ? super T, ? extends T> op) {\n        Objects.requireNonNull(op, \"op is null\");\n        if (isEmpty()) {\n            throw new NoSuchElementException(\"reduceRight on Nil\");\n        } else {\n            final Stream<T> reversed = Stream.ofAll(this).reverse();\n            return reversed.reduceLeft((xs, x) -> op.apply(x, xs));\n        }\n    }\n\n    @Override\n    default Iterator<T> replace(T currentElement, T newElement) {\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n                boolean isFirst = true;\n\n                @Override\n                public boolean hasNext() {\n                    return that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    final T elem = that.next();\n                    if (isFirst && Objects.equals(currentElement, elem)) {\n                        isFirst = false;\n                        return newElement;\n                    } else {\n                        return elem;\n                    }\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> replaceAll(T currentElement, T newElement) {\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                @Override\n                public boolean hasNext() {\n                    return that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    final T elem = that.next();\n                    if (Objects.equals(currentElement, elem)) {\n                        return newElement;\n                    } else {\n                        return elem;\n                    }\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> retainAll(Iterable<? extends T> elements) {\n        return io.vavr.collection.Collections.retainAll(this, elements);\n    }\n\n    @Override\n    default Traversable<T> scan(T zero, BiFunction<? super T, ? super T, ? extends T> operation) {\n        return scanLeft(zero, operation);\n    }\n\n    @Override\n    default <U> Iterator<U> scanLeft(U zero, BiFunction<? super U, ? super T, ? extends U> operation) {\n        Objects.requireNonNull(operation, \"operation is null\");\n        if (isEmpty()) {\n            return of(zero);\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<U>() {\n\n                boolean isFirst = true;\n                U acc = zero;\n\n                @Override\n                public boolean hasNext() {\n                    return isFirst || that.hasNext();\n                }\n\n                @Override\n                public U getNext() {\n                    if (isFirst) {\n                        isFirst = false;\n                        return acc;\n                    } else {\n                        acc = operation.apply(acc, that.next());\n                        return acc;\n                    }\n                }\n            };\n        }\n    }\n\n    // not lazy!\n    @Override\n    default <U> Iterator<U> scanRight(U zero, BiFunction<? super T, ? super U, ? extends U> operation) {\n        Objects.requireNonNull(operation, \"operation is null\");\n        if (isEmpty()) {\n            return of(zero);\n        } else {\n            return io.vavr.collection.Collections.scanRight(this, zero, operation, Function.identity());\n        }\n    }\n\n    @Override\n    default Iterator<Seq<T>> slideBy(Function<? super T, ?> classifier) {\n        Objects.requireNonNull(classifier, \"classifier is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final CachedIterator<T> source = new CachedIterator<>(this);\n            return new AbstractIterator<Seq<T>>() {\n                private Stream<T> next = null;\n\n                @Override\n                public boolean hasNext() {\n                    if (next == null && source.hasNext()) {\n                        final Object key = classifier.apply(source.touch());\n                        final java.util.List<T> acc = new ArrayList<>();\n                        while (source.hasNext() && key.equals(classifier.apply(source.touch()))) {\n                            acc.add(source.getNext());\n                        }\n                        next = Stream.ofAll(acc);\n                    }\n                    return next != null;\n                }\n\n                @Override\n                public Stream<T> getNext() {\n                    final Stream<T> result = next;\n                    next = null;\n                    return result;\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<Seq<T>> sliding(int size) {\n        return sliding(size, 1);\n    }\n\n    @Override\n    default Iterator<Seq<T>> sliding(int size, int step) {\n        return new GroupedIterator<>(this, size, step);\n    }\n    \n    @Override\n    default Tuple2<Iterator<T>, Iterator<T>> span(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        if (!hasNext()) {\n            return Tuple.of(empty(), empty());\n        } else {\n            final Stream<T> that = Stream.ofAll(this);\n            return Tuple.of(that.iterator().takeWhile(predicate), that.iterator().dropWhile(predicate));\n        }\n    }\n\n    @Override\n    default String stringPrefix() {\n        return \"Iterator\";\n    }\n\n    @Override\n    default Iterator<T> tail() {\n        if (!hasNext()) {\n            throw new UnsupportedOperationException();\n        } else {\n            next(); // remove first element\n            return this;\n        }\n    }\n\n    @Override\n    default Option<Iterator<T>> tailOption() {\n        if (hasNext()) {\n            next();\n            return Option.some(this);\n        } else {\n            return Option.none();\n        }\n    }\n\n    /**\n     * Take the first n elements from this iterator.\n     *\n     * @param n A number\n     * @return The empty iterator, if {@code n <= 0} or this is empty, otherwise a new iterator without the first n elements.\n     */\n    @Override\n    default Iterator<T> take(int n) {\n        if (n <= 0 || !hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                long count = n;\n\n                @Override\n                public boolean hasNext() {\n                    return count > 0 && that.hasNext();\n                }\n\n                @Override\n                public T getNext() {\n                    count--;\n                    return that.next();\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> takeRight(int n) {\n        if (n <= 0) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n                private io.vavr.collection.Queue<T> queue = io.vavr.collection.Queue.empty();\n\n                @Override\n                public boolean hasNext() {\n                    while (that.hasNext()) {\n                        queue = queue.enqueue(that.next());\n                        if (queue.length() > n) {\n                            queue = queue.dequeue()._2;\n                        }\n                    }\n                    return queue.length() > 0;\n                }\n\n                @Override\n                public T getNext() {\n                    final Tuple2<T, io.vavr.collection.Queue<T>> t = queue.dequeue();\n                    queue = t._2;\n                    return t._1;\n                }\n            };\n        }\n    }\n\n    @Override\n    default Iterator<T> takeUntil(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        return takeWhile(predicate.negate());\n    }\n\n    @Override\n    default Iterator<T> takeWhile(Predicate<? super T> predicate) {\n        Objects.requireNonNull(predicate, \"predicate is null\");\n        if (!hasNext()) {\n            return empty();\n        } else {\n            final Iterator<T> that = this;\n            return new AbstractIterator<T>() {\n\n                private T next;\n                private boolean cached = false;\n                private boolean finished = false;\n\n                @Override\n                public boolean hasNext() {\n                    if (cached) {\n                        return true;\n                    } else if (finished) {\n                        return false;\n                    } else if (that.hasNext()) {\n                        next = that.next();\n                        if (predicate.test(next)) {\n                            cached = true;\n                            return true;\n                        }\n                    }\n                    finished = true;\n                    return false;\n                }\n\n                @Override\n                public T getNext() {\n                    cached = false;\n                    return next;\n                }\n            };\n        }\n    }\n}\n\ninterface IteratorModule {\n\n    final class ConcatIterator<T> implements Iterator<T> {\n\n        private static final class Iterators<T> {\n\n            private final java.util.Iterator<T> head;\n            private Iterators<T> tail;\n\n            @SuppressWarnings(\"unchecked\")\n            Iterators(java.util.Iterator<? extends T> head) {\n                this.head = (java.util.Iterator<T>) head;\n            }\n        }\n\n        private Iterators<T> curr;\n        private Iterators<T> last;\n        private boolean nextCalculated = false;\n\n        ConcatIterator(java.util.Iterator<? extends java.util.Iterator<? extends T>> iterators) {\n            this.curr = this.last = iterators.hasNext() ? new Iterators<>(iterators.next()) : null;\n            while (iterators.hasNext()) {\n                this.last = this.last.tail = new Iterators<>(iterators.next());\n            }\n        }\n\n        @Override\n        public boolean hasNext() {\n            if (nextCalculated) {\n                return curr != null;\n            } else {\n                nextCalculated = true;\n                while (true) {\n                    if (curr.head.hasNext()) {\n                        return true;\n                    } else {\n                        curr = curr.tail;\n                        if (curr == null) {\n                            last = null; // release reference\n                            return false;\n                        }\n                    }\n                }\n            }\n        }\n\n        @Override\n        public T next() {\n            if (!hasNext()) {\n                throw new NoSuchElementException();\n            }\n            nextCalculated = false;\n            return curr.head.next();\n        }\n\n        @Override\n        public Iterator<T> concat(java.util.Iterator<? extends T> that) {\n            if (curr == null) {\n                nextCalculated = false;\n                curr = last = new Iterators<>(that);\n            } else {\n                last = last.tail = new Iterators<>(that);\n            }\n            return this;\n        }\n    }\n\n    final class DistinctIterator<T, U> extends AbstractIterator<T> {\n\n        private final Iterator<? extends T> that;\n        private io.vavr.collection.Set<U> known;\n        private final Function<? super T, ? extends U> keyExtractor;\n        private boolean nextDefined = false;\n        private T next;\n\n        DistinctIterator(Iterator<? extends T> that, Set<U> set, Function<? super T, ? extends U> keyExtractor) {\n            this.that = that;\n            this.known = set;\n            this.keyExtractor = keyExtractor;\n        }\n\n        @Override\n        public boolean hasNext() {\n            return nextDefined || searchNext();\n        }\n\n        private boolean searchNext() {\n            while (that.hasNext()) {\n                final T elem = that.next();\n                final U key = keyExtractor.apply(elem);\n                if (!known.contains(key)) {\n                    known = known.add(key);\n                    nextDefined = true;\n                    next = elem;\n                    return true;\n                }\n            }\n            return false;\n        }\n\n        @Override\n        public T getNext() {\n            final T result = next;\n            nextDefined = false;\n            next = null;\n            return result;\n        }\n    }\n\n    final class EmptyIterator implements Iterator<Object> {\n\n        static final EmptyIterator INSTANCE = new EmptyIterator();\n\n        @Override\n        public boolean hasNext() { return false; }\n\n        @Override\n        public Object next() { throw new NoSuchElementException(stringPrefix() + \".next()\"); }\n\n        @Override\n        public String stringPrefix() {\n            return \"EmptyIterator\";\n        }\n\n        @Override\n        public String toString() {\n            return stringPrefix() + \"()\";\n        }\n    }\n\n    final class GroupedIterator<T> implements Iterator<Seq<T>> {\n\n        private final Iterator<T> that;\n        private final int size;\n        private final int step;\n        private final int gap;\n        private final int preserve;\n\n        private Object[] buffer;\n\n        GroupedIterator(Iterator<T> that, int size, int step) {\n            if (size < 1 || step < 1) {\n                throw new IllegalArgumentException(\"size (\" + size + \") and step (\" + step + \") must both be positive\");\n            }\n            this.that = that;\n            this.size = size;\n            this.step = step;\n            this.gap = Math.max(step - size, 0);\n            this.preserve = Math.max(size - step, 0);\n            this.buffer = take(that, new Object[size], 0, size);\n        }\n\n        @Override\n        public boolean hasNext() {\n            return buffer.length > 0;\n        }\n\n        @Override\n        public Seq<T> next() {\n            if (buffer.length == 0) {\n                throw new NoSuchElementException();\n            }\n            final Object[] result = buffer;\n            if (that.hasNext()) {\n                buffer = new Object[size];\n                if (preserve > 0) {\n                    System.arraycopy(result, step, buffer, 0, preserve);\n                }\n                if (gap > 0) {\n                    drop(that, gap);\n                    buffer = take(that, buffer, preserve, size);\n                } else {\n                    buffer = take(that, buffer, preserve, step);\n                }\n            } else {\n                buffer = new Object[0];\n            }\n            return Array.wrap(result);\n        }\n\n        private static void drop(Iterator<?> source, int count) {\n            for (int i = 0; i < count && source.hasNext(); i++) {\n                source.next();\n            }\n        }\n\n        private static Object[] take(Iterator<?> source, Object[] target, int offset, int count) {\n            int i = offset;\n            while (i < count + offset && source.hasNext()) {\n                target[i] = source.next();\n                i++;\n            }\n            if (i < target.length) {\n                final Object[] result = new Object[i];\n                System.arraycopy(target, 0, result, 0, i);\n                return result;\n            } else {\n                return target;\n            }\n        }\n    }\n\n    final class CachedIterator<T> extends AbstractIterator<T> {\n\n        private final Iterator<T> that;\n\n        private T next;\n        private boolean cached = false;\n\n        CachedIterator(Iterator<T> that) {\n            this.that = that;\n        }\n\n        @Override\n        public boolean hasNext() {\n            return cached || that.hasNext();\n        }\n\n        @Override\n        public T getNext() {\n            if (cached) {\n                T result = next;\n                next = null;\n                cached = false;\n                return result;\n            } else {\n                return that.next();\n            }\n        }\n\n        T touch() {\n            next = next();\n            cached = true;\n            return next;\n        }\n    }\n\n    final class BigDecimalHelper {\n\n        @GwtIncompatible(\"Math::nextDown is not implemented\")\n        private static final Lazy<BigDecimal> INFINITY_DISTANCE = Lazy.of(() -> {\n            final BigDecimal two = BigDecimal.valueOf(2);\n            final BigDecimal supremum = BigDecimal.valueOf(Math.nextDown(Double.POSITIVE_INFINITY));\n            BigDecimal lowerBound = supremum;\n            BigDecimal upperBound = two.pow(Double.MAX_EXPONENT + 1);\n            while (true) {\n                final BigDecimal magicValue = lowerBound.add(upperBound).divide(two, HALF_UP);\n                if (Double.isInfinite(magicValue.doubleValue())) {\n                    if (areEqual(magicValue, upperBound)) {\n                        return magicValue.subtract(supremum);\n                    }\n                    upperBound = magicValue;\n                } else {\n                    lowerBound = magicValue;\n                }\n            }\n        });\n\n        /* scale-independent equality */\n        static boolean areEqual(BigDecimal from, BigDecimal toExclusive) {\n            return from.compareTo(toExclusive) == 0;\n        }\n\n        /* parse infinite values also */\n        @GwtIncompatible(\"Math::nextUp is not implemented\")\n        static BigDecimal asDecimal(double number) {\n            if (number == NEGATIVE_INFINITY) {\n                final BigDecimal result = BigDecimal.valueOf(Math.nextUp(NEGATIVE_INFINITY));\n                return result.subtract(INFINITY_DISTANCE.get());\n            } else if (number == POSITIVE_INFINITY) {\n                final BigDecimal result = BigDecimal.valueOf(Math.nextDown(POSITIVE_INFINITY));\n                return result.add(INFINITY_DISTANCE.get());\n            } else {\n                return BigDecimal.valueOf(number);\n            }\n        }\n    }\n}\n", "idx": 40, "id": 13124, "msg": "", "proj": "vavr-io-vavr", "lang": "java"}
{"patch": "@@ -799,7 +799,7 @@ Collection.prototype.updateOne = function(filter, update, options, callback) {\n       ignoreUndefined: this.s.options.ignoreUndefined,\n       multi: false\n     },\n-    { validationLevel: DEFAULT_VALIDATION }\n+    { optionsValidationLevel: this.s.optionsValidationLevel }\n   );\n \n   return executeOperation(this.s.topology, updateOne, [this, filter, update, options, callback]);", "y": 0, "oldf": "'use strict';\n\nconst deprecate = require('util').deprecate;\nconst deprecateOptions = require('./utils').deprecateOptions;\nconst checkCollectionName = require('./utils').checkCollectionName;\nconst ObjectID = require('mongodb-core').BSON.ObjectID;\nconst AggregationCursor = require('./aggregation_cursor');\nconst MongoError = require('mongodb-core').MongoError;\nconst toError = require('./utils').toError;\nconst normalizeHintField = require('./utils').normalizeHintField;\nconst handleCallback = require('./utils').handleCallback;\nconst decorateCommand = require('./utils').decorateCommand;\nconst decorateWithCollation = require('./utils').decorateWithCollation;\nconst decorateWithReadConcern = require('./utils').decorateWithReadConcern;\nconst formattedOrderClause = require('./utils').formattedOrderClause;\nconst ReadPreference = require('mongodb-core').ReadPreference;\nconst CommandCursor = require('./command_cursor');\nconst unordered = require('./bulk/unordered');\nconst ordered = require('./bulk/ordered');\nconst ChangeStream = require('./change_stream');\nconst executeOperation = require('./utils').executeOperation;\nconst applyWriteConcern = require('./utils').applyWriteConcern;\nconst resolveReadPreference = require('./utils').resolveReadPreference;\nconst ClientSession = require('mongodb-core').Sessions.ClientSession;\nconst validate = require('./options_validator').validate;\n\n// Operations\nconst bulkWrite = require('./operations/collection_ops').bulkWrite;\nconst checkForAtomicOperators = require('./operations/collection_ops').checkForAtomicOperators;\nconst count = require('./operations/collection_ops').count;\nconst countDocuments = require('./operations/collection_ops').countDocuments;\nconst createIndex = require('./operations/collection_ops').createIndex;\nconst createIndexes = require('./operations/collection_ops').createIndexes;\nconst deleteMany = require('./operations/collection_ops').deleteMany;\nconst deleteOne = require('./operations/collection_ops').deleteOne;\nconst distinct = require('./operations/collection_ops').distinct;\nconst dropCollection = require('./operations/db_ops').dropCollection;\nconst dropIndex = require('./operations/collection_ops').dropIndex;\nconst dropIndexes = require('./operations/collection_ops').dropIndexes;\nconst ensureIndex = require('./operations/collection_ops').ensureIndex;\nconst findAndModify = require('./operations/collection_ops').findAndModify;\nconst findAndRemove = require('./operations/collection_ops').findAndRemove;\nconst findOne = require('./operations/collection_ops').findOne;\nconst findOneAndDelete = require('./operations/collection_ops').findOneAndDelete;\nconst findOneAndReplace = require('./operations/collection_ops').findOneAndReplace;\nconst findOneAndUpdate = require('./operations/collection_ops').findOneAndUpdate;\nconst geoHaystackSearch = require('./operations/collection_ops').geoHaystackSearch;\nconst group = require('./operations/collection_ops').group;\nconst indexes = require('./operations/collection_ops').indexes;\nconst indexExists = require('./operations/collection_ops').indexExists;\nconst indexInformation = require('./operations/collection_ops').indexInformation;\nconst insertOne = require('./operations/collection_ops').insertOne;\nconst isCapped = require('./operations/collection_ops').isCapped;\nconst mapReduce = require('./operations/collection_ops').mapReduce;\nconst optionsOp = require('./operations/collection_ops').optionsOp;\nconst parallelCollectionScan = require('./operations/collection_ops').parallelCollectionScan;\nconst prepareDocs = require('./operations/collection_ops').prepareDocs;\nconst reIndex = require('./operations/collection_ops').reIndex;\nconst removeDocuments = require('./operations/collection_ops').removeDocuments;\nconst rename = require('./operations/collection_ops').rename;\nconst replaceOne = require('./operations/collection_ops').replaceOne;\nconst save = require('./operations/collection_ops').save;\nconst stats = require('./operations/collection_ops').stats;\nconst updateDocuments = require('./operations/collection_ops').updateDocuments;\nconst updateMany = require('./operations/collection_ops').updateMany;\nconst updateOne = require('./operations/collection_ops').updateOne;\n\nconst DEFAULT_VALIDATION = 'error';\n\n/**\n * @fileOverview The **Collection** class is an internal class that embodies a MongoDB collection\n * allowing for insert/update/remove/find and other command operation on that MongoDB collection.\n *\n * **COLLECTION Cannot directly be instantiated**\n * @example\n * const MongoClient = require('mongodb').MongoClient;\n * const test = require('assert');\n * // Connection url\n * const url = 'mongodb://localhost:27017';\n * // Database Name\n * const dbName = 'test';\n * // Connect using MongoClient\n * MongoClient.connect(url, function(err, client) {\n *   // Create a collection we want to drop later\n *   const col = client.db(dbName).collection('createIndexExample1');\n *   // Show that duplicate records got dropped\n *   col.find({}).toArray(function(err, items) {\n *     test.equal(null, err);\n *     test.equal(4, items.length);\n *     client.close();\n *   });\n * });\n */\n\n/**\n * Create a new Collection instance (INTERNAL TYPE, do not instantiate directly)\n * @class\n * @property {string} collectionName Get the collection name.\n * @property {string} namespace Get the full collection namespace.\n * @property {object} writeConcern The current write concern values.\n * @property {object} readConcern The current read concern values.\n * @property {object} hint Get current index hint for collection.\n * @return {Collection} a Collection instance.\n */\nfunction Collection(db, topology, dbName, name, pkFactory, options) {\n  checkCollectionName(name);\n\n  // Unpack variables\n  const internalHint = null;\n  const slaveOk = options == null || options.slaveOk == null ? db.slaveOk : options.slaveOk;\n  const serializeFunctions =\n    options == null || options.serializeFunctions == null\n      ? db.s.options.serializeFunctions\n      : options.serializeFunctions;\n  const raw = options == null || options.raw == null ? db.s.options.raw : options.raw;\n  const promoteLongs =\n    options == null || options.promoteLongs == null\n      ? db.s.options.promoteLongs\n      : options.promoteLongs;\n  const promoteValues =\n    options == null || options.promoteValues == null\n      ? db.s.options.promoteValues\n      : options.promoteValues;\n  const promoteBuffers =\n    options == null || options.promoteBuffers == null\n      ? db.s.options.promoteBuffers\n      : options.promoteBuffers;\n  let readPreference = null;\n  const collectionHint = null;\n  const namespace = `${dbName}.${name}`;\n\n  // Get the promiseLibrary\n  const promiseLibrary = options.promiseLibrary || Promise;\n\n  // Assign the right collection level readPreference\n  if (options && options.readPreference) {\n    readPreference = options.readPreference;\n  } else if (db.options.readPreference) {\n    readPreference = db.options.readPreference;\n  }\n\n  // Set custom primary key factory if provided\n  pkFactory = pkFactory == null ? ObjectID : pkFactory;\n\n  // Internal state\n  this.s = {\n    // Set custom primary key factory if provided\n    pkFactory: pkFactory,\n    // Db\n    db: db,\n    // Topology\n    topology: topology,\n    // dbName\n    dbName: dbName,\n    // Options\n    options: options,\n    // Namespace\n    namespace: namespace,\n    // Read preference\n    readPreference: readPreference,\n    // SlaveOK\n    slaveOk: slaveOk,\n    // Serialize functions\n    serializeFunctions: serializeFunctions,\n    // Raw\n    raw: raw,\n    // promoteLongs\n    promoteLongs: promoteLongs,\n    // promoteValues\n    promoteValues: promoteValues,\n    // promoteBuffers\n    promoteBuffers: promoteBuffers,\n    // internalHint\n    internalHint: internalHint,\n    // collectionHint\n    collectionHint: collectionHint,\n    // Name\n    name: name,\n    // Promise library\n    promiseLibrary: promiseLibrary,\n    // Read Concern\n    readConcern: options.readConcern,\n    // Write Concern\n    writeConcern: options.writeConcern\n  };\n}\n\nObject.defineProperty(Collection.prototype, 'dbName', {\n  enumerable: true,\n  get: function() {\n    return this.s.dbName;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'collectionName', {\n  enumerable: true,\n  get: function() {\n    return this.s.name;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'namespace', {\n  enumerable: true,\n  get: function() {\n    return this.s.namespace;\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'readConcern', {\n  enumerable: true,\n  get: function() {\n    return this.s.readConcern || { level: 'local' };\n  }\n});\n\nObject.defineProperty(Collection.prototype, 'writeConcern', {\n  enumerable: true,\n  get: function() {\n    let ops = {};\n    if (this.s.writeConcern) {\n      return this.s.writeConcern;\n    }\n\n    if (this.s.options.w != null) ops.w = this.s.options.w;\n    if (this.s.options.j != null) ops.j = this.s.options.j;\n    if (this.s.options.fsync != null) ops.fsync = this.s.options.fsync;\n    if (this.s.options.wtimeout != null) ops.wtimeout = this.s.options.wtimeout;\n    return ops;\n  }\n});\n\n/**\n * @ignore\n */\nObject.defineProperty(Collection.prototype, 'hint', {\n  enumerable: true,\n  get: function() {\n    return this.s.collectionHint;\n  },\n  set: function(v) {\n    this.s.collectionHint = normalizeHintField(v);\n  }\n});\n\nconst DEPRECATED_FIND_OPTIONS = ['maxScan', 'fields', 'snapshot'];\n\n/**\n * Creates a cursor for a query that can be used to iterate over results from MongoDB\n * @method\n * @param {object} [query={}] The cursor query object.\n * @param {object} [options] Optional settings.\n * @param {number} [options.limit=0] Sets the limit of documents returned in the query.\n * @param {(array|object)} [options.sort] Set to sort the documents coming back from the query. Array of indexes, [['a', 1]] etc.\n * @param {object} [options.projection] The fields to return in the query. Object of fields to include or exclude (not both), {'a':1}\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {number} [options.skip=0] Set to skip N documents ahead in your query (useful for pagination).\n * @param {string|object} [options.hint] Tell the query to use specific indexes in the query. Object of indexes to use, {'_id':1}\n * @param {boolean} [options.explain] Explain the query instead of returning the data.\n * @param {boolean} [options.snapshot] DEPRECATED: Snapshot query.\n * @param {boolean} [options.timeout] Specify if the cursor can timeout.\n * @param {boolean} [options.tailable] Specify if the cursor is tailable.\n * @param {number} [options.batchSize] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {boolean} [options.returnKey] Only return the index key.\n * @param {number} [options.maxScan] DEPRECATED: Limit the number of items to scan.\n * @param {number} [options.min] Set index bounds.\n * @param {number} [options.max] Set index bounds.\n * @param {boolean} [options.showDiskLoc=false] DEPRECATED. Show disk location of results.\n * @param {boolean} [options.showRecordId] Return the record identifier for each document.\n * @param {string} [options.comment] You can put a $comment field on a query to make looking in the profiler logs simpler.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {boolean} [options.partial] Specify if the cursor should return partial results when querying against a sharded system\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {boolean} [options.awaitData] Sets the cursor to block and await data for a while rather than returning no data\n * @throws {MongoError}\n * @return {Cursor}\n */\nconst findSchema = {\n  limit: { type: 'number', default: 0 },\n  sort: { type: ['array', 'object', 'string'] },\n  projection: { type: 'object' },\n  fields: { type: 'object' },\n  skip: { type: 'number', default: 0 },\n  hint: { type: ['string', 'object'] },\n  explain: { type: 'boolean' },\n  snapshot: { type: 'boolean' },\n  timeout: { type: 'boolean' },\n  tailable: { type: 'boolean' },\n  batchSize: { type: 'number' },\n  returnKey: { type: 'boolean' },\n  maxScan: { type: 'number' },\n  min: { type: 'number' },\n  max: { type: 'number' },\n  showDiskLoc: { type: 'boolean' },\n  showRecordId: { type: 'boolean' },\n  comment: { type: 'string' },\n  raw: { type: 'boolean' },\n  promoteLongs: { type: 'boolean' },\n  promoteValues: { type: 'boolean' },\n  promoteBuffers: { type: 'boolean' },\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  partial: { type: 'boolean' },\n  maxTimeMS: { type: 'number' },\n  collation: { type: 'object' },\n  session: { type: ClientSession },\n  promiseLibrary: { overrideOnly: true },\n  slaveOk: { type: 'boolean' },\n  awaitData: { type: 'boolean' },\n  noCursorTimeout: { type: 'boolean', overrideOnly: true },\n  ignoreUndefined: { overrideOnly: true }\n};\nCollection.prototype.find = deprecateOptions(\n  {\n    name: 'collection.find',\n    deprecatedOptions: DEPRECATED_FIND_OPTIONS,\n    optionsIndex: 1\n  },\n  function(query, options, callback) {\n    if (typeof callback === 'object') {\n      // TODO(MAJOR): throw in the future\n      console.warn('Third parameter to `find()` must be a callback or undefined');\n    }\n\n    let selector = query;\n    // figuring out arguments\n    if (typeof callback !== 'function') {\n      if (typeof options === 'function') {\n        callback = options;\n        options = undefined;\n      } else if (options == null) {\n        callback = typeof selector === 'function' ? selector : undefined;\n        selector = typeof selector === 'object' ? selector : undefined;\n      }\n    }\n\n    // Ensure selector is not null\n    selector = selector == null ? {} : selector;\n    // Validate correctness off the selector\n    const object = selector;\n    if (Buffer.isBuffer(object)) {\n      const object_size = object[0] | (object[1] << 8) | (object[2] << 16) | (object[3] << 24);\n      if (object_size !== object.length) {\n        const error = new Error(\n          'query selector raw message size does not match message header size [' +\n            object.length +\n            '] != [' +\n            object_size +\n            ']'\n        );\n        error.name = 'MongoError';\n        throw error;\n      }\n    }\n\n    // Check special case where we are using an objectId\n    if (selector != null && selector._bsontype === 'ObjectID') {\n      selector = { _id: selector };\n    }\n\n    const finalOptions = validate(\n      findSchema,\n      options,\n      {\n        raw: this.s.raw,\n        promoteLongs: this.s.promoteLongs,\n        promoteValues: this.s.promoteValues,\n        promoteBuffers: this.s.promoteBuffers,\n        slaveOk: this.s.db.slaveOk\n      },\n      {\n        readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }),\n        hint:\n          options && options.hint != null\n            ? normalizeHintField(options.hint)\n            : this.s.collectionHint,\n        noCursorTimeout: options ? options.timeout : null,\n        promiseLibrary: this.s.promiseLibrary,\n        ignoreUndefined: this.s.options.ignoreUndefined\n      },\n      { validationLevel: DEFAULT_VALIDATION }\n    );\n\n    let projection = finalOptions.projection || finalOptions.fields;\n\n    if (projection && !Buffer.isBuffer(projection) && Array.isArray(projection)) {\n      projection = projection.length\n        ? projection.reduce((result, field) => {\n            result[field] = 1;\n            return result;\n          }, {})\n        : { _id: 1 };\n    }\n\n    // Set slave ok to true if read preference different from primary\n    if (\n      finalOptions.readPreference != null &&\n      (finalOptions.readPreference !== 'primary' || finalOptions.readPreference.mode !== 'primary')\n    ) {\n      finalOptions.slaveOk = true;\n    }\n\n    // Ensure the query is an object\n    if (selector != null && typeof selector !== 'object') {\n      throw MongoError.create({ message: 'query selector must be an object', driver: true });\n    }\n\n    // Build the find command\n    const findCommand = {\n      find: this.s.namespace,\n      limit: finalOptions.limit,\n      skip: finalOptions.skip,\n      query: selector\n    };\n\n    // Merge in options to command\n    decorateCommand(findCommand, finalOptions, ['session', 'collation']);\n\n    if (projection) findCommand.fields = projection;\n\n    // Sort options\n    if (findCommand.sort) {\n      findCommand.sort = formattedOrderClause(findCommand.sort);\n    }\n\n    // Set the readConcern\n    decorateWithReadConcern(findCommand, this, finalOptions);\n\n    // Decorate find command with collation options\n    try {\n      decorateWithCollation(findCommand, this, finalOptions);\n    } catch (err) {\n      if (typeof callback === 'function') return callback(err, null);\n      throw err;\n    }\n\n    const cursor = this.s.topology.cursor(this.s.namespace, findCommand, finalOptions);\n\n    return typeof callback === 'function' ? handleCallback(callback, null, cursor) : cursor;\n  }\n);\n\n/**\n * Inserts a single document into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object} doc Document to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertOneWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst insertOneSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  serializeFunctions: { type: 'boolean' },\n  forceServerObjectId: { type: 'boolean' },\n  bypassDocumentValidation: { type: 'boolean' },\n  session: { type: ClientSession },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true }\n};\nCollection.prototype.insertOne = function(doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    insertOneSchema,\n    options,\n    {},\n    { ignoreUndefined: this.s.options.ignoreUndefined },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, insertOne, [this, doc, options, callback]);\n};\n\nfunction mapInsertManyResults(docs, r) {\n  const finalResult = {\n    result: { ok: 1, n: r.insertedCount },\n    ops: docs,\n    insertedCount: r.insertedCount,\n    insertedIds: r.insertedIds\n  };\n\n  if (r.getLastOp()) {\n    finalResult.result.opTime = r.getLastOp();\n  }\n\n  return finalResult;\n}\n\n/**\n * Inserts an array of documents into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object[]} docs Documents to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {boolean} [options.ordered=true] If true, when an insert fails, don't execute the remaining writes. If false, continue with remaining inserts when one fails.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst insertManySchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  serializeFunctions: { type: 'boolean' },\n  bypassDocumentValidation: { type: 'boolean' },\n  ordered: { type: 'boolean', default: true },\n  session: { type: ClientSession },\n  ignoreUndefined: { overrideOnly: true }\n};\nCollection.prototype.insertMany = function(docs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options ? Object.assign({}, options) : { ordered: true };\n\n  options = validate(\n    insertManySchema,\n    options,\n    { serializeFunctions: this.s.serializeFunctions },\n    { ignoreUndefined: this.s.options.ignoreUndefined },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  if (!Array.isArray(docs) && typeof callback === 'function') {\n    return callback(\n      MongoError.create({ message: 'docs parameter must be an array of documents', driver: true })\n    );\n  } else if (!Array.isArray(docs)) {\n    return new this.s.promiseLibrary((resolve, reject) => {\n      reject(\n        MongoError.create({ message: 'docs parameter must be an array of documents', driver: true })\n      );\n    });\n  }\n\n  docs = prepareDocs(this, docs, options);\n\n  // Generate the bulk write operations\n  const operations = [\n    {\n      insertMany: docs\n    }\n  ];\n\n  return executeOperation(this.s.topology, bulkWrite, [this, operations, options, callback], {\n    resultMutator: result => mapInsertManyResults(docs, result)\n  });\n};\n\n/**\n * @typedef {Object} Collection~BulkWriteOpResult\n * @property {number} insertedCount Number of documents inserted.\n * @property {number} matchedCount Number of documents matched for update.\n * @property {number} modifiedCount Number of documents modified.\n * @property {number} deletedCount Number of documents deleted.\n * @property {number} upsertedCount Number of documents upserted.\n * @property {object} insertedIds Inserted document generated Id's, hash key is the index of the originating operation\n * @property {object} upsertedIds Upserted document generated Id's, hash key is the index of the originating operation\n * @property {object} result The command result object.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~bulkWriteOpCallback\n * @param {BulkWriteError} error An error instance representing the error during the execution.\n * @param {Collection~BulkWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Perform a bulkWrite operation without a fluent API\n *\n * Legal operation types are\n *\n *  { insertOne: { document: { a: 1 } } }\n *\n *  { updateOne: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n *\n *  { updateMany: { filter: {a:2}, update: {$set: {a:2}}, upsert:true } }\n *\n *  { deleteOne: { filter: {c:1} } }\n *\n *  { deleteMany: { filter: {c:1} } }\n *\n *  { replaceOne: { filter: {c:3}, replacement: {c:4}, upsert:true}}\n *\n * If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {object[]} operations Bulk operations to perform.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.ordered=true] Execute write operation in ordered or unordered fashion.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~bulkWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst bulkWriteSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  serializeFunctions: { type: 'boolean' },\n  ordered: { type: 'boolean', default: true },\n  bypassDocumentValidation: { type: 'boolean' },\n  ignoreUndefined: { overrideOnly: true },\n  session: { type: ClientSession }\n};\nCollection.prototype.bulkWrite = function(operations, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  if (!Array.isArray(operations)) {\n    throw MongoError.create({ message: 'operations must be an array of documents', driver: true });\n  }\n\n  options = validate(\n    bulkWriteSchema,\n    options,\n    {},\n    { ignoreUndefined: this.s.options.ignoreUndefined },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, bulkWrite, [this, operations, options, callback]);\n};\n\n/**\n * @typedef {Object} Collection~WriteOpResult\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The command result object.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~writeOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~WriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * @typedef {Object} Collection~insertWriteOpResult\n * @property {Number} insertedCount The total amount of documents inserted.\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {Object.<Number, ObjectId>} insertedIds Map of the index of the inserted document to the id of the inserted document.\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The raw command result object returned from MongoDB (content might vary by server version).\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents inserted.\n */\n\n/**\n * @typedef {Object} Collection~insertOneWriteOpResult\n * @property {Number} insertedCount The total amount of documents inserted.\n * @property {object[]} ops All the documents inserted using insertOne/insertMany/replaceOne. Documents contain the _id field if forceServerObjectId == false for insertOne/insertMany\n * @property {ObjectId} insertedId The driver generated ObjectId for the insert operation.\n * @property {object} connection The connection object used for the operation.\n * @property {object} result The raw command result object returned from MongoDB (content might vary by server version).\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents inserted.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~insertWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~insertWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~insertOneWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~insertOneWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Inserts a single document or a an array of documents into MongoDB. If documents passed in do not contain the **_id** field,\n * one will be added to each of the documents missing it by the driver, mutating the document. This behavior\n * can be overridden by setting the **forceServerObjectId** flag.\n *\n * @method\n * @param {(object|object[])} docs Documents to insert.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.serializeFunctions=false] Serialize functions on any object.\n * @param {boolean} [options.forceServerObjectId=false] Force server to assign _id values instead of driver.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~insertWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated Use insertOne, insertMany or bulkWrite\n */\nCollection.prototype.insert = deprecate(function(docs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || { ordered: false };\n  docs = !Array.isArray(docs) ? [docs] : docs;\n\n  if (options.keepGoing === true) {\n    options.ordered = false;\n  }\n\n  return this.insertMany(docs, options, callback);\n}, 'collection.insert is deprecated. Use insertOne, insertMany or bulkWrite instead.');\n\n/**\n * @typedef {Object} Collection~updateWriteOpResult\n * @property {Object} result The raw result returned from MongoDB. Will vary depending on server version.\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents scanned.\n * @property {Number} result.nModified The total count of documents modified.\n * @property {Object} connection The connection object used for the operation.\n * @property {Number} matchedCount The number of documents that matched the filter.\n * @property {Number} modifiedCount The number of documents that were modified.\n * @property {Number} upsertedCount The number of documents upserted.\n * @property {Object} upsertedId The upserted id.\n * @property {ObjectId} upsertedId._id The upserted _id returned from the server.\n * @property {Object} message\n * @property {Array} ops\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~updateWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~updateWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Update a single document in a collection\n * @method\n * @param {object} filter The Filter used to select the document to update\n * @param {object} update The update operations to be applied to the document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst updateOneSchema = {\n  upsert: { type: 'boolean' },\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  bypassDocumentValidation: { type: 'boolean' },\n  arrayFilters: { type: 'array' },\n  collation: { type: 'object' },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true },\n  session: { type: ClientSession },\n  multi: { overrideOnly: true }\n};\nCollection.prototype.updateOne = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  options = validate(\n    updateOneSchema,\n    options,\n    {},\n    {\n      ignoreUndefined: this.s.options.ignoreUndefined,\n      multi: false\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, updateOne, [this, filter, update, options, callback]);\n};\n\n/**\n * Replace a document in a collection with another document\n * @method\n * @param {object} filter The Filter used to select the document to replace\n * @param {object} doc The Document that replaces the matching document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for replace operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise<Collection~updatewriteOpResultObject>} returns Promise if no callback passed\n */\nconst replaceOneSchema = {\n  upsert: { type: 'boolean' },\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  bypassDocumentValidation: { type: 'boolean' },\n  collation: { type: 'object' },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true },\n  session: { type: ClientSession },\n  multi: { overrideOnly: true }\n};\nCollection.prototype.replaceOne = function(filter, doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    replaceOneSchema,\n    options,\n    {},\n    {\n      ignoreUndefined: this.s.options.ignoreUndefined,\n      multi: false\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, replaceOne, [this, filter, doc, options, callback]);\n};\n\n/**\n * Update multiple documents in a collection\n * @method\n * @param {object} filter The Filter used to select the documents to update\n * @param {object} update The update operations to be applied to the documents\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.upsert] Update operation is an upsert.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~updateWriteOpCallback} [callback] The command result callback\n * @return {Promise<Collection~updateWriteOpResultObject>} returns Promise if no callback passed\n */\nconst updateManySchema = {\n  upsert: { type: 'boolean' },\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  arrayFilters: { type: 'array' },\n  bypassDocumentValidation: { type: 'boolean' },\n  collation: { type: 'object' },\n  session: { type: ClientSession },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true },\n  multi: { overrideOnly: true }\n};\nCollection.prototype.updateMany = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  options = validate(\n    updateManySchema,\n    options,\n    {},\n    {\n      ignoreUndefined: this.s.options.ignoreUndefined,\n      multi: true\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, updateMany, [this, filter, update, options, callback]);\n};\n\n/**\n * Updates documents.\n * @method\n * @param {object} selector The selector for the update operation.\n * @param {object} document The update document.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.upsert=false] Update operation is an upsert.\n * @param {boolean} [options.multi=false] Update one/all documents with operation.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @throws {MongoError}\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use updateOne, updateMany or bulkWrite\n */\nCollection.prototype.update = deprecate(function(selector, document, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, updateDocuments, [\n    this,\n    selector,\n    document,\n    options,\n    callback\n  ]);\n}, 'collection.update is deprecated. Use updateOne, updateMany, or bulkWrite instead.');\n\n/**\n * @typedef {Object} Collection~deleteWriteOpResult\n * @property {Object} result The raw result returned from MongoDB. Will vary depending on server version.\n * @property {Number} result.ok Is 1 if the command executed correctly.\n * @property {Number} result.n The total count of documents deleted.\n * @property {Object} connection The connection object used for the operation.\n * @property {Number} deletedCount The number of documents deleted.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~deleteWriteOpCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~deleteWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Delete a document from a collection\n * @method\n * @param {object} filter The Filter used to select the document to remove\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for remove operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~deleteWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst deleteOneSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  collation: { type: 'object' },\n  session: { type: ClientSession },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true },\n  single: { overrideOnly: true }\n};\nCollection.prototype.deleteOne = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    deleteOneSchema,\n    options,\n    {},\n    {\n      ignoreUndefined: this.s.options.ignoreUndefined,\n      single: true\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, deleteOne, [this, filter, options, callback]);\n};\n\nCollection.prototype.removeOne = Collection.prototype.deleteOne;\n\n/**\n * Delete multiple documents from a collection\n * @method\n * @param {object} filter The Filter used to select the documents to remove\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for remove operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~deleteWriteOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst deleteManySchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  collation: { type: 'object' },\n  session: { type: ClientSession },\n  ignoreUndefined: { type: 'boolean', overrideOnly: true },\n  single: { overrideOnly: true }\n};\nCollection.prototype.deleteMany = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    deleteManySchema,\n    options,\n    {},\n    {\n      ignoreUndefined: this.s.options.ignoreUndefined,\n      single: false\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, deleteMany, [this, filter, options, callback]);\n};\n\nCollection.prototype.removeMany = Collection.prototype.deleteMany;\n\n/**\n * Remove documents.\n * @method\n * @param {object} selector The selector for the update operation.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.single=false] Removes the first document found.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use deleteOne, deleteMany or bulkWrite\n */\nCollection.prototype.remove = deprecate(function(selector, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, removeDocuments, [this, selector, options, callback]);\n}, 'collection.remove is deprecated. Use deleteOne, deleteMany, or bulkWrite instead.');\n\n/**\n * Save a document. Simple full document replacement function. Not recommended for efficiency, use atomic\n * operators and update instead for more efficient operations.\n * @method\n * @param {object} doc Document to save\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~writeOpCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use insertOne, insertMany, updateOne or updateMany\n */\nCollection.prototype.save = deprecate(function(doc, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  // Add ignoreUndfined\n  if (this.s.options.ignoreUndefined) {\n    options = Object.assign({}, options);\n    options.ignoreUndefined = this.s.options.ignoreUndefined;\n  }\n\n  return executeOperation(this.s.topology, save, [this, doc, options, callback]);\n}, 'collection.save is deprecated. Use insertOne, insertMany, updateOne, or updateMany instead.');\n\n/**\n * The callback format for results\n * @callback Collection~resultCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {object} result The result object if the command was executed successfully.\n */\n\n/**\n * The callback format for an aggregation call\n * @callback Collection~aggregationCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {AggregationCursor} cursor The cursor if the aggregation command was executed successfully.\n */\n\n/**\n * Fetches the first document that matches the query\n * @method\n * @param {object} query Query for find Operation\n * @param {object} [options] Optional settings.\n * @param {object} [options.projection] The fields to return in the query. Object of fields to include or exclude (not both), {'a':1}\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {string|object} [options.hint] Tell the query to use specific indexes in the query. Object of indexes to use, {'_id':1}\n * @param {boolean} [options.explain] Explain the query instead of returning the data.\n * @param {boolean} [options.snapshot] DEPRECATED: Snapshot query.\n * @param {boolean} [options.timeout] Specify if the cursor can timeout.\n * @param {boolean} [options.tailable] Specify if the cursor is tailable.\n * @param {number} [options.batchSize] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {boolean} [options.returnKey] Only return the index key.\n * @param {number} [options.maxScan] DEPRECATED: Limit the number of items to scan.\n * @param {number} [options.min] Set index bounds.\n * @param {number} [options.max] Set index bounds.\n * @param {boolean} [options.showDiskLoc=false] DEPRECATED. Show disk location of results.\n * @param {boolean} [options.showRecordId] Return the record identifier for each document.\n * @param {string} [options.comment] You can put a $comment field on a query to make looking in the profiler logs simpler.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {boolean} [options.partial] Specify if the cursor should return partial results when querying against a sharded system\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst findOneSchema = {\n  projection: { type: 'object' },\n  fields: { type: 'object' },\n  hint: { type: ['string', 'object'] },\n  explain: { type: 'boolean' },\n  snapshot: { type: 'boolean' },\n  timeout: { type: 'boolean' },\n  tailable: { type: 'boolean' },\n  batchSize: { type: 'number' },\n  returnKey: { type: 'boolean' },\n  maxScan: { type: 'number' },\n  min: { type: 'number' },\n  max: { type: 'number' },\n  showDiskLoc: { type: 'boolean' },\n  showRecordId: { type: 'boolean' },\n  comment: { type: 'string' },\n  raw: { type: 'boolean' },\n  promoteLongs: { type: 'boolean' },\n  promoteValues: { type: 'boolean' },\n  promoteBuffers: { type: 'boolean' },\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  partial: { type: 'boolean' },\n  maxTimeMS: { type: 'number' },\n  collation: { type: 'object' },\n  session: { type: ClientSession }\n};\nCollection.prototype.findOne = deprecateOptions(\n  {\n    name: 'collection.find',\n    deprecatedOptions: DEPRECATED_FIND_OPTIONS,\n    optionsIndex: 1\n  },\n  function(query, options, callback) {\n    if (typeof callback === 'object') {\n      // TODO(MAJOR): throw in the future\n      console.warn('Third parameter to `findOne()` must be a callback or undefined');\n    }\n\n    if (typeof query === 'function') (callback = query), (query = {}), (options = {});\n    if (typeof options === 'function') (callback = options), (options = {});\n    query = query || {};\n\n    options = validate(findOneSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n    return executeOperation(this.s.topology, findOne, [this, query, options, callback]);\n  }\n);\n\n/**\n * The callback format for the collection method, must be used if strict is specified\n * @callback Collection~collectionResultCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection} collection The collection instance.\n */\n\n/**\n * Rename the collection.\n *\n * @method\n * @param {string} newName New name of of the collection.\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.dropTarget=false] Drop the target name collection if it previously exists.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~collectionResultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst renameSchema = {\n  dropTarget: { type: 'boolean', default: false },\n  readPreference: { type: [ReadPreference, 'string', 'object'], overrideOnly: true },\n  session: { type: ClientSession }\n};\nCollection.prototype.rename = function(newName, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    renameSchema,\n    options,\n    {},\n    { readPreference: ReadPreference.primary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, rename, [this, newName, options, callback]);\n};\n\n/**\n * Drop the collection from the database, removing it permanently. New accesses will create a new collection.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst dropSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  readPreference: { type: [ReadPreference, 'string', 'object'], overrideOnly: true },\n  session: { type: ClientSession }\n};\nCollection.prototype.drop = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    dropSchema,\n    options,\n    {},\n    { readPreference: ReadPreference.primary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  // Command to execute\n  const cmd = { drop: this.s.name };\n\n  // Decorate with write concern\n  applyWriteConcern(cmd, { db: this }, options);\n\n  return executeOperation(this.s.topology, dropCollection, [this.s.db, cmd, options, callback]);\n};\n\n/**\n * Returns the options of the collection.\n *\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst optionsSchema = {\n  session: { type: ClientSession }\n};\nCollection.prototype.options = function(opts, callback) {\n  if (typeof opts === 'function') (callback = opts), (opts = {});\n\n  const finalOpts = validate(optionsSchema, opts, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, optionsOp, [this, finalOpts, callback]);\n};\n\n/**\n * Returns if the collection is a capped collection\n *\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The results callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst isCappedSchema = {\n  session: { type: ClientSession }\n};\nCollection.prototype.isCapped = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(isCappedSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, isCapped, [this, options, callback]);\n};\n\n/**\n * Creates an index on the db and collection collection.\n * @method\n * @param {(string|object)} fieldOrSpec Defines the index.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {boolean} [options.unique=false] Creates an unique index.\n * @param {boolean} [options.sparse] Creates a sparse index.\n * @param {boolean} [options.background] Creates the index in the background, yielding whenever possible.\n * @param {boolean} [options.dropDups] A unique index cannot be created on a key that has pre-existing duplicate values. If you would like to create the index anyway, keeping the first document the database indexes and deleting all subsequent documents that have duplicate value\n * @param {number} [options.min] For geospatial indexes set the lower bound for the co-ordinates.\n * @param {number} [options.max] For geospatial indexes set the high bound for the co-ordinates.\n * @param {number} [options.v] Specify the format version of the indexes.\n * @param {number} [options.expireAfterSeconds] Allows you to expire data on indexes applied to a data (MongoDB 2.2 or higher)\n * @param {string} [options.name] Override the autogenerated index name (useful if the resulting name is larger than 128 bytes)\n * @param {object} [options.partialFilterExpression] Creates a partial index based on the given filter object (MongoDB 3.2 or higher)\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for createIndex operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst createIndexSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  unique: { type: 'boolean', default: false },\n  sparse: { type: 'boolean' },\n  background: { type: 'boolean' },\n  dropDups: { type: 'boolean' },\n  min: { type: 'number' },\n  max: { type: 'number' },\n  v: { type: 'number' },\n  expireAfterSeconds: { type: 'number' },\n  name: { type: 'string' },\n  partialFilterExpression: { type: 'object' },\n  collation: { type: 'object' },\n  session: { type: ClientSession },\n  readPreference: { overrideOnly: true }\n};\nCollection.prototype.createIndex = function(fieldOrSpec, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    createIndexSchema,\n    options,\n    {},\n    { readPreference: ReadPreference.primary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, createIndex, [this, fieldOrSpec, options, callback]);\n};\n\n/**\n * Creates multiple indexes in the collection, this method is only supported for\n * MongoDB 2.6 or higher. Earlier version of MongoDB will throw a command not supported\n * error. Index specifications are defined at http://docs.mongodb.org/manual/reference/command/createIndexes/.\n * @method\n * @param {array} indexSpecs An array of index specifications to be created\n * @param {Object} [options] Optional settings\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {number} [options.maxTimeMS] Number of milliseconds to wait before aborting the query.\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst createIndexesSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  maxTimeMS: { type: 'number' },\n  readPreference: { overrideOnly: true },\n  session: { type: ClientSession }\n};\nCollection.prototype.createIndexes = function(indexSpecs, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    createIndexesSchema,\n    options,\n    {},\n    { readPreference: ReadPreference.primary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, createIndexes, [this, indexSpecs, options, callback]);\n};\n\n/**\n * Drops an index from this collection.\n * @method\n * @param {string} indexName Name of the index to drop.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {number} [options.maxTimeMS] Number of milliseconds to wait before aborting the query.\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst dropIndexSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  readPreference: { type: [ReadPreference, 'string', 'object'], overrideOnly: true },\n  maxTimeMS: { type: 'number' },\n  session: { type: ClientSession }\n};\nCollection.prototype.dropIndex = function(indexName, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  options = args.length ? args.shift() || {} : {};\n\n  options = validate(\n    dropIndexSchema,\n    options,\n    {},\n    { readPreference: ReadPreference.primary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, dropIndex, [this, indexName, options, callback]);\n};\n\n/**\n * Drops all indexes from this collection.\n * @method\n * @param {Object} [options] Optional settings\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {number} [options.maxTimeMS] Number of milliseconds to wait before aborting the query.\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst dropIndexesSchema = {\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  maxTimeMS: { type: 'number' },\n  session: { type: ClientSession }\n};\nCollection.prototype.dropIndexes = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(dropIndexesSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, dropIndexes, [this, options, callback]);\n};\n\n/**\n * Drops all indexes from this collection.\n * @method\n * @deprecated use dropIndexes\n * @param {Collection~resultCallback} callback The command result callback\n * @return {Promise} returns Promise if no [callback] passed\n */\nCollection.prototype.dropAllIndexes = deprecate(\n  Collection.prototype.dropIndexes,\n  'collection.dropAllIndexes is deprecated. Use dropIndexes instead.'\n);\n\n/**\n * Reindex all indexes on the collection\n * Warning: reIndex is a blocking operation (indexes are rebuilt in the foreground) and will be slow for large collections.\n * @method\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst reIndexSchema = {\n  session: { type: ClientSession }\n};\nCollection.prototype.reIndex = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(reIndexSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, reIndex, [this, options, callback]);\n};\n\n/**\n * Get the list of all indexes information for the collection.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {number} [options.batchSize] The batchSize for the returned command cursor or if pre 2.8 the systems batch collection\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {CommandCursor}\n */\nconst listIndexesSchema = {\n  batchSize: { type: 'number' },\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  session: { type: ClientSession },\n  cursorFactory: { type: 'function', overrideOnly: true },\n  promiseLibrary: { overrideOnly: true }\n};\nCollection.prototype.listIndexes = function(options) {\n  if (!this.s.topology.capabilities()) {\n    throw new MongoError('cannot connect to server');\n  }\n\n  options = validate(\n    listIndexesSchema,\n    options,\n    {},\n    {\n      readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }),\n      cursorFactory: CommandCursor,\n      promiseLibrary: this.s.promiseLibrary\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  // Cursor options\n  let cursor = options.batchSize ? { batchSize: options.batchSize } : {};\n\n  // We have a list collections command\n  if (this.s.topology.capabilities().hasListIndexesCommand) {\n    // Build the command\n    const command = { listIndexes: this.s.name, cursor: cursor };\n    // Execute the cursor\n    cursor = this.s.topology.cursor(`${this.s.dbName}.$cmd`, command, options);\n    // Do we have a readPreference, apply it\n    if (options.readPreference) cursor.setReadPreference(options.readPreference);\n    // Return the cursor\n    return cursor;\n  }\n\n  // Get the namespace\n  const ns = `${this.s.dbName}.system.indexes`;\n  // Get the query\n  cursor = this.s.topology.cursor(ns, { find: ns, query: { ns: this.s.namespace } }, options);\n  // Do we have a readPreference, apply it\n  if (options.readPreference) cursor.setReadPreference(options.readPreference);\n  // Set the passed in batch size if one was provided\n  if (options.batchSize) cursor = cursor.batchSize(options.batchSize);\n  // Return the cursor\n  return cursor;\n};\n\n/**\n * Ensures that an index exists, if it does not it creates it\n * @method\n * @deprecated use createIndexes instead\n * @param {(string|object)} fieldOrSpec Defines the index.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.unique=false] Creates an unique index.\n * @param {boolean} [options.sparse=false] Creates a sparse index.\n * @param {boolean} [options.background=false] Creates the index in the background, yielding whenever possible.\n * @param {boolean} [options.dropDups=false] A unique index cannot be created on a key that has pre-existing duplicate values. If you would like to create the index anyway, keeping the first document the database indexes and deleting all subsequent documents that have duplicate value\n * @param {number} [options.min] For geospatial indexes set the lower bound for the co-ordinates.\n * @param {number} [options.max] For geospatial indexes set the high bound for the co-ordinates.\n * @param {number} [options.v] Specify the format version of the indexes.\n * @param {number} [options.expireAfterSeconds] Allows you to expire data on indexes applied to a data (MongoDB 2.2 or higher)\n * @param {number} [options.name] Override the autogenerated index name (useful if the resulting name is larger than 128 bytes)\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for update operation (see 3.4 documentation for available fields).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nCollection.prototype.ensureIndex = deprecate(function(fieldOrSpec, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n  options = options || {};\n\n  return executeOperation(this.s.topology, ensureIndex, [this, fieldOrSpec, options, callback]);\n}, 'collection.ensureIndex is deprecated. Use createIndexes instead.');\n\n/**\n * Checks if one or more indexes exist on the collection, fails on first non-existing index\n * @method\n * @param {(string|array)} indexes One or more index names to check.\n * @param {Object} [options] Optional settings\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst indexExistsSchema = {\n  session: { type: ClientSession },\n  full: { type: 'boolean', default: false }\n};\nCollection.prototype.indexExists = function(indexes, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(indexExistsSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, indexExists, [this, indexes, options, callback]);\n};\n\n/**\n * Retrieves this collections index info.\n * @method\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.full=false] Returns the full raw index information.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst indexInformationSchema = {\n  full: { type: 'boolean', default: false },\n  session: { type: ClientSession }\n};\nCollection.prototype.indexInformation = function(options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n\n  options = validate(indexInformationSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, indexInformation, [this, options, callback]);\n};\n\n/**\n * The callback format for results\n * @callback Collection~countCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {number} result The count of documents that matched the query.\n */\n\n/**\n * Count number of matching documents in the db to a query.\n * @method\n * @param {object} [query={}] The query for the count.\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.limit] The limit of documents to count.\n * @param {boolean} [options.skip] The number of documents to skip for the count.\n * @param {string} [options.hint] An index name hint for the query.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxTimeMS] Number of miliseconds to wait before aborting the query.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~countCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use {@link Collection#countDocuments countDocuments} or {@link Collection#estimatedDocumentCount estimatedDocumentCount} instead\n */\nCollection.prototype.count = deprecate(function(query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  query = args.length ? args.shift() || {} : {};\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, count, [this, query, options, callback]);\n}, 'collection.count is deprecated, and will be removed in a future version.' +\n  ' Use collection.countDocuments or collection.estimatedDocumentCount instead');\n\n/**\n * Gets an estimate of the count of documents in a collection using collection metadata.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the operation to run.\n * @param {Collection~countCallback} [callback] The command result callback.\n * @return {Promise} returns Promise if no callback passed.\n */\nconst estimatedDocumentCountSchema = {\n  maxTimeMS: { type: 'number' }\n};\nCollection.prototype.estimatedDocumentCount = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(\n    estimatedDocumentCountSchema,\n    options,\n    {},\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, count, [this, null, options, callback]);\n};\n\n/**\n * Gets the number of documents matching the filter.\n *\n * **Note**: When migrating from {@link Collection#count count} to {@link Collection#countDocuments countDocuments}\n * the following query operators must be replaced:\n *\n * | Operator | Replacement |\n * | -------- | ----------- |\n * | `$where`   | [`$expr`][1] |\n * | `$near`    | [`$geoWithin`][2] with [`$center`][3] |\n * | `$nearSphere` | [`$geoWithin`][2] with [`$centerSphere`][4] |\n *\n * [1]: https://docs.mongodb.com/manual/reference/operator/query/expr/\n * [2]: https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n * [3]: https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n * [4]: https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n *\n * @param {object} [query] the query for the count\n * @param {object} [options] Optional settings.\n * @param {object} [options.collation] Specifies a collation.\n * @param {string|object} [options.hint] The index to use.\n * @param {number} [options.limit] The maximum number of document to count.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the operation to run.\n * @param {number} [options.skip] The number of documents to skip before counting.\n * @param {Collection~countCallback} [callback] The command result callback.\n * @return {Promise} returns Promise if no callback passed.\n * @see https://docs.mongodb.com/manual/reference/operator/query/expr/\n * @see https://docs.mongodb.com/manual/reference/operator/query/geoWithin/\n * @see https://docs.mongodb.com/manual/reference/operator/query/center/#op._S_center\n * @see https://docs.mongodb.com/manual/reference/operator/query/centerSphere/#op._S_centerSphere\n */\nconst countDocumentsSchema = {\n  collation: { type: 'object' },\n  hint: { type: ['string', 'object'] },\n  limit: { type: 'number' },\n  maxTimeMS: { type: 'number' },\n  skip: { type: 'number' }\n};\nCollection.prototype.countDocuments = function(query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  query = args.length ? args.shift() || {} : {};\n  options = args.length ? args.shift() || {} : {};\n\n  options = validate(countDocumentsSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, countDocuments, [this, query, options, callback]);\n};\n\n/**\n * The distinct command returns returns a list of distinct values for the given key across a collection.\n * @method\n * @param {string} key Field of the document to find distinct values for.\n * @param {object} query The query for filtering the set of documents to which we apply the distinct filter.\n * @param {object} [options] Optional settings.\n * @param {object} [options.collation] Specifies a collation.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxTimeMS] Number of milliseconds to wait before aborting the query.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst distinctSchema = {\n  collation: { type: 'object' },\n  readPreference: { type: [ReadPreference, 'string', 'object'] },\n  maxTimeMS: { type: 'number' },\n  session: { type: ClientSession }\n};\nCollection.prototype.distinct = function(key, query, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  const queryOption = args.length ? args.shift() || {} : {};\n  const optionsOption = args.length ? args.shift() || {} : {};\n\n  options = validate(\n    distinctSchema,\n    optionsOption,\n    {},\n    { readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }) },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, distinct, [this, key, queryOption, options, callback]);\n};\n\n/**\n * Retrieve all the indexes on the collection.\n * @method\n * @param {Object} [options] Optional settings\n * @param {number} [options.batchSize] The batchSize for the returned command cursor or if pre 2.8 the systems batch collection\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst indexesSchema = {\n  batchSize: { type: 'number' },\n  readPreference: { type: [ReadPreference, 'string', 'object'] },\n  session: { type: ClientSession },\n  full: { type: 'boolean', default: true }\n};\nCollection.prototype.indexes = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  options = validate(indexesSchema, options, {}, { validationLevel: DEFAULT_VALIDATION });\n\n  return executeOperation(this.s.topology, indexes, [this, options, callback]);\n};\n\n/**\n * Get all the collection statistics.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.scale] Divide the returned sizes by scale value.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The collection result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst statsSchema = {\n  readPreference: { type: [ReadPreference, 'string', 'object'] },\n  scale: { type: 'number' },\n  session: { type: ClientSession }\n};\nCollection.prototype.stats = function(options, callback) {\n  const args = Array.prototype.slice.call(arguments, 0);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n\n  options = validate(\n    statsSchema,\n    options,\n    {},\n    { readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }) },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, stats, [this, options, callback]);\n};\n\n/**\n * @typedef {Object} Collection~findAndModifyWriteOpResult\n * @property {object} value Document returned from findAndModify command.\n * @property {object} lastErrorObject The raw lastErrorObject returned from the command.\n * @property {Number} ok Is 1 if the command executed correctly.\n */\n\n/**\n * The callback format for inserts\n * @callback Collection~findAndModifyCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Collection~findAndModifyWriteOpResult} result The result object if the command was executed successfully.\n */\n\n/**\n * Find a document and delete it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to remove\n * @param {object} [options] Optional settings.\n * @param {object} [options.collation] Specifies a collation.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nconst findOneAndDeleteSchema = {\n  collation: { type: 'object' },\n  projection: { type: 'object' },\n  sort: { type: ['object', 'string'] },\n  maxTimeMS: { type: 'number' },\n  session: { type: ClientSession },\n  fields: { overrideOnly: true },\n  remove: { overrideOnly: true }\n};\nCollection.prototype.findOneAndDelete = function(filter, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n\n  options = validate(\n    findOneAndDeleteSchema,\n    options,\n    {},\n    {\n      fields: options ? options.projection : null,\n      remove: true\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, findOneAndDelete, [this, filter, options, callback]);\n};\n\n/**\n * Find a document and replace it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to replace\n * @param {object} replacement The Document that replaces the matching document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specifies a collation.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {boolean} [options.upsert=false] Upsert the document if it does not exist.\n * @param {boolean} [options.returnOriginal] When false, returns the updated document rather than the original. The default is true.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nconst findOneAndReplaceSchema = {\n  bypassDocumentValidation: { type: 'boolean' },\n  collation: { type: 'object' },\n  projection: { type: 'object' },\n  sort: { type: ['object', 'string'] },\n  maxTimeMS: { type: 'number' },\n  upsert: { type: 'boolean', default: false },\n  returnOriginal: { type: 'boolean' },\n  session: { type: ClientSession },\n  fields: { overrideOnly: true },\n  update: { overrideOnly: true },\n  new: { overrideOnly: true }\n};\nCollection.prototype.findOneAndReplace = function(filter, replacement, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n  if (replacement == null || typeof replacement !== 'object')\n    throw toError('replacement parameter must be an object');\n\n  options = validate(\n    findOneAndReplaceSchema,\n    options,\n    {},\n    {\n      fields: options ? options.projection : null,\n      update: true,\n      new: options ? (options.returnOriginal !== void 0 ? !options.returnOriginal : false) : null\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, findOneAndReplace, [\n    this,\n    filter,\n    replacement,\n    options,\n    callback\n  ]);\n};\n\n/**\n * Find a document and update it in one atomic operation. Requires a write lock for the duration of the operation.\n *\n * @method\n * @param {object} filter The Filter used to select the document to update\n * @param {object} update Update operations to be performed on the document\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {object} [options.collation] Specifies a collation.\n * @param {object} [options.projection] Limits the fields to return for all matching documents.\n * @param {object} [options.sort] Determines which document the operation modifies if the query selects multiple documents.\n * @param {number} [options.maxTimeMS] The maximum amount of time to allow the query to run.\n * @param {boolean} [options.upsert=false] Upsert the document if it does not exist.\n * @param {boolean} [options.returnOriginal] When false, returns the updated document rather than the original. The default is true.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {Collection~findAndModifyCallback} [callback] The collection result callback\n * @return {Promise<Collection~findAndModifyWriteOpResultObject>} returns Promise if no callback passed\n */\nconst findOneAndUpdateSchema = {\n  bypassDocumentValidation: { type: 'boolean' },\n  collation: { type: 'object' },\n  projection: { type: 'object' },\n  sort: { type: ['object', 'string'] },\n  maxTimeMS: { type: 'number' },\n  upsert: { type: 'boolean', default: false },\n  returnOriginal: { type: 'boolean' },\n  session: { type: ClientSession },\n  arrayFilters: { type: 'array' },\n  fields: { overrideOnly: true },\n  update: { overrideOnly: true },\n  new: { overrideOnly: true }\n};\nCollection.prototype.findOneAndUpdate = function(filter, update, options, callback) {\n  if (typeof options === 'function') (callback = options), (options = {});\n\n  // Basic validation\n  if (filter == null || typeof filter !== 'object')\n    throw toError('filter parameter must be an object');\n  if (update == null || typeof update !== 'object')\n    throw toError('update parameter must be an object');\n\n  const err = checkForAtomicOperators(update);\n  if (err) {\n    if (typeof callback === 'function') return callback(err);\n    return this.s.promiseLibrary.reject(err);\n  }\n\n  options = validate(\n    findOneAndUpdateSchema,\n    options,\n    {},\n    {\n      fields: options ? options.projection : null,\n      update: true,\n      new: options ? (options.returnOriginal !== void 0 ? !options.returnOriginal : false) : null\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, findOneAndUpdate, [\n    this,\n    filter,\n    update,\n    options,\n    callback\n  ]);\n};\n\n/**\n * Find and update a document.\n * @method\n * @param {object} query Query object to locate the object to modify.\n * @param {array} sort If multiple docs match, choose the first one in the specified sort order as the object to manipulate.\n * @param {object} doc The fields/vals to be updated.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {boolean} [options.remove=false] Set to true to remove the object before returning.\n * @param {boolean} [options.upsert=false] Perform an upsert operation.\n * @param {boolean} [options.new=false] Set to true if you want to return the modified object rather than the original. Ignored for remove.\n * @param {object} [options.projection] Object containing the field projection for the result returned from the operation.\n * @param {object} [options.fields] **Deprecated** Use `options.projection` instead\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Array} [options.arrayFilters] optional list of array filters referenced in filtered positional operators\n * @param {Collection~findAndModifyCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use findOneAndUpdate, findOneAndReplace or findOneAndDelete instead\n */\nCollection.prototype.findAndModify = deprecate(function(query, sort, doc, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  sort = args.length ? args.shift() || [] : [];\n  doc = args.length ? args.shift() : null;\n  options = args.length ? args.shift() || {} : {};\n\n  // Clone options\n  options = Object.assign({}, options);\n  // Force read preference primary\n  options.readPreference = ReadPreference.primary;\n\n  return executeOperation(this.s.topology, findAndModify, [\n    this,\n    query,\n    sort,\n    doc,\n    options,\n    callback\n  ]);\n}, 'collection.findAndModify is deprecated. Use findOneAndUpdate, findOneAndReplace or findOneAndDelete instead.');\n\n/**\n * Find and remove a document.\n * @method\n * @param {object} query Query object to locate the object to modify.\n * @param {array} sort If multiple docs match, choose the first one in the specified sort order as the object to manipulate.\n * @param {object} [options] Optional settings.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j=false] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated use findOneAndDelete instead\n */\nCollection.prototype.findAndRemove = deprecate(function(query, sort, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 1);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  sort = args.length ? args.shift() || [] : [];\n  options = args.length ? args.shift() || {} : {};\n\n  return executeOperation(this.s.topology, findAndRemove, [this, query, sort, options, callback]);\n}, 'collection.findAndRemove is deprecated. Use findOneAndDelete instead.');\n\n/**\n * Execute an aggregation framework pipeline against the collection, needs MongoDB >= 2.2\n * @method\n * @param {object} [pipeline=[]] Array containing all the aggregation framework commands for the execution.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {object} [options.cursor] Return the query as cursor, on 2.6 > it returns as a real cursor on pre 2.6 it returns as an emulated cursor.\n * @param {number} [options.cursor.batchSize] The batchSize for the cursor\n * @param {boolean} [options.explain] Explain returns the aggregation execution plan (requires mongodb 2.6 >).\n * @param {boolean} [options.allowDiskUse] allowDiskUse lets the server know if it can use disk to store temporary results for the aggregation (requires mongodb 2.6 >).\n * @param {number} [options.maxTimeMS] maxTimeMS specifies a cumulative time limit in milliseconds for processing operations on the cursor. MongoDB interrupts the operation at the earliest following interrupt point.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {boolean} [options.raw=false] Return document results as raw BSON buffers.\n * @param {boolean} [options.promoteLongs=true] Promotes Long values to number if they fit inside the 53 bits resolution.\n * @param {boolean} [options.promoteValues=true] Promotes BSON values to native types where possible, set to false to only receive wrapper types.\n * @param {boolean} [options.promoteBuffers=false] Promotes Binary BSON values to native Node Buffers.\n * @param {object} [options.collation] Specify collation (MongoDB 3.4 or higher) settings for aggregate operation (see 3.4 documentation for available fields).\n * @param {string} [options.comment] Add a comment to an aggregation command\n * @param {object|string} [options.hint] Tell the query to use specific indexes in the query. Object of indexes to use, {'_id':1}\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~aggregationCallback} callback The command result callback\n * @return {(null|AggregationCursor)}\n */\nconst aggregateSchema = {\n  readPreference: { type: [ReadPreference, 'string', 'object'] },\n  cursor: { type: 'object', default: {} },\n  batchSize: { type: 'number' },\n  explain: { type: 'boolean' },\n  allowDiskUse: { type: 'boolean' },\n  maxTimeMS: { type: 'number' },\n  bypassDocumentValidation: { type: 'boolean' },\n  raw: { type: 'boolean' },\n  promoteLongs: { type: 'boolean' },\n  promoteValues: { type: 'boolean' },\n  promoteBuffers: { type: 'boolean' },\n  collation: { type: 'object' },\n  comment: { type: 'string' },\n  hint: { type: ['object', 'string'] },\n  session: { type: ClientSession },\n  promiseLibrary: { type: 'function', overrideOnly: true },\n  cursorFactory: { overrideOnly: true }\n};\nCollection.prototype.aggregate = function(pipeline, options, callback) {\n  if (Array.isArray(pipeline)) {\n    // Set up callback if one is provided\n    if (typeof options === 'function') {\n      callback = options;\n      options = {};\n    }\n\n    // If we have no options or callback we are doing\n    // a cursor based aggregation\n    if (options == null && callback == null) {\n      options = {};\n    }\n  } else {\n    // Aggregation pipeline passed as arguments on the method\n    const args = Array.prototype.slice.call(arguments, 0);\n    // Get the callback\n    callback = args.pop();\n    // Get the possible options object\n    const opts = args[args.length - 1];\n    // If it contains any of the admissible options pop it of the args\n    options =\n      opts &&\n      (opts.readPreference ||\n        opts.explain ||\n        opts.cursor ||\n        opts.out ||\n        opts.maxTimeMS ||\n        opts.hint ||\n        opts.allowDiskUse)\n        ? args.pop()\n        : {};\n    // Left over arguments is the pipeline\n    pipeline = args;\n  }\n\n  // Ignore readConcern option\n  let ignoreReadConcern = false;\n\n  // Build the command\n  const command = { aggregate: this.s.name, pipeline: pipeline };\n\n  options = validate(\n    aggregateSchema,\n    options,\n    {},\n    {\n      readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }),\n      promiseLibrary: this.s.promiseLibrary,\n      cursorFactory: AggregationCursor\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  // If out was specified\n  if (typeof options.out === 'string') {\n    pipeline.push({ $out: options.out });\n    // Ignore read concern\n    ignoreReadConcern = true;\n  } else if (pipeline.length > 0 && pipeline[pipeline.length - 1]['$out']) {\n    ignoreReadConcern = true;\n  }\n\n  // Decorate command with writeConcern if out has been specified\n  if (\n    pipeline.length > 0 &&\n    pipeline[pipeline.length - 1]['$out'] &&\n    this.s.topology.capabilities().commandsTakeWriteConcern\n  ) {\n    applyWriteConcern(command, { db: this.s.db, collection: this }, options);\n  }\n\n  // Have we specified collation\n  try {\n    decorateWithCollation(command, this, options);\n  } catch (err) {\n    if (typeof callback === 'function') return callback(err, null);\n    throw err;\n  }\n\n  // If we have bypassDocumentValidation set\n  if (options.bypassDocumentValidation === true) {\n    command.bypassDocumentValidation = options.bypassDocumentValidation;\n  }\n\n  // Do we have a readConcern specified\n  if (!ignoreReadConcern) {\n    decorateWithReadConcern(command, this, options);\n  }\n\n  // If we have allowDiskUse defined\n  if (options.allowDiskUse) command.allowDiskUse = options.allowDiskUse;\n  if (typeof options.maxTimeMS === 'number') command.maxTimeMS = options.maxTimeMS;\n\n  // If we are giving a hint\n  if (options.hint) command.hint = options.hint;\n\n  // If explain has been specified add it\n  if (options.explain) {\n    if (command.readConcern || command.writeConcern) {\n      throw toError('\"explain\" cannot be used on an aggregate call with readConcern/writeConcern');\n    }\n    command.explain = options.explain;\n  }\n\n  if (typeof options.comment === 'string') command.comment = options.comment;\n\n  command.cursor = options.cursor;\n  if (options.batchSize) command.cursor.batchSize = options.batchSize;\n\n  if (typeof callback !== 'function') {\n    if (!this.s.topology.capabilities()) {\n      throw new MongoError('cannot connect to server');\n    }\n\n    // Allow disk usage command\n    if (typeof options.allowDiskUse === 'boolean') {\n      command.allowDiskUse = options.allowDiskUse;\n    }\n    if (typeof options.maxTimeMS === 'number') command.maxTimeMS = options.maxTimeMS;\n\n    // Execute the cursor\n    return this.s.topology.cursor(this.s.namespace, command, options);\n  }\n\n  return handleCallback(callback, null, this.s.topology.cursor(this.s.namespace, command, options));\n};\n\n/**\n * Create a new Change Stream, watching for new changes (insertions, updates, replacements, deletions, and invalidations) in this collection.\n * @method\n * @since 3.0.0\n * @param {Array} [pipeline] An array of {@link https://docs.mongodb.com/manual/reference/operator/aggregation-pipeline/|aggregation pipeline stages} through which to pass change stream documents. This allows for filtering (using $match) and manipulating the change stream documents.\n * @param {object} [options] Optional settings\n * @param {string} [options.fullDocument='default'] Allowed values: \u2018default\u2019, \u2018updateLookup\u2019. When set to \u2018updateLookup\u2019, the change stream will include both a delta describing the changes to the document, as well as a copy of the entire document that was changed from some time after the change occurred.\n * @param {object} [options.resumeAfter] Specifies the logical starting point for the new change stream. This should be the _id field from a previously returned change stream document.\n * @param {number} [options.maxAwaitTimeMS] The maximum amount of time for the server to wait on new documents to satisfy a change stream query\n * @param {number} [options.batchSize] The number of documents to return per batch. See {@link https://docs.mongodb.com/manual/reference/command/aggregate|aggregation documentation}.\n * @param {object} [options.collation] Specify collation settings for operation. See {@link https://docs.mongodb.com/manual/reference/command/aggregate|aggregation documentation}.\n * @param {ReadPreference} [options.readPreference] The read preference. Defaults to the read preference of the database or collection. See {@link https://docs.mongodb.com/manual/reference/read-preference|read preference documentation}.\n * @param {Timestamp} [options.startAtClusterTime] receive change events that occur after the specified timestamp\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {ChangeStream} a ChangeStream instance.\n */\nconst watchSchema = {\n  fullDocument: { type: 'string', default: 'default' },\n  resumeAfter: { type: 'object' },\n  maxAwaitTimeMS: { type: 'number' },\n  batchSize: { type: 'number' },\n  collation: { type: 'object' },\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  startAtClusterTime: { type: 'Timestamp' },\n  session: { type: ClientSession }\n};\nCollection.prototype.watch = function(pipeline, options) {\n  pipeline = pipeline || [];\n\n  // Allow optionally not specifying a pipeline\n  if (!Array.isArray(pipeline)) {\n    options = pipeline;\n    pipeline = [];\n  }\n\n  options = validate(\n    watchSchema,\n    options,\n    {},\n    { readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }) },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return new ChangeStream(this, pipeline, options);\n};\n\n/**\n * The callback format for results\n * @callback Collection~parallelCollectionScanCallback\n * @param {MongoError} error An error instance representing the error during the execution.\n * @param {Cursor[]} cursors A list of cursors returned allowing for parallel reading of collection.\n */\n\n/**\n * Return N number of parallel cursors for a collection allowing parallel reading of entire collection. There are\n * no ordering guarantees for returned results.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.batchSize] Set the batchSize for the getMoreCommand when iterating over the query results.\n * @param {number} [options.numCursors=1] The maximum number of parallel command cursors to return (the number of returned cursors will be in the range 1:numCursors)\n * @param {boolean} [options.raw=false] Return all BSON documents as Raw Buffer documents.\n * @param {Collection~parallelCollectionScanCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst parallelCollectionScanSchema = {\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  batchSize: { type: 'number', default: 1000 },\n  numCursors: { type: 'number', default: 1 },\n  raw: { type: 'boolean', default: false },\n  promiseLibrary: { overrideOnly: true },\n  session: { overrideOnly: true }\n};\nCollection.prototype.parallelCollectionScan = function(options, callback) {\n  if (typeof options === 'function') (callback = options), (options = { numCursors: 1 });\n\n  options = validate(\n    parallelCollectionScanSchema,\n    options,\n    {},\n    {\n      readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }),\n      promiseLibrary: this.s.promiseLibrary,\n      session: undefined\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, parallelCollectionScan, [this, options, callback], {\n    skipSessions: true\n  });\n};\n\n/**\n * Execute a geo search using a geo haystack index on a collection.\n *\n * @method\n * @param {number} x Point to search on the x axis, ensure the indexes are ordered in the same order.\n * @param {number} y Point to search on the y axis, ensure the indexes are ordered in the same order.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {number} [options.maxDistance] Include results up to maxDistance from the point.\n * @param {object} [options.search] Filter the results by a query.\n * @param {number} [options.limit] Max number of results to return.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n */\nconst geoHaystackSearchSchema = {\n  readPreference: { type: [ReadPreference, 'object', 'string'] },\n  maxDistance: { type: 'number' },\n  search: { type: 'object' },\n  limit: { type: 'number' },\n  session: { type: ClientSession }\n};\nCollection.prototype.geoHaystackSearch = function(x, y, options, callback) {\n  const args = Array.prototype.slice.call(arguments, 2);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  options = args.length ? args.shift() || {} : {};\n\n  options = validate(\n    geoHaystackSearchSchema,\n    options,\n    {},\n    { readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }) },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, geoHaystackSearch, [this, x, y, options, callback]);\n};\n\n/**\n * Run a group command across a collection\n *\n * @method\n * @param {(object|array|function|code)} keys An object, array or function expressing the keys to group by.\n * @param {object} condition An optional condition that must be true for a row to be considered.\n * @param {object} initial Initial value of the aggregation counter object.\n * @param {(function|Code)} reduce The reduce function aggregates (reduces) the objects iterated\n * @param {(function|Code)} finalize An optional function to be run on each item in the result set just before the item is returned.\n * @param {boolean} command Specify if you wish to run using the internal group command or using eval, default is true.\n * @param {object} [options] Optional settings.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @return {Promise} returns Promise if no callback passed\n * @deprecated MongoDB 3.6 or higher no longer supports the group command. We recommend rewriting using the aggregation framework.\n */\nCollection.prototype.group = deprecate(function(\n  keys,\n  condition,\n  initial,\n  reduce,\n  finalize,\n  command,\n  options,\n  callback\n) {\n  const args = Array.prototype.slice.call(arguments, 3);\n  callback = typeof args[args.length - 1] === 'function' ? args.pop() : undefined;\n  reduce = args.length ? args.shift() : null;\n  finalize = args.length ? args.shift() : null;\n  command = args.length ? args.shift() : null;\n  options = args.length ? args.shift() || {} : {};\n\n  // Make sure we are backward compatible\n  if (!(typeof finalize === 'function')) {\n    command = finalize;\n    finalize = null;\n  }\n\n  if (\n    !Array.isArray(keys) &&\n    keys instanceof Object &&\n    typeof keys !== 'function' &&\n    !(keys._bsontype === 'Code')\n  ) {\n    keys = Object.keys(keys);\n  }\n\n  if (typeof reduce === 'function') {\n    reduce = reduce.toString();\n  }\n\n  if (typeof finalize === 'function') {\n    finalize = finalize.toString();\n  }\n\n  // Set up the command as default\n  command = command == null ? true : command;\n\n  return executeOperation(this.s.topology, group, [\n    this,\n    keys,\n    condition,\n    initial,\n    reduce,\n    finalize,\n    command,\n    options,\n    callback\n  ]);\n},\n'MongoDB 3.6 or higher no longer supports the group command. We recommend rewriting using the aggregation framework.');\n\n/**\n * Run Map Reduce across a collection. Be aware that the inline option for out will return an array of results not a collection.\n *\n * @method\n * @param {(function|string)} map The mapping function.\n * @param {(function|string)} reduce The reduce function.\n * @param {object} [options] Optional settings.\n * @param {object} [options.collation] Specifies a collation.\n * @param {(ReadPreference|string)} [options.readPreference] The preferred read preference (ReadPreference.PRIMARY, ReadPreference.PRIMARY_PREFERRED, ReadPreference.SECONDARY, ReadPreference.SECONDARY_PREFERRED, ReadPreference.NEAREST).\n * @param {(string|object)} [options.out] Sets the output target for the map reduce job. *{inline:1} | {replace:'collectionName'} | {merge:'collectionName'} | {reduce:'collectionName'}*\n * @param {object} [options.query] Query filter object.\n * @param {object} [options.sort] Sorts the input objects using this key. Useful for optimization, like sorting by the emit key for fewer reduces.\n * @param {number} [options.limit] Number of objects to return from collection.\n * @param {boolean} [options.keeptemp] Keep temporary data.\n * @param {(function|string)} [options.finalize] Finalize function.\n * @param {object} [options.scope] Can pass in variables that can be access from map/reduce/finalize.\n * @param {boolean} [options.jsMode] It is possible to make the execution stay in JS. Provided in MongoDB > 2.0.X.\n * @param {boolean} [options.verbose] Provide statistics on job execution time.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {Collection~resultCallback} [callback] The command result callback\n * @throws {MongoError}\n * @return {Promise} returns Promise if no callback passed\n */\nconst mapReduceSchema = {\n  collation: { type: 'object' },\n  readPreference: { type: [ReadPreference, 'string', 'object'] },\n  out: { type: ['string', 'object'], required: true },\n  query: { type: 'object' },\n  sort: { type: 'object' },\n  limit: { type: 'number' },\n  keeptemp: { type: 'boolean' },\n  finalize: { type: ['function', 'string'] },\n  scope: { type: 'object' },\n  jsMode: { type: 'boolean' },\n  verbose: { type: 'boolean' },\n  bypassDocumentValidation: { type: 'boolean' },\n  session: { type: ClientSession }\n};\nCollection.prototype.mapReduce = function(map, reduce, options, callback) {\n  if ('function' === typeof options) (callback = options), (options = {});\n\n  if ('function' === typeof map) {\n    map = map.toString();\n  }\n\n  if ('function' === typeof reduce) {\n    reduce = reduce.toString();\n  }\n\n  if ('function' === typeof options.finalize) {\n    options.finalize = options.finalize.toString();\n  }\n\n  options = validate(\n    mapReduceSchema,\n    options,\n    {},\n    {\n      readPreference: resolveReadPreference(options, { db: this.s.db, collection: this }),\n      finalize:\n        typeof options.finalize === 'function' ? options.finalize.toString() : options.finalize\n    },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return executeOperation(this.s.topology, mapReduce, [this, map, reduce, options, callback]);\n};\n\n/**\n * Initiate an Out of order batch write operation. All operations will be buffered into insert/update/remove commands executed out of order.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @return {UnorderedBulkOperation}\n */\nconst initializeUnorderedBulkOpSchema = {\n  bypassDocumentValidation: { type: 'boolean' },\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  session: { type: ClientSession },\n  promiseLibrary: { overrideOnly: true }\n};\nCollection.prototype.initializeUnorderedBulkOp = function(options) {\n  options = validate(\n    initializeUnorderedBulkOpSchema,\n    options,\n    {},\n    { promiseLibrary: this.s.promiseLibrary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return unordered(this.s.topology, this, options);\n};\n\n/**\n * Initiate an In order bulk write operation. Operations will be serially executed in the order they are added, creating a new operation for each switch in types.\n *\n * @method\n * @param {object} [options] Optional settings.\n * @param {boolean} [options.bypassDocumentValidation] Allow driver to bypass schema validation in MongoDB 3.2 or higher.\n * @param {(number|string)} [options.w] The write concern.\n * @param {number} [options.wtimeout] The write concern timeout.\n * @param {boolean} [options.j] Specify a journal write concern.\n * @param {ClientSession} [options.session] optional session to use for this operation\n * @param {OrderedBulkOperation} callback The command result callback\n * @return {null}\n */\nconst initializeOrderedBulkOpSchema = {\n  bypassDocumentValidation: { type: 'boolean' },\n  w: { type: ['number', 'string'] },\n  wtimeout: { type: 'number' },\n  j: { type: 'boolean' },\n  session: { type: ClientSession },\n  promiseLibrary: { overrideOnly: true }\n};\nCollection.prototype.initializeOrderedBulkOp = function(options) {\n  options = validate(\n    initializeOrderedBulkOpSchema,\n    options,\n    {},\n    { promiseLibrary: this.s.promiseLibrary },\n    { validationLevel: DEFAULT_VALIDATION }\n  );\n\n  return ordered(this.s.topology, this, options);\n};\n\n/**\n * Return the db logger\n * @method\n * @return {Logger} return the db logger\n * @ignore\n */\nCollection.prototype.getLogger = function() {\n  return this.s.db.s.logger;\n};\n\nmodule.exports = Collection;\n", "idx": 7, "id": 15015, "msg": "", "proj": "mongodb-node-mongodb-native", "lang": "js"}
{"patch": "@@ -16,17 +16,22 @@ import { SCRIPT_IDENTIFIER, DATA_LAYER } from './constants';\n export default function createEnableTracking( config, dataLayerTarget ) {\n \tconst dataLayerPush = createDataLayerPush( dataLayerTarget );\n \n+\tlet hasInsertedTag;\n+\n \t/**\n-\t * Enables tracking by injecting the necessary script tag if not present.\n+\t * Injects the necessary script tag if not present.\n \t */\n-\treturn function enableTracking() {\n-\t\tconfig.trackingEnabled = true;\n-\n+\tconst initializeSnippet = () => {\n \t\tconst { document } = global;\n-\n-\t\tif ( document.querySelector( `script[${ SCRIPT_IDENTIFIER }]` ) ) {\n+\t\tif ( undefined === hasInsertedTag ) {\n+\t\t\thasInsertedTag = document.querySelector(\n+\t\t\t\t`script[${ SCRIPT_IDENTIFIER }]`\n+\t\t\t);\n+\t\t}\n+\t\tif ( hasInsertedTag ) {\n \t\t\treturn;\n \t\t}\n+\t\tconfig.trackingEnabled = true;\n \n \t\t// If not present, inject it and initialize dataLayer.\n \t\tconst scriptTag = document.createElement( 'script' );", "y": 1, "oldf": "/**\n * Internal dependencies\n */\nimport createDataLayerPush from './createDataLayerPush';\nimport { SCRIPT_IDENTIFIER, DATA_LAYER } from './constants';\n\n/**\n * Returns a function which, when invoked enables tracking and injects the gtag script if necessary.\n *\n * @since 1.3.0\n *\n * @param {Object} config          Tracking configuration.\n * @param {Object} dataLayerTarget Data layer parent object.\n * @return {Function} Function that tracks an event.\n */\nexport default function createEnableTracking( config, dataLayerTarget ) {\n\tconst dataLayerPush = createDataLayerPush( dataLayerTarget );\n\n\t/**\n\t * Enables tracking by injecting the necessary script tag if not present.\n\t */\n\treturn function enableTracking() {\n\t\tconfig.trackingEnabled = true;\n\n\t\tconst { document } = global;\n\n\t\tif ( document.querySelector( `script[${ SCRIPT_IDENTIFIER }]` ) ) {\n\t\t\treturn;\n\t\t}\n\n\t\t// If not present, inject it and initialize dataLayer.\n\t\tconst scriptTag = document.createElement( 'script' );\n\t\tscriptTag.setAttribute( SCRIPT_IDENTIFIER, '' );\n\t\tscriptTag.async = true;\n\t\tscriptTag.src = `https://www.googletagmanager.com/gtag/js?id=${ config.trackingID }&l=${ DATA_LAYER }`;\n\t\tdocument.head.appendChild( scriptTag );\n\n\t\tdataLayerPush( 'js', new Date() );\n\t\tdataLayerPush( 'config', config.trackingID );\n\t};\n}\n", "idx": 1, "id": 41334, "msg": "This function is only concerned with injecting the snippet, it should not change the `trackingEnabled` state - that's what `enableTracking` is for  ", "proj": "google-site-kit-wp", "lang": "js"}
{"patch": "@@ -216,13 +216,14 @@ void HIPInternal::initialize(int hip_device_id, hipStream_t stream) {\n     m_maxBlock = hipProp.maxGridSize[0];\n \n     // theoretically, we can get 40 WF's / CU, but only can sustain 32\n+    // see\n+    // https://github.com/ROCm-Developer-Tools/HIP/blob/a0b5dfd625d99af7e288629747b40dd057183173/vdi/hip_platform.cpp#L742\n     m_maxBlocksPerSM = 32;\n     // FIXME_HIP - Nick to implement this upstream\n-    m_regsPerSM          = 262144 / 32;\n-    m_shmemPerSM         = hipProp.maxSharedMemoryPerMultiProcessor;\n-    m_maxShmemPerBlock   = hipProp.sharedMemPerBlock;\n-    m_maxThreadsPerSM    = m_maxBlocksPerSM * HIPTraits::WarpSize;\n-    m_maxThreadsPerBlock = hipProp.maxThreadsPerBlock;\n+    m_regsPerSM        = 262144 / m_maxBlocksPerSM;\n+    m_shmemPerSM       = hipProp.maxSharedMemoryPerMultiProcessor;\n+    m_maxShmemPerBlock = hipProp.sharedMemPerBlock;\n+    m_maxThreadsPerSM  = m_maxBlocksPerSM * HIPTraits::WarpSize;\n \n     //----------------------------------\n     // Multiblock reduction uses scratch flags for counters", "y": 1, "oldf": "/*\n//@HEADER\n// ************************************************************************\n//\n//                        Kokkos v. 3.0\n//       Copyright (2020) National Technology & Engineering\n//               Solutions of Sandia, LLC (NTESS).\n//\n// Under the terms of Contract DE-NA0003525 with NTESS,\n// the U.S. Government retains certain rights in this software.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n// 1. Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//\n// 2. Redistributions in binary form must reproduce the above copyright\n// notice, this list of conditions and the following disclaimer in the\n// documentation and/or other materials provided with the distribution.\n//\n// 3. Neither the name of the Corporation nor the names of the\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY NTESS \"AS IS\" AND ANY\n// EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n// IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n// PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL NTESS OR THE\n// CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\n// EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\n// PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n// PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n// LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n// NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n// SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Questions? Contact Christian R. Trott (crtrott@sandia.gov)\n//\n// ************************************************************************\n//@HEADER\n*/\n\n/*--------------------------------------------------------------------------*/\n/* Kokkos interfaces */\n\n#include <Kokkos_Core.hpp>\n\n#include <HIP/Kokkos_HIP_Instance.hpp>\n#include <Kokkos_HIP.hpp>\n#include <Kokkos_HIP_Space.hpp>\n#include <impl/Kokkos_Error.hpp>\n\n/*--------------------------------------------------------------------------*/\n/* Standard 'C' libraries */\n#include <stdlib.h>\n\n/* Standard 'C++' libraries */\n#include <iostream>\n#include <sstream>\n#include <string>\n#include <vector>\n\nnamespace Kokkos {\nnamespace Experimental {\nnamespace {\nclass HIPInternalDevices {\n public:\n  enum { MAXIMUM_DEVICE_COUNT = 64 };\n  struct hipDeviceProp_t m_hipProp[MAXIMUM_DEVICE_COUNT];\n  int m_hipDevCount;\n\n  HIPInternalDevices();\n\n  static HIPInternalDevices const &singleton();\n};\n\nHIPInternalDevices::HIPInternalDevices() {\n  HIP_SAFE_CALL(hipGetDeviceCount(&m_hipDevCount));\n\n  if (m_hipDevCount > MAXIMUM_DEVICE_COUNT) {\n    Kokkos::abort(\n        \"Sorry, you have more GPUs per node than we thought anybody would ever \"\n        \"have. Please report this to github.com/kokkos/kokkos.\");\n  }\n  for (int i = 0; i < m_hipDevCount; ++i) {\n    HIP_SAFE_CALL(hipGetDeviceProperties(m_hipProp + i, i));\n  }\n}\n\nconst HIPInternalDevices &HIPInternalDevices::singleton() {\n  static HIPInternalDevices self;\n  return self;\n}\n}  // namespace\n\nnamespace Impl {\n\n//----------------------------------------------------------------------------\n\nvoid HIPInternal::print_configuration(std::ostream &s) const {\n  const HIPInternalDevices &dev_info = HIPInternalDevices::singleton();\n\n  s << \"macro  KOKKOS_ENABLE_HIP : defined\" << '\\n';\n#if defined(HIP_VERSION)\n  s << \"macro  HIP_VERSION = \" << HIP_VERSION << \" = version \"\n    << HIP_VERSION / 100 << \".\" << HIP_VERSION % 100 << '\\n';\n#endif\n\n  for (int i = 0; i < dev_info.m_hipDevCount; ++i) {\n    s << \"Kokkos::Experimental::HIP[ \" << i << \" ] \"\n      << dev_info.m_hipProp[i].name << \" version \"\n      << (dev_info.m_hipProp[i].major) << \".\" << dev_info.m_hipProp[i].minor\n      << \", Total Global Memory: \"\n      << ::Kokkos::Impl::human_memory_size(dev_info.m_hipProp[i].totalGlobalMem)\n      << \", Shared Memory per Wavefront: \"\n      << ::Kokkos::Impl::human_memory_size(\n             dev_info.m_hipProp[i].sharedMemPerBlock);\n    if (m_hipDev == i) s << \" : Selected\";\n    s << '\\n';\n  }\n}\n\n//----------------------------------------------------------------------------\n\nHIPInternal::~HIPInternal() {\n  if (m_scratchSpace || m_scratchFlags || m_scratchConcurrentBitset) {\n    std::cerr << \"Kokkos::Experimental::HIP ERROR: Failed to call \"\n                 \"Kokkos::Experimental::HIP::finalize()\"\n              << std::endl;\n    std::cerr.flush();\n  }\n\n  m_hipDev                  = -1;\n  m_hipArch                 = -1;\n  m_multiProcCount          = 0;\n  m_maxWarpCount            = 0;\n  m_maxSharedWords          = 0;\n  m_maxShmemPerBlock        = 0;\n  m_scratchSpaceCount       = 0;\n  m_scratchFlagsCount       = 0;\n  m_scratchSpace            = 0;\n  m_scratchFlags            = 0;\n  m_scratchConcurrentBitset = nullptr;\n  m_stream                  = 0;\n}\n\nint HIPInternal::verify_is_initialized(const char *const label) const {\n  if (m_hipDev < 0) {\n    std::cerr << \"Kokkos::Experimental::HIP::\" << label\n              << \" : ERROR device not initialized\" << std::endl;\n  }\n  return 0 <= m_hipDev;\n}\n\nHIPInternal &HIPInternal::singleton() {\n  static HIPInternal *self = nullptr;\n  if (!self) {\n    self = new HIPInternal();\n  }\n  return *self;\n}\n\nvoid HIPInternal::fence() const {\n  HIP_SAFE_CALL(hipStreamSynchronize(m_stream));\n}\n\nvoid HIPInternal::initialize(int hip_device_id, hipStream_t stream) {\n  if (was_finalized)\n    Kokkos::abort(\"Calling HIP::initialize after HIP::finalize is illegal\\n\");\n\n  if (is_initialized()) return;\n\n  int constexpr WordSize = sizeof(size_type);\n\n  if (!HostSpace::execution_space::impl_is_initialized()) {\n    const std::string msg(\n        \"HIP::initialize ERROR : HostSpace::execution_space \"\n        \"is not initialized\");\n    Kokkos::Impl::throw_runtime_exception(msg);\n  }\n\n  const HIPInternalDevices &dev_info = HIPInternalDevices::singleton();\n\n  const bool ok_init = 0 == m_scratchSpace || 0 == m_scratchFlags;\n\n  // Need at least a GPU device\n  const bool ok_id =\n      0 <= hip_device_id && hip_device_id < dev_info.m_hipDevCount;\n\n  if (ok_init && ok_id) {\n    const struct hipDeviceProp_t &hipProp = dev_info.m_hipProp[hip_device_id];\n\n    m_hipDev     = hip_device_id;\n    m_deviceProp = hipProp;\n\n    hipSetDevice(m_hipDev);\n\n    m_stream = stream;\n\n    // number of multiprocessors\n    m_multiProcCount = hipProp.multiProcessorCount;\n\n    //----------------------------------\n    // Maximum number of warps,\n    // at most one warp per thread in a warp for reduction.\n    m_maxWarpCount = hipProp.maxThreadsPerBlock / Impl::HIPTraits::WarpSize;\n    if (HIPTraits::WarpSize < m_maxWarpCount) {\n      m_maxWarpCount = Impl::HIPTraits::WarpSize;\n    }\n    m_maxSharedWords = hipProp.sharedMemPerBlock / WordSize;\n\n    //----------------------------------\n    // Maximum number of blocks\n    m_maxBlock = hipProp.maxGridSize[0];\n\n    // theoretically, we can get 40 WF's / CU, but only can sustain 32\n    m_maxBlocksPerSM = 32;\n    // FIXME_HIP - Nick to implement this upstream\n    m_regsPerSM          = 262144 / 32;\n    m_shmemPerSM         = hipProp.maxSharedMemoryPerMultiProcessor;\n    m_maxShmemPerBlock   = hipProp.sharedMemPerBlock;\n    m_maxThreadsPerSM    = m_maxBlocksPerSM * HIPTraits::WarpSize;\n    m_maxThreadsPerBlock = hipProp.maxThreadsPerBlock;\n\n    //----------------------------------\n    // Multiblock reduction uses scratch flags for counters\n    // and scratch space for partial reduction values.\n    // Allocate some initial space.  This will grow as needed.\n    {\n      const unsigned reduce_block_count =\n          m_maxWarpCount * Impl::HIPTraits::WarpSize;\n\n      (void)scratch_flags(reduce_block_count * 2 * sizeof(size_type));\n      (void)scratch_space(reduce_block_count * 16 * sizeof(size_type));\n    }\n    //----------------------------------\n    // Concurrent bitset for obtaining unique tokens from within\n    // an executing kernel.\n    {\n      const int32_t buffer_bound =\n          Kokkos::Impl::concurrent_bitset::buffer_bound(HIP::concurrency());\n\n      // Allocate and initialize uint32_t[ buffer_bound ]\n\n      using Record =\n          Kokkos::Impl::SharedAllocationRecord<Kokkos::Experimental::HIPSpace,\n                                               void>;\n\n      Record *const r = Record::allocate(Kokkos::Experimental::HIPSpace(),\n                                         \"InternalScratchBitset\",\n                                         sizeof(uint32_t) * buffer_bound);\n\n      Record::increment(r);\n\n      m_scratchConcurrentBitset = reinterpret_cast<uint32_t *>(r->data());\n\n      HIP_SAFE_CALL(hipMemset(m_scratchConcurrentBitset, 0,\n                              sizeof(uint32_t) * buffer_bound));\n    }\n    //----------------------------------\n\n  } else {\n    std::ostringstream msg;\n    msg << \"Kokkos::Experimental::HIP::initialize(\" << hip_device_id\n        << \") FAILED\";\n\n    if (!ok_init) {\n      msg << \" : Already initialized\";\n    }\n    if (!ok_id) {\n      msg << \" : Device identifier out of range \"\n          << \"[0..\" << dev_info.m_hipDevCount - 1 << \"]\";\n    }\n    Kokkos::Impl::throw_runtime_exception(msg.str());\n  }\n\n  // Init the array for used for arbitrarily sized atomics\n  // FIXME_HIP uncomment this when global variable works\n  // if (m_stream == 0) ::Kokkos::Impl::initialize_host_hip_lock_arrays();\n}\n\n//----------------------------------------------------------------------------\n\nusing ScratchGrain =\n    Kokkos::Experimental::HIP::size_type[Impl::HIPTraits::WarpSize];\nenum { sizeScratchGrain = sizeof(ScratchGrain) };\n\nKokkos::Experimental::HIP::size_type *HIPInternal::scratch_space(\n    const Kokkos::Experimental::HIP::size_type size) {\n  if (verify_is_initialized(\"scratch_space\") &&\n      m_scratchSpaceCount * sizeScratchGrain < size) {\n    m_scratchSpaceCount = (size + sizeScratchGrain - 1) / sizeScratchGrain;\n\n    using Record =\n        Kokkos::Impl::SharedAllocationRecord<Kokkos::Experimental::HIPSpace,\n                                             void>;\n\n    static Record *const r = Record::allocate(\n        Kokkos::Experimental::HIPSpace(), \"InternalScratchSpace\",\n        (sizeScratchGrain * m_scratchSpaceCount));\n\n    Record::increment(r);\n\n    m_scratchSpace = reinterpret_cast<size_type *>(r->data());\n  }\n\n  return m_scratchSpace;\n}\n\nKokkos::Experimental::HIP::size_type *HIPInternal::scratch_flags(\n    const Kokkos::Experimental::HIP::size_type size) {\n  if (verify_is_initialized(\"scratch_flags\") &&\n      m_scratchFlagsCount * sizeScratchGrain < size) {\n    m_scratchFlagsCount = (size + sizeScratchGrain - 1) / sizeScratchGrain;\n\n    using Record =\n        Kokkos::Impl::SharedAllocationRecord<Kokkos::Experimental::HIPSpace,\n                                             void>;\n\n    Record *const r = Record::allocate(\n        Kokkos::Experimental::HIPSpace(), \"InternalScratchFlags\",\n        (sizeScratchGrain * m_scratchFlagsCount));\n\n    Record::increment(r);\n\n    m_scratchFlags = reinterpret_cast<size_type *>(r->data());\n\n    hipMemset(m_scratchFlags, 0, m_scratchFlagsCount * sizeScratchGrain);\n  }\n\n  return m_scratchFlags;\n}\n\n//----------------------------------------------------------------------------\n\nvoid HIPInternal::finalize() {\n  HIP().fence();\n  was_finalized = true;\n  if (0 != m_scratchSpace || 0 != m_scratchFlags) {\n    using RecordHIP =\n        Kokkos::Impl::SharedAllocationRecord<Kokkos::Experimental::HIPSpace>;\n\n    RecordHIP::decrement(RecordHIP::get_record(m_scratchFlags));\n    RecordHIP::decrement(RecordHIP::get_record(m_scratchSpace));\n    RecordHIP::decrement(RecordHIP::get_record(m_scratchConcurrentBitset));\n\n    m_hipDev                  = -1;\n    m_hipArch                 = -1;\n    m_multiProcCount          = 0;\n    m_maxWarpCount            = 0;\n    m_maxBlock                = 0;\n    m_maxSharedWords          = 0;\n    m_maxShmemPerBlock        = 0;\n    m_scratchSpaceCount       = 0;\n    m_scratchFlagsCount       = 0;\n    m_scratchSpace            = 0;\n    m_scratchFlags            = 0;\n    m_scratchConcurrentBitset = nullptr;\n    m_stream                  = 0;\n  }\n}\n\n//----------------------------------------------------------------------------\n\nKokkos::Experimental::HIP::size_type hip_internal_multiprocessor_count() {\n  return HIPInternal::singleton().m_multiProcCount;\n}\n\nKokkos::Experimental::HIP::size_type hip_internal_maximum_warp_count() {\n  return HIPInternal::singleton().m_maxWarpCount;\n}\n\nKokkos::Experimental::HIP::size_type hip_internal_maximum_grid_count() {\n  return HIPInternal::singleton().m_maxBlock;\n}\n\nKokkos::Experimental::HIP::size_type *hip_internal_scratch_space(\n    const Kokkos::Experimental::HIP::size_type size) {\n  return HIPInternal::singleton().scratch_space(size);\n}\n\nKokkos::Experimental::HIP::size_type *hip_internal_scratch_flags(\n    const Kokkos::Experimental::HIP::size_type size) {\n  return HIPInternal::singleton().scratch_flags(size);\n}\n\n}  // namespace Impl\n}  // namespace Experimental\n}  // namespace Kokkos\n\n//----------------------------------------------------------------------------\n\nnamespace Kokkos {\nnamespace Impl {\nvoid hip_device_synchronize() { HIP_SAFE_CALL(hipDeviceSynchronize()); }\n\nvoid hip_internal_error_throw(hipError_t e, const char *name, const char *file,\n                              const int line) {\n  std::ostringstream out;\n  out << name << \" error( \" << hipGetErrorName(e)\n      << \"): \" << hipGetErrorString(e);\n  if (file) {\n    out << \" \" << file << \":\" << line;\n  }\n  throw_runtime_exception(out.str());\n}\n}  // namespace Impl\n}  // namespace Kokkos\n\n//----------------------------------------------------------------------------\n\nnamespace Kokkos {\nnamespace Experimental {\nHIP::size_type HIP::detect_device_count() {\n  return HIPInternalDevices::singleton().m_hipDevCount;\n}\n}  // namespace Experimental\n}  // namespace Kokkos\n", "idx": 1, "id": 24740, "msg": "Did you mean to not initialize `m_maxThreadsPerBlock` any more?", "proj": "kokkos-kokkos", "lang": "cpp"}
{"patch": "@@ -39,8 +39,8 @@ namespace OpenTelemetry.Trace\n         /// <summary>\n         /// Adds given activitysource names to the list of subscribed sources.\n         /// </summary>\n-        /// <param name=\"names\">Activity source names.</param>\n+        /// <param name=\"sources\">Activity source names.</param>\n         /// <returns>Returns <see cref=\"TracerProviderBuilder\"/> for chaining.</returns>\n-        public abstract TracerProviderBuilder AddSource(params string[] names);\n+        public abstract TracerProviderBuilder AddSource(params Source[] sources);\n     }\n }", "y": 1, "oldf": "// <copyright file=\"TracerProviderBuilder.cs\" company=\"OpenTelemetry Authors\">\n// Copyright The OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n// </copyright>\nusing System;\n\nnamespace OpenTelemetry.Trace\n{\n    /// <summary>\n    /// TracerProviderBuilder base class.\n    /// </summary>\n    public abstract class TracerProviderBuilder\n    {\n        internal TracerProviderBuilder()\n        {\n        }\n\n        /// <summary>\n        /// Adds an instrumentation to the provider.\n        /// </summary>\n        /// <typeparam name=\"TInstrumentation\">Type of instrumentation class.</typeparam>\n        /// <param name=\"instrumentationFactory\">Function that builds instrumentation.</param>\n        /// <returns>Returns <see cref=\"TracerProviderBuilder\"/> for chaining.</returns>\n        public abstract TracerProviderBuilder AddInstrumentation<TInstrumentation>(\n            Func<TInstrumentation> instrumentationFactory)\n            where TInstrumentation : class;\n\n        /// <summary>\n        /// Adds given activitysource names to the list of subscribed sources.\n        /// </summary>\n        /// <param name=\"names\">Activity source names.</param>\n        /// <returns>Returns <see cref=\"TracerProviderBuilder\"/> for chaining.</returns>\n        public abstract TracerProviderBuilder AddSource(params string[] names);\n    }\n}\n", "idx": 1, "id": 18301, "msg": "We can keep the string version as well, and internally convert that to use Source. Most users don't care about Version, so they can use the easy version.", "proj": "open-telemetry-opentelemetry-dotnet", "lang": ".cs"}
{"patch": "@@ -595,7 +595,7 @@ class LGBMModel(_LGBMModelBase):\n         \"\"\"\n         if self._n_features is None:\n             raise LGBMNotFittedError(\"Estimator not fitted, call `fit` before exploiting the model.\")\n-        if not isinstance(X, DataFrame):\n+        if not isinstance(X, DataFrame) and not isinstance(X, DataTable):\n             X = _LGBMCheckArray(X, accept_sparse=True, force_all_finite=False)\n         n_features = X.shape[1]\n         if self._n_features != n_features:", "y": 0, "oldf": "# coding: utf-8\n# pylint: disable = invalid-name, W0105, C0111, C0301\n\"\"\"Scikit-learn wrapper interface for LightGBM.\"\"\"\nfrom __future__ import absolute_import\n\nimport numpy as np\nimport warnings\n\nfrom .basic import Dataset, LightGBMError\nfrom .compat import (SKLEARN_INSTALLED, _LGBMClassifierBase,\n                     LGBMNotFittedError, _LGBMLabelEncoder, _LGBMModelBase,\n                     _LGBMRegressorBase, _LGBMCheckXY, _LGBMCheckArray, _LGBMCheckConsistentLength,\n                     _LGBMAssertAllFinite, _LGBMCheckClassificationTargets, _LGBMComputeSampleWeight,\n                     argc_, range_, string_type, DataFrame)\nfrom .engine import train\n\n\ndef _objective_function_wrapper(func):\n    \"\"\"Decorate an objective function.\n\n    Note\n    ----\n    For multi-class task, the y_pred is group by class_id first, then group by row_id.\n    If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n    and you should group grad and hess in this way as well.\n\n    Parameters\n    ----------\n    func : callable\n        Expects a callable with signature ``func(y_true, y_pred)`` or ``func(y_true, y_pred, group):\n\n            y_true : array-like of shape = [n_samples]\n                The target values.\n            y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            group : array-like\n                Group/query data, used for ranking task.\n\n    Returns\n    -------\n    new_func : callable\n        The new objective function as expected by ``lightgbm.engine.train``.\n        The signature is ``new_func(preds, dataset)``:\n\n            preds : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            dataset : Dataset\n                The training set from which the labels will be extracted using ``dataset.get_label()``.\n    \"\"\"\n    def inner(preds, dataset):\n        \"\"\"Call passed function with appropriate arguments.\"\"\"\n        labels = dataset.get_label()\n        argc = argc_(func)\n        if argc == 2:\n            grad, hess = func(labels, preds)\n        elif argc == 3:\n            grad, hess = func(labels, preds, dataset.get_group())\n        else:\n            raise TypeError(\"Self-defined objective function should have 2 or 3 arguments, got %d\" % argc)\n        \"\"\"weighted for objective\"\"\"\n        weight = dataset.get_weight()\n        if weight is not None:\n            \"\"\"only one class\"\"\"\n            if len(weight) == len(grad):\n                grad = np.multiply(grad, weight)\n                hess = np.multiply(hess, weight)\n            else:\n                num_data = len(weight)\n                num_class = len(grad) // num_data\n                if num_class * num_data != len(grad):\n                    raise ValueError(\"Length of grad and hess should equal to num_class * num_data\")\n                for k in range_(num_class):\n                    for i in range_(num_data):\n                        idx = k * num_data + i\n                        grad[idx] *= weight[i]\n                        hess[idx] *= weight[i]\n        return grad, hess\n    return inner\n\n\ndef _eval_function_wrapper(func):\n    \"\"\"Decorate an eval function.\n\n    Note\n    ----\n    For multi-class task, the y_pred is group by class_id first, then group by row_id.\n    If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n\n    Parameters\n    ----------\n    func : callable\n        Expects a callable with following signatures:\n        ``func(y_true, y_pred)``,\n        ``func(y_true, y_pred, weight)``\n        or ``func(y_true, y_pred, weight, group)``\n        and returns (eval_name->string, eval_result->float, is_bigger_better->bool):\n\n            y_true : array-like of shape = [n_samples]\n                The target values.\n            y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            weight : array-like of shape = [n_samples]\n                The weight of samples.\n            group : array-like\n                Group/query data, used for ranking task.\n\n    Returns\n    -------\n    new_func : callable\n        The new eval function as expected by ``lightgbm.engine.train``.\n        The signature is ``new_func(preds, dataset)``:\n\n            preds : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            dataset : Dataset\n                The training set from which the labels will be extracted using ``dataset.get_label()``.\n    \"\"\"\n    def inner(preds, dataset):\n        \"\"\"Call passed function with appropriate arguments.\"\"\"\n        labels = dataset.get_label()\n        argc = argc_(func)\n        if argc == 2:\n            return func(labels, preds)\n        elif argc == 3:\n            return func(labels, preds, dataset.get_weight())\n        elif argc == 4:\n            return func(labels, preds, dataset.get_weight(), dataset.get_group())\n        else:\n            raise TypeError(\"Self-defined eval function should have 2, 3 or 4 arguments, got %d\" % argc)\n    return inner\n\n\nclass LGBMModel(_LGBMModelBase):\n    \"\"\"Implementation of the scikit-learn API for LightGBM.\"\"\"\n\n    def __init__(self, boosting_type='gbdt', num_leaves=31, max_depth=-1,\n                 learning_rate=0.1, n_estimators=100,\n                 subsample_for_bin=200000, objective=None, class_weight=None,\n                 min_split_gain=0., min_child_weight=1e-3, min_child_samples=20,\n                 subsample=1., subsample_freq=0, colsample_bytree=1.,\n                 reg_alpha=0., reg_lambda=0., random_state=None,\n                 n_jobs=-1, silent=True, importance_type='split', **kwargs):\n        r\"\"\"Construct a gradient boosting model.\n\n        Parameters\n        ----------\n        boosting_type : string, optional (default='gbdt')\n            'gbdt', traditional Gradient Boosting Decision Tree.\n            'dart', Dropouts meet Multiple Additive Regression Trees.\n            'goss', Gradient-based One-Side Sampling.\n            'rf', Random Forest.\n        num_leaves : int, optional (default=31)\n            Maximum tree leaves for base learners.\n        max_depth : int, optional (default=-1)\n            Maximum tree depth for base learners, -1 means no limit.\n        learning_rate : float, optional (default=0.1)\n            Boosting learning rate.\n            You can use ``callbacks`` parameter of ``fit`` method to shrink/adapt learning rate\n            in training using ``reset_parameter`` callback.\n            Note, that this will ignore the ``learning_rate`` argument in training.\n        n_estimators : int, optional (default=100)\n            Number of boosted trees to fit.\n        subsample_for_bin : int, optional (default=200000)\n            Number of samples for constructing bins.\n        objective : string, callable or None, optional (default=None)\n            Specify the learning task and the corresponding learning objective or\n            a custom objective function to be used (see note below).\n            Default: 'regression' for LGBMRegressor, 'binary' or 'multiclass' for LGBMClassifier, 'lambdarank' for LGBMRanker.\n        class_weight : dict, 'balanced' or None, optional (default=None)\n            Weights associated with classes in the form ``{class_label: weight}``.\n            Use this parameter only for multi-class classification task;\n            for binary classification task you may use ``is_unbalance`` or ``scale_pos_weight`` parameters.\n            The 'balanced' mode uses the values of y to automatically adjust weights\n            inversely proportional to class frequencies in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n            If None, all classes are supposed to have weight one.\n            Note, that these weights will be multiplied with ``sample_weight`` (passed through the ``fit`` method)\n            if ``sample_weight`` is specified.\n        min_split_gain : float, optional (default=0.)\n            Minimum loss reduction required to make a further partition on a leaf node of the tree.\n        min_child_weight : float, optional (default=1e-3)\n            Minimum sum of instance weight (hessian) needed in a child (leaf).\n        min_child_samples : int, optional (default=20)\n            Minimum number of data needed in a child (leaf).\n        subsample : float, optional (default=1.)\n            Subsample ratio of the training instance.\n        subsample_freq : int, optional (default=0)\n            Frequence of subsample, <=0 means no enable.\n        colsample_bytree : float, optional (default=1.)\n            Subsample ratio of columns when constructing each tree.\n        reg_alpha : float, optional (default=0.)\n            L1 regularization term on weights.\n        reg_lambda : float, optional (default=0.)\n            L2 regularization term on weights.\n        random_state : int or None, optional (default=None)\n            Random number seed.\n            If None, default seeds in C++ code will be used.\n        n_jobs : int, optional (default=-1)\n            Number of parallel threads.\n        silent : bool, optional (default=True)\n            Whether to print messages while running boosting.\n        importance_type : string, optional (default='split')\n            The type of feature importance to be filled into ``feature_importances_``.\n            If 'split', result contains numbers of times the feature is used in a model.\n            If 'gain', result contains total gains of splits which use the feature.\n        **kwargs\n            Other parameters for the model.\n            Check http://lightgbm.readthedocs.io/en/latest/Parameters.html for more parameters.\n\n            Note\n            ----\n            \\*\\*kwargs is not supported in sklearn, it may cause unexpected issues.\n\n        Attributes\n        ----------\n        n_features_ : int\n            The number of features of fitted model.\n        classes_ : array of shape = [n_classes]\n            The class label array (only for classification problem).\n        n_classes_ : int\n            The number of classes (only for classification problem).\n        best_score_ : dict or None\n            The best score of fitted model.\n        best_iteration_ : int or None\n            The best iteration of fitted model if ``early_stopping_rounds`` has been specified.\n        objective_ : string or callable\n            The concrete objective used while fitting this model.\n        booster_ : Booster\n            The underlying Booster of this model.\n        evals_result_ : dict or None\n            The evaluation results if ``early_stopping_rounds`` has been specified.\n        feature_importances_ : array of shape = [n_features]\n            The feature importances (the higher, the more important the feature).\n\n        Note\n        ----\n        A custom objective function can be provided for the ``objective`` parameter.\n        In this case, it should have the signature\n        ``objective(y_true, y_pred) -> grad, hess`` or\n        ``objective(y_true, y_pred, group) -> grad, hess``:\n\n            y_true : array-like of shape = [n_samples]\n                The target values.\n            y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            group : array-like\n                Group/query data, used for ranking task.\n            grad : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The value of the gradient for each sample point.\n            hess : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The value of the second derivative for each sample point.\n\n        For multi-class task, the y_pred is group by class_id first, then group by row_id.\n        If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i]\n        and you should group grad and hess in this way as well.\n        \"\"\"\n        if not SKLEARN_INSTALLED:\n            raise LightGBMError('Scikit-learn is required for this module')\n\n        self.boosting_type = boosting_type\n        self.objective = objective\n        self.num_leaves = num_leaves\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.n_estimators = n_estimators\n        self.subsample_for_bin = subsample_for_bin\n        self.min_split_gain = min_split_gain\n        self.min_child_weight = min_child_weight\n        self.min_child_samples = min_child_samples\n        self.subsample = subsample\n        self.subsample_freq = subsample_freq\n        self.colsample_bytree = colsample_bytree\n        self.reg_alpha = reg_alpha\n        self.reg_lambda = reg_lambda\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n        self.silent = silent\n        self.importance_type = importance_type\n        self._Booster = None\n        self._evals_result = None\n        self._best_score = None\n        self._best_iteration = None\n        self._other_params = {}\n        self._objective = objective\n        self.class_weight = class_weight\n        self._n_features = None\n        self._classes = None\n        self._n_classes = None\n        self.set_params(**kwargs)\n\n    def get_params(self, deep=True):\n        \"\"\"Get parameters for this estimator.\n\n        Parameters\n        ----------\n        deep : bool, optional (default=True)\n            If True, will return the parameters for this estimator and\n            contained subobjects that are estimators.\n\n        Returns\n        -------\n        params : dict\n            Parameter names mapped to their values.\n        \"\"\"\n        params = super(LGBMModel, self).get_params(deep=deep)\n        params.update(self._other_params)\n        return params\n\n    # minor change to support `**kwargs`\n    def set_params(self, **params):\n        \"\"\"Set the parameters of this estimator.\n\n        Parameters\n        ----------\n        **params\n            Parameter names with their new values.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n        \"\"\"\n        for key, value in params.items():\n            setattr(self, key, value)\n            if hasattr(self, '_' + key):\n                setattr(self, '_' + key, value)\n            self._other_params[key] = value\n        return self\n\n    def fit(self, X, y,\n            sample_weight=None, init_score=None, group=None,\n            eval_set=None, eval_names=None, eval_sample_weight=None,\n            eval_class_weight=None, eval_init_score=None, eval_group=None,\n            eval_metric=None, early_stopping_rounds=None, verbose=True,\n            feature_name='auto', categorical_feature='auto', callbacks=None):\n        \"\"\"Build a gradient boosting model from the training set (X, y).\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            Input feature matrix.\n        y : array-like of shape = [n_samples]\n            The target values (class labels in classification, real numbers in regression).\n        sample_weight : array-like of shape = [n_samples] or None, optional (default=None)\n            Weights of training data.\n        init_score : array-like of shape = [n_samples] or None, optional (default=None)\n            Init score of training data.\n        group : array-like or None, optional (default=None)\n            Group data of training data.\n        eval_set : list or None, optional (default=None)\n            A list of (X, y) tuple pairs to use as validation sets.\n        eval_names : list of strings or None, optional (default=None)\n            Names of eval_set.\n        eval_sample_weight : list of arrays or None, optional (default=None)\n            Weights of eval data.\n        eval_class_weight : list or None, optional (default=None)\n            Class weights of eval data.\n        eval_init_score : list of arrays or None, optional (default=None)\n            Init score of eval data.\n        eval_group : list of arrays or None, optional (default=None)\n            Group data of eval data.\n        eval_metric : string, list of strings, callable or None, optional (default=None)\n            If string, it should be a built-in evaluation metric to use.\n            If callable, it should be a custom evaluation metric, see note below for more details.\n            In either case, the ``metric`` from the model parameters will be evaluated and used as well.\n            Default: 'l2' for LGBMRegressor, 'logloss' for LGBMClassifier, 'ndcg' for LGBMRanker.\n        early_stopping_rounds : int or None, optional (default=None)\n            Activates early stopping. The model will train until the validation score stops improving.\n            Validation score needs to improve at least every ``early_stopping_rounds`` round(s)\n            to continue training.\n            Requires at least one validation data and one metric.\n            If there's more than one, will check all of them. But the training data is ignored anyway.\n        verbose : bool or int, optional (default=True)\n            Requires at least one evaluation data.\n            If True, the eval metric on the eval set is printed at each boosting stage.\n            If int, the eval metric on the eval set is printed at every ``verbose`` boosting stage.\n            The last boosting stage or the boosting stage found by using ``early_stopping_rounds`` is also printed.\n\n            Example\n            -------\n            With ``verbose`` = 4 and at least one item in ``eval_set``,\n            an evaluation metric is printed every 4 (instead of 1) boosting stages.\n\n        feature_name : list of strings or 'auto', optional (default='auto')\n            Feature names.\n            If 'auto' and data is pandas DataFrame, data columns names are used.\n        categorical_feature : list of strings or int, or 'auto', optional (default='auto')\n            Categorical features.\n            If list of int, interpreted as indices.\n            If list of strings, interpreted as feature names (need to specify ``feature_name`` as well).\n            If 'auto' and data is pandas DataFrame, pandas categorical columns are used.\n            All values in categorical features should be less than int32 max value (2147483647).\n            Large values could be memory consuming. Consider using consecutive integers starting from zero.\n            All negative values in categorical features will be treated as missing values.\n        callbacks : list of callback functions or None, optional (default=None)\n            List of callback functions that are applied at each iteration.\n            See Callbacks in Python API for more information.\n\n        Returns\n        -------\n        self : object\n            Returns self.\n\n        Note\n        ----\n        Custom eval function expects a callable with following signatures:\n        ``func(y_true, y_pred)``, ``func(y_true, y_pred, weight)`` or\n        ``func(y_true, y_pred, weight, group)``\n        and returns (eval_name, eval_result, is_bigger_better) or\n        list of (eval_name, eval_result, is_bigger_better):\n\n            y_true : array-like of shape = [n_samples]\n                The target values.\n            y_pred : array-like of shape = [n_samples] or shape = [n_samples * n_classes] (for multi-class task)\n                The predicted values.\n            weight : array-like of shape = [n_samples]\n                The weight of samples.\n            group : array-like\n                Group/query data, used for ranking task.\n            eval_name : string\n                The name of evaluation.\n            eval_result : float\n                The eval result.\n            is_bigger_better : bool\n                Is eval result bigger better, e.g. AUC is bigger_better.\n\n        For multi-class task, the y_pred is group by class_id first, then group by row_id.\n        If you want to get i-th row y_pred in j-th class, the access way is y_pred[j * num_data + i].\n        \"\"\"\n        if self._objective is None:\n            if isinstance(self, LGBMRegressor):\n                self._objective = \"regression\"\n            elif isinstance(self, LGBMClassifier):\n                self._objective = \"binary\"\n            elif isinstance(self, LGBMRanker):\n                self._objective = \"lambdarank\"\n            else:\n                raise ValueError(\"Unknown LGBMModel type.\")\n        if callable(self._objective):\n            self._fobj = _objective_function_wrapper(self._objective)\n        else:\n            self._fobj = None\n        evals_result = {}\n        params = self.get_params()\n        # user can set verbose with kwargs, it has higher priority\n        if not any(verbose_alias in params for verbose_alias in ('verbose', 'verbosity')) and self.silent:\n            params['verbose'] = -1\n        params.pop('silent', None)\n        params.pop('importance_type', None)\n        params.pop('n_estimators', None)\n        params.pop('class_weight', None)\n        if self._n_classes is not None and self._n_classes > 2:\n            params['num_class'] = self._n_classes\n        if hasattr(self, '_eval_at'):\n            params['eval_at'] = self._eval_at\n        params['objective'] = self._objective\n        if self._fobj:\n            params['objective'] = 'None'  # objective = nullptr for unknown objective\n\n        if callable(eval_metric):\n            feval = _eval_function_wrapper(eval_metric)\n        else:\n            feval = None\n            # register default metric for consistency with callable eval_metric case\n            original_metric = self._objective if isinstance(self._objective, string_type) else None\n            if original_metric is None:\n                # try to deduce from class instance\n                if isinstance(self, LGBMRegressor):\n                    original_metric = \"l2\"\n                elif isinstance(self, LGBMClassifier):\n                    original_metric = \"multi_logloss\" if self._n_classes > 2 else \"binary_logloss\"\n                elif isinstance(self, LGBMRanker):\n                    original_metric = \"ndcg\"\n            # overwrite default metric by explicitly set metric\n            for metric_alias in ['metric', 'metrics', 'metric_types']:\n                if metric_alias in params:\n                    original_metric = params.pop(metric_alias)\n            # concatenate metric from params (or default if not provided in params) and eval_metric\n            original_metric = [original_metric] if isinstance(original_metric, (string_type, type(None))) else original_metric\n            eval_metric = [eval_metric] if isinstance(eval_metric, (string_type, type(None))) else eval_metric\n            params['metric'] = set(original_metric + eval_metric)\n\n        if not isinstance(X, DataFrame):\n            _X, _y = _LGBMCheckXY(X, y, accept_sparse=True, force_all_finite=False, ensure_min_samples=2)\n            _LGBMCheckConsistentLength(_X, _y, sample_weight)\n        else:\n            _X, _y = X, y\n\n        if self.class_weight is not None:\n            class_sample_weight = _LGBMComputeSampleWeight(self.class_weight, y)\n            if sample_weight is None or len(sample_weight) == 0:\n                sample_weight = class_sample_weight\n            else:\n                sample_weight = np.multiply(sample_weight, class_sample_weight)\n\n        self._n_features = _X.shape[1]\n\n        def _construct_dataset(X, y, sample_weight, init_score, group, params):\n            ret = Dataset(X, label=y, weight=sample_weight, group=group, params=params)\n            return ret.set_init_score(init_score)\n\n        train_set = _construct_dataset(_X, _y, sample_weight, init_score, group, params)\n\n        valid_sets = []\n        if eval_set is not None:\n\n            def _get_meta_data(collection, i):\n                if collection is None:\n                    return None\n                elif isinstance(collection, list):\n                    return collection[i] if len(collection) > i else None\n                elif isinstance(collection, dict):\n                    return collection.get(i, None)\n                else:\n                    raise TypeError('eval_sample_weight, eval_class_weight, eval_init_score, and eval_group '\n                                    'should be dict or list')\n\n            if isinstance(eval_set, tuple):\n                eval_set = [eval_set]\n            for i, valid_data in enumerate(eval_set):\n                # reduce cost for prediction training data\n                if valid_data[0] is X and valid_data[1] is y:\n                    valid_set = train_set\n                else:\n                    valid_weight = _get_meta_data(eval_sample_weight, i)\n                    if _get_meta_data(eval_class_weight, i) is not None:\n                        valid_class_sample_weight = _LGBMComputeSampleWeight(_get_meta_data(eval_class_weight, i),\n                                                                             valid_data[1])\n                        if valid_weight is None or len(valid_weight) == 0:\n                            valid_weight = valid_class_sample_weight\n                        else:\n                            valid_weight = np.multiply(valid_weight, valid_class_sample_weight)\n                    valid_init_score = _get_meta_data(eval_init_score, i)\n                    valid_group = _get_meta_data(eval_group, i)\n                    valid_set = _construct_dataset(valid_data[0], valid_data[1],\n                                                   valid_weight, valid_init_score, valid_group, params)\n                valid_sets.append(valid_set)\n\n        self._Booster = train(params, train_set,\n                              self.n_estimators, valid_sets=valid_sets, valid_names=eval_names,\n                              early_stopping_rounds=early_stopping_rounds,\n                              evals_result=evals_result, fobj=self._fobj, feval=feval,\n                              verbose_eval=verbose, feature_name=feature_name,\n                              categorical_feature=categorical_feature,\n                              callbacks=callbacks)\n\n        if evals_result:\n            self._evals_result = evals_result\n\n        if early_stopping_rounds is not None:\n            self._best_iteration = self._Booster.best_iteration\n\n        self._best_score = self._Booster.best_score\n\n        # free dataset\n        self.booster_.free_dataset()\n        del train_set, valid_sets\n        return self\n\n    def predict(self, X, raw_score=False, num_iteration=None,\n                pred_leaf=False, pred_contrib=False, **kwargs):\n        \"\"\"Return the predicted value for each sample.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            Input features matrix.\n        raw_score : bool, optional (default=False)\n            Whether to predict raw scores.\n        num_iteration : int or None, optional (default=None)\n            Limit number of iterations in the prediction.\n            If None, if the best iteration exists, it is used; otherwise, all trees are used.\n            If <= 0, all trees are used (no limits).\n        pred_leaf : bool, optional (default=False)\n            Whether to predict leaf index.\n        pred_contrib : bool, optional (default=False)\n            Whether to predict feature contributions.\n\n            Note\n            ----\n            If you want to get more explanation for your model's predictions using SHAP values\n            like SHAP interaction values,\n            you can install shap package (https://github.com/slundberg/shap).\n\n        **kwargs\n            Other parameters for the prediction.\n\n        Returns\n        -------\n        predicted_result : array-like of shape = [n_samples] or shape = [n_samples, n_classes]\n            The predicted values.\n        X_leaves : array-like of shape = [n_samples, n_trees] or shape [n_samples, n_trees * n_classes]\n            If ``pred_leaf=True``, the predicted leaf every tree for each sample.\n        X_SHAP_values : array-like of shape = [n_samples, n_features + 1] or shape [n_samples, (n_features + 1) * n_classes]\n            If ``pred_contrib=True``, the each feature contributions for each sample.\n        \"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError(\"Estimator not fitted, call `fit` before exploiting the model.\")\n        if not isinstance(X, DataFrame):\n            X = _LGBMCheckArray(X, accept_sparse=True, force_all_finite=False)\n        n_features = X.shape[1]\n        if self._n_features != n_features:\n            raise ValueError(\"Number of features of the model must \"\n                             \"match the input. Model n_features_ is %s and \"\n                             \"input n_features is %s \"\n                             % (self._n_features, n_features))\n        return self.booster_.predict(X, raw_score=raw_score, num_iteration=num_iteration,\n                                     pred_leaf=pred_leaf, pred_contrib=pred_contrib, **kwargs)\n\n    @property\n    def n_features_(self):\n        \"\"\"Get the number of features of fitted model.\"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No n_features found. Need to call fit beforehand.')\n        return self._n_features\n\n    @property\n    def best_score_(self):\n        \"\"\"Get the best score of fitted model.\"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No best_score found. Need to call fit beforehand.')\n        return self._best_score\n\n    @property\n    def best_iteration_(self):\n        \"\"\"Get the best iteration of fitted model.\"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No best_iteration found. Need to call fit with early_stopping_rounds beforehand.')\n        return self._best_iteration\n\n    @property\n    def objective_(self):\n        \"\"\"Get the concrete objective used while fitting this model.\"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No objective found. Need to call fit beforehand.')\n        return self._objective\n\n    @property\n    def booster_(self):\n        \"\"\"Get the underlying lightgbm Booster of this model.\"\"\"\n        if self._Booster is None:\n            raise LGBMNotFittedError('No booster found. Need to call fit beforehand.')\n        return self._Booster\n\n    @property\n    def evals_result_(self):\n        \"\"\"Get the evaluation results.\"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No results found. Need to call fit with eval_set beforehand.')\n        return self._evals_result\n\n    @property\n    def feature_importances_(self):\n        \"\"\"Get feature importances.\n\n        Note\n        ----\n        Feature importance in sklearn interface used to normalize to 1,\n        it's deprecated after 2.0.4 and is the same as Booster.feature_importance() now.\n        ``importance_type`` attribute is passed to the function\n        to configure the type of importance values to be extracted.\n        \"\"\"\n        if self._n_features is None:\n            raise LGBMNotFittedError('No feature_importances found. Need to call fit beforehand.')\n        return self.booster_.feature_importance(importance_type=self.importance_type)\n\n\nclass LGBMRegressor(LGBMModel, _LGBMRegressorBase):\n    \"\"\"LightGBM regressor.\"\"\"\n\n    def fit(self, X, y,\n            sample_weight=None, init_score=None,\n            eval_set=None, eval_names=None, eval_sample_weight=None,\n            eval_init_score=None, eval_metric=None, early_stopping_rounds=None,\n            verbose=True, feature_name='auto', categorical_feature='auto', callbacks=None):\n        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n        super(LGBMRegressor, self).fit(X, y, sample_weight=sample_weight,\n                                       init_score=init_score, eval_set=eval_set,\n                                       eval_names=eval_names,\n                                       eval_sample_weight=eval_sample_weight,\n                                       eval_init_score=eval_init_score,\n                                       eval_metric=eval_metric,\n                                       early_stopping_rounds=early_stopping_rounds,\n                                       verbose=verbose, feature_name=feature_name,\n                                       categorical_feature=categorical_feature,\n                                       callbacks=callbacks)\n        return self\n\n    _base_doc = LGBMModel.fit.__doc__\n    fit.__doc__ = (_base_doc[:_base_doc.find('eval_class_weight :')]\n                   + _base_doc[_base_doc.find('eval_init_score :'):])\n\n\nclass LGBMClassifier(LGBMModel, _LGBMClassifierBase):\n    \"\"\"LightGBM classifier.\"\"\"\n\n    def fit(self, X, y,\n            sample_weight=None, init_score=None,\n            eval_set=None, eval_names=None, eval_sample_weight=None,\n            eval_class_weight=None, eval_init_score=None, eval_metric=None,\n            early_stopping_rounds=None, verbose=True,\n            feature_name='auto', categorical_feature='auto', callbacks=None):\n        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n        _LGBMAssertAllFinite(y)\n        _LGBMCheckClassificationTargets(y)\n        self._le = _LGBMLabelEncoder().fit(y)\n        _y = self._le.transform(y)\n\n        self._classes = self._le.classes_\n        self._n_classes = len(self._classes)\n        if self._n_classes > 2:\n            # Switch to using a multiclass objective in the underlying LGBM instance\n            ova_aliases = (\"multiclassova\", \"multiclass_ova\", \"ova\", \"ovr\")\n            if self._objective not in ova_aliases and not callable(self._objective):\n                self._objective = \"multiclass\"\n            if eval_metric in ('logloss', 'binary_logloss'):\n                eval_metric = \"multi_logloss\"\n            elif eval_metric in ('error', 'binary_error'):\n                eval_metric = \"multi_error\"\n        else:\n            if eval_metric in ('logloss', 'multi_logloss'):\n                eval_metric = 'binary_logloss'\n            elif eval_metric in ('error', 'multi_error'):\n                eval_metric = 'binary_error'\n\n        if eval_set is not None:\n            if isinstance(eval_set, tuple):\n                eval_set = [eval_set]\n            for i, (valid_x, valid_y) in enumerate(eval_set):\n                if valid_x is X and valid_y is y:\n                    eval_set[i] = (valid_x, _y)\n                else:\n                    eval_set[i] = (valid_x, self._le.transform(valid_y))\n\n        super(LGBMClassifier, self).fit(X, _y, sample_weight=sample_weight,\n                                        init_score=init_score, eval_set=eval_set,\n                                        eval_names=eval_names,\n                                        eval_sample_weight=eval_sample_weight,\n                                        eval_class_weight=eval_class_weight,\n                                        eval_init_score=eval_init_score,\n                                        eval_metric=eval_metric,\n                                        early_stopping_rounds=early_stopping_rounds,\n                                        verbose=verbose, feature_name=feature_name,\n                                        categorical_feature=categorical_feature,\n                                        callbacks=callbacks)\n        return self\n\n    fit.__doc__ = LGBMModel.fit.__doc__\n\n    def predict(self, X, raw_score=False, num_iteration=None,\n                pred_leaf=False, pred_contrib=False, **kwargs):\n        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n        result = self.predict_proba(X, raw_score, num_iteration,\n                                    pred_leaf, pred_contrib, **kwargs)\n        if raw_score or pred_leaf or pred_contrib:\n            return result\n        else:\n            class_index = np.argmax(result, axis=1)\n            return self._le.inverse_transform(class_index)\n\n    predict.__doc__ = LGBMModel.predict.__doc__\n\n    def predict_proba(self, X, raw_score=False, num_iteration=None,\n                      pred_leaf=False, pred_contrib=False, **kwargs):\n        \"\"\"Return the predicted probability for each class for each sample.\n\n        Parameters\n        ----------\n        X : array-like or sparse matrix of shape = [n_samples, n_features]\n            Input features matrix.\n        raw_score : bool, optional (default=False)\n            Whether to predict raw scores.\n        num_iteration : int or None, optional (default=None)\n            Limit number of iterations in the prediction.\n            If None, if the best iteration exists, it is used; otherwise, all trees are used.\n            If <= 0, all trees are used (no limits).\n        pred_leaf : bool, optional (default=False)\n            Whether to predict leaf index.\n        pred_contrib : bool, optional (default=False)\n            Whether to predict feature contributions.\n\n            Note\n            ----\n            If you want to get more explanation for your model's predictions using SHAP values\n            like SHAP interaction values,\n            you can install shap package (https://github.com/slundberg/shap).\n\n        **kwargs\n            Other parameters for the prediction.\n\n        Returns\n        -------\n        predicted_probability : array-like of shape = [n_samples, n_classes]\n            The predicted probability for each class for each sample.\n        X_leaves : array-like of shape = [n_samples, n_trees * n_classes]\n            If ``pred_leaf=True``, the predicted leaf every tree for each sample.\n        X_SHAP_values : array-like of shape = [n_samples, (n_features + 1) * n_classes]\n            If ``pred_contrib=True``, the each feature contributions for each sample.\n        \"\"\"\n        result = super(LGBMClassifier, self).predict(X, raw_score, num_iteration,\n                                                     pred_leaf, pred_contrib, **kwargs)\n        if self._n_classes > 2 or raw_score or pred_leaf or pred_contrib:\n            return result\n        else:\n            return np.vstack((1. - result, result)).transpose()\n\n    @property\n    def classes_(self):\n        \"\"\"Get the class label array.\"\"\"\n        if self._classes is None:\n            raise LGBMNotFittedError('No classes found. Need to call fit beforehand.')\n        return self._classes\n\n    @property\n    def n_classes_(self):\n        \"\"\"Get the number of classes.\"\"\"\n        if self._n_classes is None:\n            raise LGBMNotFittedError('No classes found. Need to call fit beforehand.')\n        return self._n_classes\n\n\nclass LGBMRanker(LGBMModel):\n    \"\"\"LightGBM ranker.\"\"\"\n\n    def fit(self, X, y,\n            sample_weight=None, init_score=None, group=None,\n            eval_set=None, eval_names=None, eval_sample_weight=None,\n            eval_init_score=None, eval_group=None, eval_metric=None,\n            eval_at=[1], early_stopping_rounds=None, verbose=True,\n            feature_name='auto', categorical_feature='auto', callbacks=None):\n        \"\"\"Docstring is inherited from the LGBMModel.\"\"\"\n        # check group data\n        if group is None:\n            raise ValueError(\"Should set group for ranking task\")\n\n        if eval_set is not None:\n            if eval_group is None:\n                raise ValueError(\"Eval_group cannot be None when eval_set is not None\")\n            elif len(eval_group) != len(eval_set):\n                raise ValueError(\"Length of eval_group should be equal to eval_set\")\n            elif (isinstance(eval_group, dict)\n                  and any(i not in eval_group or eval_group[i] is None for i in range_(len(eval_group)))\n                  or isinstance(eval_group, list)\n                  and any(group is None for group in eval_group)):\n                raise ValueError(\"Should set group for all eval datasets for ranking task; \"\n                                 \"if you use dict, the index should start from 0\")\n\n        self._eval_at = eval_at\n        super(LGBMRanker, self).fit(X, y, sample_weight=sample_weight,\n                                    init_score=init_score, group=group,\n                                    eval_set=eval_set, eval_names=eval_names,\n                                    eval_sample_weight=eval_sample_weight,\n                                    eval_init_score=eval_init_score, eval_group=eval_group,\n                                    eval_metric=eval_metric,\n                                    early_stopping_rounds=early_stopping_rounds,\n                                    verbose=verbose, feature_name=feature_name,\n                                    categorical_feature=categorical_feature,\n                                    callbacks=callbacks)\n        return self\n\n    _base_doc = LGBMModel.fit.__doc__\n    fit.__doc__ = (_base_doc[:_base_doc.find('eval_class_weight :')]\n                   + _base_doc[_base_doc.find('eval_init_score :'):])\n    _base_doc = fit.__doc__\n    _before_early_stop, _early_stop, _after_early_stop = _base_doc.partition('early_stopping_rounds :')\n    fit.__doc__ = (_before_early_stop\n                   + 'eval_at : list of int, optional (default=[1])\\n'\n                   + ' ' * 12 + 'The evaluation positions of the specified metric.\\n'\n                   + ' ' * 8 + _early_stop + _after_early_stop)\n", "idx": 3, "id": 19667, "msg": "", "proj": "microsoft-LightGBM", "lang": "cpp"}
{"patch": "@@ -218,7 +218,8 @@ class Plan < ActiveRecord::Base\n \n         if self.save!\n           # Send an email confirmation to the owners and co-owners\n-          deliver_if(recipients: self.owner_and_coowners, key: 'users.feedback_requested') do |r|\n+          owners = User.joins(:roles).where('roles.plan_id =? AND roles.access IN (?)', self.id, Role.access_values_for(:administrator))\n+          deliver_if(recipients: owners, key: 'users.feedback_requested') do |r|\n             UserMailer.feedback_confirmation(r, self, user).deliver_now\n           end\n           # Send an email to all of the org admins as well as the Org's administrator email", "y": 1, "oldf": "class Plan < ActiveRecord::Base\n  include ConditionalUserMailer\n  include ExportablePlan\n  before_validation :set_creation_defaults\n\n  ##\n  # Associations\n  belongs_to :template\n  has_many :phases, through: :template\n  has_many :sections, through: :phases\n  has_many :questions, through: :sections\n  has_many :themes, through: :questions\n  has_many :answers, dependent: :destroy\n  has_many :notes, through: :answers\n  has_many :roles, dependent: :destroy\n  has_many :users, through: :roles\n  has_and_belongs_to_many :guidance_groups, join_table: :plans_guidance_groups\n\n  accepts_nested_attributes_for :template\n  has_many :exported_plans\n\n  has_many :roles\n\n# COMMENTED OUT THE DIRECT CONNECTION HERE TO Users to prevent assignment of users without an access_level specified (currently defaults to creator)\n#  has_many :users, through: :roles\n\n\n  ##\n  # Possibly needed for active_admin\n  #   -relies on protected_attributes gem as syntax depricated in rails 4.2\n  attr_accessible :locked, :project_id, :version_id, :version, :plan_sections,\n                  :exported_plans, :project, :title, :template, :grant_number,\n                  :identifier, :principal_investigator, :principal_investigator_identifier,\n                  :description, :data_contact, :funder_name, :visibility, :exported_plans,\n                  :roles, :users, :org, :data_contact_email, :data_contact_phone, :feedback_requested,\n                  :principal_investigator_email, :as => [:default, :admin]\n  accepts_nested_attributes_for :roles\n\n  # public is a Ruby keyword so using publicly\n  enum visibility: [:organisationally_visible, :publicly_visible, :is_test, :privately_visible]\n\n  #TODO: work out why this messes up plan creation :\n  #   briley: Removed reliance on :users, its really on :roles (shouldn't have a plan without at least a creator right?) It should be ok like this though now\n#  validates :template, :title, presence: true\n\n  ##\n  # Constants\n  A4_PAGE_HEIGHT = 297 #(in mm)\n  A4_PAGE_WIDTH = 210 #(in mm)\n  ROUNDING = 5 #round estimate up to nearest 5%\n  FONT_HEIGHT_CONVERSION_FACTOR = 0.35278 #convert font point size to mm\n  FONT_WIDTH_HEIGHT_RATIO = 0.4 #Assume glyph width averages 2/5 the height\n\n  # Scope queries\n  # Note that in ActiveRecord::Enum the mappings are exposed through a class method with the pluralized attribute name (e.g visibilities rather than visibility)\n  scope :publicly_visible, -> { includes(:template).where(:visibility => visibilities[:publicly_visible]) }\n\n  # Retrieves any plan in which the user has an active role and it is not a reviewer\n  scope :active, -> (user) {\n    includes([:template, :roles]).where({ \"roles.active\": true, \"roles.user_id\": user.id }).where(Role.not_reviewer_condition)\n  }\n\n  # Retrieves any plan organisationally or publicly visible for a given org id\n  scope :organisationally_or_publicly_visible, -> (user) {\n    includes(:template, {roles: :user})\n      .where({\n        visibility: [visibilities[:organisationally_visible], visibilities[:publicly_visible]],\n        \"roles.access\": Role.access_values_for(:creator, :administrator, :editor, :commenter).min,\n        \"users.org_id\": user.org_id})\n      .where(['NOT EXISTS (SELECT 1 FROM roles WHERE plan_id = plans.id AND user_id = ?)', user.id])\n  }\n\n  scope :search, -> (term) {\n    search_pattern = \"%#{term}%\"\n    joins(:template).where(\"plans.title LIKE ? OR templates.title LIKE ?\", search_pattern, search_pattern)\n  }\n\n  # Retrieves plan, template, org, phases, sections and questions\n  scope :overview, -> (id) {\n    Plan.includes(:phases, :sections, :questions, template: [ :org ]).find(id)\n  }\n  ##\n  # Settings for the template\n  has_settings :export, class_name: 'Settings::Template' do |s|\n    s.key :export, defaults: Settings::Template::DEFAULT_SETTINGS\n  end\n  alias_method :super_settings, :settings\n\n\n  ##\n  # Proxy through to the template settings (or defaults if this plan doesn't have\n  # an associated template) if there are no settings stored for this plan.\n  # `key` is required by rails-settings, so it's required here, too.\n  #\n  # @param key [Key] a key required by rails\n  # @return [Settings] settings for this plan's template\n  def settings(key)\n    self_settings = self.super_settings(key)\n    return self_settings if self_settings.value?\n#    self.dmptemplate.settings(key)\n    self.template.settings(key) unless self.template.nil?\n  end\n\n  ##\n  # returns the template for this plan, or generates an empty template and returns that\n  #\n  # @return [Dmptemplate] the template associated with this plan\n  def dmptemplate\n    #self.project.try(:dmptemplate) || Dmptemplate.new\n    self.template\n  end\n\n\n\n  def base_template\n    base = nil\n    t = self.template\n    if t.customization_of.present?\n      base = Template.where(\"dmptemplate_id = ? and created_at < ?\", t.customization_of, self.created_at).order(version: :desc).first\n    end\n    return base\n  end\n\n\n\n  ##\n  # returns the most recent answer to the given question id\n  # optionally can create an answer if none exists\n  #\n  # @param qid [Integer] the id for the question to find the answer for\n  # @param create_if_missing [Boolean] if true, will genereate a default answer to the question\n  # @return [Answer,nil] the most recent answer to the question, or a new question with default value, or nil\n  def answer(qid, create_if_missing = true)\n    answer = answers.where(:question_id => qid).order(\"created_at DESC\").first\n    question = Question.find(qid)\n    if answer.nil? && create_if_missing then\n      answer = Answer.new\n      answer.plan_id = id\n      answer.question_id = qid\n      answer.text = question.default_value\n      default_options = Array.new\n      question.question_options.each do |option|\n        if option.is_default\n          default_options << option\n        end\n      end\n      answer.question_options = default_options\n    end\n    return answer\n  end\n\n# TODO: This just retrieves all of the guidance associated with the themes within the template\n#       so why are we transferring it here to the plan?\n  ##\n  # returns all of the sections for this version of the plan, and for the project's organisation\n  #\n  # @return [Array<Section>,nil] either a list of sections, or nil if none were found\n  def set_possible_guidance_groups\n    # find all the themes in this plan\n    # and get the guidance groups they belong to\n    ggroups = []\n    self.template.phases.each do |phase|\n      phase.sections.each do |section|\n        section.questions.each do |question|\n          question.themes.each do |theme|\n            theme.guidances.each do |guidance|\n              ggroups << guidance.guidance_group if guidance.guidance_group.published\n              # only show published guidance groups\n            end\n          end\n        end\n      end\n    end\n\n    self.guidance_groups = ggroups.uniq\n  end\n\n  ##\n  # returns all of the possible guidance groups for the plan (all options to\n  # be selected by the user to display)\n  #\n  # @return Array<Guidance>\n  def get_guidance_group_options\n    # find all the themes in this plan\n    # and get the guidance groups they belong to\n    ggroups = []\n    Template.includes(phases: [sections: [questions: [themes: [guidances: [guidance_group: :org]]]]]).find(self.template_id).phases.each do |phase|\n      phase.sections.each do |section|\n        section.questions.each do |question|\n          question.themes.each do |theme|\n            theme.guidances.each do |guidance|\n              ggroups << guidance.guidance_group if guidance.guidance_group.published\n              # only show published guidance groups\n            end\n          end\n        end\n      end\n    end\n    return ggroups.uniq\n  end\n  \n  ##\n  # Sets up the plan for feedback:\n  #  emails confirmation messages to owners\n  #  emails org admins and org contact \n  #  adds org admins to plan with the 'reviewer' Role\n  def request_feedback(user)\n    Plan.transaction do\n      begin\n        val = Role.access_values_for(:reviewer, :commenter).min\n        self.feedback_requested = true\n    \n        # Share the plan with each org admin as the reviewer role\n        admins = user.org.org_admins\n        admins.each do |admin|\n          self.roles << Role.new(user: admin, access: val)\n        end \n\n        if self.save!\n          # Send an email confirmation to the owners and co-owners\n          deliver_if(recipients: self.owner_and_coowners, key: 'users.feedback_requested') do |r|\n            UserMailer.feedback_confirmation(r, self, user).deliver_now\n          end\n          # Send an email to all of the org admins as well as the Org's administrator email\n          if user.org.contact_email.present? && !admins.collect{ |u| u.email }.include?(user.org.contact_email)\n            admins << User.new(email: user.org.contact_email, firstname: user.org.contact_name)\n          end\n          deliver_if(recipients: admins, key: 'admins.feedback_requested') do |r|\n            UserMailer.feedback_notification(r, self, user).deliver_now\n          end\n          true\n        else\n          false\n        end\n      rescue Exception => e\n        Rails.logger.error e\n        false\n      end\n    end\n  end\n\n  ##\n  # Finalizes the feedback for the plan:\n  #  emails confirmation messages to owners\n  #  sets flag on plans.feedback_requested to false\n  #  removes org admins from the 'reviewer' Role for the Plan\n  def complete_feedback(org_admin)\n    Plan.transaction do\n      begin\n        self.feedback_requested = false\n        \n        # Remove the org admins reviewer role from the plan \n        vals = Role.access_values_for(:reviewer)\n        self.roles.delete(Role.where(plan: self, access: vals))\n        \n        if self.save!\n          # Send an email confirmation to the owners and co-owners\n          deliver_if(recipients: self.owner_and_coowners, key: 'users.feedback_provided') do |r|\n            UserMailer.feedback_notification(r, self, org_admin).deliver_now\n          end\n          true\n        else\n          false\n        end\n      rescue Exception => e\n        Rails.logger.error e\n        false\n      end\n    end\n  end\n\n  # Returns all of the plan's available guidance by question as a hash for use on the write plan page\n  # {\n  #   QUESTION: {\n  #     GUIDANCE_GROUP: {\n  #       THEME: [GUIDANCE, GUIDANCE],\n  #       THEME: [GUIDANCE]\n  #     }\n  #   }\n  # }\n  def guidance_by_question_as_hash\n    # Get all of the selected guidance groups for the plan\n    guidance_groups_ids = self.guidance_groups.collect(&:id)\n    guidance_groups =  GuidanceGroup.joins(:org).where(\"guidance_groups.published = ? AND guidance_groups.id IN (?) AND orgs.id != ?\", \n                                                       true, guidance_groups_ids, self.template.org.id)\n\n    # Gather all of the Themes used in the plan as a hash\n    # {\n    #  QUESTION: [THEME, THEME], \n    #  QUESTION: [THEME]\n    # }\n    question_themes = {}\n    themes_used = []\n    self.questions.joins(:themes).pluck('questions.id', 'themes.title').each do |qt|\n      themes_used << qt[1] unless themes_used.include?(qt[1])\n      question_themes[qt[0]] = [] unless question_themes[qt[0]].present?\n      question_themes[qt[0]] << qt[1] unless question_themes[qt[0]].include?(qt[1])\n    end\n\n    # Gather all of the Guidance available for the themes used in the plan as a hash\n    # {\n    #  THEME: {\n    #    GUIDANCE_GROUP: [GUIDANCE, GUIDANCE], \n    #    GUIDANCE_GROUP: [GUIDANCE]\n    #  }\n    # }\n    theme_guidance = {}\n    GuidanceGroup.includes(guidances: :themes).joins(:guidances).\n          where('guidance_groups.published = ? AND guidances.published = ? AND themes.title IN (?) AND guidance_groups.id IN (?)', true, true, themes_used, guidance_groups.collect(&:id)).\n          pluck('guidance_groups.name', 'themes.title', 'guidances.text').each do |tg|\n      \n      theme_guidance[tg[1]] = {} unless theme_guidance[tg[1]].present?\n      theme_guidance[tg[1]][tg[0]] = [] unless theme_guidance[tg[1]][tg[0]].present?\n      theme_guidance[tg[1]][tg[0]] << tg[2] unless theme_guidance[tg[1]][tg[0]].include?(tg[2])\n    end\n    \n    # Generate a hash for the view that contains all of a question guidance\n    # {\n    #   QUESTION: {\n    #     GUIDANCE_GROUP: {\n    #       THEME: [GUIDANCE, GUIDANCE],\n    #       THEME: [GUIDANCE]\n    #     }\n    #   }\n    # }\n    question_guidance = {}\n    question_themes.keys.each do |question|\n      ggs = {}\n      # Gather all of the guidance groups applicable to the themes assigned to the question\n      groups = []\n      question_themes[question].each do |theme|\n        groups << theme_guidance[theme].keys if theme_guidance[theme].present?\n      end\n        \n      # Loop through all of the applicable guidance groups and collect their themed guidance\n      groups.flatten.uniq.each do |guidance_group|\n        guidances_by_theme = {}\n        \n        # Collect all of the guidances for each theme used by the question\n        question_themes[question].each do |theme|\n          if theme_guidance[theme].present? && theme_guidance[theme][guidance_group].present?\n            guidances_by_theme[theme] = [] unless guidances_by_theme[theme].present?\n            guidances_by_theme[theme] = theme_guidance[theme][guidance_group]\n          end\n        end\n\n        ggs[guidance_group] = guidances_by_theme unless ggs[guidance_group]\n      end\n      \n      question_guidance[question] = ggs\n    end\n    \n    question_guidance\n  end\n\n  ##\n  # determines if the plan is editable by the specified user\n  #\n  # @param user_id [Integer] the id for a user\n  # @return [Boolean] true if user can edit the plan\n  def editable_by?(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    has_role(user_id, :editor)\n  end\n\n  ##\n  # determines if the plan is readable by the specified user\n  #\n  # @param user_id [Integer] the id for a user\n  # @return [Boolean] true if the user can read the plan\n  def readable_by?(user_id)\n    user = user_id.is_a?(User) ? user_id : User.find(user_id)\n    owner_orgs = self.owner_and_coowners.collect(&:org)\n    \n    # Super Admins can view plans read-only, Org Admins can view their Org's plans \n    # otherwise the user must have the commenter role\n    (user.can_super_admin? ||\n     user.can_org_admin? && owner_orgs.include?(user.org) ||\n     has_role(user.id, :commenter))\n  end\n\n  ##\n  # determines if the plan is readable by the specified user\n  #\n  # @param user_id [Integer] the id for a user\n  # @return [Boolean] true if the user can read the plan\n  def commentable_by?(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    has_role(user_id, :commenter)\n  end\n\n  ##\n  # determines if the plan is administerable by the specified user\n  #\n  # @param user_id [Integer] the id for the user\n  # @return [Boolean] true if the user can administer the plan\n  def administerable_by?(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    has_role(user_id, :administrator)\n  end\n\n  ##\n  # determines if the plan is owned by the specified user\n  #\n  # @param user_id [Integer] the id for the user\n  # @return [Boolean] true if the user can administer the plan\n  def owned_by?(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    has_role(user_id, :creator)\n  end\n\n  ##\n  # determines if the plan is reviewable by the specified user\n  #\n  # @param user_id [Integer] the id for the user\n  # @return [Boolean] true if the user can administer the plan\n  def reviewable_by?(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    has_role(user_id, :reviewer)\n  end\n\n  ##\n  # determines whether or not the specified user has any rol on the plan\n  #\n  # @param user_id [Integer] the id for the user\n  # @return [Boolean] true if the user has any rol\n  def any_role?(user)\n    user_id = user.id if user.is_a?(User)\n    !self.roles.index{ |rol| rol.user_id == user_id }.nil?\n  end\n\n  ##\n  # defines and returns the status of the plan\n  # status consists of a hash of the num_questions, num_answers, sections, questions, and spaced used.\n  # For each section, it contains the id's of each of the questions\n  # for each question, it contains the answer_id, answer_created_by, answer_text, answer_options_id, aand answered_by\n  #\n  # @return [Status]\n\n  def status\n    status = {\n      \"num_questions\" => 0,\n      \"num_answers\" => 0,\n      \"sections\" => {},\n      \"questions\" => {},\n      \"space_used\" => 0 # percentage of available space in pdf used\n    }\n\n    space_used = height_of_text(self.title, 2, 2)\n\n    section_ids = sections.map {|s| s.id}\n\n    # we retrieve this is 2 joins:\n    #   1. sections and questions\n    #   2. questions and answers\n    # why? because Rails 4 doesn't have any sensible left outer join.\n    # when we change to RAILS 5 it is meant to have so this can be fixed then\n\n    records = Section.joins(questions: :question_format)\n                     .select('sections.id as sectionid,\n                              sections.title as stitle,\n                              questions.id as questionid,\n                              questions.text as questiontext,\n                              question_formats.title as qformat')\n                     .where(\"sections.id in (?) \", section_ids)\n                     .to_a\n\n    # extract question ids to get answers\n    question_ids = records.map {|r| r.questionid}.uniq\n    status[\"num_questions\"] = question_ids.count\n\n    arecords = Question.joins(answers: :user)\n                       .select('questions.id as questionid,\n                                answers.id as answerid,\n                                answers.plan_id as plan_id,\n                                answers.text as answertext,\n                                answers.updated_at as updated,\n                                users.email as username')\n                       .where(\"questions.id in (?) and answers.plan_id = ?\",question_ids, self.id)\n                       .to_a\n\n    # we want answerids to extract options later\n    answer_ids = arecords.map {|r| r.answerid}.uniq\n    status[\"num_answers\"] = answer_ids.count\n\n    # create map from questionid to answer structure\n    qa_map = {}\n    arecords.each do |rec|\n      qa_map[rec.questionid] = {\n        plan: rec.plan_id,\n        id: rec.answerid,\n        text: rec.answertext,\n        updated: rec.updated,\n        user: rec.username\n      }\n    end\n\n\n    # build main status structure\n    records.each do |rec|\n      sid = rec.sectionid\n      stitle = rec.stitle\n      qid = rec.questionid\n      qtext = rec.questiontext\n      format = rec.qformat\n\n      answer = nil\n      if qa_map.has_key?(qid)\n        answer = qa_map[qid]\n      end\n\n      aid = answer.nil? ? nil : answer[:id]\n      atext = answer.nil? ? nil : answer[:text]\n      updated = answer.nil? ? nil : answer[:updated]\n      uname = answer.nil? ? nil : answer[:user]\n\n      space_used += height_of_text(stitle, 1, 1)\n\n      shash = status[\"sections\"]\n      if !shash.has_key?(sid)\n        shash[sid] = {}\n        shash[sid][\"num_questions\"] = 0\n        shash[sid][\"num_answers\"] = 0\n        shash[sid][\"questions\"] = Array.new\n      end\n\n      shash[sid][\"questions\"] << qid\n      shash[sid][\"num_questions\"] += 1\n\n      space_used += height_of_text(qtext) unless qtext == stitle\n      if atext.present?\n        space_used += height_of_text(atext)\n      else\n        space_used += height_of_text(_('Question not answered.'))\n      end\n\n      if answer.present? then\n        shash[sid][\"num_answers\"] += 1\n      end\n\n      status[\"questions\"][qid] = {\n        \"format\" => format,\n        \"answer_id\" => aid,\n        \"answer_updated_at\" => updated.to_i,\n        \"answer_text\" => atext,\n        \"answered_by\" => uname\n      }\n\n    end\n\n    records = Answer.joins(:question_options).select('answers.id as answerid, question_options.id as optid').where(id: answer_ids).to_a\n    opt_hash = {}\n    records.each do |rec|\n      aid = rec.answerid\n      optid = rec.optid\n      if !opt_hash.has_key?(aid)\n        opt_hash[aid] = Array.new\n      end\n      opt_hash[aid] << optid\n    end\n\n    status[\"questions\"].each_key do |questionid|\n      answerid = status[\"questions\"][questionid][\"answer_id\"]\n      status[\"questions\"][questionid][\"answer_option_ids\"] = opt_hash[answerid]\n    end\n\n    status['space_used'] = estimate_space_used(space_used)\n\n    return status\n  end\n\n\n  ##\n  # assigns the passed user_id to the creater_role for the project\n  # gives the user rights to read, edit, administrate, and defines them as creator\n  #\n  # @param user_id [Integer] the user to be given priveleges' id\n  def assign_creator(user_id)\n    user_id = user_id.id if user_id.is_a?(User)\n    add_user(user_id, true, true, true)\n  end\n\n  ##\n  # returns the funder id for the plan\n  #\n  # @return [Integer, nil] the id for the funder\n  def funder_id\n    if self.template.nil? then\n      return nil\n    end\n    return self.template.org\n  end\n\n  ##\n  # returns the funder organisation for the project or nil if none is specified\n  #\n  # @return [Organisation, nil] the funder for project, or nil if none exists\n  def funder\n    template = self.template\n    if template.nil? then\n      return nil\n    end\n\n    if template.customization_of\n      return template.customization_of.org\n    else\n      return template.org\n    end\n  end\n\n  ##\n  # assigns the passed user_id as an editor for the project\n  # gives the user rights to read and edit\n  #\n  # @param user_id [Integer] the user to be given priveleges' id\n  def assign_editor(user_id)\n    add_user(user_id, true)\n  end\n\n  ##\n  # assigns the passed user_id as a reader for the project\n  # gives the user rights to read\n  #\n  # @param user_id [Integer] the user to be given priveleges' id\n  def assign_reader(user_id)\n    add_user(user_id)\n  end\n\n  ##\n  # assigns the passed user_id as an administrator for the project\n  # gives the user rights to read, adit, and administrate the project\n  #\n  # @param user_id [Integer] the user to be given priveleges' id\n  def assign_administrator(user_id)\n    add_user(user_id, true, true)\n  end\n\n  ##\n  # the datetime for the latest update of this plan\n  #\n  # @return [DateTime] the time of latest update\n  def latest_update\n    latest_update = updated_at\n    phases.each do |phase|\n      if phase.updated_at > latest_update then\n        latest_update = phase.updated_at\n      end\n    end\n    return latest_update\n  end\n\n  # Getters to match 'My plans' columns\n\n  ##\n  # the title of the project\n  #\n  # @return [String] the title of the project\n  def name\n    self.title\n  end\n\n  ##\n  # the owner of the project\n  #\n  # @return [User] the creater of the project\n  def owner\n    vals = Role.access_values_for(:creator)\n    User.joins(:roles).where('roles.plan_id = ? AND roles.access IN (?)', self.id, vals).first\n  end\n\n  ##\n  # returns the shared roles of a plan, excluding the creator\n  def shared\n    role_values = Role.where(plan: self).where(Role.not_creator_condition).any? \n  end\n\n  ##\n  # the owner and co-owners of the project\n  #\n  # @return [Users]\n  def owner_and_coowners\n    vals = Role.access_values_for(:creator).concat(Role.access_values_for(:administrator))\n    User.joins(:roles).where(\"roles.plan_id = ? AND roles.access IN (?)\", self.id, vals)\n  end\n\n  ##\n  # the time the project was last updated, formatted as a date\n  #\n  # @return [Date] last update as a date\n  def last_edited\n    self.latest_update.to_date\n  end\n\n  # Returns the number of answered questions from the entire plan\n  def num_answered_questions\n    return Answer.where(id: answers.map(&:id)).includes({ question: :question_format }, :question_options).reduce(0) do |m, a|\n      if a.is_valid?\n        m+=1\n      end\n      m\n    end\n  end\n\n  # Returns a section given its id or nil if does not exist for the current plan\n  def get_section(section_id)\n    self.sections.find { |s| s.id == section_id }\n  end\n\n  # Returns the number of questions for a plan.\n  def num_questions\n    return sections.includes(:questions).joins(:questions).reduce(0){ |m, s| m + s.questions.length }\n  end\n  # the following two methods are for eager loading. One gets used for the plan/show\n  # page and the oter for the plan/edit. The difference is just that one pulls in more than\n  # the other.\n  # TODO: revisit this and work out for sure that maintaining the difference is worthwhile.\n  # it may not be. Also make sure nether is doing more thanit needs to.\n  #\n  def self.eager_load(id)\n    Plan.includes(\n      [{template: [\n                   {phases: {sections: {questions: :answers}}},\n                   {customizations: :org}\n                  ]},\n       {plans_guidance_groups: {guidance_group: :guidances}}\n      ]).find(id)\n  end\n\n  # Pre-fetched a plan phase together with its sections and questions associated. It also pre-fetches the answers and notes associated to the plan\n  def self.load_for_phase(id, phase_id)\n    plan = Plan\n      .joins(template: { phases: { sections: :questions }})\n      .preload(template: { phases: { sections: :questions }}) # Preserves the default order defined in the model relationships\n      .where(\"plans.id = :id AND phases.id = :phase_id\", { id: id, phase_id: phase_id })\n      .merge(Plan.includes(answers: :notes))[0]\n    phase = plan.template.phases.first\n    return plan, phase\n  end\n\n  # deep copy the given plan and all of it's associations\n  #\n  # @params [Plan] plan to be deep copied\n  # @return [Plan] saved copied plan\n  def self.deep_copy(plan)\n    plan_copy = plan.dup\n    plan_copy.title = \"Copy of \" + plan.title\n    plan_copy.save!\n    plan.answers.each do |answer|\n      answer_copy = Answer.deep_copy(answer)\n      answer_copy.plan_id = plan_copy.id\n      answer_copy.save!\n    end\n    plan.guidance_groups.each do |guidance_group|\n      if guidance_group.present?\n        plan_copy.guidance_groups << GuidanceGroup.where(id: guidance_group.id).first\n      end\n    end\n    return plan_copy\n  end\n\n  # Returns visibility message given a Symbol type visibility passed, otherwise nil\n  def self.visibility_message(type)\n    message = {\n      :organisationally_visible => _('organisational'),\n      :publicly_visible => _('public'),\n      :is_test => _('test'),\n      :privately_visible => _('private')\n    }\n    message[type]\n  end\n\n  # Determines whether or not visibility changes are permitted according to the\n  # percentage of the plan answered in respect to a threshold defined at application.config\n  def visibility_allowed?\n    value=(self.num_answered_questions().to_f/self.num_questions()*100).round(2)\n    !self.is_test? && value >= Rails.application.config.default_plan_percentage_answered\n  end\n\n  # Determines whether or not a question (given its id) exists for the self plan\n  def question_exists?(question_id)\n    Plan.joins(:questions).exists?(id: self.id, \"questions.id\": question_id)\n  end\n\n  # Checks whether or not the number of questions matches the number of valid answers\n  def no_questions_matches_no_answers?\n    num_questions = question_ids.length\n    pre_fetched_answers = Answer\n      .includes({ question: :question_format }, :question_options)\n      .where(id: answer_ids)\n    num_answers = pre_fetched_answers.reduce(0) do |m, a|\n      if a.is_valid? \n        m+=1\n      end\n      m\n    end\n    return num_questions == num_answers\n  end\n\n  private\n\n  # Returns whether or not the user has the specified role for the plan\n  def has_role(user_id, role_as_sym)\n    if user_id.is_a?(Integer) && role_as_sym.is_a?(Symbol)\n      vals = Role.access_values_for(role_as_sym)\n      self.roles.where(user_id: user_id, access: vals, active: true).first.present?\n    else\n      false\n    end\n  end\n\n  ##\n  # adds a user to the project\n  # if no flags are specified, the user is given read privleges\n  #\n  # @param user_id [Integer] the user to be given privleges\n  # @param is_editor [Boolean] whether or not the user can edit the project\n  # @param is_administrator [Boolean] whether or not the user can administrate the project\n  # @param is_creator [Boolean] wheter or not the user created the project\n  # @return [Array<ProjectGroup>]\n  #\n  # TODO: change this to specifying uniqueness of user/plan association and handle\n  # that way\n  #\n  def add_user(user_id, is_editor = false, is_administrator = false, is_creator = false)\n    Role.where(plan_id: self.id, user_id: user_id).each do |r|\n      r.destroy\n    end\n\n    role = Role.new\n    role.user_id = user_id\n    role.plan_id = id\n\n    # if you get assigned a role you can comment\n    role.commenter= true\n\n    # the rest of the roles are inclusing so creator => administrator => editor\n    if is_creator\n      role.creator =  true\n      role.administrator = true\n      role.editor = true\n    end\n\n    if is_administrator\n      role.administrator = true\n      role.editor = true\n    end\n\n    if is_editor\n      role.editor = true\n    end\n\n    role.save\n\n    # This is necessary because we're creating the associated record but not assigning it\n    # to roles. Auto-saving like this may be confusing when coding upstream in a controller,\n    # view or api. Should probably change this to:\n    #    self.roles << role\n    # and then let the save be called manually via:\n    #    plan.save!\n    #self.reload\n  end\n\n  ##\n  # creates a plan for each phase in the dmptemplate associated with this project\n  # unless the phase is unpublished, it creates a new plan, and a new version of the plan and adds them to the project's plans\n  #\n  # @return [Array<Plan>]\n  def create_plans\n    dmptemplate.phases.each do |phase|\n      latest_published_version = phase.latest_published_version\n      unless latest_published_version.nil?\n        new_plan = Plan.new\n        new_plan.version = latest_published_version\n        plans << new_plan\n      end\n    end\n  end\n\n\n\n  ##\n  # Based on the height of the text gathered so far and the available vertical\n  # space of the pdf, estimate a percentage of how much space has been used.\n  # This is highly dependent on the layout in the pdf. A more accurate approach\n  # would be to render the pdf and check how much space had been used, but that\n  # could be very slow.\n  # NOTE: This is only an estimate, rounded up to the nearest 5%; it is intended\n  # for guidance when editing plan data, not to be 100% accurate.\n  #\n  # @param used_height [Integer] an estimate of the height used so far\n  # @return [Integer] the estimate of space used of an A4 portrain\n  def estimate_space_used(used_height)\n    @formatting ||= self.settings(:export).formatting\n\n    return 0 unless @formatting[:font_size] > 0\n\n    margin_height    = @formatting[:margin][:top].to_i + @formatting[:margin][:bottom].to_i\n    page_height      = A4_PAGE_HEIGHT - margin_height # 297mm for A4 portrait\n    available_height = page_height * self.dmptemplate.settings(:export).max_pages\n\n    percentage = (used_height / available_height) * 100\n    (percentage / ROUNDING).ceil * ROUNDING # round up to nearest five\n  end\n\n  ##\n  # Take a guess at the vertical height (in mm) of the given text based on the\n  # font-size and left/right margins stored in the plan's settings.\n  # This assumes a fixed-width for each glyph, which is obviously\n  # incorrect for the font-face choices available; the idea is that\n  # they'll hopefully average out to that in the long-run.\n  # Allows for hinting different font sizes (offset from base via font_size_inc)\n  # and vertical margins (i.e. for heading text)\n  #\n  # @param text [String] the text to estimate size of\n  # @param font_size_inc [Integer] the size of the font of the text, defaults to 0\n  # @param vertical_margin [Integer] the top margin above the text, defaults to 0\n  def height_of_text(text, font_size_inc = 0, vertical_margin = 0)\n    @formatting     ||= self.settings(:export).formatting\n    @margin_width   ||= @formatting[:margin][:left].to_i + @formatting[:margin][:right].to_i\n    @base_font_size ||= @formatting[:font_size]\n\n    return 0 unless @base_font_size > 0\n\n    font_height = FONT_HEIGHT_CONVERSION_FACTOR * (@base_font_size + font_size_inc)\n    font_width  = font_height * FONT_WIDTH_HEIGHT_RATIO # Assume glyph width averages at 2/5s the height\n    leading     = font_height / 2\n\n    chars_in_line = (A4_PAGE_WIDTH - @margin_width) / font_width # 210mm for A4 portrait\n    num_lines = (text.length / chars_in_line).ceil\n\n    (num_lines * font_height) + vertical_margin + leading\n  end\n\n  # Initialize the title and dirty flags for new templates\n  # --------------------------------------------------------\n  def set_creation_defaults\n    # Only run this before_validation because rails fires this before save/create\n    if self.id.nil?\n      self.title = \"My plan (#{self.template.title})\" if self.title.nil? && !self.template.nil?\n    end\n  end\nend\n", "idx": 1, "id": 17452, "msg": "owner_and_coowners class method is still used after the change? If not, please remove it!", "proj": "DMPRoadmap-roadmap", "lang": "rb"}
{"patch": "@@ -925,7 +925,7 @@ class GTPUpdatePDPContextRequest(Packet):\n         IE_Dispatcher)]\n \n     def hashret(self):\n-        return struct.pack(\"H\", self.seq)\n+        return struct.pack(\"H\", getattr(self.underlayer, \"seq\"))\n \n \n class GTPUpdatePDPContextResponse(Packet):", "y": 0, "oldf": "# Copyright (C) 2018 Leonardo Monteiro <decastromonteiro@gmail.com>\n#               2017 Alexis Sultan    <alexis.sultan@sfr.com>\n#               2017 Alessio Deiana <adeiana@gmail.com>\n#               2014 Guillaume Valadon <guillaume.valadon@ssi.gouv.fr>\n#               2012 ffranz <ffranz@iniqua.com>\n##\n# This program is published under a GPLv2 license\n\n# scapy.contrib.description = GPRS Tunneling Protocol (GTP)\n# scapy.contrib.status = loads\n\nfrom __future__ import absolute_import\nimport struct\n\n\nfrom scapy.compat import chb, orb, bytes_encode\nfrom scapy.config import conf\nfrom scapy.error import warning\nfrom scapy.fields import BitEnumField, BitField, ByteEnumField, ByteField, \\\n    ConditionalField, FieldLenField, FieldListField, FlagsField, IntField, \\\n    IPField, PacketListField, ShortField, StrFixedLenField, StrLenField, \\\n    XBitField, XByteField, XIntField\nfrom scapy.layers.inet import IP, UDP\nfrom scapy.layers.inet6 import IPv6, IP6Field\nfrom scapy.layers.ppp import PPP\nfrom scapy.modules.six.moves import range\nfrom scapy.packet import bind_layers, bind_bottom_up, bind_top_down, \\\n    Packet, Raw\nfrom scapy.volatile import RandInt, RandIP, RandNum, RandString\n\n\n# GTP Data types\n\nRATType = {\n    1: \"UTRAN\",\n    2: \"GETRAN\",\n    3: \"WLAN\",\n    4: \"GAN\",\n    5: \"HSPA\"\n}\n\nGTPmessageType = {1: \"echo_request\",\n                  2: \"echo_response\",\n                  16: \"create_pdp_context_req\",\n                  17: \"create_pdp_context_res\",\n                  18: \"update_pdp_context_req\",\n                  19: \"update_pdp_context_resp\",\n                  20: \"delete_pdp_context_req\",\n                  21: \"delete_pdp_context_res\",\n                  26: \"error_indication\",\n                  27: \"pdu_notification_req\",\n                  31: \"supported_extension_headers_notification\",\n                  254: \"end_marker\",\n                  255: \"g_pdu\"}\n\nIEType = {1: \"Cause\",\n             2: \"IMSI\",\n             3: \"RAI\",\n             4: \"TLLI\",\n             5: \"P_TMSI\",\n             8: \"IE_ReorderingRequired\",\n          14: \"Recovery\",\n          15: \"SelectionMode\",\n          16: \"TEIDI\",\n          17: \"TEICP\",\n          19: \"TeardownInd\",\n          20: \"NSAPI\",\n          26: \"ChargingChrt\",\n          27: \"TraceReference\",\n          28: \"TraceType\",\n          127: \"ChargingId\",\n          128: \"EndUserAddress\",\n          131: \"AccessPointName\",\n          132: \"ProtocolConfigurationOptions\",\n          133: \"GSNAddress\",\n          134: \"MSInternationalNumber\",\n          135: \"QoS\",\n          148: \"CommonFlags\",\n          149: \"APNRestriction\",\n          151: \"RatType\",\n          152: \"UserLocationInformation\",\n          153: \"MSTimeZone\",\n          154: \"IMEI\",\n          181: \"MSInfoChangeReportingAction\",\n          184: \"BearerControlMode\",\n          191: \"EvolvedAllocationRetentionPriority\",\n          255: \"PrivateExtention\"}\n\nCauseValues = {0: \"Request IMSI\",\n               1: \"Request IMEI\",\n               2: \"Request IMSI and IMEI\",\n               3: \"No identity needed\",\n               4: \"MS Refuses\",\n               5: \"MS is not GPRS Responding\",\n               128: \"Request accepted\",\n               129: \"New PDP type due to network preference\",\n               130: \"New PDP type due to single address bearer only\",\n               192: \"Non-existent\",\n               193: \"Invalid message format\",\n               194: \"IMSI not known\",\n               195: \"MS is GPRS Detached\",\n               196: \"MS is not GPRS Responding\",\n               197: \"MS Refuses\",\n               198: \"Version not supported\",\n               199: \"No resources available\",\n               200: \"Service not supported\",\n               201: \"Mandatory IE incorrect\",\n               202: \"Mandatory IE missing\",\n               203: \"Optional IE incorrect\",\n               204: \"System failure\",\n               205: \"Roaming restriction\",\n               206: \"P-TMSI Signature mismatch\",\n               207: \"GPRS connection suspended\",\n               208: \"Authentication failure\",\n               209: \"User authentication failed\",\n               210: \"Context not found\",\n               211: \"All dynamic PDP addresses are occupied\",\n               212: \"No memory is available\",\n               213: \"Reallocation failure\",\n               214: \"Unknown mandatory extension header\",\n               215: \"Semantic error in the TFT operation\",\n               216: \"Syntactic error in TFT operation\",\n               217: \"Semantic errors in packet filter(s)\",\n               218: \"Syntactic errors in packet filter(s)\",\n               219: \"Missing or unknown APN\",\n               220: \"Unknown PDP address or PDP type\",\n               221: \"PDP context without TFT already activated\",\n               222: \"APN access denied : no subscription\",\n               223: \"APN Restriction type incompatibility with currently active PDP Contexts\",  # noqa: E501\n               224: \"MS MBMS Capabilities Insufficient\",\n               225: \"Invalid Correlation : ID\",\n               226: \"MBMS Bearer Context Superseded\",\n               227: \"Bearer Control Mode violation\",\n               228: \"Collision with network initiated request\"}\n\nSelection_Mode = {11111100: \"MS or APN\",\n                  11111101: \"MS\",\n                  11111110: \"NET\",\n                  11111111: \"FutureUse\"}\n\nTrueFalse_value = {254: \"False\",\n                   255: \"True\"}\n\n# http://www.arib.or.jp/IMT-2000/V720Mar09/5_Appendix/Rel8/29/29281-800.pdf\nExtensionHeadersTypes = {\n    0: \"No more extension headers\",\n    1: \"Reserved\",\n    2: \"Reserved\",\n    64: \"UDP Port\",\n    133: \"PDU Session Container\",\n    192: \"PDCP PDU Number\",\n    193: \"Reserved\",\n    194: \"Reserved\"\n}\n\n\nclass TBCDByteField(StrFixedLenField):\n\n    def i2h(self, pkt, val):\n        return val\n\n    def m2i(self, pkt, val):\n        ret = []\n        for v in val:\n            byte = orb(v)\n            left = byte >> 4\n            right = byte & 0xf\n            if left == 0xf:\n                ret.append(TBCD_TO_ASCII[right:right + 1])\n            else:\n                ret += [\n                    TBCD_TO_ASCII[right:right + 1],\n                    TBCD_TO_ASCII[left:left + 1]\n                ]\n        return b\"\".join(ret)\n\n    def i2m(self, pkt, val):\n        if not isinstance(val, bytes):\n            val = bytes_encode(val)\n        ret_string = b\"\"\n        for i in range(0, len(val), 2):\n            tmp = val[i:i + 2]\n            if len(tmp) == 2:\n                ret_string += chb(int(tmp[::-1], 16))\n            else:\n                ret_string += chb(int(b\"F\" + tmp[:1], 16))\n        return ret_string\n\n\nTBCD_TO_ASCII = b\"0123456789*#abc\"\n\n\nclass GTP_ExtensionHeader(Packet):\n    @classmethod\n    def dispatch_hook(cls, _pkt=None, *args, **kargs):\n        if _pkt is None:\n            return GTP_UDPPort_ExtensionHeader\n        return cls\n\n\nclass GTP_UDPPort_ExtensionHeader(GTP_ExtensionHeader):\n    fields_desc = [ByteField(\"length\", 0x40),\n                   ShortField(\"udp_port\", None),\n                   ByteEnumField(\"next_ex\", 0, ExtensionHeadersTypes), ]\n\n\nclass GTP_PDCP_PDU_ExtensionHeader(GTP_ExtensionHeader):\n    fields_desc = [ByteField(\"length\", 0x01),\n                   ShortField(\"pdcp_pdu\", None),\n                   ByteEnumField(\"next_ex\", 0, ExtensionHeadersTypes), ]\n\n\nclass GTPHeader(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP-C Header\"\n    fields_desc = [BitField(\"version\", 1, 3),\n                   BitField(\"PT\", 1, 1),\n                   BitField(\"reserved\", 0, 1),\n                   BitField(\"E\", 0, 1),\n                   BitField(\"S\", 0, 1),\n                   BitField(\"PN\", 0, 1),\n                   ByteEnumField(\"gtp_type\", None, GTPmessageType),\n                   ShortField(\"length\", None),\n                   IntField(\"teid\", 0),\n                   ConditionalField(XBitField(\"seq\", 0, 16), lambda pkt:pkt.E == 1 or pkt.S == 1 or pkt.PN == 1),  # noqa: E501\n                   ConditionalField(ByteField(\"npdu\", 0), lambda pkt:pkt.E == 1 or pkt.S == 1 or pkt.PN == 1),  # noqa: E501\n                   ConditionalField(ByteEnumField(\"next_ex\", 0, ExtensionHeadersTypes), lambda pkt:pkt.E == 1 or pkt.S == 1 or pkt.PN == 1), ]  # noqa: E501\n\n    def post_build(self, p, pay):\n        p += pay\n        if self.length is None:\n            tmp_len = len(p) - 8\n            p = p[:2] + struct.pack(\"!H\", tmp_len) + p[4:]\n        return p\n\n    def hashret(self):\n        return struct.pack(\"B\", self.version) + self.payload.hashret()\n\n    def answers(self, other):\n        return (isinstance(other, GTPHeader) and\n                self.version == other.version and\n                self.payload.answers(other.payload))\n\n    @classmethod\n    def dispatch_hook(cls, _pkt=None, *args, **kargs):\n        if _pkt and len(_pkt) >= 1:\n            if (orb(_pkt[0]) >> 5) & 0x7 == 2:\n                from . import gtp_v2\n                return gtp_v2.GTPHeader\n        if _pkt and len(_pkt) >= 8:\n            _gtp_type = orb(_pkt[1:2])\n            return GTPforcedTypes.get(_gtp_type, GTPHeader)\n        return cls\n\n\nclass GTP_U_Header(GTPHeader):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP-U Header\"\n    # GTP-U protocol is used to transmit T-PDUs between GSN pairs (or between an SGSN and an RNC in UMTS),  # noqa: E501\n    # encapsulated in G-PDUs. A G-PDU is a packet including a GTP-U header and a T-PDU. The Path Protocol  # noqa: E501\n    # defines the path and the GTP-U header defines the tunnel. Several tunnels may be multiplexed on a single path.  # noqa: E501\n\n    def guess_payload_class(self, payload):\n        # Snooped from Wireshark\n        # https://github.com/boundary/wireshark/blob/07eade8124fd1d5386161591b52e177ee6ea849f/epan/dissectors/packet-gtp.c#L8195  # noqa: E501\n        if self.E == 1:\n            if self.next_ex == 0x85:\n                return GTPPDUSessionContainer\n            return GTPHeader.guess_payload_class(self, payload)\n\n        if self.gtp_type == 255:\n            sub_proto = orb(payload[0])\n            if sub_proto >= 0x45 and sub_proto <= 0x4e:\n                return IP\n            elif (sub_proto & 0xf0) == 0x60:\n                return IPv6\n            else:\n                return PPP\n        return GTPHeader.guess_payload_class(self, payload)\n\n\n# Some gtp_types have to be associated with a certain type of header\nGTPforcedTypes = {\n    16: GTPHeader,\n    17: GTPHeader,\n    18: GTPHeader,\n    19: GTPHeader,\n    20: GTPHeader,\n    21: GTPHeader,\n    26: GTP_U_Header,\n    27: GTPHeader,\n    254: GTP_U_Header,\n    255: GTP_U_Header\n}\n\n\nclass GTPPDUSessionContainer(Packet):\n    name = \"GTP PDU Session Container\"\n    fields_desc = [ByteField(\"ExtHdrLen\", None),\n                   BitField(\"type\", 0, 4),\n                   BitField(\"spare1\", 0, 4),\n                   BitField(\"P\", 0, 1),\n                   BitField(\"R\", 0, 1),\n                   BitField(\"QFI\", 0, 6),\n                   ConditionalField(XBitField(\"PPI\", 0, 3),\n                                    lambda pkt: pkt.P == 1),\n                   ConditionalField(XBitField(\"spare2\", 0, 5),\n                                    lambda pkt: pkt.P == 1),\n                   ConditionalField(ByteField(\"pad1\", 0),\n                                    lambda pkt: pkt.P == 1),\n                   ConditionalField(ByteField(\"pad2\", 0),\n                                    lambda pkt: pkt.P == 1),\n                   ConditionalField(ByteField(\"pad3\", 0),\n                                    lambda pkt: pkt.P == 1),\n                   ConditionalField(StrLenField(\n                       \"extraPadding\",\n                       \"\",\n                       length_from=lambda pkt: 4 * (pkt.ExtHdrLen) - 4),\n                       lambda pkt: pkt.ExtHdrLen and pkt.ExtHdrLen > 1),\n                   ByteEnumField(\"NextExtHdr\", 0, ExtensionHeadersTypes), ]\n\n    def guess_payload_class(self, payload):\n        if self.NextExtHdr == 0:\n            sub_proto = orb(payload[0])\n            if sub_proto >= 0x45 and sub_proto <= 0x4e:\n                return IP\n            elif (sub_proto & 0xf0) == 0x60:\n                return IPv6\n            else:\n                return PPP\n        return GTPHeader.guess_payload_class(self, payload)\n\n    def post_build(self, p, pay):\n        p += pay\n        if self.ExtHdrLen is None:\n            if self.P == 1:\n                hdr_len = 2\n            else:\n                hdr_len = 1\n            p = struct.pack(\"!B\", hdr_len) + p[1:]\n        return p\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n\nclass GTPEchoRequest(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Echo Request\"\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n\nclass IE_Base(Packet):\n    def extract_padding(self, pkt):\n        return \"\", pkt\n\n    def post_build(self, p, pay):\n        if self.fields_desc[1].name == \"length\":\n            if self.length is None:\n                tmp_len = len(p)\n                if isinstance(self.payload, conf.padding_layer):\n                    tmp_len += len(self.payload.load)\n                p = p[:1] + struct.pack(\"!H\", tmp_len - 2) + p[3:]\n        return p + pay\n\n\nclass IE_Cause(IE_Base):\n    name = \"Cause\"\n    fields_desc = [ByteEnumField(\"ietype\", 1, IEType),\n                   ByteEnumField(\"CauseValue\", None, CauseValues)]\n\n\nclass IE_IMSI(IE_Base):\n    name = \"IMSI - Subscriber identity of the MS\"\n    fields_desc = [ByteEnumField(\"ietype\", 2, IEType),\n                   TBCDByteField(\"imsi\", str(RandNum(0, 999999999999999)), 8)]\n\n\nclass IE_Routing(IE_Base):\n    name = \"Routing Area Identity\"\n    fields_desc = [ByteEnumField(\"ietype\", 3, IEType),\n                   TBCDByteField(\"MCC\", \"\", 2),\n                   # MNC: if the third digit of MCC is 0xf,\n                   # then the length of MNC is 1 byte\n                   TBCDByteField(\"MNC\", \"\", 1),\n                   ShortField(\"LAC\", None),\n                   ByteField(\"RAC\", None)]\n\n\nclass IE_ReorderingRequired(IE_Base):\n    name = \"Recovery\"\n    fields_desc = [ByteEnumField(\"ietype\", 8, IEType),\n                   ByteEnumField(\"reordering_required\", 254, TrueFalse_value)]\n\n\nclass IE_Recovery(IE_Base):\n    name = \"Recovery\"\n    fields_desc = [ByteEnumField(\"ietype\", 14, IEType),\n                   ByteField(\"restart_counter\", 24)]\n\n\nclass IE_SelectionMode(IE_Base):\n    # Indicates the origin of the APN in the message\n    name = \"Selection Mode\"\n    fields_desc = [ByteEnumField(\"ietype\", 15, IEType),\n                   BitEnumField(\"SelectionMode\", \"MS or APN\",\n                                8, Selection_Mode)]\n\n\nclass IE_TEIDI(IE_Base):\n    name = \"Tunnel Endpoint Identifier Data\"\n    fields_desc = [ByteEnumField(\"ietype\", 16, IEType),\n                   XIntField(\"TEIDI\", RandInt())]\n\n\nclass IE_TEICP(IE_Base):\n    name = \"Tunnel Endpoint Identifier Control Plane\"\n    fields_desc = [ByteEnumField(\"ietype\", 17, IEType),\n                   XIntField(\"TEICI\", RandInt())]\n\n\nclass IE_Teardown(IE_Base):\n    name = \"Teardown Indicator\"\n    fields_desc = [ByteEnumField(\"ietype\", 19, IEType),\n                   ByteEnumField(\"indicator\", \"True\", TrueFalse_value)]\n\n\nclass IE_NSAPI(IE_Base):\n    # Identifies a PDP context in a mobility management context specified by TEICP  # noqa: E501\n    name = \"NSAPI\"\n    fields_desc = [ByteEnumField(\"ietype\", 20, IEType),\n                   XBitField(\"sparebits\", 0x0000, 4),\n                   XBitField(\"NSAPI\", RandNum(0, 15), 4)]\n\n\nclass IE_ChargingCharacteristics(IE_Base):\n    # Way of informing both the SGSN and GGSN of the rules for\n    name = \"Charging Characteristics\"\n    fields_desc = [ByteEnumField(\"ietype\", 26, IEType),\n                   # producing charging information based on operator configured triggers.  # noqa: E501\n                   #    0000 .... .... .... : spare\n                   #    .... 1... .... .... : normal charging\n                   #    .... .0.. .... .... : prepaid charging\n                   #    .... ..0. .... .... : flat rate charging\n                   #    .... ...0 .... .... : hot billing charging\n                   #    .... .... 0000 0000 : reserved\n                   XBitField(\"Ch_ChSpare\", None, 4),\n                   XBitField(\"normal_charging\", None, 1),\n                   XBitField(\"prepaid_charging\", None, 1),\n                   XBitField(\"flat_rate_charging\", None, 1),\n                   XBitField(\"hot_billing_charging\", None, 1),\n                   XBitField(\"Ch_ChReserved\", 0, 8)]\n\n\nclass IE_TraceReference(IE_Base):\n    # Identifies a record or a collection of records for a particular trace.\n    name = \"Trace Reference\"\n    fields_desc = [ByteEnumField(\"ietype\", 27, IEType),\n                   XBitField(\"Trace_reference\", None, 16)]\n\n\nclass IE_TraceType(IE_Base):\n    # Indicates the type of the trace\n    name = \"Trace Type\"\n    fields_desc = [ByteEnumField(\"ietype\", 28, IEType),\n                   XBitField(\"Trace_type\", None, 16)]\n\n\nclass IE_ChargingId(IE_Base):\n    name = \"Charging ID\"\n    fields_desc = [ByteEnumField(\"ietype\", 127, IEType),\n                   XIntField(\"Charging_id\", RandInt())]\n\n\nclass IE_EndUserAddress(IE_Base):\n    # Supply protocol specific information of the external packet\n    name = \"End User Address\"\n    fields_desc = [ByteEnumField(\"ietype\", 128, IEType),\n                   #         data network accessed by the GGPRS subscribers.\n                   #            - Request\n                   #                1    Type (1byte)\n                   #                2-3    Length (2bytes) - value 2\n                   #                4    Spare + PDP Type Organization\n                   #                5    PDP Type Number\n                   #            - Response\n                   #                6-n    PDP Address\n                   ShortField(\"length\", 2),\n                   BitField(\"SPARE\", 15, 4),\n                   BitField(\"PDPTypeOrganization\", 1, 4),\n                   XByteField(\"PDPTypeNumber\", None),\n                   ConditionalField(IPField(\"PDPAddress\", RandIP()),\n                                    lambda pkt: pkt.length == 6 or pkt.length == 22),  # noqa: E501\n                   ConditionalField(IP6Field(\"IPv6_PDPAddress\", '::1'),\n                                    lambda pkt: pkt.length == 18 or pkt.length == 22)]  # noqa: E501\n\n\nclass APNStrLenField(StrLenField):\n    # Inspired by DNSStrField\n    def m2i(self, pkt, s):\n        ret_s = b\"\"\n        tmp_s = s\n        while tmp_s:\n            tmp_len = orb(tmp_s[0]) + 1\n            if tmp_len > len(tmp_s):\n                warning(\"APN prematured end of character-string (size=%i, remaining bytes=%i)\" % (tmp_len, len(tmp_s)))  # noqa: E501\n            ret_s += tmp_s[1:tmp_len]\n            tmp_s = tmp_s[tmp_len:]\n            if len(tmp_s):\n                ret_s += b\".\"\n        s = ret_s\n        return s\n\n    def i2m(self, pkt, s):\n        if not isinstance(s, bytes):\n            s = bytes_encode(s)\n        s = b\"\".join(chb(len(x)) + x for x in s.split(b\".\"))\n        return s\n\n\nclass IE_AccessPointName(IE_Base):\n    # Sent by SGSN or by GGSN as defined in 3GPP TS 23.060\n    name = \"Access Point Name\"\n    fields_desc = [ByteEnumField(\"ietype\", 131, IEType),\n                   ShortField(\"length\", None),\n                   APNStrLenField(\"APN\", \"nternet\", length_from=lambda x: x.length)]  # noqa: E501\n\n    def post_build(self, p, pay):\n        if self.length is None:\n            tmp_len = len(p) - 3\n            p = p[:2] + struct.pack(\"!B\", tmp_len) + p[3:]\n        return p\n\n\nclass IE_ProtocolConfigurationOptions(IE_Base):\n    name = \"Protocol Configuration Options\"\n    fields_desc = [ByteEnumField(\"ietype\", 132, IEType),\n                   ShortField(\"length\", 4),\n                   StrLenField(\"Protocol_Configuration\", \"\",\n                               length_from=lambda x: x.length)]\n\n\nclass IE_GSNAddress(IE_Base):\n    name = \"GSN Address\"\n    fields_desc = [ByteEnumField(\"ietype\", 133, IEType),\n                   ShortField(\"length\", None),\n                   ConditionalField(IPField(\"ipv4_address\", RandIP()),\n                                    lambda pkt: pkt.length == 4),\n                   ConditionalField(IP6Field(\"ipv6_address\", '::1'),\n                                    lambda pkt: pkt.length == 16)]\n\n    def post_build(self, p, pay):\n        if self.length is None:\n            tmp_len = len(p) - 3\n            p = p[:2] + struct.pack(\"!B\", tmp_len) + p[3:]\n        return p\n\n\nclass IE_MSInternationalNumber(IE_Base):\n    name = \"MS International Number\"\n    fields_desc = [ByteEnumField(\"ietype\", 134, IEType),\n                   ShortField(\"length\", None),\n                   FlagsField(\"flags\", 0x91, 8, [\"Extension\", \"\", \"\", \"International Number\", \"\", \"\", \"\", \"ISDN numbering\"]),  # noqa: E501\n                   TBCDByteField(\"digits\", \"33607080910\", length_from=lambda x: x.length - 1)]  # noqa: E501\n\n\nclass QoS_Profile(IE_Base):\n    name = \"QoS profile\"\n    fields_desc = [ByteField(\"qos_ei\", 0),\n                   ByteField(\"length\", None),\n                   XBitField(\"spare\", 0x00, 2),\n                   XBitField(\"delay_class\", 0x000, 3),\n                   XBitField(\"reliability_class\", 0x000, 3),\n                   XBitField(\"peak_troughput\", 0x0000, 4),\n                   BitField(\"spare\", 0, 1),\n                   XBitField(\"precedence_class\", 0x000, 3),\n                   XBitField(\"spare\", 0x000, 3),\n                   XBitField(\"mean_troughput\", 0x00000, 5),\n                   XBitField(\"traffic_class\", 0x000, 3),\n                   XBitField(\"delivery_order\", 0x00, 2),\n                   XBitField(\"delivery_of_err_sdu\", 0x000, 3),\n                   ByteField(\"max_sdu_size\", None),\n                   ByteField(\"max_bitrate_up\", None),\n                   ByteField(\"max_bitrate_down\", None),\n                   XBitField(\"redidual_ber\", 0x0000, 4),\n                   XBitField(\"sdu_err_ratio\", 0x0000, 4),\n                   XBitField(\"transfer_delay\", 0x00000, 5),\n                   XBitField(\"traffic_handling_prio\", 0x000, 3),\n                   ByteField(\"guaranteed_bit_rate_up\", None),\n                   ByteField(\"guaranteed_bit_rate_down\", None)]\n\n\nclass IE_QoS(IE_Base):\n    name = \"QoS\"\n    fields_desc = [ByteEnumField(\"ietype\", 135, IEType),\n                   ShortField(\"length\", None),\n                   ByteField(\"allocation_retention_prioiry\", 1),\n\n                   ConditionalField(XBitField(\"spare\", 0x00, 2),\n                                    lambda p: p.length and p.length > 1),\n                   ConditionalField(XBitField(\"delay_class\", 0x000, 3),\n                                    lambda p: p.length and p.length > 1),\n                   ConditionalField(XBitField(\"reliability_class\", 0x000, 3),\n                                    lambda p: p.length and p.length > 1),\n\n                   ConditionalField(XBitField(\"peak_troughput\", 0x0000, 4),\n                                    lambda p: p.length and p.length > 2),\n                   ConditionalField(BitField(\"spare\", 0, 1),\n                                    lambda p: p.length and p.length > 2),\n                   ConditionalField(XBitField(\"precedence_class\", 0x000, 3),\n                                    lambda p: p.length and p.length > 2),\n\n                   ConditionalField(XBitField(\"spare\", 0x000, 3),\n                                    lambda p: p.length and p.length > 3),\n                   ConditionalField(XBitField(\"mean_troughput\", 0x00000, 5),\n                                    lambda p: p.length and p.length > 3),\n\n                   ConditionalField(XBitField(\"traffic_class\", 0x000, 3),\n                                    lambda p: p.length and p.length > 4),\n                   ConditionalField(XBitField(\"delivery_order\", 0x00, 2),\n                                    lambda p: p.length and p.length > 4),\n                   ConditionalField(XBitField(\"delivery_of_err_sdu\", 0x000, 3),\n                                    lambda p: p.length and p.length > 4),\n\n                   ConditionalField(ByteField(\"max_sdu_size\", None),\n                                    lambda p: p.length and p.length > 5),\n                   ConditionalField(ByteField(\"max_bitrate_up\", None),\n                                    lambda p: p.length and p.length > 6),\n                   ConditionalField(ByteField(\"max_bitrate_down\", None),\n                                    lambda p: p.length and p.length > 7),\n\n                   ConditionalField(XBitField(\"redidual_ber\", 0x0000, 4),\n                                    lambda p: p.length and p.length > 8),\n                   ConditionalField(XBitField(\"sdu_err_ratio\", 0x0000, 4),\n                                    lambda p: p.length and p.length > 8),\n                   ConditionalField(XBitField(\"transfer_delay\", 0x00000, 6),\n                                    lambda p: p.length and p.length > 9),\n                   ConditionalField(XBitField(\"traffic_handling_prio\",\n                                              0x000,\n                                              2),\n                                    lambda p: p.length and p.length > 9),\n\n                   ConditionalField(ByteField(\"guaranteed_bit_rate_up\", None),\n                                    lambda p: p.length and p.length > 10),\n                   ConditionalField(ByteField(\"guaranteed_bit_rate_down\",\n                                              None),\n                                    lambda p: p.length and p.length > 11),\n\n                   ConditionalField(XBitField(\"spare\", 0x000, 3),\n                                    lambda p: p.length and p.length > 12),\n                   ConditionalField(BitField(\"signaling_indication\", 0, 1),\n                                    lambda p: p.length and p.length > 12),\n                   ConditionalField(XBitField(\"source_stats_desc\", 0x0000, 4),\n                                    lambda p: p.length and p.length > 12),\n\n                   ConditionalField(ByteField(\"max_bitrate_down_ext\", None),\n                                    lambda p: p.length and p.length > 13),\n                   ConditionalField(ByteField(\"guaranteed_bitrate_down_ext\",\n                                              None),\n                                    lambda p: p.length and p.length > 14),\n                   ConditionalField(ByteField(\"max_bitrate_up_ext\", None),\n                                    lambda p: p.length and p.length > 15),\n                   ConditionalField(ByteField(\"guaranteed_bitrate_up_ext\",\n                                              None),\n                                    lambda p: p.length and p.length > 16),\n                   ConditionalField(ByteField(\"max_bitrate_down_ext2\", None),\n                                    lambda p: p.length and p.length > 17),\n                   ConditionalField(ByteField(\"guaranteed_bitrate_down_ext2\",\n                                              None),\n                                    lambda p: p.length and p.length > 18),\n                   ConditionalField(ByteField(\"max_bitrate_up_ext2\", None),\n                                    lambda p: p.length and p.length > 19),\n                   ConditionalField(ByteField(\"guaranteed_bitrate_up_ext2\",\n                                              None),\n                                    lambda p: p.length and p.length > 20)]\n\n\nclass IE_CommonFlags(IE_Base):\n    name = \"Common Flags\"\n    fields_desc = [ByteEnumField(\"ietype\", 148, IEType),\n                   ShortField(\"length\", None),\n                   BitField(\"dual_addr_bearer_fl\", 0, 1),\n                   BitField(\"upgrade_qos_supported\", 0, 1),\n                   BitField(\"nrsn\", 0, 1),\n                   BitField(\"no_qos_nego\", 0, 1),\n                   BitField(\"mbms_cnting_info\", 0, 1),\n                   BitField(\"ran_procedure_ready\", 0, 1),\n                   BitField(\"mbms_service_type\", 0, 1),\n                   BitField(\"prohibit_payload_compression\", 0, 1)]\n\n\nclass IE_APNRestriction(IE_Base):\n    name = \"APN Restriction\"\n    fields_desc = [ByteEnumField(\"ietype\", 149, IEType),\n                   ShortField(\"length\", 1),\n                   ByteField(\"restriction_type_value\", 0)]\n\n\nclass IE_RATType(IE_Base):\n    name = \"Rat Type\"\n    fields_desc = [ByteEnumField(\"ietype\", 151, IEType),\n                   ShortField(\"length\", 1),\n                   ByteEnumField(\"RAT_Type\", None, RATType)]\n\n\nclass IE_UserLocationInformation(IE_Base):\n    name = \"User Location Information\"\n    fields_desc = [ByteEnumField(\"ietype\", 152, IEType),\n                   ShortField(\"length\", None),\n                   ByteField(\"type\", 1),\n                   # Only type 1 is currently supported\n                   TBCDByteField(\"MCC\", \"\", 2),\n                   # MNC: if the third digit of MCC is 0xf, then the length of MNC is 1 byte  # noqa: E501\n                   TBCDByteField(\"MNC\", \"\", 1),\n                   ShortField(\"LAC\", None),\n                   ShortField(\"SAC\", None)]\n\n\nclass IE_MSTimeZone(IE_Base):\n    name = \"MS Time Zone\"\n    fields_desc = [ByteEnumField(\"ietype\", 153, IEType),\n                   ShortField(\"length\", None),\n                   ByteField(\"timezone\", 0),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   XBitField(\"daylight_saving_time\", 0x00, 2)]\n\n\nclass IE_IMEI(IE_Base):\n    name = \"IMEI\"\n    fields_desc = [ByteEnumField(\"ietype\", 154, IEType),\n                   ShortField(\"length\", None),\n                   TBCDByteField(\"IMEI\", \"\", length_from=lambda x: x.length)]\n\n\nclass IE_MSInfoChangeReportingAction(IE_Base):\n    name = \"MS Info Change Reporting Action\"\n    fields_desc = [ByteEnumField(\"ietype\", 181, IEType),\n                   ShortField(\"length\", 1),\n                   ByteField(\"Action\", 0)]\n\n\nclass IE_DirectTunnelFlags(IE_Base):\n    name = \"Direct Tunnel Flags\"\n    fields_desc = [ByteEnumField(\"ietype\", 182, IEType),\n                   ShortField(\"length\", 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"EI\", 0, 1),\n                   BitField(\"GCSI\", 0, 1),\n                   BitField(\"DTI\", 0, 1)]\n\n\nclass IE_BearerControlMode(IE_Base):\n    name = \"Bearer Control Mode\"\n    fields_desc = [ByteEnumField(\"ietype\", 184, IEType),\n                   ShortField(\"length\", 1),\n                   ByteField(\"bearer_control_mode\", 0)]\n\n\nclass IE_EvolvedAllocationRetentionPriority(IE_Base):\n    name = \"Evolved Allocation/Retention Priority\"\n    fields_desc = [ByteEnumField(\"ietype\", 191, IEType),\n                   ShortField(\"length\", 1),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"PCI\", 0, 1),\n                   XBitField(\"PL\", 0x0000, 4),\n                   BitField(\"Spare\", 0, 1),\n                   BitField(\"PVI\", 0, 1)]\n\n\nclass IE_CharginGatewayAddress(IE_Base):\n    name = \"Chargin Gateway Address\"\n    fields_desc = [ByteEnumField(\"ietype\", 251, IEType),\n                   ShortField(\"length\", 4),\n                   ConditionalField(IPField(\"ipv4_address\", \"127.0.0.1\"),\n                                    lambda\n                                    pkt: pkt.length == 4),\n                   ConditionalField(IP6Field(\"ipv6_address\", \"::1\"), lambda\n                                    pkt: pkt.length == 16)]\n\n\nclass IE_PrivateExtension(IE_Base):\n    name = \"Private Extension\"\n    fields_desc = [ByteEnumField(\"ietype\", 255, IEType),\n                   ShortField(\"length\", 1),\n                   ByteField(\"extension identifier\", 0),\n                   StrLenField(\"extention_value\", \"\",\n                               length_from=lambda x: x.length)]\n\n\nclass IE_ExtensionHeaderList(IE_Base):\n    name = \"Extension Header List\"\n    fields_desc = [ByteEnumField(\"ietype\", 141, IEType),\n                   FieldLenField(\"length\", None, length_of=\"extension_headers\"),  # noqa: E501\n                   FieldListField(\"extension_headers\", [64, 192], ByteField(\"\", 0))]  # noqa: E501\n\n\nclass IE_NotImplementedTLV(Packet):\n    name = \"IE not implemented\"\n    fields_desc = [ByteEnumField(\"ietype\", 0, IEType),\n                   ShortField(\"length\", None),\n                   StrLenField(\"data\", \"\", length_from=lambda x: x.length)]\n\n    def extract_padding(self, pkt):\n        return \"\", pkt\n\n\nietypecls = {1: IE_Cause,\n             2: IE_IMSI,\n             3: IE_Routing,\n             8: IE_ReorderingRequired,\n             14: IE_Recovery,\n             15: IE_SelectionMode,\n             16: IE_TEIDI,\n             17: IE_TEICP,\n             19: IE_Teardown,\n             20: IE_NSAPI,\n             26: IE_ChargingCharacteristics,\n             27: IE_TraceReference,\n             28: IE_TraceType,\n             127: IE_ChargingId,\n             128: IE_EndUserAddress,\n             131: IE_AccessPointName,\n             132: IE_ProtocolConfigurationOptions,\n             133: IE_GSNAddress,\n             134: IE_MSInternationalNumber,\n             135: IE_QoS,\n             141: IE_ExtensionHeaderList,\n             148: IE_CommonFlags,\n             149: IE_APNRestriction,\n             151: IE_RATType,\n             152: IE_UserLocationInformation,\n             153: IE_MSTimeZone,\n             154: IE_IMEI,\n             181: IE_MSInfoChangeReportingAction,\n             182: IE_DirectTunnelFlags,\n             184: IE_BearerControlMode,\n             191: IE_EvolvedAllocationRetentionPriority,\n             251: IE_CharginGatewayAddress,\n             255: IE_PrivateExtension}\n\n\ndef IE_Dispatcher(s):\n    \"\"\"Choose the correct Information Element class.\"\"\"\n    if len(s) < 1:\n        return Raw(s)\n    # Get the IE type\n    ietype = orb(s[0])\n    cls = ietypecls.get(ietype, Raw)\n\n    # if ietype greater than 128 are TLVs\n    if cls == Raw and ietype & 128 == 128:\n        cls = IE_NotImplementedTLV\n    return cls(s)\n\n\nclass GTPEchoResponse(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Echo Response\"\n    fields_desc = [PacketListField(\"IE_list\", [], IE_Dispatcher)]\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n    def answers(self, other):\n        return self.seq == other.seq\n\n\nclass GTPCreatePDPContextRequest(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Create PDP Context Request\"\n    fields_desc = [PacketListField(\"IE_list\", [IE_TEIDI(), IE_NSAPI(), IE_GSNAddress(length=4, ipv4_address=RandIP()),  # noqa: E501\n                                               IE_GSNAddress(length=4, ipv4_address=RandIP()),  # noqa: E501\n                                               IE_NotImplementedTLV(ietype=135, length=15, data=RandString(15))],  # noqa: E501\n                                   IE_Dispatcher)]\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n\nclass GTPCreatePDPContextResponse(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Create PDP Context Response\"\n    fields_desc = [PacketListField(\"IE_list\", [], IE_Dispatcher)]\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n    def answers(self, other):\n        return self.seq == other.seq\n\n\nclass GTPUpdatePDPContextRequest(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Update PDP Context Request\"\n    fields_desc = [PacketListField(\"IE_list\", [\n        IE_Cause(),\n        IE_Recovery(),\n        IE_TEIDI(),\n        IE_TEICP(),\n        IE_ChargingId(),\n        IE_ProtocolConfigurationOptions(),\n        IE_GSNAddress(),\n        IE_GSNAddress(),\n        IE_GSNAddress(),\n        IE_GSNAddress(),\n        IE_QoS(),\n        IE_CharginGatewayAddress(),\n        IE_CharginGatewayAddress(),\n        IE_CommonFlags(),\n        IE_APNRestriction(),\n        IE_BearerControlMode(),\n        IE_MSInfoChangeReportingAction(),\n        IE_EvolvedAllocationRetentionPriority(),\n        IE_PrivateExtension()],\n        IE_Dispatcher)]\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n\nclass GTPUpdatePDPContextResponse(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Update PDP Context Response\"\n    fields_desc = [PacketListField(\"IE_list\", None, IE_Dispatcher)]\n\n    def hashret(self):\n        return struct.pack(\"H\", self.seq)\n\n\nclass GTPErrorIndication(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Error Indication\"\n    fields_desc = [PacketListField(\"IE_list\", [], IE_Dispatcher)]\n\n\nclass GTPDeletePDPContextRequest(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Delete PDP Context Request\"\n    fields_desc = [PacketListField(\"IE_list\", [], IE_Dispatcher)]\n\n\nclass GTPDeletePDPContextResponse(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP Delete PDP Context Response\"\n    fields_desc = [PacketListField(\"IE_list\", [], IE_Dispatcher)]\n\n\nclass GTPPDUNotificationRequest(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP PDU Notification Request\"\n    fields_desc = [PacketListField(\"IE_list\", [IE_IMSI(),\n                                               IE_TEICP(TEICI=RandInt()),\n                                               IE_EndUserAddress(PDPTypeNumber=0x21),  # noqa: E501\n                                               IE_AccessPointName(),\n                                               IE_GSNAddress(ipv4_address=\"127.0.0.1\"),  # noqa: E501\n                                               ], IE_Dispatcher)]\n\n\nclass GTPSupportedExtensionHeadersNotification(Packet):\n    name = \"GTP Supported Extension Headers Notification\"\n    fields_desc = [PacketListField(\"IE_list\", [IE_ExtensionHeaderList(),\n                                               ], IE_Dispatcher)]\n\n\nclass GTPmorethan1500(Packet):\n    # 3GPP TS 29.060 V9.1.0 (2009-12)\n    name = \"GTP More than 1500\"\n    fields_desc = [ByteEnumField(\"IE_Cause\", \"Cause\", IEType),\n                   BitField(\"IE\", 1, 12000), ]\n\n\n# Bind GTP-C\nbind_bottom_up(UDP, GTPHeader, dport=2123)\nbind_bottom_up(UDP, GTPHeader, sport=2123)\nbind_layers(UDP, GTPHeader, dport=2123, sport=2123)\nbind_layers(GTPHeader, GTPEchoRequest, gtp_type=1, S=1)\nbind_layers(GTPHeader, GTPEchoResponse, gtp_type=2, S=1)\nbind_layers(GTPHeader, GTPCreatePDPContextRequest, gtp_type=16)\nbind_layers(GTPHeader, GTPCreatePDPContextResponse, gtp_type=17)\nbind_layers(GTPHeader, GTPUpdatePDPContextRequest, gtp_type=18)\nbind_layers(GTPHeader, GTPUpdatePDPContextResponse, gtp_type=19)\nbind_layers(GTPHeader, GTPDeletePDPContextRequest, gtp_type=20)\nbind_layers(GTPHeader, GTPDeletePDPContextResponse, gtp_type=21)\nbind_layers(GTPHeader, GTPPDUNotificationRequest, gtp_type=27)\nbind_layers(GTPHeader, GTPSupportedExtensionHeadersNotification, gtp_type=31, S=1)  # noqa: E501\nbind_layers(GTPHeader, GTP_UDPPort_ExtensionHeader, next_ex=64, E=1)\nbind_layers(GTPHeader, GTP_PDCP_PDU_ExtensionHeader, next_ex=192, E=1)\n\n# Bind GTP-U\nbind_bottom_up(UDP, GTP_U_Header, dport=2152)\nbind_bottom_up(UDP, GTP_U_Header, sport=2152)\nbind_layers(UDP, GTP_U_Header, dport=2152, sport=2152)\nbind_layers(GTP_U_Header, GTPErrorIndication, gtp_type=26, S=1)\nbind_layers(GTP_U_Header, GTPPDUSessionContainer,\n            gtp_type=255, E=1, next_ex=0x85)\nbind_top_down(GTP_U_Header, IP, gtp_type=255)\nbind_top_down(GTP_U_Header, IPv6, gtp_type=255)\nbind_top_down(GTP_U_Header, PPP, gtp_type=255)\n", "idx": 6, "id": 17405, "msg": "", "proj": "secdev-scapy", "lang": "py"}
{"patch": "@@ -75,7 +75,8 @@ def _depth_first_search(set_tasks, current_task, visited):\n             if task in set_tasks[\"run_by_other_worker\"] or task in set_tasks[\"upstream_run_by_other_worker\"]:\n                 set_tasks[\"upstream_run_by_other_worker\"].add(current_task)\n                 upstream_run_by_other_worker = True\n-        if not upstream_failure and not upstream_missing_dependency and not upstream_run_by_other_worker and current_task not in set_tasks[\"run_by_other_worker\"]:\n+        if not upstream_failure and not upstream_missing_dependency and \\\n+                not upstream_run_by_other_worker and current_task not in set_tasks[\"run_by_other_worker\"]:\n             set_tasks[\"unknown_reason\"].add(current_task)\n \n ", "y": 0, "oldf": "# -*- coding: utf-8 -*-\n#\n# Copyright 2015-2015 Spotify AB\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\"\"\"\nThis module provide the function :py:func:`summary` that is used for printing\nan `execution summary\n<https://github.com/spotify/luigi/blob/master/examples/execution_summary_example.py>`_\nat the end of luigi invocations.\n\"\"\"\n\nimport textwrap\n\n\ndef _partition_tasks(worker):\n    \"\"\"\n    Takes a worker and sorts out tasks based on their status.\n    Still_pending_not_ext is only used to get upstream_failure, upstream_missing_dependency and run_by_other_worker\n    \"\"\"\n    task_history = worker._add_task_history\n    pending_tasks = {task for(task, status, ext) in task_history if status == 'PENDING'}\n    set_tasks = {}\n    set_tasks[\"completed\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task in pending_tasks}\n    set_tasks[\"already_done\"] = {task for (task, status, ext) in task_history if status == 'DONE' and task not in pending_tasks and task not in set_tasks[\"completed\"]}\n    set_tasks[\"failed\"] = {task for (task, status, ext) in task_history if status == 'FAILED'}\n    set_tasks[\"still_pending_ext\"] = {task for (task, status, ext) in task_history if status == 'PENDING' and task not in set_tasks[\"failed\"] and task not in set_tasks[\"completed\"] and not ext}\n    set_tasks[\"still_pending_not_ext\"] = {task for (task, status, ext) in task_history if status == 'PENDING' and task not in set_tasks[\"failed\"] and task not in set_tasks[\"completed\"] and ext}\n    set_tasks[\"run_by_other_worker\"] = set()\n    set_tasks[\"upstream_failure\"] = set()\n    set_tasks[\"upstream_missing_dependency\"] = set()\n    set_tasks[\"upstream_run_by_other_worker\"] = set()\n    set_tasks[\"unknown_reason\"] = set()\n    return set_tasks\n\n\ndef _populate_unknown_statuses(set_tasks):\n    \"\"\"\n    Add the \"upstream_*\" and \"unknown_reason\" statuses my mutating set_tasks.\n    \"\"\"\n    visited = set()\n    for task in set_tasks[\"still_pending_not_ext\"]:\n        _depth_first_search(set_tasks, task, visited)\n\n\ndef _depth_first_search(set_tasks, current_task, visited):\n    \"\"\"\n    This dfs checks why tasks are still pending.\n    \"\"\"\n    visited.add(current_task)\n    if current_task in set_tasks[\"still_pending_not_ext\"]:\n        upstream_failure = False\n        upstream_missing_dependency = False\n        upstream_run_by_other_worker = False\n        for task in current_task._requires():\n            if task not in visited:\n                _depth_first_search(set_tasks, task, visited)\n            if task in set_tasks[\"failed\"] or task in set_tasks[\"upstream_failure\"]:\n                set_tasks[\"upstream_failure\"].add(current_task)\n                upstream_failure = True\n            if task in set_tasks[\"still_pending_ext\"] or task in set_tasks[\"upstream_missing_dependency\"]:\n                set_tasks[\"upstream_missing_dependency\"].add(current_task)\n                upstream_missing_dependency = True\n            if task in set_tasks[\"run_by_other_worker\"] or task in set_tasks[\"upstream_run_by_other_worker\"]:\n                set_tasks[\"upstream_run_by_other_worker\"].add(current_task)\n                upstream_run_by_other_worker = True\n        if not upstream_failure and not upstream_missing_dependency and not upstream_run_by_other_worker and current_task not in set_tasks[\"run_by_other_worker\"]:\n            set_tasks[\"unknown_reason\"].add(current_task)\n\n\ndef _get_str(task_dict, extra_indent):\n    \"\"\"\n    This returns a string for each status\n    \"\"\"\n    lines = []\n    for task_family, tasks in task_dict.items():\n        row = '    '\n        if extra_indent:\n            row += '    '\n        if len(lines) >= 5:\n            \"\"\"\n            This is how many rows will be printed for each status. If you want fewer rows you can lower the limit.\n            \"\"\"\n            row += '...'\n            lines.append(row)\n            break\n        if len(tasks[0].get_params()) == 0:\n            row += '- {0} {1}()'.format(len(tasks), str(task_family))\n        elif _get_len_of_params(tasks[0]) > 60 or (len(tasks) == 2 and len(tasks[0].get_params()) > 1 and (_get_len_of_params(tasks[0]) > 40 or len(str(tasks[0])) > 100)) or len(str(tasks[0])) > 200:\n            \"\"\"\n            This is to make sure that there is no really long task in the output\n            \"\"\"\n            row += '- {0} {1}(...)'.format(len(tasks), task_family)\n        elif len((tasks[0].get_params())) == 1:\n            attributes = sorted({getattr(task, tasks[0].get_params()[0][0]) for task in tasks})\n            row += '- {0} {1}({2}='.format(len(tasks), task_family, tasks[0].get_params()[0][0])\n            if _ranging_attributes(attributes, tasks[0].get_params()[0]) and len(attributes) > 3:\n                row += '{0}...{1}'.format(tasks[0].get_params()[0][1].serialize(attributes[0]), tasks[0].get_params()[0][1].serialize(attributes[-1]))\n            else:\n                row += '{0}'.format(_get_str_one_parameter(tasks))\n            row += \")\"\n        else:\n            ranging = False\n            params = _get_set_of_params(tasks)\n            unique_param_keys = list(_get_unique_param_keys(params))\n            if len(unique_param_keys) == 1:\n                unique_param, = unique_param_keys\n                attributes = sorted(params[unique_param])\n                if _ranging_attributes(attributes, unique_param) and len(attributes) > 2:\n                    ranging = True\n                    row += '- {0} {1}({2}'.format(len(tasks), task_family, _get_str_ranging_multiple_parameters(attributes, tasks, unique_param))\n            if not ranging:\n                if len(tasks) == 1:\n                    row += '- {0} {1}'.format(len(tasks), tasks[0])\n                if len(tasks) == 2:\n                    row += '- {0} and {1}'.format(tasks[0], tasks[1])\n                if len(tasks) > 2:\n                    row += '- {0} and {1} other {2}'.format(tasks[0], len(tasks) - 1, task_family)\n        lines.append(row)\n    return '\\n'.join(lines)\n\n\ndef _get_len_of_params(task):\n    return sum(len(param[0]) for param in task.get_params())\n\n\ndef _get_str_ranging_multiple_parameters(attributes, tasks, unique_param):\n    row = ''\n    str_unique_param = '{0}...{1}'.format(unique_param[1].serialize(attributes[0]), unique_param[1].serialize(attributes[-1]))\n    for param in tasks[0].get_params():\n        row += '{0}='.format(param[0])\n        if param[0] == unique_param[0]:\n            row += '{0}'.format(str_unique_param)\n        else:\n            row += '{0}'.format(param[1].serialize(getattr(tasks[0], param[0])))\n        if param != tasks[0].get_params()[-1]:\n            row += \", \"\n    row += ')'\n    return row\n\n\ndef _get_set_of_params(tasks):\n    params = {}\n    for param in tasks[0].get_params():\n        params[param] = {getattr(task, param[0]) for task in tasks}\n    return params\n\n\ndef _get_unique_param_keys(params):\n    for param_key, param_values in params.items():\n        if len(param_values) > 1:\n            yield param_key\n\n\ndef _ranging_attributes(attributes, unique_param):\n    \"\"\"\n    Checks if there is a continuous range\n    \"\"\"\n    if len(attributes) > 2:\n        if unique_param[1].next_in_enumeration(attributes[0]) is None:\n            return False\n        for i in range(1, len(attributes)):\n            if unique_param[1].next_in_enumeration(attributes[i - 1]) != attributes[i]:\n                return False\n    return True\n\n\ndef _get_str_one_parameter(tasks):\n    row = ''\n    count = 0\n    for task in tasks:\n        if (len(row) >= 30 and count > 2 and count != len(tasks) - 1) or len(row) > 200:\n            row += '...'\n            break\n        row += '{0}'.format(getattr(task, task.get_params()[0][0]))\n        if count < len(tasks) - 1:\n            row += ','\n        count += 1\n    return row\n\n\ndef _serialize_first_param(task):\n    return task.get_params()[0][1].serialize(getattr(task, task.get_params()[0][0]))\n\n\ndef _get_number_of_tasks_for(status, group_tasks):\n    if status == \"still_pending\":\n        return (_get_number_of_tasks(group_tasks[\"still_pending_ext\"]) +\n                _get_number_of_tasks(group_tasks[\"still_pending_not_ext\"]))\n    return _get_number_of_tasks(group_tasks[status])\n\n\ndef _get_number_of_tasks(task_dict):\n    return sum(len(tasks) for tasks in task_dict.values())\n\n\ndef _get_comments(group_tasks):\n    \"\"\"\n    Get the human readable comments and quantities for the task types.\n    \"\"\"\n    comments = {}\n    for status, human in _COMMENTS:\n        num_tasks = _get_number_of_tasks_for(status, group_tasks)\n        if num_tasks:\n            space = \"    \" if status in _PENDING_SUB_STATUSES else \"\"\n            comments[status] = '{space}* {num_tasks} {human}:\\n'.format(\n                space=space,\n                num_tasks=num_tasks,\n                human=human)\n    return comments\n\n\n# Oredered in the sense that they'll be printed in this order\n_ORDERED_STATUSES = (\n    \"already_done\",\n    \"completed\",\n    \"failed\",\n    \"still_pending\",\n    \"still_pending_ext\",\n    \"run_by_other_worker\",\n    \"upstream_failure\",\n    \"upstream_missing_dependency\",\n    \"upstream_run_by_other_worker\",\n    \"unknown_reason\",\n)\n_PENDING_SUB_STATUSES = set(_ORDERED_STATUSES[_ORDERED_STATUSES.index(\"still_pending_ext\"):])\n_COMMENTS = set((\n    (\"already_done\", 'present dependencies were encountered'),\n    (\"completed\", 'ran successfully'),\n    (\"failed\", 'failed'),\n    (\"still_pending\", 'were left pending, among these'),\n    (\"still_pending_ext\", 'were missing external dependencies'),\n    (\"run_by_other_worker\", 'were being run by another worker'),\n    (\"upstream_failure\", 'had failed dependencies'),\n    (\"upstream_missing_dependency\", 'had missing external dependencies'),\n    (\"upstream_run_by_other_worker\", 'had dependencies that were being run by other worker'),\n    (\"unknown_reason\", 'were left pending because of unknown reason'),\n))\n\n\ndef _get_run_by_other_worker(worker):\n    \"\"\"\n    This returns a set of the tasks that are being run by other worker\n    \"\"\"\n    worker_that_blocked_task = dict()\n    get_work_response_history = worker._get_work_response_history\n    for get_work_response in get_work_response_history:\n        if get_work_response['task_id'] is None:\n            for running_task in get_work_response['running_tasks']:\n                other_worker_id = running_task['worker']\n                other_task_id = running_task['task_id']\n                other_task = worker._scheduled_tasks.get(other_task_id)\n                if other_task:\n                    worker_that_blocked_task[other_task] = other_worker_id\n    return set(worker_that_blocked_task.keys())\n\n\ndef _get_external_workers(worker):\n    \"\"\"\n    This returns a dict with a set of tasks for all of the other workers\n    \"\"\"\n    worker_that_blocked_task = dict()\n    get_work_response_history = worker._get_work_response_history\n    for get_work_response in get_work_response_history:\n        if get_work_response['task_id'] is None:\n            for running_task in get_work_response['running_tasks']:\n                other_worker_id = running_task['worker']\n                other_task_id = running_task['task_id']\n                other_task = worker._scheduled_tasks.get(other_task_id)\n                if other_task:\n                    if other_worker_id not in worker_that_blocked_task.keys():\n                        worker_that_blocked_task[other_worker_id] = set()\n                    worker_that_blocked_task[other_worker_id].add(other_task)\n    return worker_that_blocked_task\n\n\ndef _group_tasks_by_name_and_status(task_dict):\n    \"\"\"\n    Takes a dictionary with sets of tasks grouped by their status and returns a dictionary with dictionaries with an array of tasks grouped by their status and task name\n    \"\"\"\n    group_status = {}\n    for task in task_dict:\n        if task.task_family not in group_status:\n            group_status[task.task_family] = []\n        group_status[task.task_family].append(task)\n    return group_status\n\n\ndef _summary_dict(worker):\n    set_tasks = _partition_tasks(worker)\n    set_tasks[\"run_by_other_worker\"] = _get_run_by_other_worker(worker)\n    _populate_unknown_statuses(set_tasks)\n    return set_tasks\n\n\ndef _summary_format(set_tasks, worker):\n    group_tasks = {}\n    for status, task_dict in set_tasks.items():\n        group_tasks[status] = _group_tasks_by_name_and_status(task_dict)\n    comments = _get_comments(group_tasks)\n    num_all_tasks = len(set_tasks[\"already_done\"]) + len(set_tasks[\"completed\"]) + len(set_tasks[\"failed\"]) + len(set_tasks[\"still_pending_ext\"]) + len(set_tasks[\"still_pending_not_ext\"])\n    str_output = ''\n    str_output += 'Scheduled {0} tasks of which:\\n'.format(num_all_tasks)\n    for status in _ORDERED_STATUSES:\n        if status not in comments:\n            continue\n        str_output += '{0}'.format(comments[status])\n        if status != 'still_pending':\n            str_output += '{0}\\n'.format(_get_str(group_tasks[status], status in _PENDING_SUB_STATUSES))\n    ext_workers = _get_external_workers(worker)\n    group_tasks_ext_workers = {}\n    for ext_worker, task_dict in ext_workers.items():\n        group_tasks_ext_workers[ext_worker] = _group_tasks_by_name_and_status(task_dict)\n    if len(ext_workers) > 0:\n        str_output += \"\\nThe other workers were:\\n\"\n        count = 0\n        for ext_worker, task_dict in ext_workers.items():\n            if count > 3 and count < len(ext_workers) - 1:\n                str_output += \"    and {0} other workers\".format(len(ext_workers) - count)\n                break\n            str_output += \"    - {0} ran {1} tasks\\n\".format(ext_worker, len(task_dict))\n            count += 1\n        str_output += '\\n'\n    if num_all_tasks == len(set_tasks[\"already_done\"]) + len(set_tasks[\"still_pending_ext\"]) + len(set_tasks[\"still_pending_not_ext\"]):\n        if len(ext_workers) == 0:\n            str_output += '\\n'\n        str_output += 'Did not run any tasks'\n    smiley = \"\"\n    reason = \"\"\n    if len(set_tasks[\"failed\"]):\n        smiley = \":(\"\n        reason = \"there were failed tasks\"\n    elif len(set_tasks[\"still_pending_ext\"]):\n        smiley = \":|\"\n        reason = \"there were missing external dependencies\"\n    else:\n        smiley = \":)\"\n        reason = \"there were no failed tasks or missing external dependencies\"\n    str_output += \"\\nThis progress looks {0} because {1}\".format(smiley, reason)\n    if num_all_tasks == 0:\n        str_output = 'Did not schedule any tasks'\n    return str_output\n\n\ndef _summary_wrap(str_output):\n    return textwrap.dedent(\"\"\"\n    ===== Luigi Execution Summary =====\n\n    {str_output}\n\n    ===== Luigi Execution Summary =====\n    \"\"\").format(str_output=str_output)\n\n\ndef summary(worker):\n    \"\"\"\n    Given a worker, return a human readable summary of what the worker have\n    done.\n    \"\"\"\n    return _summary_wrap(_summary_format(_summary_dict(worker), worker))\n# 5\n", "idx": 2, "id": 12846, "msg": "", "proj": "spotify-luigi", "lang": "py"}
{"patch": "@@ -151,7 +151,7 @@ class Org < ActiveRecord::Base\n   end\n \n   def grant_api!(token_permission_type)\n-    org.token_permission_types << token_permission_type unless org.tokenpermission_types.include? token_permission_type\n+    self.token_permission_types << token_permission_type unless self.token_permission_types.include? token_permission_type\n   end\n \n   private", "y": 1, "oldf": "class Org < ActiveRecord::Base\n  include GlobalHelpers\n  include FlagShihTzu\n  extend Dragonfly::Model::Validations\n  validates_with OrgLinksValidator\n\n  # Stores links as an JSON object: { org: [{\"link\":\"www.example.com\",\"text\":\"foo\"}, ...] }\n  # The links are validated against custom validator allocated at validators/template_links_validator.rb\n  serialize :links, JSON\n\n  ##\n  # Associations\n#  belongs_to :organisation_type   # depricated, but cannot be removed until migration run\n  belongs_to :language\n  has_many :guidance_groups\n  has_many :templates\n  has_many :users\n  has_many :annotations\n\n  has_and_belongs_to_many :token_permission_types, join_table: \"org_token_permissions\", unique: true\n\n  has_many :org_identifiers\n  has_many :identifier_schemes, through: :org_identifiers\n\n  ##\n  # Possibly needed for active_admin\n  #   -relies on protected_attributes gem as syntax depricated in rails 4.2\n\tattr_accessible :abbreviation, :logo, :remove_logo,\n                  :logo_file_name, :name, :links,\n                  :organisation_type_id, :wayfless_entity, :parent_id, :sort_name,\n                  :token_permission_type_ids, :language_id, :contact_email, :contact_name,\n                  :language, :org_type, :region, :token_permission_types,\n                  :guidance_group_ids, :is_other, :region_id, :logo_uid, :logo_name,\n                  :feedback_enabled, :feedback_email_subject, :feedback_email_msg\n  ##\n  # Validators\n#  validates :contact_email, email: true, allow_nil: true\n  validates :name, presence: {message: _(\"can't be blank\")}, uniqueness: {message: _(\"must be unique\")}\n  # allow validations for logo upload\n  dragonfly_accessor :logo do\n    after_assign :resize_image\n  end\n  validates_property :format, of: :logo, in: ['jpeg', 'png', 'gif','jpg','bmp'], message: _(\"must be one of the following formats: jpeg, jpg, png, gif, bmp\")\n  validates_size_of :logo, maximum: 500.kilobytes, message: _(\"can't be larger than 500KB\")\n\n  ##\n  # Define Bit Field values\n  # Column org_type\n  has_flags 1 => :institution,\n            2 => :funder,\n            3 => :organisation,\n            4 => :research_institute,\n            5 => :project,\n            6 => :school,\n            column: 'org_type'\n\n  # Predefined queries for retrieving the managain organisation and funders\n  scope :managing_orgs, -> { where(abbreviation: Rails.configuration.branding[:organisation][:abbreviation]) }\n\n  scope :search, -> (term) {\n    search_pattern = \"%#{term}%\"\n    where(\"orgs.name LIKE ? OR orgs.contact_email LIKE ?\", search_pattern, search_pattern)\n  }\n\n  after_create :create_guidance_group\n\n  # EVALUATE CLASS AND INSTANCE METHODS BELOW\n  #\n  # What do they do? do they do it efficiently, and do we need them?\n\n  # Determines the locale set for the organisation\n  # @return String or nil\n  def get_locale\n    if !self.language.nil?\n      return self.language.abbreviation\n    else\n      return nil\n    end\n  end\n\n# TODO: Should these be hardcoded? Also, an Org can currently be multiple org_types at one time.\n#       For example you can do: funder = true; project = true; school = true\n#       Calling type in the above scenario returns \"Funder\" which is a bit misleading\n#       Is FlagShihTzu's Bit flag the appropriate structure here or should we use an enum?\n#       Tests are setup currently to work with this issue.\n  ##\n  # returns the name of the type of the organisation as a string\n  # defaults to none if no org type present\n  #\n  # @return [String]\n  def org_type_to_s\n    ret = []\n    ret << \"Institution\" if self.institution?\n    ret << \"Funder\" if self.funder?\n    ret << \"Organisation\" if self.organisation?\n    ret << \"Research Institute\" if self.research_institute?\n    ret << \"Project\" if self.project?\n    ret << \"School\" if self.school?\n    return (ret.length > 0 ? ret.join(', ') : \"None\")\n  end\n\n  def funder_only?\n    self.org_type == Org.org_type_values_for(:funder).min\n  end\n\n  ##\n  # returns the name of the organisation\n  #\n  # @return [String]\n  def to_s\n    name\n  end\n\n  ##\n  # returns the abbreviation for the organisation if it exists, or the name if not\n  #\n  # @return [String] name or abbreviation of the organisation\n  def short_name\n    if abbreviation.nil? then\n      return name\n    else\n      return abbreviation\n    end\n  end\n\n  ##\n  # returns all published templates belonging to the organisation\n  #\n  # @return [Array<Dmptemplate>] published dmptemplates\n\tdef published_templates\n\t\treturn templates.where(\"published = ?\", true)\n\tend\n\n  def check_api_credentials\n    if token_permission_types.count == 0\n      users.each do |user|\n        user.api_token = \"\"\n        user.save!\n      end\n    end\n  end\n\n  def org_admins\n    User.joins(:perms).where(\"users.org_id = ? AND perms.name IN (?)\", self.id,\n      ['grant_permissions', 'modify_templates', 'modify_guidance', 'change_org_details'])\n  end\n\n  def plans\n    Plan.includes(:template, :phases, :roles, :users).joins(:roles, :users).where('users.org_id = ? AND roles.access IN (?)',\n      self.id, Role.access_values_for(:owner).concat(Role.access_values_for(:administrator)))\n  end\n\n  def grant_api!(token_permission_type)\n    org.token_permission_types << token_permission_type unless org.tokenpermission_types.include? token_permission_type\n  end\n\n  private\n    ##\n    # checks size of logo and resizes if necessary\n    #\n    def resize_image\n      unless logo.nil?\n        if logo.height != 75\n          self.logo = logo.thumb('x75')  # resize height and maintain aspect ratio\n        end\n      end\n    end\n\n    # creates a dfefault Guidance Group on create on the Org\n    def create_guidance_group\n      GuidanceGroup.create(name: self.abbreviation? ? self.abbreviation : self.name , org_id: self.id)\n    end\n\nend\n", "idx": 1, "id": 17448, "msg": "self is implicit", "proj": "DMPRoadmap-roadmap", "lang": "rb"}
{"patch": "@@ -118,7 +118,7 @@ func (q *EventuallyConsistentQuotaUsage) Get(\n \t}()\n \tpast := q.config.Clock().Now().Sub(c.timestamp)\n \tswitch {\n-\tcase past > blockTolerance:\n+\tcase past > blockTolerance || c.timestamp.IsZero():\n \t\tq.log.CDebugf(ctx, \"Blocking on getAndCache. Cached data is %s old.\", past)\n \t\t// TODO: optimize this to make sure there's only one outstanding RPC. In\n \t\t// other words, wait for it to finish if one is already in progress.", "y": 1, "oldf": "// Copyright 2017 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"sync\"\n\t\"sync/atomic\"\n\t\"time\"\n\n\t\"github.com/keybase/client/go/logger\"\n\t\"github.com/keybase/client/go/protocol/keybase1\"\n\t\"github.com/keybase/kbfs/kbfsblock\"\n\t\"golang.org/x/net/context\"\n)\n\n// ECQUCtxTagKey is the type for unique ECQU background opertaion IDs.\ntype ECQUCtxTagKey struct{}\n\n// ECQUID is used in EventuallyConsistentQuotaUsage for only background RPCs.\n// More specifically, when we need to spawn a background goroutine for\n// GetUserQuotaInfo, a new context with this tag is created and used. This is\n// also used as a prefix for the logger module name in\n// EventuallyConsistentQuotaUsage.\nconst ECQUID = \"ECQU\"\n\ntype cachedQuotaUsage struct {\n\ttimestamp  time.Time\n\tusageBytes int64\n\tlimitBytes int64\n}\n\n// EventuallyConsistentQuotaUsage keeps tracks of quota usage, in a way user of\n// which can choose to accept stale data to reduce calls into block servers.\ntype EventuallyConsistentQuotaUsage struct {\n\tconfig Config\n\tlog    logger.Logger\n\ttid    keybase1.TeamID\n\n\tbackgroundInProcess int32\n\n\tmu     sync.RWMutex\n\tcached cachedQuotaUsage\n}\n\n// NewEventuallyConsistentQuotaUsage creates a new\n// EventuallyConsistentQuotaUsage object.\nfunc NewEventuallyConsistentQuotaUsage(\n\tconfig Config, loggerSuffix string) *EventuallyConsistentQuotaUsage {\n\treturn &EventuallyConsistentQuotaUsage{\n\t\tconfig: config,\n\t\tlog:    config.MakeLogger(ECQUID + \"-\" + loggerSuffix),\n\t}\n}\n\n// NewEventuallyConsistentTeamQuotaUsage creates a new\n// EventuallyConsistentQuotaUsage object.\nfunc NewEventuallyConsistentTeamQuotaUsage(\n\tconfig Config, tid keybase1.TeamID,\n\tloggerSuffix string) *EventuallyConsistentQuotaUsage {\n\treturn &EventuallyConsistentQuotaUsage{\n\t\tconfig: config,\n\t\tlog:    config.MakeLogger(ECQUID + \"-\" + loggerSuffix),\n\t\ttid:    tid,\n\t}\n}\n\nfunc (q *EventuallyConsistentQuotaUsage) getAndCache(\n\tctx context.Context) (usage cachedQuotaUsage, err error) {\n\tdefer func() {\n\t\tq.log.CDebugf(ctx, \"getAndCache: error=%v\", err)\n\t}()\n\tvar quotaInfo *kbfsblock.QuotaInfo\n\tif q.tid.IsNil() {\n\t\tquotaInfo, err = q.config.BlockServer().GetUserQuotaInfo(ctx)\n\t} else {\n\t\tquotaInfo, err = q.config.BlockServer().GetTeamQuotaInfo(ctx, q.tid)\n\t}\n\tif err != nil {\n\t\treturn cachedQuotaUsage{}, err\n\t}\n\n\tusage.limitBytes = quotaInfo.Limit\n\tif quotaInfo.Total != nil {\n\t\tusage.usageBytes = quotaInfo.Total.Bytes[kbfsblock.UsageWrite]\n\t} else {\n\t\tusage.usageBytes = 0\n\t}\n\tusage.timestamp = q.config.Clock().Now()\n\n\tq.mu.Lock()\n\tdefer q.mu.Unlock()\n\tq.cached = usage\n\n\treturn usage, nil\n}\n\n// Get returns KBFS bytes used and limit for user. To help avoid having too\n// frequent calls into bserver, caller can provide a positive tolerance, to\n// accept stale LimitBytes and UsageBytes data. If tolerance is 0 or negative,\n// this always makes a blocking RPC to bserver and return latest quota usage.\n//\n// 1) If the age of cached data is more than blockTolerance, a blocking RPC is\n// issued and the function only returns after RPC finishes, with the newest\n// data from RPC. The RPC causes cached data to be refreshed as well.\n// 2) Otherwise, if the age of cached data is more than bgTolerance,\n// a background RPC is spawned to refresh cached data, and the stale\n// data is returned immediately.\n// 3) Otherwise, the cached stale data is returned immediately.\nfunc (q *EventuallyConsistentQuotaUsage) Get(\n\tctx context.Context, bgTolerance, blockTolerance time.Duration) (\n\ttimestamp time.Time, usageBytes, limitBytes int64, err error) {\n\tc := func() cachedQuotaUsage {\n\t\tq.mu.RLock()\n\t\tdefer q.mu.RUnlock()\n\t\treturn q.cached\n\t}()\n\tpast := q.config.Clock().Now().Sub(c.timestamp)\n\tswitch {\n\tcase past > blockTolerance:\n\t\tq.log.CDebugf(ctx, \"Blocking on getAndCache. Cached data is %s old.\", past)\n\t\t// TODO: optimize this to make sure there's only one outstanding RPC. In\n\t\t// other words, wait for it to finish if one is already in progress.\n\t\tc, err = q.getAndCache(ctx)\n\t\tif err != nil {\n\t\t\treturn time.Time{}, -1, -1, err\n\t\t}\n\tcase past > bgTolerance:\n\t\tif atomic.CompareAndSwapInt32(&q.backgroundInProcess, 0, 1) {\n\t\t\tid, err := MakeRandomRequestID()\n\t\t\tif err != nil {\n\t\t\t\tq.log.Warning(\"Couldn't generate a random request ID: %v\", err)\n\t\t\t}\n\t\t\tq.log.CDebugf(ctx, \"Cached data is %s old. Spawning getAndCache in \"+\n\t\t\t\t\"background with tag:%s=%v.\", past, ECQUID, id)\n\t\t\tgo func() {\n\t\t\t\t// Make a new context so that it doesn't get canceled when returned.\n\t\t\t\tlogTags := make(logger.CtxLogTags)\n\t\t\t\tlogTags[ECQUCtxTagKey{}] = ECQUID\n\t\t\t\tbgCtx := logger.NewContextWithLogTags(context.Background(), logTags)\n\t\t\t\tbgCtx = context.WithValue(bgCtx, ECQUCtxTagKey{}, id)\n\t\t\t\t// Make sure a timeout is on the context, in case the RPC blocks\n\t\t\t\t// forever somehow, where we'd end up with never resetting\n\t\t\t\t// backgroundInProcess flag again.\n\t\t\t\tbgCtx, cancel := context.WithTimeout(bgCtx, 10*time.Second)\n\t\t\t\tdefer cancel()\n\t\t\t\t// The error is igonred here without logging since getAndCache already\n\t\t\t\t// logs it.\n\t\t\t\t_, _ = q.getAndCache(bgCtx)\n\t\t\t\tatomic.StoreInt32(&q.backgroundInProcess, 0)\n\t\t\t}()\n\t\t} else {\n\t\t\tq.log.CDebugf(ctx,\n\t\t\t\t\"Cached data is %s old, but background getAndCache is already running.\", past)\n\t\t}\n\tdefault:\n\t\tq.log.CDebugf(ctx, \"Returning cached data from %s ago.\", past)\n\t}\n\treturn c.timestamp, c.usageBytes, c.limitBytes, nil\n}\n", "idx": 1, "id": 17302, "msg": "When `c.timestamp` is zero value, shouldn't `past > blockTolerance` be `true` almost for sure?", "proj": "keybase-kbfs", "lang": "go"}
{"patch": "@@ -511,6 +511,11 @@ class DictSet(VaspInputSet):\n \n         if isinstance(settings, Kpoints):\n             return settings\n+        \n+        # Return None if KSPACING is present in the INCAR, because this will\n+        # cause VASP to generate the KPOINTS file automatically\n+        if self.user_incar_settings.get(\"KSPACING\", None) is not None:\n+            return None\n \n         # If grid_density is in the kpoints_settings use\n         # Kpoints.automatic_density", "y": 0, "oldf": "# coding: utf-8\n# Copyright (c) Pymatgen Development Team.\n# Distributed under the terms of the MIT License.\n\n\"\"\"\nThis module defines the VaspInputSet abstract base class and a concrete\nimplementation for the parameters developed and tested by the core team\nof pymatgen, including the Materials Virtual Lab, Materials Project and the MIT\nhigh throughput project.  The basic concept behind an input set is to specify\na scheme to generate a consistent set of VASP inputs from a structure\nwithout further user intervention. This ensures comparability across\nruns.\n\nRead the following carefully before implementing new input sets:\n\n1. 99% of what needs to be done can be done by specifying user_incar_settings\n   to override some of the defaults of various input sets. Unless there is an\n   extremely good reason to add a new set, DO NOT add one. E.g., if you want\n   to turn the hubbard U off, just set \"LDAU\": False as a user_incar_setting.\n2. All derivative input sets should inherit from one of the usual MPRelaxSet or\n   MITRelaxSet, and proper superclass delegation should be used where possible.\n   In particular, you are not supposed to implement your own as_dict or\n   from_dict for derivative sets unless you know what you are doing.\n   Improper overriding the as_dict and from_dict protocols is the major\n   cause of implementation headaches. If you need an example, look at how the\n   MPStaticSet or MPNonSCFSets are constructed.\n\nThe above are recommendations. The following are UNBREAKABLE rules:\n\n1. All input sets must take in a structure or list of structures as the first\n   argument.\n2. user_incar_settings, user_kpoints_settings and user_<whatever>_settings are\n   ABSOLUTE. Any new sets you implement must obey this. If a user wants to\n   override your settings, you assume he knows what he is doing. Do not\n   magically override user supplied settings. You can issue a warning if you\n   think the user is wrong.\n3. All input sets must save all supplied args and kwargs as instance variables.\n   E.g., self.my_arg = my_arg and self.kwargs = kwargs in the __init__. This\n   ensures the as_dict and from_dict work correctly.\n\"\"\"\n\nimport abc\nimport re\nimport glob\nimport shutil\nimport warnings\nfrom itertools import chain\nfrom copy import deepcopy\nfrom pathlib import Path\nfrom typing import List, Union\nimport numpy as np\nfrom monty.serialization import loadfn\nfrom monty.io import zopen\nfrom monty.dev import deprecated\nfrom zipfile import ZipFile\n\nfrom pymatgen.core.periodic_table import Specie, Element\nfrom pymatgen.core.structure import Structure\nfrom pymatgen.io.vasp.inputs import Incar, Poscar, Potcar, Kpoints, VaspInput\nfrom pymatgen.io.vasp.outputs import Vasprun, Outcar\nfrom pymatgen.io.lobster import Lobsterin\nfrom monty.json import MSONable\nfrom pymatgen.symmetry.analyzer import SpacegroupAnalyzer\nfrom pymatgen.symmetry.bandstructure import HighSymmKpath\nfrom pymatgen.analysis.structure_matcher import StructureMatcher\nfrom pymatgen.core.sites import PeriodicSite\n\n__author__ = \"Shyue Ping Ong, Wei Chen, Will Richards, Geoffroy Hautier, \" \\\n             \"Anubhav Jain\"\n__copyright__ = \"Copyright 2011, The Materials Project\"\n__version__ = \"1.0\"\n__maintainer__ = \"Shyue Ping Ong\"\n__email__ = \"shyuep@gmail.com\"\n__date__ = \"May 28 2016\"\n\nMODULE_DIR = Path(__file__).resolve().parent\n\n\nclass VaspInputSet(MSONable, metaclass=abc.ABCMeta):\n    \"\"\"\n    Base class representing a set of Vasp input parameters with a structure\n    supplied as init parameters. Typically, you should not inherit from this\n    class. Start from DictSet or MPRelaxSet or MITRelaxSet.\n    \"\"\"\n\n    @property\n    @abc.abstractmethod\n    def incar(self):\n        \"\"\"Incar object\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def kpoints(self):\n        \"\"\"Kpoints object\"\"\"\n        pass\n\n    @property\n    @abc.abstractmethod\n    def poscar(self):\n        \"\"\"Poscar object\"\"\"\n        pass\n\n    @property\n    def potcar_symbols(self):\n        \"\"\"\n        List of POTCAR symbols.\n        \"\"\"\n        elements = self.poscar.site_symbols\n        potcar_symbols = []\n        settings = self._config_dict[\"POTCAR\"]\n\n        if isinstance(settings[elements[-1]], dict):\n            for el in elements:\n                potcar_symbols.append(settings[el]['symbol']\n                                      if el in settings else el)\n        else:\n            for el in elements:\n                potcar_symbols.append(settings.get(el, el))\n\n        return potcar_symbols\n\n    @property\n    def potcar(self):\n        \"\"\"\n        Potcar object.\n        \"\"\"\n        return Potcar(self.potcar_symbols, functional=self.potcar_functional)\n\n    @property  # type: ignore\n    @deprecated(message=\"Use the get_vasp_input() method instead.\")\n    def all_input(self):\n        \"\"\"\n        Returns all input files as a dict of {filename: vasp object}\n\n        Returns:\n            dict of {filename: object}, e.g., {'INCAR': Incar object, ...}\n        \"\"\"\n        return {'INCAR': self.incar,\n                'KPOINTS': self.kpoints,\n                'POSCAR': self.poscar,\n                'POTCAR': self.potcar}\n\n    def get_vasp_input(self) -> VaspInput:\n        \"\"\"\n\n        Returns:\n            VaspInput\n        \"\"\"\n        return VaspInput(incar=self.incar,\n                         kpoints=self.kpoints,\n                         poscar=self.poscar,\n                         potcar=self.potcar)\n\n    def write_input(self, output_dir,\n                    make_dir_if_not_present=True, include_cif=False):\n        \"\"\"\n        Writes a set of VASP input to a directory.\n\n        Args:\n            output_dir (str): Directory to output the VASP input files\n            make_dir_if_not_present (bool): Set to True if you want the\n                directory (and the whole path) to be created if it is not\n                present.\n            include_cif (bool): Whether to write a CIF file in the output\n                directory for easier opening by VESTA.\n        \"\"\"\n        vinput = self.get_vasp_input()\n        vinput.write_input(\n            output_dir, make_dir_if_not_present=make_dir_if_not_present)\n        if include_cif:\n            s = vinput[\"POSCAR\"].structure\n            fname = Path(output_dir) / (\"%s.cif\" % re.sub(r'\\s', \"\", s.formula))\n            s.to(filename=fname)\n\n    def write_spec(self, filename=None, readme=None):\n        \"\"\"\n        Write a set of the VASP input files to a zip file, WITHOUT the Potcar\n        file. The Potcar file will instead be written as a \"POTCAR.spec\". This is\n        intended to help sharing an input set with people who might not have a license\n        to specific Potcar files.\n\n        Given a \"POTCAR.spec\", the specific POTCAR file can be re-generated using\n        pymatgen with the \"generate_potcar\" function in the pymatgen CLI when set\n        up with a corresponding directory of POTCAR files.\n\n        Args:\n            filename (str): Filename to output as zip file, will default to name\n            of input set.\n            readme (str): Additional file to include as README\n        \"\"\"\n\n        if not filename:\n            filename = self.__class__.__name__ + \"_spec.zip\"\n        if not filename.endswith(\".zip\"):\n            filename += \".zip\"\n\n        with ZipFile(filename, 'w') as zip:\n            zip.writestr(\"INCAR\", str(self.incar))\n            zip.writestr(\"POSCAR\", str(self.poscar))\n            zip.writestr(\"KPOINTS\", str(self.kpoints))\n            zip.writestr(\"POTCAR.spec\", \"\\n\".join(self.potcar_symbols))\n            if readme:\n                zip.writestr(\"README\", readme)\n            # TODO: should write transformations.json also where appropriate\n            # cannot import TransformedStructure due to circular import\n\n    def as_dict(self, verbosity=2):\n        \"\"\"\n        Args:\n            verbosity: Verbosity for generated dict. If 1, structure is\n            excluded.\n\n        Returns:\n            MSONable dict\n        \"\"\"\n        d = MSONable.as_dict(self)\n        if verbosity == 1:\n            d.pop(\"structure\", None)\n        return d\n\n\ndef _load_yaml_config(fname):\n    config = loadfn(str(MODULE_DIR / (\"%s.yaml\" % fname)))\n    config[\"INCAR\"].update(loadfn(str(MODULE_DIR / \"VASPIncarBase.yaml\")))\n    return config\n\n\nclass DictSet(VaspInputSet):\n    \"\"\"\n    Concrete implementation of VaspInputSet that is initialized from a dict\n    settings. This allows arbitrary settings to be input. In general,\n    this is rarely used directly unless there is a source of settings in yaml\n    format (e.g., from a REST interface). It is typically used by other\n    VaspInputSets for initialization.\n\n    Special consideration should be paid to the way the MAGMOM initialization\n    for the INCAR is done. The initialization differs depending on the type of\n    structure and the configuration settings. The order in which the magmom is\n    determined is as follows:\n\n    1. If the site itself has a magmom setting, that is used.\n    2. If the species on the site has a spin setting, that is used.\n    3. If the species itself has a particular setting in the config file, that\n       is used, e.g., Mn3+ may have a different magmom than Mn4+.\n    4. Lastly, the element symbol itself is checked in the config file. If\n       there are no settings, VASP's default of 0.6 is used.\n    \"\"\"\n\n    def __init__(self, structure, config_dict,\n                 files_to_transfer=None, user_incar_settings=None,\n                 user_kpoints_settings=None, user_potcar_settings=None,\n                 constrain_total_magmom=False, sort_structure=True,\n                 potcar_functional=\"PBE\", force_gamma=False,\n                 reduce_structure=None, vdw=None,\n                 use_structure_charge=False, standardize=False, sym_prec=0.1,\n                 international_monoclinic=True):\n        \"\"\"\n        Args:\n            structure (Structure): The Structure to create inputs for.\n            config_dict (dict): The config dictionary to use.\n            files_to_transfer (dict): A dictionary of {filename: filepath}. This\n                allows the transfer of files from a previous calculation.\n            user_incar_settings (dict): User INCAR settings. This allows a user\n                to override INCAR settings, e.g., setting a different MAGMOM for\n                various elements or species. Note that in the new scheme,\n                ediff_per_atom and hubbard_u are no longer args. Instead, the\n                config_dict supports EDIFF_PER_ATOM and EDIFF keys. The former\n                scales with # of atoms, the latter does not. If both are\n                present, EDIFF is preferred. To force such settings, just supply\n                user_incar_settings={\"EDIFF\": 1e-5, \"LDAU\": False} for example.\n                The keys 'LDAUU', 'LDAUJ', 'LDAUL' are special cases since\n                pymatgen defines different values depending on what anions are\n                present in the structure, so these keys can be defined in one\n                of two ways, e.g. either {\"LDAUU\":{\"O\":{\"Fe\":5}}} to set LDAUU\n                for Fe to 5 in an oxide, or {\"LDAUU\":{\"Fe\":5}} to set LDAUU to\n                5 regardless of the input structure.\n\n                If a None value is given, that key is unset. For example,\n                {\"ENCUT\": None} will remove ENCUT from the incar settings.\n            user_kpoints_settings (dict or Kpoints): Allow user to override kpoints\n                setting by supplying a dict E.g., {\"reciprocal_density\": 1000}.\n                User can also supply Kpoints object. Default is None.\n            user_potcar_settings (dict: Allow user to override POTCARs. E.g.,\n                {\"Gd\": \"Gd_3\"}. This is generally not recommended. Default is None.\n            constrain_total_magmom (bool): Whether to constrain the total magmom\n                (NUPDOWN in INCAR) to be the sum of the expected MAGMOM for all\n                species. Defaults to False.\n            sort_structure (bool): Whether to sort the structure (using the\n                default sort order of electronegativity) before generating input\n                files. Defaults to True, the behavior you would want most of the\n                time. This ensures that similar atomic species are grouped\n                together.\n            potcar_functional (str): Functional to use. Default (None) is to use\n                the functional in Potcar.DEFAULT_FUNCTIONAL. Valid values:\n                \"PBE\", \"PBE_52\", \"PBE_54\", \"LDA\", \"LDA_52\", \"LDA_54\", \"PW91\",\n                \"LDA_US\", \"PW91_US\".\n            force_gamma (bool): Force gamma centered kpoint generation. Default\n                (False) is to use the Automatic Density kpoint scheme, which\n                will use the Gamma centered generation scheme for hexagonal\n                cells, and Monkhorst-Pack otherwise.\n            reduce_structure (None/str): Before generating the input files,\n                generate the reduced structure. Default (None), does not\n                alter the structure. Valid values: None, \"niggli\", \"LLL\".\n            vdw: Adds default parameters for van-der-Waals functionals supported\n                by VASP to INCAR. Supported functionals are: DFT-D2, undamped\n                DFT-D3, DFT-D3 with Becke-Jonson damping, Tkatchenko-Scheffler,\n                Tkatchenko-Scheffler with iterative Hirshfeld partitioning,\n                MBD@rSC, dDsC, Dion's vdW-DF, DF2, optPBE, optB88, optB86b and\n                rVV10.\n            use_structure_charge (bool): If set to True, then the public\n                variable used for setting the overall charge of the\n                structure (structure.charge) is used to set the NELECT\n                variable in the INCAR\n                Default is False (structure's overall charge is not used)\n            standardize (float): Whether to standardize to a primitive standard\n                cell. Defaults to False.\n            sym_prec (float): Tolerance for symmetry finding.\n            international_monoclinic (bool): Whether to use international convention\n                (vs Curtarolo) for monoclinic. Defaults True.\n        \"\"\"\n        if reduce_structure:\n            structure = structure.get_reduced_structure(reduce_structure)\n        if sort_structure:\n            structure = structure.get_sorted_structure()\n\n        self._structure = structure\n        self._config_dict = deepcopy(config_dict)\n        self.files_to_transfer = files_to_transfer or {}\n        self.constrain_total_magmom = constrain_total_magmom\n        self.sort_structure = sort_structure\n        self.potcar_functional = potcar_functional\n        self.force_gamma = force_gamma\n        self.reduce_structure = reduce_structure\n        self.user_incar_settings = user_incar_settings or {}\n        self.user_kpoints_settings = user_kpoints_settings or {}\n        self.user_potcar_settings = user_potcar_settings\n        self.vdw = vdw.lower() if vdw is not None else None\n        self.use_structure_charge = use_structure_charge\n        self.standardize = standardize\n        self.sym_prec = sym_prec\n        self.international_monoclinic = international_monoclinic\n\n        if self.vdw:\n            vdw_par = loadfn(str(MODULE_DIR / \"vdW_parameters.yaml\"))\n            try:\n                self._config_dict[\"INCAR\"].update(vdw_par[self.vdw])\n            except KeyError:\n                raise KeyError(\"Invalid or unsupported van-der-Waals \"\n                               \"functional. Supported functionals are \"\n                               \"%s.\" % vdw_par.keys())\n        if self.user_potcar_settings:\n            warnings.warn(\n                \"Overriding POTCARs is generally not recommended as it \"\n                \"significantly affect the results of calculations and \"\n                \"compatibility with other calculations done with the same \"\n                \"input set. In many instances, it is better to write a \"\n                \"subclass of a desired input set and override the POTCAR in \"\n                \"the subclass to be explicit on the differences.\",\n                BadInputSetWarning)\n            for k, v in self.user_potcar_settings.items():\n                self._config_dict[\"POTCAR\"][k] = v\n\n    @property\n    def structure(self) -> Structure:\n        \"\"\"\n        :return: Structure\n        \"\"\"\n        if self.standardize and self.sym_prec:\n            return standardize_structure(\n                self._structure, sym_prec=self.sym_prec,\n                international_monoclinic=self.international_monoclinic)\n        else:\n            return self._structure\n\n    @property\n    def incar(self) -> Incar:\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        settings = dict(self._config_dict[\"INCAR\"])\n        for k, v in self.user_incar_settings.items():\n            if v is None:\n                try:\n                    del settings[k]\n                except KeyError:\n                    settings[k] = v\n            else:\n                settings[k] = v\n        structure = self.structure\n        incar = Incar()\n        comp = structure.composition\n        elements = sorted([el for el in comp.elements if comp[el] > 0],\n                          key=lambda e: e.X)\n        most_electroneg = elements[-1].symbol\n        poscar = Poscar(structure)\n        hubbard_u = settings.get(\"LDAU\", False)\n\n        for k, v in settings.items():\n            if k == \"MAGMOM\":\n                mag = []\n                for site in structure:\n                    if hasattr(site, 'magmom'):\n                        mag.append(site.magmom)\n                    elif hasattr(site.specie, 'spin'):\n                        mag.append(site.specie.spin)\n                    elif str(site.specie) in v:\n                        mag.append(v.get(str(site.specie)))\n                    else:\n                        mag.append(v.get(site.specie.symbol, 0.6))\n                incar[k] = mag\n            elif k in ('LDAUU', 'LDAUJ', 'LDAUL'):\n                if hubbard_u:\n                    if hasattr(structure[0], k.lower()):\n                        m = dict([(site.specie.symbol, getattr(site, k.lower()))\n                                  for site in structure])\n                        incar[k] = [m[sym] for sym in poscar.site_symbols]\n                    # lookup specific LDAU if specified for most_electroneg atom\n                    elif most_electroneg in v.keys() and \\\n                            isinstance(v[most_electroneg], dict):\n                        incar[k] = [v[most_electroneg].get(sym, 0)\n                                    for sym in poscar.site_symbols]\n                    # else, use fallback LDAU value if it exists\n                    else:\n                        incar[k] = [v.get(sym, 0)\n                                    if isinstance(v.get(sym, 0), (float, int))\n                                    else 0 for sym in poscar.site_symbols]\n            elif k.startswith(\"EDIFF\") and k != \"EDIFFG\":\n                if \"EDIFF\" not in settings and k == \"EDIFF_PER_ATOM\":\n                    incar[\"EDIFF\"] = float(v) * structure.num_sites\n                else:\n                    incar[\"EDIFF\"] = float(settings[\"EDIFF\"])\n            else:\n                incar[k] = v\n\n        has_u = hubbard_u and sum(incar['LDAUU']) > 0\n        if has_u:\n            # modify LMAXMIX if LSDA+U and you have d or f electrons\n            # note that if the user explicitly sets LMAXMIX in settings it will\n            # override this logic.\n            if 'LMAXMIX' not in settings.keys():\n                # contains f-electrons\n                if any([el.Z > 56 for el in structure.composition]):\n                    incar['LMAXMIX'] = 6\n                # contains d-electrons\n                elif any([el.Z > 20 for el in structure.composition]):\n                    incar['LMAXMIX'] = 4\n        else:\n            for key in list(incar.keys()):\n                if key.startswith('LDAU'):\n                    del incar[key]\n\n        if self.constrain_total_magmom:\n            nupdown = sum([mag if abs(mag) > 0.6 else 0\n                           for mag in incar['MAGMOM']])\n            incar['NUPDOWN'] = nupdown\n\n        if self.use_structure_charge:\n            incar[\"NELECT\"] = self.nelect\n\n        if np.product(self.kpoints.kpts) < 4 and incar.get(\"ISMEAR\", 0) == -5:\n            incar[\"ISMEAR\"] = 0\n\n        if all([k.is_metal for k in structure.composition.keys()]):\n            if incar.get(\"NSW\", 0) > 0 and incar.get(\"ISMEAR\", 1) < 1:\n                warnings.warn(\"Relaxation of likely metal with ISMEAR < 1 \"\n                              \"detected. Please see VASP recommendations on \"\n                              \"ISMEAR for metals.\", BadInputSetWarning)\n\n        return incar\n\n    @property\n    def poscar(self) -> Poscar:\n        \"\"\"\n        :return: Poscar\n        \"\"\"\n        return Poscar(self.structure)\n\n    @property\n    def nelect(self) -> float:\n        \"\"\"\n        Gets the default number of electrons for a given structure.\n        \"\"\"\n        # if structure is not sorted this can cause problems, so must take\n        # care to remove redundant symbols when counting electrons\n        site_symbols = list(set(self.poscar.site_symbols))\n        structure = self.structure\n        nelect = 0.\n        for ps in self.potcar:\n            if ps.element in site_symbols:\n                site_symbols.remove(ps.element)\n                nelect += structure.composition.element_composition[\n                              ps.element] * ps.ZVAL\n\n        if self.use_structure_charge:\n            return nelect - structure.charge\n        else:\n            return nelect\n\n    @property\n    def kpoints(self) -> Kpoints:\n        \"\"\"\n        Returns a KPOINTS file using the fully automated grid method. Uses\n        Gamma centered meshes for hexagonal cells and Monk grids otherwise.\n\n        Algorithm:\n            Uses a simple approach scaling the number of divisions along each\n            reciprocal lattice vector proportional to its length.\n        \"\"\"\n        settings = self.user_kpoints_settings or self._config_dict[\"KPOINTS\"]\n\n        if isinstance(settings, Kpoints):\n            return settings\n\n        # If grid_density is in the kpoints_settings use\n        # Kpoints.automatic_density\n        if settings.get('grid_density'):\n            return Kpoints.automatic_density(\n                self.structure, int(settings['grid_density']),\n                self.force_gamma)\n\n        # If reciprocal_density is in the kpoints_settings use\n        # Kpoints.automatic_density_by_vol\n        elif settings.get('reciprocal_density'):\n            return Kpoints.automatic_density_by_vol(\n                self.structure, int(settings['reciprocal_density']),\n                self.force_gamma)\n\n        # If length is in the kpoints_settings use Kpoints.automatic\n        elif settings.get('length'):\n            return Kpoints.automatic(settings['length'])\n\n        # Raise error. Unsure of which kpoint generation to use\n        else:\n            raise ValueError(\n                \"Invalid KPoint Generation algo : Supported Keys are \"\n                \"grid_density: for Kpoints.automatic_density generation, \"\n                \"reciprocal_density: for KPoints.automatic_density_by_vol \"\n                \"generation, and length  : for Kpoints.automatic generation\")\n\n    def __str__(self):\n        return self.__class__.__name__\n\n    def __repr__(self):\n        return self.__class__.__name__\n\n    def write_input(self, output_dir: str,\n                    make_dir_if_not_present: bool = True,\n                    include_cif: bool = False):\n        \"\"\"\n        Writes out all input to a directory.\n\n        :param output_dir: Name of output dir.\n        :param make_dir_if_not_present: Makes the directory if it does not\n            exist.\n        :param include_cif: Whether to include a CIF file for easier reading.\n        :return:\n        \"\"\"\n        super().write_input(\n            output_dir=output_dir,\n            make_dir_if_not_present=make_dir_if_not_present,\n            include_cif=include_cif)\n        for k, v in self.files_to_transfer.items():\n            with zopen(v, \"rb\") as fin, \\\n                    zopen(str(Path(output_dir) / k), \"wb\") as fout:\n                shutil.copyfileobj(fin, fout)\n\n\nclass MITRelaxSet(DictSet):\n    \"\"\"\n    Standard implementation of VaspInputSet utilizing parameters in the MIT\n    High-throughput project.\n    The parameters are chosen specifically for a high-throughput project,\n    which means in general pseudopotentials with fewer electrons were chosen.\n\n    Please refer::\n\n        A Jain, G. Hautier, C. Moore, S. P. Ong, C. Fischer, T. Mueller,\n        K. A. Persson, G. Ceder. A high-throughput infrastructure for density\n        functional theory calculations. Computational Materials Science,\n        2011, 50(8), 2295-2310. doi:10.1016/j.commatsci.2011.02.023\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MITRelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        \"\"\"\n        :param structure: Structure\n        :param kwargs: Same as those supported by DictSet.\n        \"\"\"\n        super().__init__(structure, MITRelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPRelaxSet(DictSet):\n    \"\"\"\n    Implementation of VaspInputSet utilizing parameters in the public\n    Materials Project. Typically, the pseudopotentials chosen contain more\n    electrons than the MIT parameters, and the k-point grid is ~50% more dense.\n    The LDAUU parameters are also different due to the different psps used,\n    which result in different fitted values.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MPRelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        \"\"\"\n        :param structure: Structure\n        :param kwargs: Same as those supported by DictSet.\n        \"\"\"\n        super().__init__(structure, MPRelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPMetalRelaxSet(MPRelaxSet):\n    \"\"\"\n    Implementation of VaspInputSet utilizing parameters in the public\n    Materials Project, but with tuning for metals. Key things are a denser\n    k point density, and a\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MPRelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        \"\"\"\n        :param structure: Structure\n        :param kwargs: Same as those supported by DictSet.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        self._config_dict[\"INCAR\"].update({\n            \"ISMEAR\": 1,\n            \"SIGMA\": 0.2\n        })\n        self._config_dict[\"KPOINTS\"].update({\n            \"reciprocal_density\": 200\n        })\n        self.kwargs = kwargs\n\n\nclass MPHSERelaxSet(DictSet):\n    \"\"\"\n    Same as the MPRelaxSet, but with HSE parameters.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MPHSERelaxSet\")\n\n    def __init__(self, structure, **kwargs):\n        \"\"\"\n        :param structure: Structure\n        :param kwargs: Same as those supported by DictSet.\n        \"\"\"\n        super().__init__(structure, MPHSERelaxSet.CONFIG, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MPStaticSet(MPRelaxSet):\n    \"\"\"\n    Creates input files for a static calculation.\n    \"\"\"\n\n    def __init__(self, structure, prev_incar=None, prev_kpoints=None,\n                 lepsilon=False, lcalcpol=False, reciprocal_density=100,\n                 small_gap_multiply=None, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): Structure from previous run.\n            prev_incar (Incar): Incar file from previous run.\n            prev_kpoints (Kpoints): Kpoints from previous run.\n            lepsilon (bool): Whether to add static dielectric calculation\n            reciprocal_density (int): For static calculations, we usually set the\n                reciprocal density by volume. This is a convenience arg to change\n                that, rather than using user_kpoints_settings. Defaults to 100,\n                which is ~50% more than that of standard relaxation calculations.\n            small_gap_multiply ([float, float]): If the gap is less than\n                1st index, multiply the default reciprocal_density by the 2nd\n                index.\n            **kwargs: kwargs supported by MPRelaxSet.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        if isinstance(prev_incar, str):\n            prev_incar = Incar.from_file(prev_incar)\n        if isinstance(prev_kpoints, str):\n            prev_kpoints = Kpoints.from_file(prev_kpoints)\n\n        self.prev_incar = prev_incar\n        self.prev_kpoints = prev_kpoints\n        self.reciprocal_density = reciprocal_density\n        self.kwargs = kwargs\n        self.lepsilon = lepsilon\n        self.lcalcpol = lcalcpol\n        self.small_gap_multiply = small_gap_multiply\n\n    @property\n    def incar(self):\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        parent_incar = super().incar\n        incar = Incar(self.prev_incar) if self.prev_incar is not None else \\\n            Incar(parent_incar)\n\n        incar.update(\n            {\"IBRION\": -1, \"ISMEAR\": -5, \"LAECHG\": True, \"LCHARG\": True,\n             \"LORBIT\": 11, \"LVHAR\": True, \"LWAVE\": False, \"NSW\": 0,\n             \"ICHARG\": 0, \"ALGO\": \"Normal\"})\n\n        if self.lepsilon:\n            incar[\"IBRION\"] = 8\n            incar[\"LEPSILON\"] = True\n\n            # LPEAD=T: numerical evaluation of overlap integral prevents\n            # LRF_COMMUTATOR errors and can lead to better expt. agreement\n            # but produces slightly different results\n            incar[\"LPEAD\"] = True\n\n            # Note that DFPT calculations MUST unset NSW. NSW = 0 will fail\n            # to output ionic.\n            incar.pop(\"NSW\", None)\n            incar.pop(\"NPAR\", None)\n\n        if self.lcalcpol:\n            incar[\"LCALCPOL\"] = True\n\n        for k in [\"MAGMOM\", \"NUPDOWN\"] + list(self.kwargs.get(\n                \"user_incar_settings\", {}).keys()):\n            # For these parameters as well as user specified settings, override\n            # the incar settings.\n            if parent_incar.get(k, None) is not None:\n                incar[k] = parent_incar[k]\n            else:\n                incar.pop(k, None)\n\n        # use new LDAUU when possible b/c the Poscar might have changed\n        # representation\n        if incar.get('LDAU'):\n            u = incar.get('LDAUU', [])\n            j = incar.get('LDAUJ', [])\n            if sum([u[x] - j[x] for x, y in enumerate(u)]) > 0:\n                for tag in ('LDAUU', 'LDAUL', 'LDAUJ'):\n                    incar.update({tag: parent_incar[tag]})\n            # ensure to have LMAXMIX for GGA+U static run\n            if \"LMAXMIX\" not in incar:\n                incar.update({\"LMAXMIX\": parent_incar[\"LMAXMIX\"]})\n\n        # Compare ediff between previous and staticinputset values,\n        # choose the tighter ediff\n        incar[\"EDIFF\"] = min(incar.get(\"EDIFF\", 1), parent_incar[\"EDIFF\"])\n        return incar\n\n    @property\n    def kpoints(self) -> Kpoints:\n        \"\"\"\n        :return: Kpoints\n        \"\"\"\n        self._config_dict[\"KPOINTS\"][\"reciprocal_density\"] = \\\n            self.reciprocal_density\n        kpoints = super().kpoints\n\n        # Prefer to use k-point scheme from previous run\n        # except for when lepsilon = True is specified\n        if self.prev_kpoints and self.prev_kpoints.style != kpoints.style:\n            if (self.prev_kpoints.style == Kpoints.supported_modes.Monkhorst) \\\n                    and (not self.lepsilon):\n                k_div = [kp + 1 if kp % 2 == 1 else kp\n                         for kp in kpoints.kpts[0]]\n                kpoints = Kpoints.monkhorst_automatic(k_div)\n            else:\n                kpoints = Kpoints.gamma_automatic(kpoints.kpts[0])\n        return kpoints\n\n    def override_from_prev_calc(self, prev_calc_dir='.'):\n        \"\"\"\n        Update the input set to include settings from a previous calculation.\n\n        Args:\n            prev_calc_dir (str): The path to the previous calculation directory.\n\n        Returns:\n            The input set with the settings (structure, k-points, incar, etc)\n            updated using the previous VASP run.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        self.prev_incar = vasprun.incar\n        self.prev_kpoints = vasprun.kpoints\n\n        if self.standardize:\n            warnings.warn(\"Use of standardize=True with from_prev_run is not \"\n                          \"recommended as there is no guarantee the copied \"\n                          \"files will be appropriate for the standardized \"\n                          \"structure.\")\n\n        self._structure = get_structure_from_prev_run(vasprun, outcar)\n\n        # multiply the reciprocal density if needed\n        if self.small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= self.small_gap_multiply[0]:\n                self.reciprocal_density = (self.reciprocal_density *\n                                           self.small_gap_multiply[1])\n\n        return self\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for static calculations from a\n        directory of previous Vasp run.\n\n        Args:\n            prev_calc_dir (str): Directory containing the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            **kwargs: All kwargs supported by MPStaticSet, other than prev_incar\n                and prev_structure and prev_kpoints which are determined from\n                the prev_calc_dir.\n        \"\"\"\n        input_set = cls(_dummy_structure, **kwargs)\n        return input_set.override_from_prev_calc(prev_calc_dir=prev_calc_dir)\n\n\nclass MPHSEBSSet(MPHSERelaxSet):\n    \"\"\"\n    Implementation of a VaspInputSet for HSE band structure computations.\n    Remember that HSE band structures must be self-consistent in VASP. A\n    band structure along symmetry lines for instance needs BOTH a uniform\n    grid with appropriate weights AND a path along the lines with weight 0.\n\n    Thus, the \"Uniform\" mode is just like regular static SCF but allows\n    adding custom kpoints (e.g., corresponding to known VBM/CBM) to the\n    uniform grid that have zero weight (e.g., for better gap estimate).\n\n    The \"Gap\" mode behaves just like the \"Uniform\" mode, however, if starting\n    from a previous calculation, the VBM and CBM k-points will automatically\n    be added to ``added_kpoints``.\n\n    The \"Line\" mode is just like Uniform mode, but additionally adds\n    k-points along symmetry lines with zero weight.\n    \"\"\"\n\n    def __init__(self, structure, user_incar_settings=None, added_kpoints=None,\n                 mode=\"Gap\", reciprocal_density=None, copy_chgcar=True,\n                 kpoints_line_density=20, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): Structure to compute\n            user_incar_settings (dict): A dict specifying additional incar\n                settings\n            added_kpoints (list): a list of kpoints (list of 3 number list)\n                added to the run. The k-points are in fractional coordinates\n            mode (str): \"Line\" - generate k-points along symmetry lines for\n                bandstructure. \"Uniform\" - generate uniform k-points grid.\n            reciprocal_density (int): k-point density to use for uniform mesh.\n            copy_chgcar (bool): Whether to copy the CHGCAR of a previous run.\n            kpoints_line_density (int): k-point density for high symmetry lines\n            **kwargs (dict): Any other parameters to pass into DictSet.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        self.user_incar_settings = user_incar_settings or {}\n        self._config_dict[\"INCAR\"].update({\n            \"NSW\": 0,\n            \"ISMEAR\": 0,\n            \"SIGMA\": 0.05,\n            \"ISYM\": 3,\n            \"LCHARG\": False,\n            \"NELMIN\": 5\n        })\n        self.added_kpoints = added_kpoints if added_kpoints is not None else []\n        self.mode = mode\n\n        if (not reciprocal_density or\n                \"reciprocal_density\" not in self.user_kpoints_settings):\n            self.reciprocal_density = 50\n        else:\n            self.reciprocal_density = reciprocal_density or \\\n                                      self.user_kpoints_settings['reciprocal_density']\n\n        self.kpoints_line_density = kpoints_line_density\n        self.copy_chgcar = copy_chgcar\n\n    @property\n    def kpoints(self) -> Kpoints:\n        \"\"\"\n        :return: Kpoints\n        \"\"\"\n        kpts = []  # type: List[Union[int, float, None]]\n        weights = []  # type: List[Union[float, None]]\n        all_labels = []  # type: List[Union[str, None]]\n        structure = self.structure\n\n        # for both modes, include the Uniform mesh w/standard weights\n        grid = Kpoints.automatic_density_by_vol(structure,\n                                                self.reciprocal_density).kpts\n        ir_kpts = SpacegroupAnalyzer(structure, symprec=0.1) \\\n            .get_ir_reciprocal_mesh(grid[0])\n        for k in ir_kpts:\n            kpts.append(k[0])\n            weights.append(int(k[1]))\n            all_labels.append(None)\n\n        # for both modes, include any user-added kpoints w/zero weight\n        for k in self.added_kpoints:\n            kpts.append(k)\n            weights.append(0.0)\n            all_labels.append(\"user-defined\")\n\n        # for line mode only, add the symmetry lines w/zero weight\n        if self.mode.lower() == \"line\":\n            kpath = HighSymmKpath(structure)\n            frac_k_points, labels = kpath.get_kpoints(\n                line_density=self.kpoints_line_density,\n                coords_are_cartesian=False)\n\n            for k in range(len(frac_k_points)):\n                kpts.append(frac_k_points[k])\n                weights.append(0.0)\n                all_labels.append(labels[k])\n\n        comment = (\"HSE run along symmetry lines\"\n                   if self.mode.lower() == \"line\"\n                   else \"HSE run on uniform grid\")\n\n        return Kpoints(comment=comment,\n                       style=Kpoints.supported_modes.Reciprocal,\n                       num_kpts=len(kpts), kpts=kpts, kpts_weights=weights,\n                       labels=all_labels)\n\n    def override_from_prev_calc(self, prev_calc_dir='.'):\n        \"\"\"\n        Update the input set to include settings from a previous calculation.\n\n        Args:\n            prev_calc_dir (str): The path to the previous calculation directory.\n\n        Returns:\n            The input set with the settings (structure, k-points, incar, etc)\n            updated using the previous VASP run.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        self._structure = get_structure_from_prev_run(vasprun, outcar)\n\n        # note: recommend not standardizing the cell because we want to retain\n        # k-points\n        if self.standardize:\n            warnings.warn(\"Use of standardize=True with from_prev_calc is not \"\n                          \"recommended as there is no guarantee the copied \"\n                          \"files will be appropriate for the standardized \"\n                          \"structure.\")\n\n        if self.mode.lower() == \"gap\":\n            added_kpoints = []\n\n            bs = vasprun.get_band_structure()\n            vbm, cbm = bs.get_vbm()[\"kpoint\"], bs.get_cbm()[\"kpoint\"]\n            if vbm:\n                added_kpoints.append(vbm.frac_coords)\n            if cbm:\n                added_kpoints.append(cbm.frac_coords)\n\n            self.added_kpoints.extend(added_kpoints)\n\n        files_to_transfer = {}\n        if self.copy_chgcar:\n            chgcars = sorted(glob.glob(str(Path(prev_calc_dir) / \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        self.files_to_transfer.update(files_to_transfer)\n\n        return self\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for HSE calculations from a\n        directory of previous Vasp run.\n\n        Args:\n            prev_calc_dir (str): Directory containing the outputs\n                (vasprun.xml and OUTCAR) of previous vasp run.\n            **kwargs: All kwargs supported by MPHSEBSStaticSet, other than\n                prev_structure which is determined from the previous calc dir.\n        \"\"\"\n        input_set = cls(_dummy_structure, **kwargs)\n        return input_set.override_from_prev_calc(prev_calc_dir=prev_calc_dir)\n\n\nclass MPNonSCFSet(MPRelaxSet):\n    \"\"\"\n    Init a MPNonSCFSet. Typically, you would use the classmethod\n    from_prev_calc to initialize from a previous SCF run.\n    \"\"\"\n\n    def __init__(self, structure, prev_incar=None,\n                 mode=\"line\", nedos=2001, reciprocal_density=100, sym_prec=0.1,\n                 kpoints_line_density=20, optics=False, copy_chgcar=True,\n                 nbands_factor=1.2, small_gap_multiply=None, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): Structure to compute\n            prev_incar (Incar/string): Incar file from previous run.\n            mode (str): Line, Uniform or Boltztrap mode supported.\n            nedos (int): nedos parameter. Default to 2001.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                volume (defaults to 100)\n            sym_prec (float): Symmetry precision (for Uniform mode).\n            kpoints_line_density (int): Line density for Line mode.\n            optics (bool): whether to add dielectric function\n            copy_chgcar: Whether to copy the old CHGCAR when starting from a\n                previous calculation.\n            nbands_factor (float): Multiplicative factor for NBANDS when starting\n                from a previous calculation. Choose a higher number if you are\n                doing an LOPTICS calculation.\n            small_gap_multiply ([float, float]): When starting from a previous\n                calculation, if the gap is less than 1st index, multiply the default\n                reciprocal_density by the 2nd index.\n            **kwargs: kwargs supported by MPRelaxSet.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        if isinstance(prev_incar, str):\n            prev_incar = Incar.from_file(prev_incar)\n        self.prev_incar = prev_incar\n        self.kwargs = kwargs\n        self.nedos = nedos\n        self.reciprocal_density = reciprocal_density\n        self.sym_prec = sym_prec\n        self.kpoints_line_density = kpoints_line_density\n        self.optics = optics\n        self.mode = mode.lower()\n        self.copy_chgcar = copy_chgcar\n        self.nbands_factor = nbands_factor\n        self.small_gap_multiply = small_gap_multiply\n\n        if self.mode.lower() not in [\"line\", \"uniform\", \"boltztrap\"]:\n            raise ValueError(\"Supported modes for NonSCF runs are 'Line', \"\n                             \"'Uniform' and 'Boltztrap!\")\n\n        if (self.mode.lower() != \"uniform\" or nedos < 2000) and optics:\n            warnings.warn(\"It is recommended to use Uniform mode with a high \"\n                          \"NEDOS for optics calculations.\")\n\n    @property\n    def incar(self) -> Incar:\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        incar = super().incar\n        if self.prev_incar is not None:\n            incar.update({k: v for k, v in self.prev_incar.items()})\n\n        # Overwrite necessary INCAR parameters from previous runs\n        incar.update({\"IBRION\": -1, \"LCHARG\": False, \"LORBIT\": 11,\n                      \"LWAVE\": False, \"NSW\": 0, \"ISYM\": 0, \"ICHARG\": 11})\n\n        if self.mode.lower() == 'uniform':\n            # use tetrahedron method for DOS and optics calculations\n            incar.update({\"ISMEAR\": -5})\n        else:\n            # if line mode, can't use ISMEAR=-5; also use small sigma to avoid\n            # partial occupancies for small band gap materials.\n            # finally, explicit k-point generation (needed for bolztrap mode)\n            # is incompatible with ISMEAR = -5.\n            incar.update({\"ISMEAR\": 0, \"SIGMA\": 0.01})\n\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        if self.mode.lower() in \"uniform\":\n            # Set smaller steps for DOS and optics output\n            incar[\"NEDOS\"] = self.nedos\n\n        if self.optics:\n            incar[\"LOPTICS\"] = True\n\n        incar.pop(\"MAGMOM\", None)\n\n        return incar\n\n    @property\n    def kpoints(self) -> Kpoints:\n        \"\"\"\n        :return: Kpoints\n        \"\"\"\n        if self.mode.lower() == \"line\":\n            kpath = HighSymmKpath(self.structure)\n            frac_k_points, k_points_labels = kpath.get_kpoints(\n                line_density=self.kpoints_line_density,\n                coords_are_cartesian=False)\n            kpoints = Kpoints(\n                comment=\"Non SCF run along symmetry lines\",\n                style=Kpoints.supported_modes.Reciprocal,\n                num_kpts=len(frac_k_points),\n                kpts=frac_k_points, labels=k_points_labels,\n                kpts_weights=[1] * len(frac_k_points))\n        elif self.mode.lower() == \"boltztrap\":\n            kpoints = Kpoints.automatic_density_by_vol(self.structure,\n                                                       self.reciprocal_density)\n            mesh = kpoints.kpts[0]\n            ir_kpts = SpacegroupAnalyzer(\n                self.structure,\n                symprec=self.sym_prec).get_ir_reciprocal_mesh(mesh)\n            kpts = []\n            weights = []\n            for k in ir_kpts:\n                kpts.append(k[0])\n                weights.append(int(k[1]))\n            kpoints = Kpoints(comment=\"Non SCF run on uniform grid\",\n                              style=Kpoints.supported_modes.Reciprocal,\n                              num_kpts=len(ir_kpts),\n                              kpts=kpts, kpts_weights=weights)\n        else:\n            self._config_dict[\"KPOINTS\"][\"reciprocal_density\"] = \\\n                self.reciprocal_density\n            kpoints = super().kpoints\n\n        # override pymatgen kpoints if provided\n        user_kpoints = self.kwargs.get(\"user_kpoints_settings\", None)\n        if isinstance(user_kpoints, Kpoints):\n            kpoints = user_kpoints\n\n        return kpoints\n\n    def override_from_prev_calc(self, prev_calc_dir='.'):\n        \"\"\"\n        Update the input set to include settings from a previous calculation.\n\n        Args:\n            prev_calc_dir (str): The path to the previous calculation directory.\n\n        Returns:\n            The input set with the settings (structure, k-points, incar, etc)\n            updated using the previous VASP run.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        self.prev_incar = vasprun.incar\n\n        # Get a Magmom-decorated structure\n        self._structure = get_structure_from_prev_run(vasprun, outcar)\n\n        if self.standardize:\n            warnings.warn(\"Use of standardize=True with from_prev_run is not \"\n                          \"recommended as there is no guarantee the copied \"\n                          \"files will be appropriate for the standardized\"\n                          \" structure. copy_chgcar is enforced to be false.\")\n            self.copy_chgcar = False\n\n        # Turn off spin when magmom for every site is smaller than 0.02.\n        if outcar and outcar.magnetization:\n            site_magmom = np.array([i['tot'] for i in outcar.magnetization])\n            ispin = 2 if np.any(site_magmom[np.abs(site_magmom) > 0.02]) else 1\n\n        elif vasprun.is_spin:\n            ispin = 2\n\n        else:\n            ispin = 1\n\n        nbands = int(np.ceil(vasprun.parameters[\"NBANDS\"] * self.nbands_factor))\n        self.prev_incar.update({\"ISPIN\": ispin, \"NBANDS\": nbands})\n\n        files_to_transfer = {}\n\n        if self.copy_chgcar:\n            chgcars = sorted(glob.glob(str(Path(prev_calc_dir) / \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        self.files_to_transfer.update(files_to_transfer)\n\n        # multiply the reciprocal density if needed:\n        if self.small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= self.small_gap_multiply[0]:\n                self.reciprocal_density = (self.reciprocal_density *\n                                           self.small_gap_multiply[1])\n                self.kpoints_line_density = (self.kpoints_line_density *\n                                             self.small_gap_multiply[1])\n\n        return self\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for NonSCF calculations from a\n        directory of previous static Vasp run.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            **kwargs: All kwargs supported by MPNonSCFSet, other than structure,\n                prev_incar and prev_chgcar which are determined from the\n                prev_calc_dir.\n        \"\"\"\n        input_set = cls(_dummy_structure, **kwargs)\n        return input_set.override_from_prev_calc(prev_calc_dir=prev_calc_dir)\n\n\nclass MPSOCSet(MPStaticSet):\n    \"\"\"\n    An input set for running spin-orbit coupling (SOC) calculations.\n    \"\"\"\n\n    def __init__(self, structure, saxis=(0, 0, 1), copy_chgcar=True,\n                 nbands_factor=1.2, reciprocal_density=100,\n                 small_gap_multiply=None, magmom=None, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): the structure must have the 'magmom' site\n                property and each magnetic moment value must have 3\n                components. eg: ``magmom = [[0,0,2], ...]``\n            saxis (tuple): magnetic moment orientation\n            copy_chgcar: Whether to copy the old CHGCAR. Defaults to True.\n            nbands_factor (float): Multiplicative factor for NBANDS. Choose a\n                higher number if you are doing an LOPTICS calculation.\n            reciprocal_density (int): density of k-mesh by reciprocal volume.\n            small_gap_multiply ([float, float]): If the gap is less than\n                1st index, multiply the default reciprocal_density by the 2nd\n                index.\n            magmom (list[list[float]]): Override for the structure magmoms.\n            **kwargs: kwargs supported by MPStaticSet.\n        \"\"\"\n\n        if (not hasattr(structure[0], \"magmom\") and\n                not isinstance(structure[0].magmom, list)):\n            raise ValueError(\n                \"The structure must have the 'magmom' site \"\n                \"property and each magnetic moment value must have 3 \"\n                \"components. eg:- magmom = [0,0,2]\")\n\n        super().__init__(structure, reciprocal_density=reciprocal_density,\n                         **kwargs)\n        self.saxis = saxis\n        self.copy_chgcar = copy_chgcar\n        self.nbands_factor = nbands_factor\n        self.small_gap_multiply = small_gap_multiply\n        self.magmom = magmom\n\n    @property\n    def incar(self) -> Incar:\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        incar = super().incar\n        if self.prev_incar is not None:\n            incar.update({k: v for k, v in self.prev_incar.items()})\n\n        # Overwrite necessary INCAR parameters from previous runs\n        incar.update({\"ISYM\": -1, \"LSORBIT\": \"T\", \"ICHARG\": 11,\n                      \"SAXIS\": list(self.saxis)})\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n    def override_from_prev_calc(self, prev_calc_dir='.'):\n        \"\"\"\n        Update the input set to include settings from a previous calculation.\n\n        Args:\n            prev_calc_dir (str): The path to the previous calculation directory.\n\n        Returns:\n            The input set with the settings (structure, k-points, incar, etc)\n            updated using the previous VASP run.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n\n        self.prev_incar = vasprun.incar\n\n        # Remove magmoms from previous INCAR, since we will prefer\n        # the final calculated magmoms\n        # TODO: revisit in context of MPStaticSet incar logic\n        if 'MAGMOM' in self.prev_incar:\n            del self.prev_incar['magmom']\n\n        # Get a magmom-decorated structure\n        self._structure = get_structure_from_prev_run(vasprun, outcar)\n        if self.standardize:\n            warnings.warn(\"Use of standardize=True with from_prev_run is not \"\n                          \"recommended as there is no guarantee the copied \"\n                          \"files will be appropriate for the standardized\"\n                          \" structure. copy_chgcar is enforced to be false.\")\n            self.copy_chgcar = False\n\n        # override magmom if provided\n        if self.magmom:\n            self._structure = self._structure.copy(\n                site_properties={\"magmom\": self.magmom})\n\n        # magmom has to be 3D for SOC calculation.\n        if hasattr(self._structure[0], \"magmom\"):\n            if not isinstance(self._structure[0].magmom, list):\n                self._structure = self._structure.copy(\n                    site_properties={\"magmom\": [[0, 0, site.magmom]\n                                                for site in self._structure]})\n        else:\n            raise ValueError(\"Neither the previous structure has magmom \"\n                             \"property nor magmom provided\")\n\n        nbands = int(np.ceil(vasprun.parameters[\"NBANDS\"] * self.nbands_factor))\n        self.prev_incar.update({\"NBANDS\": nbands})\n\n        files_to_transfer = {}\n        if self.copy_chgcar:\n            chgcars = sorted(glob.glob(str(Path(prev_calc_dir) / \"CHGCAR*\")))\n            if chgcars:\n                files_to_transfer[\"CHGCAR\"] = str(chgcars[-1])\n\n        self.files_to_transfer.update(files_to_transfer)\n\n        # multiply the reciprocal density if needed:\n        if self.small_gap_multiply:\n            gap = vasprun.eigenvalue_band_properties[0]\n            if gap <= self.small_gap_multiply[0]:\n                self.reciprocal_density = (self.reciprocal_density *\n                                           self.small_gap_multiply[1])\n\n        return self\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for SOC calculations from a\n        directory of previous static Vasp run. SOC calc requires all 3\n        components for MAGMOM for each atom in the structure.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml and OUTCAR) of previous vasp run.\n            **kwargs: All kwargs supported by MPSOCSet, other than structure,\n                prev_incar and prev_chgcar which are determined from the\n                prev_calc_dir.\n        \"\"\"\n        input_set = cls(_dummy_structure, **kwargs)\n        return input_set.override_from_prev_calc(prev_calc_dir=prev_calc_dir)\n\n\nclass MPNMRSet(MPStaticSet):\n    \"\"\"\n    Init a MPNMRSet.\n    \"\"\"\n\n    def __init__(self, structure, mode=\"cs\", isotopes=None,\n                 prev_incar=None, reciprocal_density=100, **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): Structure to compute\n            mode (str): The NMR calculation to run\n                            \"cs\": for Chemical Shift\n                            \"efg\" for Electric Field Gradient\n            isotopes (list): list of Isotopes for quadrupole moments\n            prev_incar (Incar): Incar file from previous run.\n            reciprocal_density (int): density of k-mesh by reciprocal\n                                    volume (defaults to 100)\n            **kwargs: kwargs supported by MPStaticSet.\n        \"\"\"\n        self.mode = mode\n        self.isotopes = isotopes if isotopes else []\n        super().__init__(structure, prev_incar=prev_incar,\n                         reciprocal_density=reciprocal_density, **kwargs)\n\n    @property\n    def incar(self):\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        incar = super().incar\n\n        if self.mode.lower() == \"cs\":\n            incar.update({\"LCHIMAG\": True,\n                          \"EDIFF\": -1.0e-10,\n                          \"ISYM\": 0,\n                          \"LCHARG\": False,\n                          \"LNMR_SYM_RED\": True,\n                          \"NELMIN\": 10,\n                          \"NSLPLINE\": True,\n                          \"PREC\": \"ACCURATE\",\n                          \"SIGMA\": 0.01})\n        elif self.mode.lower() == \"efg\":\n\n            isotopes = {ist.split(\"-\")[0]: ist for ist in self.isotopes}\n\n            quad_efg = [\n                Specie(p).get_nmr_quadrupole_moment(isotopes.get(p, None)) for p\n                in self.poscar.site_symbols]\n\n            incar.update({\"ALGO\": \"FAST\",\n                          \"EDIFF\": -1.0e-10,\n                          \"ISYM\": 0,\n                          \"LCHARG\": False,\n                          \"LEFG\": True,\n                          \"QUAD_EFG\": quad_efg,\n                          \"NELMIN\": 10,\n                          \"PREC\": \"ACCURATE\",\n                          \"SIGMA\": 0.01})\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n\nclass MVLElasticSet(MPRelaxSet):\n    \"\"\"\n    MVL denotes VASP input sets that are implemented by the Materials Virtual\n    Lab (http://www.materialsvirtuallab.org) for various research.\n\n    This input set is used to calculate elastic constants in VASP. It is used\n    in the following work::\n\n        Z. Deng, Z. Wang, I.-H. Chu, J. Luo, S. P. Ong.\n        \u201cElastic Properties of Alkali Superionic Conductor Electrolytes\n        from First Principles Calculations\u201d, J. Electrochem. Soc.\n        2016, 163(2), A67-A74. doi: 10.1149/2.0061602jes\n\n    To read the elastic constants, you may use the Outcar class which parses the\n    elastic constants.\n    \"\"\"\n\n    def __init__(self, structure, potim=0.015, **kwargs):\n        \"\"\"\n        Args:\n            scale (float): POTIM parameter. The default of 0.015 is usually fine,\n                but some structures may require a smaller step.\n            user_incar_settings (dict): A dict specifying additional incar\n                settings.\n            kwargs:\n                Parameters supported by MPRelaxSet.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        self._config_dict[\"INCAR\"].update({\"IBRION\": 6, \"NFREE\": 2,\n                                           \"POTIM\": potim})\n        self._config_dict[\"INCAR\"].pop(\"NPAR\", None)\n\n\nclass MVLGWSet(DictSet):\n    \"\"\"\n    MVL denotes VASP input sets that are implemented by the Materials Virtual\n    Lab (http://www.materialsvirtuallab.org) for various research. This is a\n    flexible input set for GW calculations.\n\n    Note that unlike all other input sets in this module, the PBE_54 series of\n    functional is set as the default. These have much improved performance for\n    GW calculations.\n\n    A typical sequence is mode=\"STATIC\" -> mode=\"DIAG\" -> mode=\"GW\" ->\n    mode=\"BSE\". For all steps other than the first one (static), the\n    recommendation is to use from_prev_calculation on the preceding run in\n    the series.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MVLGWSet\")\n\n    SUPPORTED_MODES = (\"DIAG\", \"GW\", \"STATIC\", \"BSE\")\n\n    def __init__(self, structure, prev_incar=None, nbands=None,\n                 potcar_functional=\"PBE_54\", reciprocal_density=100,\n                 mode=\"STATIC\", copy_wavecar=True, nbands_factor=5, ncores=16,\n                 **kwargs):\n        r\"\"\"\n        Args:\n            structure (Structure): Input structure.\n            prev_incar (Incar/string): Incar file from previous run.\n            mode (str): Supported modes are \"STATIC\" (default), \"DIAG\", \"GW\",\n                and \"BSE\".\n            nbands (int): For subsequent calculations, it is generally\n                recommended to perform NBANDS convergence starting from the\n                NBANDS of the previous run for DIAG, and to use the exact same\n                NBANDS for GW and BSE. This parameter is used by\n                from_previous_calculation to set nband.\n            potcar_functional (str): Defaults to \"PBE_54\".\n            copy_wavecar: Whether to copy the old WAVECAR, WAVEDER and associated\n                files when starting from a previous calculation.\n            nbands_factor (int): Multiplicative factor for NBANDS when starting\n                from a previous calculation. Only applies if mode==\"DIAG\".\n                Need to be tested for convergence.\n            ncores (int): Numbers of cores used for the calculation. VASP will alter\n                NBANDS if it was not dividable by ncores. Only applies if\n                mode==\"DIAG\".\n            **kwargs: All kwargs supported by DictSet. Typically,\n                user_incar_settings is a commonly used option.\n        \"\"\"\n        super().__init__(structure, MVLGWSet.CONFIG, **kwargs)\n        self.prev_incar = prev_incar\n        self.nbands = nbands\n        self.potcar_functional = potcar_functional\n        self.reciprocal_density = reciprocal_density\n        self.mode = mode.upper()\n        if self.mode not in MVLGWSet.SUPPORTED_MODES:\n            raise ValueError(\"%s not one of the support modes : %s\" %\n                             (self.mode, MVLGWSet.SUPPORTED_MODES))\n        self.kwargs = kwargs\n        self.copy_wavecar = copy_wavecar\n        self.nbands_factor = nbands_factor\n        self.ncores = ncores\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        Generate gamma center k-points mesh grid for GW calc,\n        which is requested by GW calculation.\n        \"\"\"\n        return Kpoints.automatic_density_by_vol(self.structure,\n                                                self.reciprocal_density,\n                                                force_gamma=True)\n\n    @property\n    def incar(self):\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        parent_incar = super().incar\n        incar = Incar(self.prev_incar) if self.prev_incar is not None else \\\n            Incar(parent_incar)\n\n        if self.mode == \"DIAG\":\n            # Default parameters for diagonalization calculation.\n            incar.update({\n                \"ALGO\": \"Exact\",\n                \"NELM\": 1,\n                \"LOPTICS\": True,\n                \"LPEAD\": True\n            })\n        elif self.mode == \"GW\":\n            # Default parameters for GW calculation.\n            incar.update({\n                \"ALGO\": \"GW0\",\n                \"NELM\": 1,\n                \"NOMEGA\": 80,\n                \"ENCUTGW\": 250\n            })\n            incar.pop(\"EDIFF\", None)\n            incar.pop(\"LOPTICS\", None)\n            incar.pop(\"LPEAD\", None)\n        elif self.mode == \"BSE\":\n            # Default parameters for BSE calculation.\n            incar.update({\n                \"ALGO\": \"BSE\",\n                \"ANTIRES\": 0,\n                \"NBANDSO\": 20,\n                \"NBANDSV\": 20\n            })\n\n        if self.nbands:\n            incar[\"NBANDS\"] = self.nbands\n\n        # Respect user set INCAR.\n        incar.update(self.kwargs.get(\"user_incar_settings\", {}))\n\n        return incar\n\n    def override_from_prev_calc(self, prev_calc_dir='.'):\n        \"\"\"\n        Update the input set to include settings from a previous calculation.\n\n        Args:\n            prev_calc_dir (str): The path to the previous calculation directory.\n\n        Returns:\n            The input set with the settings (structure, k-points, incar, etc)\n            updated using the previous VASP run.\n        \"\"\"\n        vasprun, outcar = get_vasprun_outcar(prev_calc_dir)\n        self.prev_incar = vasprun.incar\n        self._structure = vasprun.final_structure\n\n        if self.standardize:\n            warnings.warn(\"Use of standardize=True with from_prev_run is not \"\n                          \"recommended as there is no guarantee the copied \"\n                          \"files will be appropriate for the standardized \"\n                          \"structure.\")\n\n        self.nbands = int(vasprun.parameters[\"NBANDS\"])\n        if self.mode.upper() == \"DIAG\":\n            self.nbands = int(np.ceil(self.nbands * self.nbands_factor /\n                                      self.ncores) * self.ncores)\n\n        # copy WAVECAR, WAVEDER (derivatives)\n        files_to_transfer = {}\n        if self.copy_wavecar:\n            for fname in (\"WAVECAR\", \"WAVEDER\", \"WFULL\"):\n                w = sorted(glob.glob(str(Path(prev_calc_dir) / (fname + \"*\"))))\n                if w:\n                    if fname == \"WFULL\":\n                        for f in w:\n                            fname = Path(f).name\n                            fname = fname.split(\".\")[0]\n                            files_to_transfer[fname] = f\n                    else:\n                        files_to_transfer[fname] = str(w[-1])\n\n        self.files_to_transfer.update(files_to_transfer)\n\n        return self\n\n    @classmethod\n    def from_prev_calc(cls, prev_calc_dir, mode=\"DIAG\", **kwargs):\n        \"\"\"\n        Generate a set of Vasp input files for GW or BSE calculations from a\n        directory of previous Exact Diag Vasp run.\n\n        Args:\n            prev_calc_dir (str): The directory contains the outputs(\n                vasprun.xml of previous vasp run.\n            mode (str): Supported modes are \"STATIC\", \"DIAG\" (default), \"GW\",\n                and \"BSE\".\n            **kwargs: All kwargs supported by MVLGWSet, other than structure,\n                prev_incar and mode, which are determined from the\n                prev_calc_dir.\n        \"\"\"\n        input_set = cls(_dummy_structure, mode=mode, **kwargs)\n        return input_set.override_from_prev_calc(prev_calc_dir=prev_calc_dir)\n\n\nclass MVLSlabSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a set of slab vasp runs,\n    including both slabs (along the c direction) and orient unit cells (bulk),\n    to ensure the same KPOINTS, POTCAR and INCAR criterion.\n    \"\"\"\n\n    def __init__(self, structure, k_product=50, bulk=False,\n                 auto_dipole=False, set_mix=True, sort_structure=True,\n                 **kwargs):\n        \"\"\"\n        :param structure: Structure\n        :param k_product: default to 50, kpoint number * length for a & b\n            directions, also for c direction in bulk calculations\n        :param bulk:\n        :param auto_dipole:\n        :param set_mix:\n        :param sort_structure:\n        :param kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n\n        if sort_structure:\n            structure = structure.get_sorted_structure()\n\n        self.k_product = k_product\n        self.bulk = bulk\n        self.auto_dipole = auto_dipole\n        self.kwargs = kwargs\n        self.set_mix = set_mix\n        self.kpt_calc = None\n\n        slab_incar = {\"EDIFF\": 1e-4, \"EDIFFG\": -0.02, \"ENCUT\": 400,\n                      \"ISMEAR\": 0, \"SIGMA\": 0.05, \"ISIF\": 3}\n        if not self.bulk:\n            slab_incar[\"ISIF\"] = 2\n            slab_incar[\"LVTOT\"] = True\n            if self.set_mix:\n                slab_incar[\"AMIN\"] = 0.01\n                slab_incar[\"AMIX\"] = 0.2\n                slab_incar[\"BMIX\"] = 0.001\n            slab_incar[\"NELMIN\"] = 8\n            if self.auto_dipole:\n                weights = [s.species.weight for s in structure]\n                center_of_mass = np.average(structure.frac_coords,\n                                            weights=weights, axis=0)\n\n                slab_incar[\"IDIPOL\"] = 3\n                slab_incar[\"LDIPOL\"] = True\n                slab_incar[\"DIPOL\"] = center_of_mass\n\n        self._config_dict[\"INCAR\"].update(slab_incar)\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        k_product, default to 50, is kpoint number * length for a & b\n            directions, also for c direction in bulk calculations\n        Automatic mesh & Gamma is the default setting.\n        \"\"\"\n\n        # To get input sets, the input structure has to has the same number\n        # of required parameters as a Structure object (ie. 4). Slab\n        # attributes aren't going to affect the VASP inputs anyways so\n        # converting the slab into a structure should not matter\n\n        kpt = super().kpoints\n        kpt.comment = \"Automatic mesh\"\n        kpt.style = 'Gamma'\n\n        # use k_product to calculate kpoints, k_product = kpts[0][0] * a\n        lattice_abc = self.structure.lattice.abc\n        kpt_calc = [int(self.k_product / lattice_abc[0] + 0.5),\n                    int(self.k_product / lattice_abc[1] + 0.5), 1]\n\n        self.kpt_calc = kpt_calc\n        # calculate kpts (c direction) for bulk. (for slab, set to 1)\n        if self.bulk:\n            kpt_calc[2] = int(self.k_product / lattice_abc[2] + 0.5)\n\n        kpt.kpts[0] = kpt_calc\n\n        return kpt\n\n    def as_dict(self, verbosity=2):\n        \"\"\"\n        :param verbosity: Verbosity of dict. E.g., whether to include Structure.\n        :return: MSONAble dict\n        \"\"\"\n        d = MSONable.as_dict(self)\n        if verbosity == 1:\n            d.pop(\"structure\", None)\n        return d\n\n\nclass MVLGBSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a vasp input files for grain boundary calculations, slab\n    or bulk.\n    \"\"\"\n\n    def __init__(self, structure, k_product=40, slab_mode=False, is_metal=True,\n                 **kwargs):\n        r\"\"\"\n\n        Args:\n            structure(Structure): provide the structure\n            k_product: Kpoint number * length for a & b directions, also for c\n                direction in bulk calculations. Default to 40.\n            slab_mode (bool): Defaults to False. Use default (False) for a\n                bulk supercell. Use True if you are performing calculations on a\n                slab-like (i.e., surface) of the GB, for example, when you are\n                calculating the work of separation.\n            is_metal (bool): Defaults to True. This determines whether an ISMEAR of\n                1 is used (for metals) or not (for insulators and semiconductors)\n                by default. Note that it does *not* override user_incar_settings,\n                which can be set by the user to be anything desired.\n            **kwargs:\n                Other kwargs supported by :class:`MPRelaxSet`.\n        \"\"\"\n        super().__init__(structure, **kwargs)\n        self.k_product = k_product\n        self.slab_mode = slab_mode\n        self.is_metal = is_metal\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        k_product, default to 40, is kpoint number * length for a & b\n        directions, also for c direction in bulk calculations\n        Automatic mesh & Gamma is the default setting.\n        \"\"\"\n\n        # To get input sets, the input structure has to has the same number\n        # of required parameters as a Structure object.\n\n        kpt = super().kpoints\n        kpt.comment = \"Generated by pymatgen's MVLGBSet\"\n        kpt.style = 'Gamma'\n\n        # use k_product to calculate kpoints, k_product = kpts[0][0] * a\n        lengths = self.structure.lattice.abc\n        kpt_calc = [int(self.k_product / lengths[0] + 0.5),\n                    int(self.k_product / lengths[1] + 0.5),\n                    int(self.k_product / lengths[2] + 0.5)]\n\n        if self.slab_mode:\n            kpt_calc[2] = 1\n\n        kpt.kpts[0] = kpt_calc\n\n        return kpt\n\n    @property\n    def incar(self):\n        \"\"\"\n        :return: Incar\n        \"\"\"\n        incar = super().incar\n\n        # The default incar setting is used for metallic system, for\n        # insulator or semiconductor, ISMEAR need to be changed.\n        incar.update({\n            \"LCHARG\": False,\n            \"NELM\": 60,\n            \"PREC\": \"Normal\",\n            \"EDIFFG\": -0.02,\n            \"ICHARG\": 0,\n            \"NSW\": 200,\n            \"EDIFF\": 0.0001\n        })\n\n        if self.is_metal:\n            incar[\"ISMEAR\"] = 1\n            incar[\"LDAU\"] = False\n\n        if self.slab_mode:\n            # for clean grain boundary and bulk relaxation, full optimization\n            # relaxation (ISIF=3) is used. For slab relaxation (ISIF=2) is used.\n            incar[\"ISIF\"] = 2\n            incar[\"NELMIN\"] = 8\n\n        incar.update(self.user_incar_settings)\n\n        return incar\n\n\nclass MVLRelax52Set(DictSet):\n    \"\"\"\n    Implementation of VaspInputSet utilizing the public Materials Project\n    parameters for INCAR & KPOINTS and VASP's recommended PAW potentials for\n    POTCAR.\n\n    Keynotes from VASP manual:\n        1. Recommended potentials for calculations using vasp.5.2+\n        2. If dimers with short bonds are present in the compound (O2, CO,\n            N2, F2, P2, S2, Cl2), it is recommended to use the h potentials.\n            Specifically, C_h, O_h, N_h, F_h, P_h, S_h, Cl_h\n        3. Released on Oct 28, 2018 by VASP. Please refer to VASP\n            Manual 1.2, 1.3 & 10.2.1 for more details.\n    \"\"\"\n    CONFIG = _load_yaml_config(\"MVLRelax52Set\")\n\n    def __init__(self, structure, potcar_functional=\"PBE_52\", **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): input structure.\n            potcar_functional (str): choose from \"PBE_52\" and \"PBE_54\".\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        if potcar_functional not in [\"PBE_52\", \"PBE_54\"]:\n            raise ValueError(\"Please select from PBE_52 and PBE_54!\")\n\n        super().__init__(structure, MVLRelax52Set.CONFIG,\n                         potcar_functional=potcar_functional, **kwargs)\n        self.kwargs = kwargs\n\n\nclass MITNEBSet(MITRelaxSet):\n    \"\"\"\n    Class for writing NEB inputs. Note that EDIFF is not on a per atom\n    basis for this input set.\n    \"\"\"\n\n    def __init__(self, structures, unset_encut=False, **kwargs):\n        \"\"\"\n        Args:\n            structures: List of Structure objects.\n            unset_encut (bool): Whether to unset ENCUT.\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        if len(structures) < 3:\n            raise ValueError(\"You need at least 3 structures for an NEB.\")\n        kwargs[\"sort_structure\"] = False\n        super().__init__(structures[0], **kwargs)\n        self.structures = self._process_structures(structures)\n        self.unset_encut = False\n        if unset_encut:\n            self._config_dict[\"INCAR\"].pop(\"ENCUT\", None)\n\n        if \"EDIFF\" not in self._config_dict[\"INCAR\"]:\n            self._config_dict[\"INCAR\"][\"EDIFF\"] = self._config_dict[\n                \"INCAR\"].pop(\"EDIFF_PER_ATOM\")\n\n        # NEB specific defaults\n        defaults = {'IMAGES': len(structures) - 2, 'IBRION': 1, 'ISYM': 0,\n                    'LCHARG': False, \"LDAU\": False}\n        self._config_dict[\"INCAR\"].update(defaults)\n\n    @property\n    def poscar(self):\n        \"\"\"\n        :return: Poscar for structure of first end point.\n        \"\"\"\n        return Poscar(self.structures[0])\n\n    @property\n    def poscars(self):\n        \"\"\"\n        :return: List of Poscars.\n        \"\"\"\n        return [Poscar(s) for s in self.structures]\n\n    @staticmethod\n    def _process_structures(structures):\n        \"\"\"\n        Remove any atom jumps across the cell\n        \"\"\"\n        input_structures = structures\n        structures = [input_structures[0]]\n        for s in input_structures[1:]:\n            prev = structures[-1]\n            for i in range(len(s)):\n                t = np.round(prev[i].frac_coords - s[i].frac_coords)\n                if np.any(np.abs(t) > 0.5):\n                    s.translate_sites([i], t, to_unit_cell=False)\n            structures.append(s)\n        return structures\n\n    def write_input(self, output_dir, make_dir_if_not_present=True,\n                    write_cif=False, write_path_cif=False,\n                    write_endpoint_inputs=False):\n        \"\"\"\n        NEB inputs has a special directory structure where inputs are in 00,\n        01, 02, ....\n\n        Args:\n            output_dir (str): Directory to output the VASP input files\n            make_dir_if_not_present (bool): Set to True if you want the\n                directory (and the whole path) to be created if it is not\n                present.\n            write_cif (bool): If true, writes a cif along with each POSCAR.\n            write_path_cif (bool): If true, writes a cif for each image.\n            write_endpoint_inputs (bool): If true, writes input files for\n                running endpoint calculations.\n        \"\"\"\n        output_dir = Path(output_dir)\n        if make_dir_if_not_present and not output_dir.exists():\n            output_dir.mkdir(parents=True)\n        self.incar.write_file(str(output_dir / 'INCAR'))\n        self.kpoints.write_file(str(output_dir / 'KPOINTS'))\n        self.potcar.write_file(str(output_dir / 'POTCAR'))\n\n        for i, p in enumerate(self.poscars):\n            d = output_dir / str(i).zfill(2)\n            if not d.exists():\n                d.mkdir(parents=True)\n            p.write_file(str(d / 'POSCAR'))\n            if write_cif:\n                p.structure.to(filename=str(d / '{}.cif'.format(i)))\n        if write_endpoint_inputs:\n            end_point_param = MITRelaxSet(\n                self.structures[0],\n                user_incar_settings=self.user_incar_settings)\n\n            for image in ['00', str(len(self.structures) - 1).zfill(2)]:\n                end_point_param.incar.write_file(\n                    str(output_dir / image / 'INCAR'))\n                end_point_param.kpoints.write_file(\n                    str(output_dir / image / 'KPOINTS'))\n                end_point_param.potcar.write_file(\n                    str(output_dir / image / 'POTCAR'))\n        if write_path_cif:\n            sites = set()\n            lat = self.structures[0].lattice\n            for site in chain(*(s.sites for s in self.structures)):\n                sites.add(\n                    PeriodicSite(site.species, site.frac_coords, lat))\n            nebpath = Structure.from_sites(sorted(sites))\n            nebpath.to(filename=str(output_dir / 'path.cif'))\n\n\nclass MITMDSet(MITRelaxSet):\n    \"\"\"\n    Class for writing a vasp md run. This DOES NOT do multiple stage\n    runs.\n    \"\"\"\n\n    def __init__(self, structure, start_temp, end_temp, nsteps, time_step=2,\n                 spin_polarized=False, **kwargs):\n        r\"\"\"\n\n        Args:\n            structure (Structure): Input structure.\n            start_temp (int): Starting temperature.\n            end_temp (int): Final temperature.\n            nsteps (int): Number of time steps for simulations. NSW parameter.\n            time_step (int): The time step for the simulation. The POTIM\n                parameter. Defaults to 2fs.\n            spin_polarized (bool): Whether to do spin polarized calculations.\n                The ISPIN parameter. Defaults to False.\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        # MD default settings\n        defaults = {'TEBEG': start_temp, 'TEEND': end_temp, 'NSW': nsteps,\n                    'EDIFF_PER_ATOM': 0.000001, 'LSCALU': False,\n                    'LCHARG': False,\n                    'LPLANE': False, 'LWAVE': True, 'ISMEAR': 0,\n                    'NELMIN': 4, 'LREAL': True, 'BMIX': 1,\n                    'MAXMIX': 20, 'NELM': 500, 'NSIM': 4, 'ISYM': 0,\n                    'ISIF': 0, 'IBRION': 0, 'NBLOCK': 1, 'KBLOCK': 100,\n                    'SMASS': 0, 'POTIM': time_step, 'PREC': 'Low',\n                    'ISPIN': 2 if spin_polarized else 1,\n                    \"LDAU\": False}\n\n        super().__init__(structure, **kwargs)\n\n        self.start_temp = start_temp\n        self.end_temp = end_temp\n        self.nsteps = nsteps\n        self.time_step = time_step\n        self.spin_polarized = spin_polarized\n        self.kwargs = kwargs\n\n        # use VASP default ENCUT\n        self._config_dict[\"INCAR\"].pop('ENCUT', None)\n\n        if defaults['ISPIN'] == 1:\n            self._config_dict[\"INCAR\"].pop('MAGMOM', None)\n        self._config_dict[\"INCAR\"].update(defaults)\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        :return: Kpoints\n        \"\"\"\n        return Kpoints.gamma_automatic()\n\n\nclass MPMDSet(MPRelaxSet):\n    \"\"\"\n    This a modified version of the old MITMDSet pre 2018/03/12.\n\n    This set serves as the basis for the amorphous skyline paper.\n\n    (1) Aykol, M.; Dwaraknath, S. S.; Sun, W.; Persson, K. A. Thermodynamic\n        Limit for Synthesis of Metastable Inorganic Materials. Sci. Adv. 2018,\n        4 (4).\n\n    Class for writing a vasp md run. This DOES NOT do multiple stage runs.\n    Precision remains normal, to increase accuracy of stress tensor.\n    \"\"\"\n\n    def __init__(self, structure, start_temp, end_temp, nsteps,\n                 spin_polarized=False, **kwargs):\n        r\"\"\"\n        Args:\n            structure (Structure): Input structure.\n            start_temp (int): Starting temperature.\n            end_temp (int): Final temperature.\n            nsteps (int): Number of time steps for simulations. NSW parameter.\n            time_step (int): The time step for the simulation. The POTIM\n                parameter. Defaults to 2fs.\n            spin_polarized (bool): Whether to do spin polarized calculations.\n                The ISPIN parameter. Defaults to False.\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n\n        # MD default settings\n        defaults = {'TEBEG': start_temp, 'TEEND': end_temp, 'NSW': nsteps,\n                    'EDIFF_PER_ATOM': 0.00001, 'LSCALU': False,\n                    'LCHARG': False,\n                    'LPLANE': False, 'LWAVE': True, 'ISMEAR': 0,\n                    'NELMIN': 4, 'LREAL': True, 'BMIX': 1,\n                    'MAXMIX': 20, 'NELM': 500, 'NSIM': 4, 'ISYM': 0,\n                    'ISIF': 0, 'IBRION': 0, 'NBLOCK': 1, 'KBLOCK': 100,\n                    'SMASS': 0, 'POTIM': 2, 'PREC': 'Normal',\n                    'ISPIN': 2 if spin_polarized else 1,\n                    \"LDAU\": False, 'ADDGRID': True}\n\n        if Element('H') in structure.species:\n            defaults['POTIM'] = 0.5\n            defaults['NSW'] = defaults['NSW'] * 4\n\n        super().__init__(structure, **kwargs)\n\n        self.start_temp = start_temp\n        self.end_temp = end_temp\n        self.nsteps = nsteps\n        self.spin_polarized = spin_polarized\n        self.kwargs = kwargs\n\n        # use VASP default ENCUT\n        self._config_dict[\"INCAR\"].pop('ENCUT', None)\n\n        if defaults['ISPIN'] == 1:\n            self._config_dict[\"INCAR\"].pop('MAGMOM', None)\n        self._config_dict[\"INCAR\"].update(defaults)\n\n    @property\n    def kpoints(self):\n        \"\"\"\n        :return: Kpoints\n        \"\"\"\n        return Kpoints.gamma_automatic()\n\n\nclass MVLNPTMDSet(MITMDSet):\n    \"\"\"\n    Class for writing a vasp md run in NPT ensemble.\n\n    Notes:\n        To eliminate Pulay stress, the default ENCUT is set to a rather large\n        value of ENCUT, which is 1.5 * ENMAX.\n    \"\"\"\n\n    def __init__(self, structure, start_temp, end_temp, nsteps, time_step=2,\n                 spin_polarized=False, **kwargs):\n        r\"\"\"\n        Args:\n            structure (Structure): input structure.\n            start_temp (int): Starting temperature.\n            end_temp (int): Final temperature.\n            nsteps(int): Number of time steps for simulations. NSW parameter.\n            time_step (int): The time step for the simulation. The POTIM\n                parameter. Defaults to 2fs.\n            spin_polarized (bool): Whether to do spin polarized calculations.\n                The ISPIN parameter. Defaults to False.\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        user_incar_settings = kwargs.get(\"user_incar_settings\", {})\n\n        # NPT-AIMD default settings\n        defaults = {\"IALGO\": 48,\n                    \"ISIF\": 3,\n                    \"LANGEVIN_GAMMA\": [10] * structure.ntypesp,\n                    \"LANGEVIN_GAMMA_L\": 1,\n                    \"MDALGO\": 3,\n                    \"PMASS\": 10,\n                    \"PSTRESS\": 0,\n                    \"SMASS\": 0}\n\n        defaults.update(user_incar_settings)\n        kwargs[\"user_incar_settings\"] = defaults\n\n        super().__init__(structure, start_temp, end_temp,\n                         nsteps, time_step, spin_polarized, **kwargs)\n\n        # Set NPT-AIMD ENCUT = 1.5 * VASP_default\n        enmax = [self.potcar[i].keywords['ENMAX']\n                 for i in range(structure.ntypesp)]\n        encut = max(enmax) * 1.5\n        self._config_dict[\"INCAR\"][\"ENCUT\"] = encut\n\n\nclass MVLScanRelaxSet(MPRelaxSet):\n    \"\"\"\n    Class for writing a relax input set using Strongly Constrained and\n    Appropriately Normed (SCAN) semilocal density functional.\n\n    Notes:\n        1. This functional is only available from VASP.5.4.3 upwards.\n\n        2. Meta-GGA calculations require POTCAR files that include\n        information on the kinetic energy density of the core-electrons,\n        i.e. \"PBE_52\" or \"PBE_54\". Make sure the POTCAR including the\n        following lines (see VASP wiki for more details):\n\n            $ grep kinetic POTCAR\n            kinetic energy-density\n            mkinetic energy-density pseudized\n            kinetic energy density (partial)\n    \"\"\"\n\n    def __init__(self, structure, potcar_functional=\"PBE_52\", **kwargs):\n        r\"\"\"\n        Args:\n            structure (Structure): input structure.\n            potcar_functional (str): choose from \"PBE_52\" and \"PBE_54\".\n            vdw (str): set \"rVV10\" to enable SCAN+rVV10, which is a versatile\n                van der Waals density functional by combing the SCAN functional\n                with the rVV10 non-local correlation functional.\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        if potcar_functional not in [\"PBE_52\", \"PBE_54\"]:\n            raise ValueError(\"SCAN calculations required PBE_52 or PBE_54!\")\n\n        super().__init__(structure, potcar_functional=potcar_functional,\n                         **kwargs)\n\n        self._config_dict[\"INCAR\"].update({\"ADDGRID\": True,\n                                           \"EDIFF\": 1e-05,\n                                           \"EDIFFG\": -0.05,\n                                           \"LASPH\": True,\n                                           \"LDAU\": False,\n                                           \"METAGGA\": \"SCAN\",\n                                           \"NELM\": 200})\n\n\nclass LobsterSet(MPRelaxSet):\n    \"\"\"\n    Input set to prepare VASP runs that can be digested by Lobster (See cohp.de)\n    \"\"\"\n\n    CONFIG = _load_yaml_config(\"MPRelaxSet\")\n\n    def __init__(self, structure: Structure, isym: int = -1, ismear: int = -5, reciprocal_density: int = None,\n                 potcar_functional: str = \"PBE_54\", address_basis_file: str = None, user_supplied_basis: dict = None,\n                 **kwargs):\n        \"\"\"\n        Args:\n            structure (Structure): input structure.\n            isym (int): ISYM entry for INCAR, only isym=-1 and isym=0 are allowed\n            ismear (int): ISMEAR entry for INCAR, only ismear=-5 and ismear=0 are allowed\n            reciprocal_density (int): density of k-mesh by reciprocal volume\n            potcar_functional (string): only PBE_54, PBE_52 and PBE are recommended at the moment\n            user_supplied_basis (dict): dict including basis functions for all elements in structure,\n                e.g. {\"Fe\": \"3d 3p 4s\", \"O\": \"2s 2p\"}; if not supplied, a standard basis is used\n            address_basis_file (str): address to a file similar to \"BASIS_PBE_54.yaml\" in pymatgen.io\n            **kwargs: Other kwargs supported by :class:`DictSet`.\n        \"\"\"\n        warnings.warn(\"Make sure that all parameters are okay! This is a brand new implementation.\")\n\n        if not (isym == -1 or isym == 0):\n            raise ValueError(\"Lobster cannot digest WAVEFUNCTIONS with symmetry\")\n        if not (ismear == -5 or ismear == 0):\n            raise ValueError(\"Lobster usually works with ismear=-5 or ismear=0\")\n\n        # newest potcars are preferred\n        super().__init__(structure, potcar_functional=potcar_functional, **kwargs)\n\n        # reciprocal density\n        if self.user_kpoints_settings is not None:\n            if (not reciprocal_density or \"reciprocal_density\" not in self.user_kpoints_settings):\n                # test, if this is okay\n                self.reciprocal_density = 310\n            else:\n                self.reciprocal_density = reciprocal_density or \\\n                                          self.user_kpoints_settings['reciprocal_density']\n        else:\n            if (not reciprocal_density):\n                # test, if this is okay\n                self.reciprocal_density = 310\n            else:\n                self.reciprocal_density = reciprocal_density\n\n        # might need to be adapted in the future\n        ediff_per_atom = 5e-05\n\n        self.isym = isym\n        self.ismear = ismear\n        self.user_supplied_basis = user_supplied_basis\n        self.address_basis_file = address_basis_file\n        # predefined basis! Check if the basis is okay! (charge spilling and bandoverlaps!)\n        if user_supplied_basis is None and address_basis_file is None:\n            basis = Lobsterin._get_basis(structure=structure,\n                                         potcar_symbols=self.potcar_symbols)\n        elif address_basis_file is not None:\n            basis = Lobsterin._get_basis(structure=structure,\n                                         potcar_symbols=self.potcar_symbols, address_basis_file=address_basis_file)\n        elif user_supplied_basis is not None:\n            # test if all elements from structure are in user_supplied_basis\n            for atomtype in structure.symbol_set:\n                if atomtype not in user_supplied_basis:\n                    raise ValueError(\"There are no basis functions for the atom type \" + str(atomtype))\n            basis = [key + ' ' + value for key, value in user_supplied_basis.items()]\n\n        lobsterin = Lobsterin(settingsdict={\"basisfunctions\": basis})\n        nbands = lobsterin._get_nbands(structure=structure)\n\n        update_dict = {\"EDIFF_PER_ATOM\": ediff_per_atom, \"NSW\": 0, \"LWAVE\": True, \"ISYM\": isym, \"NBANDS\": nbands,\n                       \"IBRION\": -1, \"ISMEAR\": ismear, \"LORBIT\": 11, \"ICHARG\": 0, \"ALGO\": \"Normal\"}\n\n        self._config_dict[\"INCAR\"].update(update_dict)\n        self._config_dict[\"KPOINTS\"].update({\"reciprocal_density\": self.reciprocal_density})\n\n\ndef get_vasprun_outcar(path, parse_dos=True, parse_eigen=True):\n    \"\"\"\n    :param path: Path to get the vasprun.xml and OUTCAR.\n    :param parse_dos: Whether to parse dos. Defaults to True.\n    :param parse_eigen: Whether to parse eigenvalue. Defaults to True.\n    :return:\n    \"\"\"\n    path = Path(path)\n    vruns = list(glob.glob(str(path / \"vasprun.xml*\")))\n    outcars = list(glob.glob(str(path / \"OUTCAR*\")))\n\n    if len(vruns) == 0 or len(outcars) == 0:\n        raise ValueError(\n            \"Unable to get vasprun.xml/OUTCAR from prev calculation in %s\" %\n            path)\n    vsfile_fullpath = str(path / \"vasprun.xml\")\n    outcarfile_fullpath = str(path / \"OUTCAR\")\n    vsfile = vsfile_fullpath if vsfile_fullpath in vruns else sorted(vruns)[-1]\n    outcarfile = outcarfile_fullpath if outcarfile_fullpath in outcars else \\\n        sorted(outcars)[-1]\n    return (Vasprun(vsfile, parse_dos=parse_dos, parse_eigen=parse_eigen),\n            Outcar(outcarfile))\n\n\ndef get_structure_from_prev_run(vasprun, outcar=None):\n    \"\"\"\n    Process structure from previous run.\n\n    Args:\n        vasprun (Vasprun): Vasprun that contains the final structure\n            from previous run.\n        outcar (Outcar): Outcar that contains the magnetization info from\n            previous run.\n\n    Returns:\n        Returns the magmom-decorated structure that can be passed to get\n        Vasp input files, e.g. get_kpoints.\n    \"\"\"\n    structure = vasprun.final_structure\n\n    site_properties = {}\n    # magmom\n    if vasprun.is_spin:\n        if outcar and outcar.magnetization:\n            site_properties.update({\"magmom\": [i['tot']\n                                               for i in outcar.magnetization]})\n        else:\n            site_properties.update({\"magmom\": vasprun.parameters['MAGMOM']})\n    # ldau\n    if vasprun.parameters.get(\"LDAU\", False):\n        for k in (\"LDAUU\", \"LDAUJ\", \"LDAUL\"):\n            vals = vasprun.incar[k]\n            m = {}\n            l_val = []\n            s = 0\n            for site in structure:\n                if site.specie.symbol not in m:\n                    m[site.specie.symbol] = vals[s]\n                    s += 1\n                l_val.append(m[site.specie.symbol])\n            if len(l_val) == len(structure):\n                site_properties.update({k.lower(): l_val})\n            else:\n                raise ValueError(\"length of list {} not the same as\"\n                                 \"structure\".format(l_val))\n\n    return structure.copy(site_properties=site_properties)\n\n\ndef standardize_structure(structure, sym_prec=0.1,\n                          international_monoclinic=True):\n    \"\"\"\n    Get the symmetrically standardized structure.\n\n    Args:\n        structure (Structure): The structure.\n        sym_prec (float): Tolerance for symmetry finding for standardization.\n        international_monoclinic (bool): Whether to use international\n            convention (vs Curtarolo) for monoclinic. Defaults True.\n\n    Returns:\n        The symmetrized structure.\n    \"\"\"\n    sym_finder = SpacegroupAnalyzer(structure, symprec=sym_prec)\n    new_structure = sym_finder.get_primitive_standard_structure(\n        international_monoclinic=international_monoclinic)\n\n    # the primitive structure finding has had several bugs in the past\n    # defend through validation\n    vpa_old = structure.volume / structure.num_sites\n    vpa_new = new_structure.volume / new_structure.num_sites\n\n    if abs(vpa_old - vpa_new) / vpa_old > 0.02:\n        raise ValueError(\n            \"Standardizing cell failed! VPA old: {}, VPA new: {}\".format(\n                vpa_old, vpa_new))\n\n    sm = StructureMatcher()\n    if not sm.fit(structure, new_structure):\n        raise ValueError(\n            \"Standardizing cell failed! Old structure doesn't match new.\")\n\n    return new_structure\n\n\nclass BadInputSetWarning(UserWarning):\n    \"\"\"\n    Warning class for bad but legal inputs.\n    \"\"\"\n    pass\n\n\ndef batch_write_input(structures, vasp_input_set=MPRelaxSet, output_dir=\".\",\n                      make_dir_if_not_present=True, subfolder=None,\n                      sanitize=False, include_cif=False, **kwargs):\n    \"\"\"\n    Batch write vasp input for a sequence of structures to\n    output_dir, following the format output_dir/{group}/{formula}_{number}.\n\n    Args:\n        structures ([Structure]): Sequence of Structures.\n        vasp_input_set (VaspInputSet): VaspInputSet class that creates\n            vasp input files from structures. Note that a class should be\n            supplied. Defaults to MPRelaxSet.\n        output_dir (str): Directory to output files. Defaults to current\n            directory \".\".\n        make_dir_if_not_present (bool): Create the directory if not present.\n            Defaults to True.\n        subfolder (callable): Function to create subdirectory name from\n            structure. Defaults to simply \"formula_count\".\n        sanitize (bool): Boolean indicating whether to sanitize the\n            structure before writing the VASP input files. Sanitized output\n            are generally easier for viewing and certain forms of analysis.\n            Defaults to False.\n        include_cif (bool): Whether to output a CIF as well. CIF files are\n            generally better supported in visualization programs.\n        **kwargs: Additional kwargs are passed to the vasp_input_set class\n            in addition to structure.\n    \"\"\"\n    output_dir = Path(output_dir)\n    for i, s in enumerate(structures):\n        formula = re.sub(r'\\s+', \"\", s.formula)\n        if subfolder is not None:\n            subdir = subfolder(s)\n            d = output_dir / subdir\n        else:\n            d = output_dir / '{}_{}'.format(formula, i)\n        if sanitize:\n            s = s.copy(sanitize=True)\n        v = vasp_input_set(s, **kwargs)\n        v.write_input(str(d), make_dir_if_not_present=make_dir_if_not_present,\n                      include_cif=include_cif)\n\n\n_dummy_structure = Structure([1, 0, 0, 0, 1, 0, 0, 0, 1], ['I'], [[0, 0, 0]],\n                             site_properties={\"magmom\": [[0, 0, 1]]})\n", "idx": 4, "id": 18441, "msg": "", "proj": "materialsproject-pymatgen", "lang": "py"}
{"patch": "@@ -173,19 +173,32 @@ def _get_timestamp(statuses=('SUCCESS', 'PARTIAL_SUCCESS')):\n \n     return latest_timestamp\n \n-def _flatten_violations(violations):\n+def _flatten_violations(violations, flattening_scheme):\n     \"\"\"Flatten RuleViolations into a dict for each RuleViolation member.\n \n     Args:\n         violations: The RuleViolations to flatten.\n+        flattening_scheme: Which flattening scheme to use\n \n     Yield:\n         Iterator of RuleViolations as a dict per member.\n     \"\"\"\n \n+    # TODO: Make this nicer\n     LOGGER.info('Writing violations to csv...')\n     for violation in violations:\n-        for member in violation.members:\n+        if flattening_scheme == 'policy_violations':\n+            for member in violation.members:\n+                yield {\n+                    'resource_id': violation.resource_id,\n+                    'resource_type': violation.resource_type,\n+                    'rule_index': violation.rule_index,\n+                    'rule_name': violation.rule_name,\n+                    'violation_type': violation.violation_type,\n+                    'role': violation.role,\n+                    'member': '{}:{}'.format(member.type, member.name)\n+                }\n+        if flattening_scheme == 'buckets_acl_violations':\n             yield {\n                 'resource_id': violation.resource_id,\n                 'resource_type': violation.resource_type,", "y": 0, "oldf": "# Copyright 2017 Google Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#    http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"GCP Resource scanner.\n\n\nUsage:\n\n  $ forseti_scanner --rules <rules path> \\\\\n      --output_path <output path (optional)> \\\\\n      --db_host <Cloud SQL database hostname/IP> \\\\\n      --db_user <Cloud SQL database user> \\\\\n      --db_name <Cloud SQL database name (required)> \\\\\n      --sendgrid_api_key <API key to auth SendGrid email service> \\\\\n      --email_sender <email address of the email sender> \\\\\n      --email_recipient <email address of the email recipient>\n\"\"\"\n\nimport itertools\nimport os\nimport shutil\nimport sys\n\nfrom datetime import datetime\n\nimport gflags as flags\n\nfrom google.apputils import app\nfrom google.cloud.security.common.data_access import csv_writer\nfrom google.cloud.security.common.data_access import dao\nfrom google.cloud.security.common.data_access import violation_dao\nfrom google.cloud.security.common.data_access import errors as db_errors\nfrom google.cloud.security.common.gcp_type.resource_util import ResourceUtil\nfrom google.cloud.security.common.util import log_util\nfrom google.cloud.security.common.util.email_util import EmailUtil\nfrom google.cloud.security.scanner.audit import engine_map as em\nfrom google.cloud.security.scanner.scanners import scanners_map as sm\n\n\n# Setup flags\nFLAGS = flags.FLAGS\n\n# Format: flags.DEFINE_<type>(flag_name, default_value, help_text)\n# Example:\n# https://github.com/google/python-gflags/blob/master/examples/validator.py\nflags.DEFINE_string('rules', None,\n                    ('Path to rules file (yaml/json). '\n                     'If GCS object, include full path, e.g. '\n                     ' \"gs://<bucketname>/path/to/file\".'))\n\nflags.DEFINE_string('output_path', None,\n                    ('Output path (do not include filename). If GCS location, '\n                     'the format of the path should be '\n                     '\"gs://bucket-name/path/for/output\".'))\n\nflags.DEFINE_boolean('list_engines', False, 'List all rule engines')\n\nflags.DEFINE_string('engine_name', None, 'Which engine to use')\n\nflags.DEFINE_string('use_scanner_basedir', None, 'Which rule basedir to use')\n\nLOGGER = log_util.get_logger(__name__)\nSCANNER_OUTPUT_CSV_FMT = 'scanner_output.{}.csv'\nOUTPUT_TIMESTAMP_FMT = '%Y%m%dT%H%M%SZ'\n\n\ndef main(_):\n    \"\"\"Run the scanner.\"\"\"\n\n    if FLAGS.list_engines is True:\n        _list_rules_engines()\n        sys.exit(1)\n\n    if not FLAGS.engine_name:\n        LOGGER.warn('Provide an engine name')\n        sys.exit(1)\n    else:\n        rules_engine_name = FLAGS.engine_name\n\n    LOGGER.info('Using rules engine: %s', rules_engine_name)\n\n    LOGGER.info('Initializing the rules engine:\\nUsing rules: %s', FLAGS.rules)\n\n    if not FLAGS.rules:\n        LOGGER.warn(('Provide a rules file. '\n                     'Use \"forseti_scanner --helpfull\" for help.'))\n        sys.exit(1)\n\n    snapshot_timestamp = _get_timestamp()\n    if not snapshot_timestamp:\n        LOGGER.warn('No snapshot timestamp found. Exiting.')\n        sys.exit()\n\n    # Load scanner from map\n    scanner = sm.SCANNER_MAP[rules_engine_name](snapshot_timestamp)\n\n    # TODO: Make the groups scanner run consistently with other scanners\n    # instead of it's own execution path.\n    if rules_engine_name == 'GroupsEngine':\n        all_violations = scanner.run(FLAGS.rules)\n        LOGGER.info('Found %s violation(s) in Groups.', len(all_violations))\n        sys.exit(1)\n\n    # Instantiate rules engine with supplied rules file\n    rules_engine = em.ENGINE_TO_DATA_MAP[rules_engine_name](\n        rules_file_path=FLAGS.rules)\n    rules_engine.build_rule_book()\n\n    iter_objects, resource_counts = scanner.run()\n\n    # Load violations processing function\n    all_violations = scanner.find_violations(\n        itertools.chain(\n            *iter_objects),\n        rules_engine)\n\n    # If there are violations, send results.\n    if all_violations:\n        _output_results(all_violations,\n                        snapshot_timestamp,\n                        resource_counts=resource_counts)\n\n    LOGGER.info('Done!')\n\ndef _list_rules_engines():\n    \"\"\"List rules engines.\n\n    Args:\n        audit_base_dir: base directory for rules engines\n\n    Returns:\n        None\n    \"\"\"\n    for engine in em.ENGINE_TO_DATA_MAP:\n        print engine\n\ndef _get_output_filename(now_utc):\n    \"\"\"Create the output filename.\n\n    Args:\n        now_utc: The datetime now in UTC.\n\n    Returns:\n        The output filename for the csv, formatted with the now_utc timestamp.\n    \"\"\"\n\n    output_timestamp = now_utc.strftime(OUTPUT_TIMESTAMP_FMT)\n    output_filename = SCANNER_OUTPUT_CSV_FMT.format(output_timestamp)\n    return output_filename\n\ndef _get_timestamp(statuses=('SUCCESS', 'PARTIAL_SUCCESS')):\n    \"\"\"Get latest snapshot timestamp.\n\n    Returns:\n        The latest snapshot timestamp string.\n    \"\"\"\n\n    latest_timestamp = None\n    try:\n        latest_timestamp = dao.Dao().get_latest_snapshot_timestamp(statuses)\n    except db_errors.MySQLError as err:\n        LOGGER.error('Error getting latest snapshot timestamp: %s', err)\n\n    return latest_timestamp\n\ndef _flatten_violations(violations):\n    \"\"\"Flatten RuleViolations into a dict for each RuleViolation member.\n\n    Args:\n        violations: The RuleViolations to flatten.\n\n    Yield:\n        Iterator of RuleViolations as a dict per member.\n    \"\"\"\n\n    LOGGER.info('Writing violations to csv...')\n    for violation in violations:\n        for member in violation.members:\n            yield {\n                'resource_id': violation.resource_id,\n                'resource_type': violation.resource_type,\n                'rule_index': violation.rule_index,\n                'rule_name': violation.rule_name,\n                'violation_type': violation.violation_type,\n                'role': violation.role,\n                'member': '{}:{}'.format(member.type, member.name)\n            }\n\ndef _output_results(all_violations, snapshot_timestamp, **kwargs):\n    \"\"\"Send the output results.\n\n    Args:\n        all_violations: The list of violations to report.\n        **kwargs: The rest of the args.\n    \"\"\"\n\n    # Write violations to database.\n    (inserted_row_count, violation_errors) = (0, [])\n    try:\n        vdao = violation_dao.ViolationDao()\n        (inserted_row_count, violation_errors) = vdao.insert_violations(\n            all_violations, snapshot_timestamp=snapshot_timestamp)\n    except db_errors.MySQLError as err:\n        LOGGER.error('Error importing violations to database: %s', err)\n\n    # TODO: figure out what to do with the errors. For now, just log it.\n    LOGGER.debug('Inserted %s rows with %s errors',\n                 inserted_row_count, len(violation_errors))\n\n    output_csv_name = None\n\n    # Write the CSV for all the violations.\n    with csv_writer.write_csv(\n        resource_name='policy_violations',\n        data=_flatten_violations(all_violations),\n        write_header=True) as csv_file:\n        output_csv_name = csv_file.name\n        LOGGER.info('CSV filename: %s', output_csv_name)\n\n        # Scanner timestamp for output file and email.\n        now_utc = datetime.utcnow()\n\n        # If output_path specified, upload to GCS.\n        if FLAGS.output_path:\n            output_path = FLAGS.output_path\n            if not output_path.startswith('gs://'):\n                if not os.path.exists(FLAGS.output_path):\n                    os.makedirs(output_path)\n                output_path = os.path.abspath(output_path)\n            _upload_csv(output_path, now_utc, output_csv_name)\n\n        # Send summary email.\n        if FLAGS.email_recipient is not None:\n            resource_counts = kwargs.get('resource_counts', {})\n            _send_email(output_csv_name, now_utc,\n                        all_violations, resource_counts,\n                        violation_errors)\n\ndef _upload_csv(output_path, now_utc, csv_name):\n    \"\"\"Upload CSV to Cloud Storage.\n\n    Args:\n        output_path: The output path for the csv.\n        now_utc: The UTC timestamp of \"now\".\n        csv_name: The csv_name.\n    \"\"\"\n\n    from google.cloud.security.common.gcp_api import storage\n\n    output_filename = _get_output_filename(now_utc)\n\n    # If output path was specified, copy the csv temp file either to\n    # a local file or upload it to Google Cloud Storage.\n    full_output_path = os.path.join(output_path, output_filename)\n    LOGGER.info('Output path: %s', full_output_path)\n\n    if output_path.startswith('gs://'):\n        # An output path for GCS must be the full\n        # `gs://bucket-name/path/for/output`\n        storage_client = storage.StorageClient()\n        storage_client.put_text_file(\n            csv_name, full_output_path)\n    else:\n        # Otherwise, just copy it to the output path.\n        shutil.copy(csv_name, full_output_path)\n\ndef _send_email(csv_name, now_utc, all_violations,\n                total_resources, violation_errors):\n    \"\"\"Send a summary email of the scan.\n\n    Args:\n        csv_name: The full path of the csv.\n        now_utc: The UTC datetime right now.\n        all_violations: The list of violations.\n        total_resources: A dict of the resources and their count.\n        violation_errors: Iterable of violation errors.\n    \"\"\"\n\n    mail_util = EmailUtil(FLAGS.sendgrid_api_key)\n    total_violations, resource_summaries = _build_scan_summary(\n        all_violations, total_resources)\n\n    # Render the email template with values.\n    scan_date = now_utc.strftime('%Y %b %d, %H:%M:%S (UTC)')\n    email_content = EmailUtil.render_from_template(\n        'scanner_summary.jinja', {\n            'scan_date':  scan_date,\n            'resource_summaries': resource_summaries,\n            'violation_errors': violation_errors,\n        })\n\n    # Create an attachment out of the csv file and base64 encode the content.\n    attachment = EmailUtil.create_attachment(\n        file_location=csv_name,\n        content_type='text/csv',\n        filename=_get_output_filename(now_utc),\n        disposition='attachment',\n        content_id='Scanner Violations'\n    )\n    scanner_subject = 'Policy Scan Complete - {} violations found'.format(\n        total_violations)\n    mail_util.send(email_sender=FLAGS.email_sender,\n                   email_recipient=FLAGS.email_recipient,\n                   email_subject=scanner_subject,\n                   email_content=email_content,\n                   content_type='text/html',\n                   attachment=attachment)\n\ndef _build_scan_summary(all_violations, total_resources):\n    \"\"\"Build the scan summary.\n\n    Args:\n        all_violations: List of violations.\n        total_resources: A dict of the resources and their count.\n\n    Returns:\n        Total counts and summaries.\n    \"\"\"\n\n    resource_summaries = {}\n    total_violations = 0\n    # Build a summary of the violations and counts for the email.\n    # resource summary:\n    # {\n    #     RESOURCE_TYPE: {\n    #         'total': TOTAL,\n    #         'ids': [...] # resource_ids\n    #     },\n    #     ...\n    # }\n    for violation in all_violations:\n        resource_type = violation.resource_type\n        if resource_type not in resource_summaries:\n            resource_summaries[resource_type] = {\n                'pluralized_resource_type': ResourceUtil.pluralize(\n                    resource_type),\n                'total': total_resources[resource_type],\n                'violations': {}\n            }\n\n        # Keep track of # of violations per resource id.\n        if (violation.resource_id not in\n                resource_summaries[resource_type]['violations']):\n            resource_summaries[resource_type][\n                'violations'][violation.resource_id] = 0\n\n        resource_summaries[resource_type][\n            'violations'][violation.resource_id] += len(violation.members)\n        total_violations += len(violation.members)\n\n    return total_violations, resource_summaries\n\n\nif __name__ == '__main__':\n    app.run()\n", "idx": 8, "id": 25960, "msg": "", "proj": "forseti-security-forseti-security", "lang": "py"}
{"patch": "@@ -72,6 +72,24 @@ func (s *Solver) ensurePod(ctx context.Context, ch *v1alpha1.Challenge) (*corev1\n \treturn s.createPod(ch)\n }\n \n+// getFinalNamespace resolves the right namespace for Solver Pods for the\n+// given challenge.  By default, it will be the namespace set in the\n+// Challenge.  For ClusterIssuers, the it may overriden by the\n+// Namespace set in the Solver's PodSpec.\n+func getFinalNamespace(ch *v1alpha1.Challenge) string {\n+\tnamespace := ch.Namespace\n+\n+\tif ch.Spec.Solver != nil &&\n+\t\tch.Spec.Solver.HTTP01 != nil &&\n+\t\tch.Spec.Solver.HTTP01.Ingress != nil &&\n+\t\tch.Spec.Solver.HTTP01.Ingress.PodTemplate != nil &&\n+\t\t0 < len(ch.Spec.Solver.HTTP01.Ingress.PodTemplate.Namespace) {\n+\t\tnamespace = ch.Spec.Solver.HTTP01.Ingress.PodTemplate.Namespace\n+\t}\n+\n+\treturn namespace\n+}\n+\n // getPodsForChallenge returns a list of pods that were created to solve\n // the given challenge\n func (s *Solver) getPodsForChallenge(ctx context.Context, ch *v1alpha1.Challenge) ([]*corev1.Pod, error) {", "y": 1, "oldf": "/*\nCopyright 2019 The Jetstack cert-manager contributors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage http\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"hash/adler32\"\n\n\tcorev1 \"k8s.io/api/core/v1\"\n\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n\t\"k8s.io/apimachinery/pkg/labels\"\n\t\"k8s.io/apimachinery/pkg/selection\"\n\tutilerrors \"k8s.io/apimachinery/pkg/util/errors\"\n\n\t\"github.com/jetstack/cert-manager/pkg/apis/certmanager/v1alpha1\"\n\tlogf \"github.com/jetstack/cert-manager/pkg/logs\"\n)\n\nfunc podLabels(ch *v1alpha1.Challenge) map[string]string {\n\tdomainHash := fmt.Sprintf(\"%d\", adler32.Checksum([]byte(ch.Spec.DNSName)))\n\ttokenHash := fmt.Sprintf(\"%d\", adler32.Checksum([]byte(ch.Spec.Token)))\n\tsolverIdent := \"true\"\n\treturn map[string]string{\n\t\t// TODO: we need to support domains longer than 63 characters\n\t\t// this value should probably be hashed, and then the full plain text\n\t\t// value stored as an annotation to make it easier for users to read\n\t\t// see #425 for details: https://github.com/jetstack/cert-manager/issues/425\n\t\tdomainLabelKey:               domainHash,\n\t\ttokenLabelKey:                tokenHash,\n\t\tsolverIdentificationLabelKey: solverIdent,\n\t}\n}\n\nfunc (s *Solver) ensurePod(ctx context.Context, ch *v1alpha1.Challenge) (*corev1.Pod, error) {\n\tlog := logf.FromContext(ctx).WithName(\"ensurePod\")\n\n\tlog.V(logf.DebugLevel).Info(\"checking for existing HTTP01 solver pods\")\n\texistingPods, err := s.getPodsForChallenge(ctx, ch)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(existingPods) == 1 {\n\t\tlogf.WithRelatedResource(log, existingPods[0]).Info(\"found one existing HTTP01 solver pod\")\n\t\treturn existingPods[0], nil\n\t}\n\tif len(existingPods) > 1 {\n\t\tlog.Info(\"multiple challenge solver pods found for challenge. cleaning up all existing pods.\")\n\t\terr := s.cleanupPods(ctx, ch)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn nil, fmt.Errorf(\"multiple existing challenge solver pods found and cleaned up. retrying challenge sync\")\n\t}\n\n\tlog.Info(\"creating HTTP01 challenge solver pod\")\n\n\treturn s.createPod(ch)\n}\n\n// getPodsForChallenge returns a list of pods that were created to solve\n// the given challenge\nfunc (s *Solver) getPodsForChallenge(ctx context.Context, ch *v1alpha1.Challenge) ([]*corev1.Pod, error) {\n\tlog := logf.FromContext(ctx)\n\n\tpodLabels := podLabels(ch)\n\torderSelector := labels.NewSelector()\n\tfor key, val := range podLabels {\n\t\treq, err := labels.NewRequirement(key, selection.Equals, []string{val})\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\torderSelector = orderSelector.Add(*req)\n\t}\n\n\tpodList, err := s.podLister.Pods(ch.Namespace).List(orderSelector)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar relevantPods []*corev1.Pod\n\tfor _, pod := range podList {\n\t\tif !metav1.IsControlledBy(pod, ch) {\n\t\t\tlogf.WithRelatedResource(log, pod).Info(\"found existing solver pod for this challenge resource, however \" +\n\t\t\t\t\"it does not have an appropriate OwnerReference referencing this challenge. Skipping it altogether.\")\n\t\t\tcontinue\n\t\t}\n\t\trelevantPods = append(relevantPods, pod)\n\t}\n\n\treturn relevantPods, nil\n}\n\nfunc (s *Solver) cleanupPods(ctx context.Context, ch *v1alpha1.Challenge) error {\n\tlog := logf.FromContext(ctx, \"cleanupPods\")\n\n\tpods, err := s.getPodsForChallenge(ctx, ch)\n\tif err != nil {\n\t\treturn err\n\t}\n\tvar errs []error\n\tfor _, pod := range pods {\n\t\tlog := logf.WithRelatedResource(log, pod).V(logf.DebugLevel)\n\t\tlog.Info(\"deleting pod resource\")\n\n\t\terr := s.Client.CoreV1().Pods(pod.Namespace).Delete(pod.Name, nil)\n\t\tif err != nil {\n\t\t\tlog.Info(\"failed to delete pod resource\", \"error\", err)\n\t\t\terrs = append(errs, err)\n\t\t\tcontinue\n\t\t}\n\t\tlog.Info(\"successfully deleted pod resource\")\n\t}\n\n\treturn utilerrors.NewAggregate(errs)\n}\n\n// createPod will create a challenge solving pod for the given certificate,\n// domain, token and key.\nfunc (s *Solver) createPod(ch *v1alpha1.Challenge) (*corev1.Pod, error) {\n\treturn s.Client.CoreV1().Pods(ch.Namespace).Create(\n\t\ts.buildPod(ch))\n}\n\n// buildPod will build a challenge solving pod for the given certificate,\n// domain, token and key. It will not create it in the API server\nfunc (s *Solver) buildPod(ch *v1alpha1.Challenge) *corev1.Pod {\n\tpod := s.buildDefaultPod(ch)\n\n\t// Override defaults if they have changed in the pod template.\n\tif ch.Spec.Solver != nil &&\n\t\tch.Spec.Solver.HTTP01 != nil &&\n\t\tch.Spec.Solver.HTTP01.Ingress != nil {\n\t\tpod = s.mergePodObjectMetaWithPodTemplate(pod,\n\t\t\tch.Spec.Solver.HTTP01.Ingress.PodTemplate)\n\t}\n\n\treturn pod\n}\n\nfunc (s *Solver) buildDefaultPod(ch *v1alpha1.Challenge) *corev1.Pod {\n\tpodLabels := podLabels(ch)\n\n\treturn &corev1.Pod{\n\t\tObjectMeta: metav1.ObjectMeta{\n\t\t\tGenerateName: \"cm-acme-http-solver-\",\n\t\t\tNamespace:    ch.Namespace,\n\t\t\tLabels:       podLabels,\n\t\t\tAnnotations: map[string]string{\n\t\t\t\t\"sidecar.istio.io/inject\": \"false\",\n\t\t\t},\n\t\t\tOwnerReferences: []metav1.OwnerReference{*metav1.NewControllerRef(ch, challengeGvk)},\n\t\t},\n\t\tSpec: corev1.PodSpec{\n\t\t\tRestartPolicy: corev1.RestartPolicyOnFailure,\n\t\t\tContainers: []corev1.Container{\n\t\t\t\t{\n\t\t\t\t\tName: \"acmesolver\",\n\t\t\t\t\t// TODO: use an image as specified as a config option\n\t\t\t\t\tImage:           s.Context.HTTP01SolverImage,\n\t\t\t\t\tImagePullPolicy: corev1.PullIfNotPresent,\n\t\t\t\t\t// TODO: replace this with some kind of cmdline generator\n\t\t\t\t\tArgs: []string{\n\t\t\t\t\t\tfmt.Sprintf(\"--listen-port=%d\", acmeSolverListenPort),\n\t\t\t\t\t\tfmt.Sprintf(\"--domain=%s\", ch.Spec.DNSName),\n\t\t\t\t\t\tfmt.Sprintf(\"--token=%s\", ch.Spec.Token),\n\t\t\t\t\t\tfmt.Sprintf(\"--key=%s\", ch.Spec.Key),\n\t\t\t\t\t},\n\t\t\t\t\tResources: corev1.ResourceRequirements{\n\t\t\t\t\t\tRequests: corev1.ResourceList{\n\t\t\t\t\t\t\tcorev1.ResourceCPU:    s.ACMEOptions.HTTP01SolverResourceRequestCPU,\n\t\t\t\t\t\t\tcorev1.ResourceMemory: s.ACMEOptions.HTTP01SolverResourceRequestMemory,\n\t\t\t\t\t\t},\n\t\t\t\t\t\tLimits: corev1.ResourceList{\n\t\t\t\t\t\t\tcorev1.ResourceCPU:    s.ACMEOptions.HTTP01SolverResourceLimitsCPU,\n\t\t\t\t\t\t\tcorev1.ResourceMemory: s.ACMEOptions.HTTP01SolverResourceLimitsMemory,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tPorts: []corev1.ContainerPort{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tName:          \"http\",\n\t\t\t\t\t\t\tContainerPort: acmeSolverListenPort,\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t}\n}\n\n// Merge object meta from the pod template. Fall back to default values.\nfunc (s *Solver) mergePodObjectMetaWithPodTemplate(pod *corev1.Pod, podTempl *v1alpha1.ACMEChallengeSolverHTTP01IngressPodTemplate) *corev1.Pod {\n\tif podTempl == nil {\n\t\treturn pod\n\t}\n\n\tif pod.Labels == nil {\n\t\tpod.Labels = make(map[string]string)\n\t}\n\n\tfor k, v := range podTempl.Labels {\n\t\tpod.Labels[k] = v\n\t}\n\n\tif pod.Annotations == nil {\n\t\tpod.Annotations = make(map[string]string)\n\t}\n\n\tfor k, v := range podTempl.Annotations {\n\t\tpod.Annotations[k] = v\n\t}\n\n\tif pod.Spec.NodeSelector == nil {\n\t\tpod.Spec.NodeSelector = make(map[string]string)\n\t}\n\n\tfor k, v := range podTempl.Spec.NodeSelector {\n\t\tpod.Spec.NodeSelector[k] = v\n\t}\n\n\tif pod.Spec.Tolerations == nil {\n\t\tpod.Spec.Tolerations = []corev1.Toleration{}\n\t}\n\n\tfor _, t := range podTempl.Spec.Tolerations {\n\t\tpod.Spec.Tolerations = append(pod.Spec.Tolerations, t)\n\t}\n\n\tif podTempl.Spec.Affinity != nil {\n\t\tpod.Spec.Affinity = podTempl.Spec.Affinity\n\t}\n\n\treturn pod\n}\n", "idx": 1, "id": 15741, "msg": "I could add a check on ch.IssuerRef here to double-check that this is only allowed in ClusterIssuer, not Issuer. Since everything is expected to go through the validation phase I didn't to keep this logic simpler, but let me know if you prefer to add it.", "proj": "jetstack-cert-manager", "lang": "go"}
{"patch": "@@ -81,26 +81,34 @@ func ParseTransportType(s string) (TransportType, error) {\n // The client dispatcher will be brought up using the given TransportType for Unary, HTTP for\n // Oneway, and the serviceName with a \"-client\" suffix.\n func WithClientConfig(serviceName string, procedures []transport.Procedure, transportType TransportType, f func(transport.ClientConfig) error) (err error) {\n-\tdispatcherConfig, err := NewDispatcherConfig(serviceName)\n+\tdispatcherConfig, close, err := NewDispatcherConfig(serviceName)\n \tif err != nil {\n \t\treturn err\n \t}\n+\tdefer func() {\n+\t\terr = multierr.Append(err, close())\n+\t}()\n+\n \tserverDispatcher, err := NewServerDispatcher(procedures, dispatcherConfig)\n \tif err != nil {\n \t\treturn err\n \t}\n+\n \tclientDispatcher, err := NewClientDispatcher(transportType, dispatcherConfig)\n \tif err != nil {\n \t\treturn err\n \t}\n+\n \tif err := serverDispatcher.Start(); err != nil {\n \t\treturn err\n \t}\n-\tdefer func() { err = errors.CombineErrors(err, serverDispatcher.Stop()) }()\n+\tdefer func() { err = multierr.Append(err, serverDispatcher.Stop()) }()\n+\n \tif err := clientDispatcher.Start(); err != nil {\n \t\treturn err\n \t}\n-\tdefer func() { err = errors.CombineErrors(err, clientDispatcher.Stop()) }()\n+\tdefer func() { err = multierr.Append(err, clientDispatcher.Stop()) }()\n+\n \treturn f(clientDispatcher.ClientConfig(serviceName))\n }\n ", "y": 0, "oldf": "// Copyright (c) 2017 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage testutils\n\nimport (\n\t\"fmt\"\n\t\"net\"\n\t\"strconv\"\n\n\t\"go.uber.org/yarpc\"\n\t\"go.uber.org/yarpc/api/transport\"\n\t\"go.uber.org/yarpc/internal/errors\"\n\t\"go.uber.org/yarpc/transport/http\"\n\t\"go.uber.org/yarpc/transport/tchannel\"\n)\n\nconst (\n\t// TransportTypeHTTP represents using HTTP.\n\tTransportTypeHTTP TransportType = iota\n\t// TransportTypeTChannel represents using TChannel.\n\tTransportTypeTChannel\n)\n\nvar (\n\t// AllTransportTypes are all TransportTypes,\n\tAllTransportTypes = []TransportType{\n\t\tTransportTypeHTTP,\n\t\tTransportTypeTChannel,\n\t}\n)\n\n// TransportType is a transport type.\ntype TransportType int\n\n// String returns a string representation of t.\nfunc (t TransportType) String() string {\n\tswitch t {\n\tcase TransportTypeHTTP:\n\t\treturn \"http\"\n\tcase TransportTypeTChannel:\n\t\treturn \"tchannel\"\n\tdefault:\n\t\treturn strconv.Itoa(int(t))\n\t}\n}\n\n// ParseTransportType parses a transport type from a string.\nfunc ParseTransportType(s string) (TransportType, error) {\n\tswitch s {\n\tcase \"http\":\n\t\treturn TransportTypeHTTP, nil\n\tcase \"tchannel\":\n\t\treturn TransportTypeTChannel, nil\n\tdefault:\n\t\treturn 0, fmt.Errorf(\"invalid TransportType: %s\", s)\n\t}\n}\n\n// WithClientConfig wraps a function by setting up a client and server dispatcher and giving\n// the function the client configuration to use in tests for the given TransportType.\n//\n// The server dispatcher will be brought up using all TransportTypes and with the serviceName.\n// The client dispatcher will be brought up using the given TransportType for Unary, HTTP for\n// Oneway, and the serviceName with a \"-client\" suffix.\nfunc WithClientConfig(serviceName string, procedures []transport.Procedure, transportType TransportType, f func(transport.ClientConfig) error) (err error) {\n\tdispatcherConfig, err := NewDispatcherConfig(serviceName)\n\tif err != nil {\n\t\treturn err\n\t}\n\tserverDispatcher, err := NewServerDispatcher(procedures, dispatcherConfig)\n\tif err != nil {\n\t\treturn err\n\t}\n\tclientDispatcher, err := NewClientDispatcher(transportType, dispatcherConfig)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif err := serverDispatcher.Start(); err != nil {\n\t\treturn err\n\t}\n\tdefer func() { err = errors.CombineErrors(err, serverDispatcher.Stop()) }()\n\tif err := clientDispatcher.Start(); err != nil {\n\t\treturn err\n\t}\n\tdefer func() { err = errors.CombineErrors(err, clientDispatcher.Stop()) }()\n\treturn f(clientDispatcher.ClientConfig(serviceName))\n}\n\n// NewClientDispatcher returns a new client Dispatcher.\n//\n// HTTP always will be configured as an outbound for Oneway\nfunc NewClientDispatcher(transportType TransportType, config *DispatcherConfig) (*yarpc.Dispatcher, error) {\n\tport, err := config.GetPort(transportType)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\thttpPort, err := config.GetPort(TransportTypeHTTP)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar onewayOutbound transport.OnewayOutbound\n\tvar unaryOutbound transport.UnaryOutbound\n\tswitch transportType {\n\tcase TransportTypeTChannel:\n\t\ttchannelTransport, err := tchannel.NewChannelTransport(tchannel.ServiceName(config.GetServiceName()))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tonewayOutbound = http.NewTransport().NewSingleOutbound(fmt.Sprintf(\"http://127.0.0.1:%d\", httpPort))\n\t\tunaryOutbound = tchannelTransport.NewSingleOutbound(fmt.Sprintf(\"localhost:%d\", port))\n\tcase TransportTypeHTTP:\n\t\thttpOutbound := http.NewTransport().NewSingleOutbound(fmt.Sprintf(\"http://127.0.0.1:%d\", port))\n\t\tonewayOutbound = httpOutbound\n\t\tunaryOutbound = httpOutbound\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"invalid TransportType: %v\", transportType)\n\t}\n\treturn yarpc.NewDispatcher(\n\t\tyarpc.Config{\n\t\t\tName: fmt.Sprintf(\"%s-client\", config.GetServiceName()),\n\t\t\tOutbounds: yarpc.Outbounds{\n\t\t\t\tconfig.GetServiceName(): {\n\t\t\t\t\tOneway: onewayOutbound,\n\t\t\t\t\tUnary:  unaryOutbound,\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t), nil\n}\n\n// NewServerDispatcher returns a new server Dispatcher.\nfunc NewServerDispatcher(procedures []transport.Procedure, config *DispatcherConfig) (*yarpc.Dispatcher, error) {\n\ttchannelPort, err := config.GetPort(TransportTypeTChannel)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\thttpPort, err := config.GetPort(TransportTypeHTTP)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ttchannelTransport, err := tchannel.NewChannelTransport(\n\t\ttchannel.ServiceName(config.GetServiceName()),\n\t\ttchannel.ListenAddr(fmt.Sprintf(\":%d\", tchannelPort)),\n\t)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdispatcher := yarpc.NewDispatcher(\n\t\tyarpc.Config{\n\t\t\tName: config.GetServiceName(),\n\t\t\tInbounds: yarpc.Inbounds{\n\t\t\t\ttchannelTransport.NewInbound(),\n\t\t\t\thttp.NewTransport().NewInbound(fmt.Sprintf(\":%d\", httpPort)),\n\t\t\t},\n\t\t},\n\t)\n\tdispatcher.Register(procedures)\n\treturn dispatcher, nil\n}\n\n// DispatcherConfig is the configuration for a Dispatcher.\ntype DispatcherConfig struct {\n\tserviceName         string\n\ttransportTypeToPort map[TransportType]uint16\n}\n\n// NewDispatcherConfig returns a new DispatcherConfig with assigned ports.\nfunc NewDispatcherConfig(serviceName string) (*DispatcherConfig, error) {\n\ttransportTypeToPort, err := getTransportTypeToPort()\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &DispatcherConfig{\n\t\tserviceName,\n\t\ttransportTypeToPort,\n\t}, nil\n}\n\n// GetServiceName gets the service name.\nfunc (d *DispatcherConfig) GetServiceName() string {\n\treturn d.serviceName\n}\n\n// GetPort gets the port for the TransportType.\nfunc (d *DispatcherConfig) GetPort(transportType TransportType) (uint16, error) {\n\tport, ok := d.transportTypeToPort[transportType]\n\tif !ok {\n\t\treturn 0, fmt.Errorf(\"no port for TransportType %v\", transportType)\n\t}\n\treturn port, nil\n}\n\nfunc getTransportTypeToPort() (map[TransportType]uint16, error) {\n\tm := make(map[TransportType]uint16, len(AllTransportTypes))\n\tfor _, transportType := range AllTransportTypes {\n\t\tport, err := getFreePort()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tm[transportType] = port\n\t}\n\treturn m, nil\n}\n\nfunc getFreePort() (uint16, error) {\n\taddress, err := net.ResolveTCPAddr(\"tcp\", \"localhost:0\")\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tlistener, err := net.ListenTCP(\"tcp\", address)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tport := uint16(listener.Addr().(*net.TCPAddr).Port)\n\tif err := listener.Close(); err != nil {\n\t\treturn 0, err\n\t}\n\treturn port, nil\n}\n", "idx": 4, "id": 13067, "msg": "", "proj": "yarpc-yarpc-go", "lang": "go"}
{"patch": "@@ -59,7 +59,7 @@ func (c *Controller) Sync(ctx context.Context, cr *v1alpha1.CertificateRequest)\n \tif apiutil.CertificateRequestHasCondition(cr, v1alpha1.CertificateRequestCondition{\n \t\tType:   v1alpha1.CertificateRequestConditionReady,\n \t\tStatus: v1alpha1.ConditionFalse,\n-\t\tReason: errorCertificateFailed,\n+\t\tReason: errorFailed,\n \t}) {\n \t\tdbg.Info(\"certificate request condition failed so skipping processing\")\n \t\treturn nil", "y": 0, "oldf": "/*\nCopyright 2019 The Jetstack cert-manager contributors.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n*/\n\npackage certificaterequests\n\nimport (\n\t\"context\"\n\t\"encoding/json\"\n\t\"reflect\"\n\n\t\"github.com/kr/pretty\"\n\tcorev1 \"k8s.io/api/core/v1\"\n\tk8sErrors \"k8s.io/apimachinery/pkg/api/errors\"\n\tutilerrors \"k8s.io/apimachinery/pkg/util/errors\"\n\n\tapiutil \"github.com/jetstack/cert-manager/pkg/api/util\"\n\t\"github.com/jetstack/cert-manager/pkg/apis/certmanager\"\n\t\"github.com/jetstack/cert-manager/pkg/apis/certmanager/v1alpha1\"\n\t\"github.com/jetstack/cert-manager/pkg/apis/certmanager/validation\"\n\t\"github.com/jetstack/cert-manager/pkg/issuer\"\n\tlogf \"github.com/jetstack/cert-manager/pkg/logs\"\n\t\"github.com/jetstack/cert-manager/pkg/util/pki\"\n)\n\nconst (\n\terrorCertificatePending = \"CertPending\"\n\terrorCertificateFailed  = \"CertFailed\"\n\n\terrorIssuerNotFound = \"IssuerNotFound\"\n\terrorIssuerInit     = \"IssuerInitError\"\n\n\tsuccessCertificateIssued = \"CertIssued\"\n)\n\nfunc (c *Controller) Sync(ctx context.Context, cr *v1alpha1.CertificateRequest) (err error) {\n\tc.metrics.IncrementSyncCallCount(ControllerName)\n\n\tlog := logf.FromContext(ctx)\n\tdbg := log.V(logf.DebugLevel)\n\n\tif !(cr.Spec.IssuerRef.Group == \"\" || cr.Spec.IssuerRef.Group == certmanager.GroupName) {\n\t\tdbg.Info(\"certificate request issuerRef group does not match certmanager group so skipping processing\")\n\t\treturn nil\n\t}\n\n\tif apiutil.CertificateRequestHasCondition(cr, v1alpha1.CertificateRequestCondition{\n\t\tType:   v1alpha1.CertificateRequestConditionReady,\n\t\tStatus: v1alpha1.ConditionFalse,\n\t\tReason: errorCertificateFailed,\n\t}) {\n\t\tdbg.Info(\"certificate request condition failed so skipping processing\")\n\t\treturn nil\n\t}\n\n\tcrCopy := cr.DeepCopy()\n\tdefer func() {\n\t\tif _, saveErr := c.updateCertificateRequestStatus(ctx, cr, crCopy); saveErr != nil {\n\t\t\terr = utilerrors.NewAggregate([]error{saveErr, err})\n\t\t}\n\t}()\n\n\tdbg.Info(\"fetching issuer object referenced by CertificateRequest\")\n\n\tissuerObj, err := c.helper.GetGenericIssuer(crCopy.Spec.IssuerRef, crCopy.Namespace)\n\tif k8sErrors.IsNotFound(err) {\n\t\tc.recorder.Eventf(crCopy, corev1.EventTypeWarning, errorIssuerNotFound, err.Error())\n\t\tlog.WithValues(\n\t\t\tlogf.RelatedResourceNameKey, crCopy.Spec.IssuerRef.Name,\n\t\t\tlogf.RelatedResourceKindKey, crCopy.Spec.IssuerRef.Kind,\n\t\t).Error(err, \"failed to find referenced issuer\")\n\t\treturn nil\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdbg.Info(\"ensuring issuer type matches this controller\")\n\n\tlog = logf.WithRelatedResource(log, issuerObj)\n\n\tissuerType, err := apiutil.NameForIssuer(issuerObj)\n\tif err != nil {\n\t\tc.recorder.Eventf(crCopy, corev1.EventTypeWarning, errorIssuerNotFound, err.Error())\n\t\tlog.Error(err, \"failed to obtain referenced issuer type\")\n\t\treturn nil\n\t}\n\n\t// This CertificateRequest is not meant for us, ignore\n\tif issuerType != c.issuerType {\n\t\tc.log.WithValues(\n\t\t\tlogf.RelatedResourceKindKey, issuerType,\n\t\t).V(5).Info(\"issuer reference type does not match controller resource kind, ignoring\")\n\t\treturn nil\n\t}\n\n\tdbg.Info(\"validating CertificateRequest resource object\")\n\n\tel := validation.ValidateCertificateRequest(crCopy)\n\tif len(el) > 0 {\n\t\tc.recorder.Eventf(crCopy, corev1.EventTypeWarning, \"BadConfig\", \"Resource validation failed: %v\", el.ToAggregate())\n\t\treturn nil\n\t}\n\n\tif len(crCopy.Status.Certificate) > 0 {\n\t\tdbg.Info(\"certificate field is already set in status so skipping processing\")\n\t\tc.setCertificateRequestStatus(crCopy)\n\t\treturn nil\n\t}\n\n\ti, err := c.issuerFactory.IssuerFor(issuerObj)\n\tif err != nil {\n\t\tc.recorder.Eventf(crCopy, corev1.EventTypeWarning, errorIssuerInit, \"Internal error initialising issuer: %v\", err)\n\t\treturn nil\n\t}\n\n\t// TODO: Metrics??\n\n\tdbg.Info(\"invoking sign function as existing certificate does not exist\")\n\treturn c.sign(ctx, crCopy, i)\n}\n\n// return an error on failure. If retrieval is succesful, the certificate data\n// will be stored in the certificate request status\nfunc (c *Controller) sign(ctx context.Context, cr *v1alpha1.CertificateRequest, issuer issuer.Interface) error {\n\tlog := logf.FromContext(ctx)\n\n\tdefer c.setCertificateRequestStatus(cr)\n\n\tresp, err := issuer.Sign(ctx, cr)\n\tif err != nil {\n\t\tlog.Error(err, \"error issuing certificate request\")\n\t\treturn err\n\t}\n\n\t// if the issuer has not returned any data, exit early\n\tif resp == nil {\n\t\treturn nil\n\t}\n\n\tif len(resp.Certificate) > 0 {\n\t\tcr.Status.Certificate = resp.Certificate\n\t\tcr.Status.CA = resp.CA\n\n\t\tc.recorder.Event(cr, corev1.EventTypeNormal, successCertificateIssued,\n\t\t\t\"Certificate fetched from issuer successfully\")\n\t}\n\n\treturn nil\n}\n\n// setCertificateRequestStatus will update the status subresource of the\n// certificate request.\nfunc (c *Controller) setCertificateRequestStatus(cr *v1alpha1.CertificateRequest) {\n\t// No cert exists yet\n\tif len(cr.Status.Certificate) == 0 {\n\t\tapiutil.SetCertificateRequestCondition(cr, v1alpha1.CertificateRequestConditionReady,\n\t\t\tv1alpha1.ConditionFalse, errorCertificatePending, \"Certificate issuance pending\")\n\t\treturn\n\t}\n\n\t// invalid cert\n\t_, err := pki.DecodeX509CertificateBytes(cr.Status.Certificate)\n\tif err != nil {\n\t\tapiutil.SetCertificateRequestCondition(cr, v1alpha1.CertificateRequestConditionReady,\n\t\t\tv1alpha1.ConditionFalse, errorCertificateFailed, \"Failed to decode certificate PEM\")\n\t\treturn\n\t}\n\n\t// cert has been issued and can be decoded so we are ready\n\tapiutil.SetCertificateRequestCondition(cr, v1alpha1.CertificateRequestConditionReady,\n\t\tv1alpha1.ConditionTrue, \"Ready\", \"Certificate has been issued successfully\")\n\treturn\n}\n\nfunc (c *Controller) updateCertificateRequestStatus(ctx context.Context, old, new *v1alpha1.CertificateRequest) (*v1alpha1.CertificateRequest, error) {\n\tlog := logf.FromContext(ctx, \"updateStatus\")\n\toldBytes, _ := json.Marshal(old.Status)\n\tnewBytes, _ := json.Marshal(new.Status)\n\tif reflect.DeepEqual(oldBytes, newBytes) {\n\t\treturn nil, nil\n\t}\n\n\tlog.V(logf.DebugLevel).Info(\"updating resource due to change in status\", \"diff\", pretty.Diff(string(oldBytes), string(newBytes)))\n\t// TODO: replace Update call with UpdateStatus. This requires a custom API\n\t// server with the /status subresource enabled and/or subresource support\n\t// for CRDs (https://github.com/kubernetes/kubernetes/issues/38113)\n\treturn c.cmClient.CertmanagerV1alpha1().CertificateRequests(new.Namespace).Update(new)\n}\n", "idx": 3, "id": 17938, "msg": "", "proj": "jetstack-cert-manager", "lang": "go"}
{"patch": "@@ -46,6 +46,18 @@ function virtualDOMfromNode(node, shadowId) {\n \t\t\t\t);\n \t\t\t}\n \t\t\treturn vNodeCache._tabbableElements;\n+\t\t},\n+\n+\t\t// abstract Node and Element APIs so we can run axe in DOM-less\n+\t\t// environments. these are static properties in the assumption\n+\t\t// that axe does not change any of them while it runs.\n+\t\telementNodeType: node.nodeType,\n+\t\telementNodeName: node.nodeName,\n+\t\telementClassName: node.className,\n+\t\telementId: node.id,\n+\n+\t\telementGetAttribute(attrName) {\n+\t\t\treturn node.getAttribute(attrName);\n \t\t}\n \t};\n \taxe._cache.get('nodeMap').set(node, vNode);", "y": 1, "oldf": "/*eslint no-use-before-define: 0*/\nvar axe = axe || { utils: {} };\n\n/**\n * This implemnts the flatten-tree algorithm specified:\n * Originally here https://drafts.csswg.org/css-scoping/#flat-tree\n * Hopefully soon published here: https://www.w3.org/TR/css-scoping-1/#flat-tree\n *\n * Some notable information:\n ******* NOTE: as of Chrome 59, this is broken in Chrome so that tests fail completely\n ******* removed functionality for now\n * 1. <slot> elements do not have boxes by default (i.e. they do not get rendered and\n *    their CSS properties are ignored)\n * 2. <slot> elements can be made to have a box by overriding the display property\n *    which is 'contents' by default\n * 3. Even boxed <slot> elements do not show up in the accessibility tree until\n *    they have a tabindex applied to them OR they have a role applied to them AND\n *    they have a box (this is observed behavior in Safari on OS X, I cannot find\n *    the spec for this)\n */\n\n/**\n * Wrap the real node and provide list of the flattened children\n *\n * @param node {Node} - the node in question\n * @param shadowId {String} - the ID of the shadow DOM to which this node belongs\n * @return {Object} - the wrapped node\n */\nfunction virtualDOMfromNode(node, shadowId) {\n\tconst vNodeCache = {};\n\tconst vNode = {\n\t\tshadowId: shadowId,\n\t\tchildren: [],\n\t\tactualNode: node,\n\t\t_isHidden: null, // will be populated by axe.utils.isHidden\n\t\tget isFocusable() {\n\t\t\tif (!vNodeCache.hasOwnProperty('_isFocusable')) {\n\t\t\t\tvNodeCache._isFocusable = axe.commons.dom.isFocusable(node);\n\t\t\t}\n\t\t\treturn vNodeCache._isFocusable;\n\t\t},\n\t\tget tabbableElements() {\n\t\t\tif (!vNodeCache.hasOwnProperty('_tabbableElements')) {\n\t\t\t\tvNodeCache._tabbableElements = axe.commons.dom.getTabbableElements(\n\t\t\t\t\tthis\n\t\t\t\t);\n\t\t\t}\n\t\t\treturn vNodeCache._tabbableElements;\n\t\t}\n\t};\n\taxe._cache.get('nodeMap').set(node, vNode);\n\treturn vNode;\n}\n\n/**\n * find all the fallback content for a <slot> and return these as an array\n * this array will also include any #text nodes\n *\n * @param node {Node} - the slot Node\n * @return Array{Nodes}\n */\nfunction getSlotChildren(node) {\n\tvar retVal = [];\n\n\tnode = node.firstChild;\n\twhile (node) {\n\t\tretVal.push(node);\n\t\tnode = node.nextSibling;\n\t}\n\treturn retVal;\n}\n\n/**\n * Recursvely returns an array of the virtual DOM nodes at this level\n * excluding comment nodes and the shadow DOM nodes <content> and <slot>\n *\n * @param {Node} node the current node\n * @param {String} shadowId, optional ID of the shadow DOM that is the closest shadow\n *                           ancestor of the node\n */\nfunction flattenTree(node, shadowId) {\n\t// using a closure here and therefore cannot easily refactor toreduce the statements\n\tvar retVal, realArray, nodeName;\n\tfunction reduceShadowDOM(res, child) {\n\t\tvar replacements = flattenTree(child, shadowId);\n\t\tif (replacements) {\n\t\t\tres = res.concat(replacements);\n\t\t}\n\t\treturn res;\n\t}\n\n\tif (node.documentElement) {\n\t\t// document\n\t\tnode = node.documentElement;\n\t}\n\tnodeName = node.nodeName.toLowerCase();\n\n\tif (axe.utils.isShadowRoot(node)) {\n\t\t// generate an ID for this shadow root and overwrite the current\n\t\t// closure shadowId with this value so that it cascades down the tree\n\t\tretVal = virtualDOMfromNode(node, shadowId);\n\t\tshadowId =\n\t\t\t'a' +\n\t\t\tMath.random()\n\t\t\t\t.toString()\n\t\t\t\t.substring(2);\n\t\trealArray = Array.from(node.shadowRoot.childNodes);\n\t\tretVal.children = realArray.reduce(reduceShadowDOM, []);\n\t\treturn [retVal];\n\t} else {\n\t\tif (nodeName === 'content') {\n\t\t\trealArray = Array.from(node.getDistributedNodes());\n\t\t\treturn realArray.reduce(reduceShadowDOM, []);\n\t\t} else if (\n\t\t\tnodeName === 'slot' &&\n\t\t\ttypeof node.assignedNodes === 'function'\n\t\t) {\n\t\t\trealArray = Array.from(node.assignedNodes());\n\t\t\tif (!realArray.length) {\n\t\t\t\t// fallback content\n\t\t\t\trealArray = getSlotChildren(node);\n\t\t\t}\n\t\t\tvar styl = window.getComputedStyle(node);\n\t\t\t// check the display property\n\t\t\tif (false && styl.display !== 'contents') {\n\t\t\t\t// intentionally commented out\n\t\t\t\t// has a box\n\t\t\t\tretVal = virtualDOMfromNode(node, shadowId);\n\t\t\t\tretVal.children = realArray.reduce(reduceShadowDOM, []);\n\t\t\t\treturn [retVal];\n\t\t\t} else {\n\t\t\t\treturn realArray.reduce(reduceShadowDOM, []);\n\t\t\t}\n\t\t} else {\n\t\t\tif (node.nodeType === 1) {\n\t\t\t\tretVal = virtualDOMfromNode(node, shadowId);\n\t\t\t\trealArray = Array.from(node.childNodes);\n\t\t\t\tretVal.children = realArray.reduce(reduceShadowDOM, []);\n\t\t\t\treturn [retVal];\n\t\t\t} else if (node.nodeType === 3) {\n\t\t\t\t// text\n\t\t\t\treturn [virtualDOMfromNode(node)];\n\t\t\t}\n\t\t\treturn undefined;\n\t\t}\n\t}\n}\n\n/**\n * Recursvely returns an array of the virtual DOM nodes at this level\n * excluding comment nodes and the shadow DOM nodes <content> and <slot>\n *\n * @param {Node} node the current node\n * @param {String} shadowId, optional ID of the shadow DOM that is the closest shadow\n *                           ancestor of the node\n */\naxe.utils.getFlattenedTree = function(node, shadowId) {\n\taxe._cache.set('nodeMap', new WeakMap());\n\treturn flattenTree(node, shadowId);\n};\n\n/**\n * Return a single node from the virtual dom tree\n *\n * @param {Object} vNode The flattened, virtual DOM tree\n * @param {Node}   node  The HTML DOM node\n */\naxe.utils.getNodeFromTree = function(vNode, node) {\n\tconst el = node || vNode;\n\treturn axe._cache.get('nodeMap') ? axe._cache.get('nodeMap').get(el) : null;\n};\n", "idx": 1, "id": 14481, "msg": "I think we should normalise this, do a `.toLowerCase()` here.", "proj": "dequelabs-axe-core", "lang": "js"}
{"patch": "@@ -36,14 +36,21 @@ const double constexpr STRAIGHT_ANGLE = 180.;\n // if a turn deviates this much from going straight, it will be kept straight\n const double constexpr MAXIMAL_ALLOWED_NO_TURN_DEVIATION = 2.;\n // angle that lies between two nearly indistinguishable roads\n-const double constexpr NARROW_TURN_ANGLE = 25.;\n+const double constexpr NARROW_TURN_ANGLE = 35.;\n // angle difference that can be classified as straight, if its the only narrow turn\n const double constexpr FUZZY_STRAIGHT_ANGLE = 15.;\n const double constexpr DISTINCTION_RATIO = 2;\n \n+using guidance::TurnInstruction;\n+using guidance::DirectionModifier;\n+using guidance::TurnType;\n+using guidance::FunctionalRoadClass;\n+\n+using guidance::isLowPriorityRoadClass;\n+using guidance::angularDeviation;\n+using guidance::getTurnDirection;\n+\n // Configuration to find representative candidate for turn angle calculations\n-const double constexpr MINIMAL_SEGMENT_LENGTH = 1.;\n-const double constexpr DESIRED_SEGMENT_LENGTH = 10.;\n \n EdgeBasedGraphFactory::EdgeBasedGraphFactory(\n     std::shared_ptr<util::NodeBasedDynamicGraph> node_based_graph,", "y": 0, "oldf": "#include \"extractor/edge_based_edge.hpp\"\n#include \"extractor/edge_based_graph_factory.hpp\"\n#include \"util/coordinate.hpp\"\n#include \"util/coordinate_calculation.hpp\"\n#include \"util/percent.hpp\"\n#include \"util/integer_range.hpp\"\n#include \"util/lua_util.hpp\"\n#include \"util/simple_logger.hpp\"\n#include \"util/timing_util.hpp\"\n#include \"util/exception.hpp\"\n\n#include \"util/debug_geometry.hpp\"\n\n#include <boost/assert.hpp>\n\n#include <algorithm>\n#include <cmath>\n#include <fstream>\n#include <iomanip>\n#include <limits>\n#include <sstream>\n#include <string>\n\nnamespace osrm\n{\nnamespace extractor\n{\n\n// configuration of turn classification\nconst bool constexpr INVERT = true;\nconst bool constexpr RESOLVE_TO_RIGHT = true;\nconst bool constexpr RESOLVE_TO_LEFT = false;\n\n// what angle is interpreted as going straight\nconst double constexpr STRAIGHT_ANGLE = 180.;\n// if a turn deviates this much from going straight, it will be kept straight\nconst double constexpr MAXIMAL_ALLOWED_NO_TURN_DEVIATION = 2.;\n// angle that lies between two nearly indistinguishable roads\nconst double constexpr NARROW_TURN_ANGLE = 25.;\n// angle difference that can be classified as straight, if its the only narrow turn\nconst double constexpr FUZZY_STRAIGHT_ANGLE = 15.;\nconst double constexpr DISTINCTION_RATIO = 2;\n\n// Configuration to find representative candidate for turn angle calculations\nconst double constexpr MINIMAL_SEGMENT_LENGTH = 1.;\nconst double constexpr DESIRED_SEGMENT_LENGTH = 10.;\n\nEdgeBasedGraphFactory::EdgeBasedGraphFactory(\n    std::shared_ptr<util::NodeBasedDynamicGraph> node_based_graph,\n    const CompressedEdgeContainer &compressed_edge_container,\n    const std::unordered_set<NodeID> &barrier_nodes,\n    const std::unordered_set<NodeID> &traffic_lights,\n    std::shared_ptr<const RestrictionMap> restriction_map,\n    const std::vector<QueryNode> &node_info_list,\n    SpeedProfileProperties speed_profile)\n    : m_max_edge_id(0), m_node_info_list(node_info_list),\n      m_node_based_graph(std::move(node_based_graph)),\n      m_restriction_map(std::move(restriction_map)), m_barrier_nodes(barrier_nodes),\n      m_traffic_lights(traffic_lights), m_compressed_edge_container(compressed_edge_container),\n      speed_profile(std::move(speed_profile))\n{\n}\n\nvoid EdgeBasedGraphFactory::GetEdgeBasedEdges(\n    util::DeallocatingVector<EdgeBasedEdge> &output_edge_list)\n{\n    BOOST_ASSERT_MSG(0 == output_edge_list.size(), \"Vector is not empty\");\n    using std::swap; // Koenig swap\n    swap(m_edge_based_edge_list, output_edge_list);\n}\n\nvoid EdgeBasedGraphFactory::GetEdgeBasedNodes(std::vector<EdgeBasedNode> &nodes)\n{\n#ifndef NDEBUG\n    for (const EdgeBasedNode &node : m_edge_based_node_list)\n    {\n        BOOST_ASSERT(m_node_info_list.at(node.u).lat != INT_MAX);\n        BOOST_ASSERT(m_node_info_list.at(node.u).lon != INT_MAX);\n        BOOST_ASSERT(m_node_info_list.at(node.v).lon != INT_MAX);\n        BOOST_ASSERT(m_node_info_list.at(node.v).lat != INT_MAX);\n    }\n#endif\n    using std::swap; // Koenig swap\n    swap(nodes, m_edge_based_node_list);\n}\n\nvoid EdgeBasedGraphFactory::GetStartPointMarkers(std::vector<bool> &node_is_startpoint)\n{\n    using std::swap; // Koenig swap\n    swap(m_edge_based_node_is_startpoint, node_is_startpoint);\n}\n\nvoid EdgeBasedGraphFactory::GetEdgeBasedNodeWeights(std::vector<EdgeWeight> &output_node_weights)\n{\n    using std::swap; // Koenig swap\n    swap(m_edge_based_node_weights, output_node_weights);\n}\n\nunsigned EdgeBasedGraphFactory::GetHighestEdgeID() { return m_max_edge_id; }\n\nvoid EdgeBasedGraphFactory::InsertEdgeBasedNode(const NodeID node_u, const NodeID node_v)\n{\n    // merge edges together into one EdgeBasedNode\n    BOOST_ASSERT(node_u != SPECIAL_NODEID);\n    BOOST_ASSERT(node_v != SPECIAL_NODEID);\n\n    // find forward edge id and\n    const EdgeID edge_id_1 = m_node_based_graph->FindEdge(node_u, node_v);\n    BOOST_ASSERT(edge_id_1 != SPECIAL_EDGEID);\n\n    const EdgeData &forward_data = m_node_based_graph->GetEdgeData(edge_id_1);\n\n    // find reverse edge id and\n    const EdgeID edge_id_2 = m_node_based_graph->FindEdge(node_v, node_u);\n    BOOST_ASSERT(edge_id_2 != SPECIAL_EDGEID);\n\n    const EdgeData &reverse_data = m_node_based_graph->GetEdgeData(edge_id_2);\n\n    if (forward_data.edge_id == SPECIAL_NODEID && reverse_data.edge_id == SPECIAL_NODEID)\n    {\n        return;\n    }\n\n    if (forward_data.edge_id != SPECIAL_NODEID && reverse_data.edge_id == SPECIAL_NODEID)\n        m_edge_based_node_weights[forward_data.edge_id] = INVALID_EDGE_WEIGHT;\n\n    BOOST_ASSERT(m_compressed_edge_container.HasEntryForID(edge_id_1) ==\n                 m_compressed_edge_container.HasEntryForID(edge_id_2));\n    if (m_compressed_edge_container.HasEntryForID(edge_id_1))\n    {\n        BOOST_ASSERT(m_compressed_edge_container.HasEntryForID(edge_id_2));\n\n        // reconstruct geometry and put in each individual edge with its offset\n        const auto &forward_geometry = m_compressed_edge_container.GetBucketReference(edge_id_1);\n        const auto &reverse_geometry = m_compressed_edge_container.GetBucketReference(edge_id_2);\n        BOOST_ASSERT(forward_geometry.size() == reverse_geometry.size());\n        BOOST_ASSERT(0 != forward_geometry.size());\n        const unsigned geometry_size = static_cast<unsigned>(forward_geometry.size());\n        BOOST_ASSERT(geometry_size > 1);\n\n        // reconstruct bidirectional edge with individual weights and put each into the NN index\n\n        std::vector<int> forward_dist_prefix_sum(forward_geometry.size(), 0);\n        std::vector<int> reverse_dist_prefix_sum(reverse_geometry.size(), 0);\n\n        // quick'n'dirty prefix sum as std::partial_sum needs addtional casts\n        // TODO: move to lambda function with C++11\n        int temp_sum = 0;\n\n        for (const auto i : util::irange(0u, geometry_size))\n        {\n            forward_dist_prefix_sum[i] = temp_sum;\n            temp_sum += forward_geometry[i].second;\n\n            BOOST_ASSERT(forward_data.distance >= temp_sum);\n        }\n\n        temp_sum = 0;\n        for (const auto i : util::irange(0u, geometry_size))\n        {\n            temp_sum += reverse_geometry[reverse_geometry.size() - 1 - i].second;\n            reverse_dist_prefix_sum[i] = reverse_data.distance - temp_sum;\n            // BOOST_ASSERT(reverse_data.distance >= temp_sum);\n        }\n\n        NodeID current_edge_source_coordinate_id = node_u;\n\n        // traverse arrays from start and end respectively\n        for (const auto i : util::irange(0u, geometry_size))\n        {\n            BOOST_ASSERT(current_edge_source_coordinate_id ==\n                         reverse_geometry[geometry_size - 1 - i].first);\n            const NodeID current_edge_target_coordinate_id = forward_geometry[i].first;\n            BOOST_ASSERT(current_edge_target_coordinate_id != current_edge_source_coordinate_id);\n\n            // build edges\n            m_edge_based_node_list.emplace_back(\n                forward_data.edge_id, reverse_data.edge_id, current_edge_source_coordinate_id,\n                current_edge_target_coordinate_id, forward_data.name_id, forward_geometry[i].second,\n                reverse_geometry[geometry_size - 1 - i].second, forward_dist_prefix_sum[i],\n                reverse_dist_prefix_sum[i], m_compressed_edge_container.GetPositionForID(edge_id_1),\n                false, INVALID_COMPONENTID, i, forward_data.travel_mode, reverse_data.travel_mode);\n\n            m_edge_based_node_is_startpoint.push_back(forward_data.startpoint ||\n                                                      reverse_data.startpoint);\n            current_edge_source_coordinate_id = current_edge_target_coordinate_id;\n\n            BOOST_ASSERT(m_edge_based_node_list.back().IsCompressed());\n\n            BOOST_ASSERT(node_u != m_edge_based_node_list.back().u ||\n                         node_v != m_edge_based_node_list.back().v);\n\n            BOOST_ASSERT(node_u != m_edge_based_node_list.back().v ||\n                         node_v != m_edge_based_node_list.back().u);\n        }\n\n        BOOST_ASSERT(current_edge_source_coordinate_id == node_v);\n        BOOST_ASSERT(m_edge_based_node_list.back().IsCompressed());\n    }\n    else\n    {\n        BOOST_ASSERT(!m_compressed_edge_container.HasEntryForID(edge_id_2));\n\n        if (forward_data.edge_id != SPECIAL_NODEID)\n        {\n            BOOST_ASSERT(!forward_data.reversed);\n        }\n        else\n        {\n            BOOST_ASSERT(forward_data.reversed);\n        }\n\n        if (reverse_data.edge_id != SPECIAL_NODEID)\n        {\n            BOOST_ASSERT(!reverse_data.reversed);\n        }\n        else\n        {\n            BOOST_ASSERT(reverse_data.reversed);\n        }\n\n        BOOST_ASSERT(forward_data.edge_id != SPECIAL_NODEID ||\n                     reverse_data.edge_id != SPECIAL_NODEID);\n\n        m_edge_based_node_list.emplace_back(\n            forward_data.edge_id, reverse_data.edge_id, node_u, node_v, forward_data.name_id,\n            forward_data.distance, reverse_data.distance, 0, 0, SPECIAL_EDGEID, false,\n            INVALID_COMPONENTID, 0, forward_data.travel_mode, reverse_data.travel_mode);\n        m_edge_based_node_is_startpoint.push_back(forward_data.startpoint ||\n                                                  reverse_data.startpoint);\n        BOOST_ASSERT(!m_edge_based_node_list.back().IsCompressed());\n    }\n}\n\nvoid EdgeBasedGraphFactory::FlushVectorToStream(\n    std::ofstream &edge_data_file, std::vector<OriginalEdgeData> &original_edge_data_vector) const\n{\n    if (original_edge_data_vector.empty())\n    {\n        return;\n    }\n    edge_data_file.write((char *)&(original_edge_data_vector[0]),\n                         original_edge_data_vector.size() * sizeof(OriginalEdgeData));\n    original_edge_data_vector.clear();\n}\n\n#ifdef DEBUG_GEOMETRY\nvoid EdgeBasedGraphFactory::Run(const std::string &original_edge_data_filename,\n                                lua_State *lua_state,\n                                const std::string &edge_segment_lookup_filename,\n                                const std::string &edge_penalty_filename,\n                                const bool generate_edge_lookup,\n                                const std::string &debug_turns_path)\n#else\nvoid EdgeBasedGraphFactory::Run(const std::string &original_edge_data_filename,\n                                lua_State *lua_state,\n                                const std::string &edge_segment_lookup_filename,\n                                const std::string &edge_penalty_filename,\n                                const bool generate_edge_lookup)\n#endif\n{\n    TIMER_START(renumber);\n    m_max_edge_id = RenumberEdges() - 1;\n    TIMER_STOP(renumber);\n\n    TIMER_START(generate_nodes);\n    m_edge_based_node_weights.reserve(m_max_edge_id + 1);\n    GenerateEdgeExpandedNodes();\n    TIMER_STOP(generate_nodes);\n\n    TIMER_START(generate_edges);\n#ifdef DEBUG_GEOMETRY\n    GenerateEdgeExpandedEdges(original_edge_data_filename, lua_state, edge_segment_lookup_filename,\n                              edge_penalty_filename, generate_edge_lookup, debug_turns_path);\n#else\n    GenerateEdgeExpandedEdges(original_edge_data_filename, lua_state, edge_segment_lookup_filename,\n                              edge_penalty_filename, generate_edge_lookup);\n#endif\n\n    TIMER_STOP(generate_edges);\n\n    util::SimpleLogger().Write() << \"Timing statistics for edge-expanded graph:\";\n    util::SimpleLogger().Write() << \"Renumbering edges: \" << TIMER_SEC(renumber) << \"s\";\n    util::SimpleLogger().Write() << \"Generating nodes: \" << TIMER_SEC(generate_nodes) << \"s\";\n    util::SimpleLogger().Write() << \"Generating edges: \" << TIMER_SEC(generate_edges) << \"s\";\n}\n\n/// Renumbers all _forward_ edges and sets the edge_id.\n/// A specific numbering is not important. Any unique ID will do.\n/// Returns the number of edge based nodes.\nunsigned EdgeBasedGraphFactory::RenumberEdges()\n{\n    // renumber edge based node of outgoing edges\n    unsigned numbered_edges_count = 0;\n    for (const auto current_node : util::irange(0u, m_node_based_graph->GetNumberOfNodes()))\n    {\n        for (const auto current_edge : m_node_based_graph->GetAdjacentEdgeRange(current_node))\n        {\n            EdgeData &edge_data = m_node_based_graph->GetEdgeData(current_edge);\n\n            // only number incoming edges\n            if (edge_data.reversed)\n            {\n                continue;\n            }\n\n            // oneway streets always require this self-loop. Other streets only if a u-turn plus\n            // traversal\n            // of the street takes longer than the loop\n            m_edge_based_node_weights.push_back(edge_data.distance + speed_profile.u_turn_penalty);\n\n            BOOST_ASSERT(numbered_edges_count < m_node_based_graph->GetNumberOfEdges());\n            edge_data.edge_id = numbered_edges_count;\n            ++numbered_edges_count;\n\n            BOOST_ASSERT(SPECIAL_NODEID != edge_data.edge_id);\n        }\n    }\n\n    return numbered_edges_count;\n}\n\n/// Creates the nodes in the edge expanded graph from edges in the node-based graph.\nvoid EdgeBasedGraphFactory::GenerateEdgeExpandedNodes()\n{\n    util::Percent progress(m_node_based_graph->GetNumberOfNodes());\n\n    // loop over all edges and generate new set of nodes\n    for (const auto node_u : util::irange(0u, m_node_based_graph->GetNumberOfNodes()))\n    {\n        BOOST_ASSERT(node_u != SPECIAL_NODEID);\n        BOOST_ASSERT(node_u < m_node_based_graph->GetNumberOfNodes());\n        progress.printStatus(node_u);\n        for (EdgeID e1 : m_node_based_graph->GetAdjacentEdgeRange(node_u))\n        {\n            const EdgeData &edge_data = m_node_based_graph->GetEdgeData(e1);\n            BOOST_ASSERT(e1 != SPECIAL_EDGEID);\n            const NodeID node_v = m_node_based_graph->GetTarget(e1);\n\n            BOOST_ASSERT(SPECIAL_NODEID != node_v);\n            // pick only every other edge, since we have every edge as an outgoing\n            // and incoming egde\n            if (node_u > node_v)\n            {\n                continue;\n            }\n\n            BOOST_ASSERT(node_u < node_v);\n\n            // if we found a non-forward edge reverse and try again\n            if (edge_data.edge_id == SPECIAL_NODEID)\n            {\n                InsertEdgeBasedNode(node_v, node_u);\n            }\n            else\n            {\n                InsertEdgeBasedNode(node_u, node_v);\n            }\n        }\n    }\n\n    BOOST_ASSERT(m_edge_based_node_list.size() == m_edge_based_node_is_startpoint.size());\n    BOOST_ASSERT(m_max_edge_id + 1 == m_edge_based_node_weights.size());\n\n    util::SimpleLogger().Write() << \"Generated \" << m_edge_based_node_list.size()\n                                 << \" nodes in edge-expanded graph\";\n}\n\n/// Actually it also generates OriginalEdgeData and serializes them...\n#ifdef DEBUG_GEOMETRY\nvoid EdgeBasedGraphFactory::GenerateEdgeExpandedEdges(\n    const std::string &original_edge_data_filename,\n    lua_State *lua_state,\n    const std::string &edge_segment_lookup_filename,\n    const std::string &edge_fixed_penalties_filename,\n    const bool generate_edge_lookup,\n    const std::string &debug_turns_path)\n#else\nvoid EdgeBasedGraphFactory::GenerateEdgeExpandedEdges(\n    const std::string &original_edge_data_filename,\n    lua_State *lua_state,\n    const std::string &edge_segment_lookup_filename,\n    const std::string &edge_fixed_penalties_filename,\n    const bool generate_edge_lookup)\n#endif\n{\n    util::SimpleLogger().Write() << \"generating edge-expanded edges\";\n\n    std::size_t node_based_edge_counter = 0;\n    std::size_t original_edges_counter = 0;\n    restricted_turns_counter = 0;\n    skipped_uturns_counter = 0;\n    skipped_barrier_turns_counter = 0;\n    std::size_t compressed = 0;\n\n    std::ofstream edge_data_file(original_edge_data_filename.c_str(), std::ios::binary);\n    std::ofstream edge_segment_file;\n    std::ofstream edge_penalty_file;\n\n    if (generate_edge_lookup)\n    {\n        edge_segment_file.open(edge_segment_lookup_filename.c_str(), std::ios::binary);\n        edge_penalty_file.open(edge_fixed_penalties_filename.c_str(), std::ios::binary);\n    }\n\n    // writes a dummy value that is updated later\n    edge_data_file.write((char *)&original_edges_counter, sizeof(unsigned));\n\n    std::vector<OriginalEdgeData> original_edge_data_vector;\n    original_edge_data_vector.reserve(1024 * 1024);\n\n    // Loop over all turns and generate new set of edges.\n    // Three nested loop look super-linear, but we are dealing with a (kind of)\n    // linear number of turns only.\n    util::Percent progress(m_node_based_graph->GetNumberOfNodes());\n\n#ifdef DEBUG_GEOMETRY\n    util::DEBUG_TURNS_START(debug_turns_path);\n#endif\n\n    for (const auto node_u : util::irange(0u, m_node_based_graph->GetNumberOfNodes()))\n    {\n        // progress.printStatus(node_u);\n        for (const EdgeID edge_form_u : m_node_based_graph->GetAdjacentEdgeRange(node_u))\n        {\n            if (m_node_based_graph->GetEdgeData(edge_form_u).reversed)\n            {\n                continue;\n            }\n\n            ++node_based_edge_counter;\n            auto turn_candidates = getTurnCandidates(node_u, edge_form_u);\n            turn_candidates = optimizeCandidates(edge_form_u, turn_candidates);\n            turn_candidates = suppressTurns(edge_form_u, turn_candidates);\n\n            const NodeID node_v = m_node_based_graph->GetTarget(edge_form_u);\n\n            for (const auto turn : turn_candidates)\n            {\n                if (!turn.valid)\n                    continue;\n\n                const double turn_angle = turn.angle;\n\n                // only add an edge if turn is not prohibited\n                const EdgeData &edge_data1 = m_node_based_graph->GetEdgeData(edge_form_u);\n                const EdgeData &edge_data2 = m_node_based_graph->GetEdgeData(turn.eid);\n\n                BOOST_ASSERT(edge_data1.edge_id != edge_data2.edge_id);\n                BOOST_ASSERT(!edge_data1.reversed);\n                BOOST_ASSERT(!edge_data2.reversed);\n\n                // the following is the core of the loop.\n                unsigned distance = edge_data1.distance;\n                if (m_traffic_lights.find(node_v) != m_traffic_lights.end())\n                {\n                    distance += speed_profile.traffic_signal_penalty;\n\n                    util::DEBUG_SIGNAL(node_v, m_node_info_list,\n                                       speed_profile.traffic_signal_penalty);\n                }\n\n                const int turn_penalty = GetTurnPenalty(turn_angle, lua_state);\n                const TurnInstruction turn_instruction = turn.instruction;\n\n                if (turn_instruction == TurnInstruction::UTurn)\n                {\n                    distance += speed_profile.u_turn_penalty;\n\n                    util::DEBUG_UTURN(node_v, m_node_info_list, speed_profile.u_turn_penalty);\n                }\n\n                distance += turn_penalty;\n\n                const bool edge_is_compressed =\n                    m_compressed_edge_container.HasEntryForID(edge_form_u);\n\n                if (edge_is_compressed)\n                {\n                    ++compressed;\n                }\n\n                original_edge_data_vector.emplace_back(\n                    (edge_is_compressed ? m_compressed_edge_container.GetPositionForID(edge_form_u)\n                                        : node_v),\n                    edge_data1.name_id, turn_instruction, edge_is_compressed,\n                    edge_data1.travel_mode);\n\n                ++original_edges_counter;\n\n                if (original_edge_data_vector.size() > 1024 * 1024 * 10)\n                {\n                    FlushVectorToStream(edge_data_file, original_edge_data_vector);\n                }\n\n                BOOST_ASSERT(SPECIAL_NODEID != edge_data1.edge_id);\n                BOOST_ASSERT(SPECIAL_NODEID != edge_data2.edge_id);\n\n                // NOTE: potential overflow here if we hit 2^32 routable edges\n                BOOST_ASSERT(m_edge_based_edge_list.size() <= std::numeric_limits<NodeID>::max());\n                m_edge_based_edge_list.emplace_back(edge_data1.edge_id, edge_data2.edge_id,\n                                                    m_edge_based_edge_list.size(), distance, true,\n                                                    false);\n\n                // Here is where we write out the mapping between the edge-expanded edges, and\n                // the node-based edges that are originally used to calculate the `distance`\n                // for the edge-expanded edges.  About 40 lines back, there is:\n                //\n                //                 unsigned distance = edge_data1.distance;\n                //\n                // This tells us that the weight for an edge-expanded-edge is based on the weight\n                // of the *source* node-based edge.  Therefore, we will look up the individual\n                // segments of the source node-based edge, and write out a mapping between\n                // those and the edge-based-edge ID.\n                // External programs can then use this mapping to quickly perform\n                // updates to the edge-expanded-edge based directly on its ID.\n                if (generate_edge_lookup)\n                {\n                    unsigned fixed_penalty = distance - edge_data1.distance;\n                    edge_penalty_file.write(reinterpret_cast<const char *>(&fixed_penalty),\n                                            sizeof(fixed_penalty));\n                    if (edge_is_compressed)\n                    {\n                        const auto node_based_edges =\n                            m_compressed_edge_container.GetBucketReference(edge_form_u);\n                        NodeID previous = node_u;\n\n                        const unsigned node_count = node_based_edges.size() + 1;\n                        edge_segment_file.write(reinterpret_cast<const char *>(&node_count),\n                                                sizeof(node_count));\n                        const QueryNode &first_node = m_node_info_list[previous];\n                        edge_segment_file.write(reinterpret_cast<const char *>(&first_node.node_id),\n                                                sizeof(first_node.node_id));\n\n                        for (auto target_node : node_based_edges)\n                        {\n                            const QueryNode &from = m_node_info_list[previous];\n                            const QueryNode &to = m_node_info_list[target_node.first];\n                            const double segment_length =\n                                util::coordinate_calculation::greatCircleDistance(\n                                    from.lat, from.lon, to.lat, to.lon);\n\n                            edge_segment_file.write(reinterpret_cast<const char *>(&to.node_id),\n                                                    sizeof(to.node_id));\n                            edge_segment_file.write(reinterpret_cast<const char *>(&segment_length),\n                                                    sizeof(segment_length));\n                            edge_segment_file.write(\n                                reinterpret_cast<const char *>(&target_node.second),\n                                sizeof(target_node.second));\n                            previous = target_node.first;\n                        }\n                    }\n                    else\n                    {\n                        static const unsigned node_count = 2;\n                        const QueryNode from = m_node_info_list[node_u];\n                        const QueryNode to = m_node_info_list[node_v];\n                        const double segment_length =\n                            util::coordinate_calculation::greatCircleDistance(from.lat, from.lon,\n                                                                              to.lat, to.lon);\n                        edge_segment_file.write(reinterpret_cast<const char *>(&node_count),\n                                                sizeof(node_count));\n                        edge_segment_file.write(reinterpret_cast<const char *>(&from.node_id),\n                                                sizeof(from.node_id));\n                        edge_segment_file.write(reinterpret_cast<const char *>(&to.node_id),\n                                                sizeof(to.node_id));\n                        edge_segment_file.write(reinterpret_cast<const char *>(&segment_length),\n                                                sizeof(segment_length));\n                        edge_segment_file.write(\n                            reinterpret_cast<const char *>(&edge_data1.distance),\n                            sizeof(edge_data1.distance));\n                    }\n                }\n            }\n        }\n    }\n\n    util::DEBUG_TURNS_STOP();\n\n    FlushVectorToStream(edge_data_file, original_edge_data_vector);\n\n    edge_data_file.seekp(std::ios::beg);\n    edge_data_file.write((char *)&original_edges_counter, sizeof(unsigned));\n    edge_data_file.close();\n\n    util::SimpleLogger().Write() << \"Generated \" << m_edge_based_node_list.size()\n                                 << \" edge based nodes\";\n    util::SimpleLogger().Write() << \"Node-based graph contains \" << node_based_edge_counter\n                                 << \" edges\";\n    util::SimpleLogger().Write() << \"Edge-expanded graph ...\";\n    util::SimpleLogger().Write() << \"  contains \" << m_edge_based_edge_list.size() << \" edges\";\n    util::SimpleLogger().Write() << \"  skips \" << restricted_turns_counter << \" turns, \"\n                                                                              \"defined by \"\n                                 << m_restriction_map->size() << \" restrictions\";\n    util::SimpleLogger().Write() << \"  skips \" << skipped_uturns_counter << \" U turns\";\n    util::SimpleLogger().Write() << \"  skips \" << skipped_barrier_turns_counter\n                                 << \" turns over barriers\";\n}\n\n// requires sorted candidates\nstd::vector<EdgeBasedGraphFactory::TurnCandidate>\nEdgeBasedGraphFactory::optimizeCandidates(NodeID via_eid,\n                                          std::vector<TurnCandidate> turn_candidates) const\n{\n    BOOST_ASSERT_MSG(std::is_sorted(turn_candidates.begin(), turn_candidates.end(),\n                                    [](const TurnCandidate &left, const TurnCandidate &right)\n                                    {\n                                        return left.angle < right.angle;\n                                    }),\n                     \"Turn Candidates not sorted by angle.\");\n    if (turn_candidates.size() <= 1)\n        return turn_candidates;\n\n    const auto getLeft = [&turn_candidates](std::size_t index)\n    {\n        return (index + 1) % turn_candidates.size();\n    };\n    const auto getRight = [&turn_candidates](std::size_t index)\n    {\n        return (index + turn_candidates.size() - 1) % turn_candidates.size();\n    };\n\n    // handle availability of multiple u-turns (e.g. street with separated small parking roads)\n    if (turn_candidates[0].instruction == TurnInstruction::UTurn && turn_candidates[0].angle == 0)\n    {\n        if (turn_candidates[getLeft(0)].instruction == TurnInstruction::UTurn)\n            turn_candidates[getLeft(0)].instruction = TurnInstruction::TurnSharpLeft;\n        if (turn_candidates[getRight(0)].instruction == TurnInstruction::UTurn)\n            turn_candidates[getRight(0)].instruction = TurnInstruction::TurnSharpRight;\n    }\n\n    const auto keepStraight = [](double angle)\n    {\n        return std::abs(angle - 180) < 5;\n    };\n\n    for (std::size_t turn_index = 0; turn_index < turn_candidates.size(); ++turn_index)\n    {\n        auto &turn = turn_candidates[turn_index];\n        if (turn.instruction > TurnInstruction::TurnSlightLeft ||\n            turn.instruction == TurnInstruction::UTurn)\n            continue;\n        auto &left = turn_candidates[getLeft(turn_index)];\n        if (turn.angle == left.angle)\n        {\n            util::SimpleLogger().Write(logDEBUG)\n                << \"[warning] conflicting turn angles, identical road duplicated? \"\n                << m_node_info_list[m_node_based_graph->GetTarget(via_eid)].lat << \" \"\n                << m_node_info_list[m_node_based_graph->GetTarget(via_eid)].lon << std::endl;\n        }\n        if (isConflict(turn.instruction, left.instruction))\n        {\n            // begin of a conflicting region\n            std::size_t conflict_begin = turn_index;\n            std::size_t conflict_end = getLeft(turn_index);\n            std::size_t conflict_size = 2;\n            while (\n                isConflict(turn_candidates[getLeft(conflict_end)].instruction, turn.instruction) &&\n                conflict_size < turn_candidates.size())\n            {\n                conflict_end = getLeft(conflict_end);\n                ++conflict_size;\n            }\n\n            turn_index = (conflict_end < conflict_begin) ? turn_candidates.size() : conflict_end;\n\n            if (conflict_size > 3)\n            {\n                // check if some turns are invalid to find out about good handling\n            }\n\n            auto &instruction_left_of_end = turn_candidates[getLeft(conflict_end)].instruction;\n            auto &instruction_right_of_begin =\n                turn_candidates[getRight(conflict_begin)].instruction;\n            auto &candidate_at_end = turn_candidates[conflict_end];\n            auto &candidate_at_begin = turn_candidates[conflict_begin];\n            if (conflict_size == 2)\n            {\n                if (turn.instruction == TurnInstruction::GoStraight)\n                {\n                    if (instruction_left_of_end != TurnInstruction::TurnSlightLeft &&\n                        instruction_right_of_begin != TurnInstruction::TurnSlightRight)\n                    {\n                        std::int32_t resolved_count = 0;\n                        //uses side-effects in resolve\n                        if (!keepStraight(candidate_at_end.angle) &&\n                            !resolve(candidate_at_end.instruction, instruction_left_of_end,\n                                     RESOLVE_TO_LEFT))\n                            util::SimpleLogger().Write(logDEBUG) << \"[warning] failed to resolve conflict\";\n                        else\n                            ++resolved_count;\n                        //uses side-effects in resolve\n                        if (!keepStraight(candidate_at_begin.angle) &&\n                            !resolve(candidate_at_begin.instruction, instruction_right_of_begin,\n                                     RESOLVE_TO_RIGHT))\n                            util::SimpleLogger().Write(logDEBUG) << \"[warning] failed to resolve conflict\";\n                        else\n                            ++resolved_count;\n                        if (resolved_count >= 1 &&\n                            (!keepStraight(candidate_at_begin.angle) ||\n                             !keepStraight(candidate_at_end.angle))) // should always be the\n                                                                     // case, theoretically\n                            continue;\n                    }\n                }\n                if (candidate_at_begin.confidence < candidate_at_end.confidence)\n                { // if right shift is cheaper, or only option\n                    if (resolve(candidate_at_begin.instruction, instruction_right_of_begin,\n                                RESOLVE_TO_RIGHT))\n                        continue;\n                    else if (resolve(candidate_at_end.instruction, instruction_left_of_end,\n                                     RESOLVE_TO_LEFT))\n                        continue;\n                }\n                else\n                {\n                    if (resolve(candidate_at_end.instruction, instruction_left_of_end,\n                                RESOLVE_TO_LEFT))\n                        continue;\n                    else if (resolve(candidate_at_begin.instruction, instruction_right_of_begin,\n                                     RESOLVE_TO_RIGHT))\n                        continue;\n                }\n                if (isSlightTurn(turn.instruction) || isSharpTurn(turn.instruction))\n                {\n                    auto resolve_direction =\n                        (turn.instruction == TurnInstruction::TurnSlightRight ||\n                         turn.instruction == TurnInstruction::TurnSharpLeft)\n                            ? RESOLVE_TO_RIGHT\n                            : RESOLVE_TO_LEFT;\n                    if (resolve_direction == RESOLVE_TO_RIGHT &&\n                        resolveTransitive(\n                            candidate_at_begin.instruction, instruction_right_of_begin,\n                            turn_candidates[getRight(getRight(conflict_begin))].instruction,\n                            RESOLVE_TO_RIGHT))\n                        continue;\n                    else if (resolve_direction == RESOLVE_TO_LEFT &&\n                             resolveTransitive(\n                                 candidate_at_end.instruction, instruction_left_of_end,\n                                 turn_candidates[getLeft(getLeft(conflict_end))].instruction,\n                                 RESOLVE_TO_LEFT))\n                        continue;\n                }\n            }\n            else if (conflict_size >= 3)\n            {\n                // a conflict of size larger than three cannot be handled with the current\n                // model.\n                // Handle it as best as possible and keep the rest of the conflicting turns\n                if (conflict_size > 3)\n                {\n                    NodeID conflict_location = m_node_based_graph->GetTarget(via_eid);\n                    util::SimpleLogger().Write(logDEBUG)\n                        << \"[warning] found conflict larget than size three at \"\n                        << m_node_info_list[conflict_location].lat << \", \"\n                        << m_node_info_list[conflict_location].lon;\n                }\n\n                if (!resolve(candidate_at_begin.instruction, instruction_right_of_begin,\n                             RESOLVE_TO_RIGHT))\n                {\n                    if (isSlightTurn(turn.instruction))\n                        resolveTransitive(\n                            candidate_at_begin.instruction, instruction_right_of_begin,\n                            turn_candidates[getRight(getRight(conflict_begin))].instruction,\n                            RESOLVE_TO_RIGHT);\n                    else if (isSharpTurn(turn.instruction))\n                        resolveTransitive(\n                            candidate_at_end.instruction, instruction_left_of_end,\n                            turn_candidates[getLeft(getLeft(conflict_end))].instruction,\n                            RESOLVE_TO_LEFT);\n                }\n                if (!resolve(candidate_at_end.instruction, instruction_left_of_end,\n                             RESOLVE_TO_LEFT))\n                {\n                    if (isSlightTurn(turn.instruction))\n                        resolveTransitive(\n                            candidate_at_end.instruction, instruction_left_of_end,\n                            turn_candidates[getLeft(getLeft(conflict_end))].instruction,\n                            RESOLVE_TO_LEFT);\n                    else if (isSharpTurn(turn.instruction))\n                        resolveTransitive(\n                            candidate_at_begin.instruction, instruction_right_of_begin,\n                            turn_candidates[getRight(getRight(conflict_begin))].instruction,\n                            RESOLVE_TO_RIGHT);\n                }\n            }\n        }\n    }\n    return turn_candidates;\n}\n\nbool EdgeBasedGraphFactory::isObviousChoice(EdgeID via_eid,\n                                            std::size_t turn_index,\n                                            const std::vector<TurnCandidate> &turn_candidates) const\n{\n    const auto getLeft = [&turn_candidates](std::size_t index)\n    {\n        return (index + 1) % turn_candidates.size();\n    };\n    const auto getRight = [&turn_candidates](std::size_t index)\n    {\n        return (index + turn_candidates.size() - 1) % turn_candidates.size();\n    };\n    const auto &candidate = turn_candidates[turn_index];\n    const EdgeData &in_data = m_node_based_graph->GetEdgeData(via_eid);\n    const EdgeData &out_data = m_node_based_graph->GetEdgeData(candidate.eid);\n    const auto &candidate_to_the_left = turn_candidates[getLeft(turn_index)];\n\n    const auto &candidate_to_the_right = turn_candidates[getRight(turn_index)];\n\n    const auto hasValidRatio = [](const TurnCandidate &left, const TurnCandidate &center,\n                                  const TurnCandidate &right)\n    {\n        auto angle_left = (left.angle > 180) ? angularDeviation(left.angle, STRAIGHT_ANGLE) : 180;\n        auto angle_right =\n            (right.angle < 180) ? angularDeviation(right.angle, STRAIGHT_ANGLE) : 180;\n        auto self_angle = angularDeviation(center.angle, STRAIGHT_ANGLE);\n        return angularDeviation(center.angle, STRAIGHT_ANGLE) < NARROW_TURN_ANGLE &&\n               ((center.angle < STRAIGHT_ANGLE)\n                    ? (angle_right > self_angle && angle_left / self_angle > DISTINCTION_RATIO)\n                    : (angle_left > self_angle && angle_right / self_angle > DISTINCTION_RATIO));\n    };\n    // only valid turn\n\n    return turn_candidates.size() == 1 ||\n           // only non u-turn\n           (turn_candidates.size() == 2 &&\n            candidate_to_the_left.instruction == TurnInstruction::UTurn) || // nearly straight turn\n           angularDeviation(candidate.angle, STRAIGHT_ANGLE) < MAXIMAL_ALLOWED_NO_TURN_DEVIATION ||\n           hasValidRatio(candidate_to_the_left, candidate, candidate_to_the_right) ||\n           (in_data.name_id != 0 && in_data.name_id == out_data.name_id &&\n            angularDeviation(candidate.angle, STRAIGHT_ANGLE) < NARROW_TURN_ANGLE / 2);\n}\n\nstd::vector<EdgeBasedGraphFactory::TurnCandidate>\nEdgeBasedGraphFactory::suppressTurns(EdgeID via_eid,\n                                     std::vector<TurnCandidate> turn_candidates) const\n{\n    // remove invalid candidates\n    BOOST_ASSERT_MSG(std::is_sorted(turn_candidates.begin(), turn_candidates.end(),\n                                    [](const TurnCandidate &left, const TurnCandidate &right)\n                                    {\n                                        return left.angle < right.angle;\n                                    }),\n                     \"Turn Candidates not sorted by angle.\");\n    const auto end_valid = std::remove_if(turn_candidates.begin(), turn_candidates.end(),\n                                          [](const TurnCandidate &candidate)\n                                          {\n                                              return !candidate.valid;\n                                          });\n    turn_candidates.erase(end_valid, turn_candidates.end());\n\n    const auto getLeft = [&turn_candidates](std::size_t index)\n    {\n        return (index + 1) % turn_candidates.size();\n    };\n    const auto getRight = [&turn_candidates](std::size_t index)\n    {\n        return (index + turn_candidates.size() - 1) % turn_candidates.size();\n    };\n\n    const EdgeData &in_data = m_node_based_graph->GetEdgeData(via_eid);\n\n    bool has_obvious_with_same_name = false;\n    double obvious_with_same_name_angle = 0;\n    for (std::size_t turn_index = 0; turn_index < turn_candidates.size(); ++turn_index)\n    {\n        if (m_node_based_graph->GetEdgeData(turn_candidates[turn_index].eid).name_id ==\n                in_data.name_id &&\n            isObviousChoice(via_eid, turn_index, turn_candidates))\n        {\n            has_obvious_with_same_name = true;\n            obvious_with_same_name_angle = turn_candidates[turn_index].angle;\n            break;\n        }\n    }\n\n    for (std::size_t turn_index = 0; turn_index < turn_candidates.size(); ++turn_index)\n    {\n        auto &candidate = turn_candidates[turn_index];\n        const EdgeData &out_data = m_node_based_graph->GetEdgeData(candidate.eid);\n        if (candidate.valid && candidate.instruction != TurnInstruction::UTurn)\n        {\n            // TODO road category would be useful to indicate obviousness of turn\n            // check if turn can be omitted or at least changed\n            const auto &left = turn_candidates[getLeft(turn_index)];\n            const auto &right = turn_candidates[getRight(turn_index)];\n\n            // make very slight instructions straight, if they are the only valid choice going with\n            // at most a slight turn\n            if (candidate.instruction < TurnInstruction::ReachViaLocation &&\n                (!isSlightTurn(getTurnDirection(left.angle)) || !left.valid) &&\n                (!isSlightTurn(getTurnDirection(right.angle)) || !right.valid) &&\n                angularDeviation(candidate.angle, STRAIGHT_ANGLE) < FUZZY_STRAIGHT_ANGLE)\n                candidate.instruction = TurnInstruction::GoStraight;\n\n            // TODO this smaller comparison for turns is DANGEROUS, has to be revised if turn\n            // instructions change\n            if (candidate.instruction < TurnInstruction::ReachViaLocation)\n            {\n                if (in_data.travel_mode ==\n                    out_data.travel_mode) // make sure to always announce mode changes\n                {\n                    if (isObviousChoice(via_eid, turn_index, turn_candidates))\n                    {\n\n                        if (in_data.name_id == out_data.name_id) // same road\n                        {\n                            candidate.instruction = TurnInstruction::NoTurn;\n                        }\n\n                        else if (!has_obvious_with_same_name)\n                        {\n                            // TODO discuss, we might want to keep the current name of the turn. But\n                            // this would mean emitting a turn when you just keep on a road\n                            candidate.instruction = TurnInstruction::NameChanges;\n                        }\n                        else if (candidate.angle < obvious_with_same_name_angle)\n                            candidate.instruction = TurnInstruction::TurnSlightRight;\n                        else\n                            candidate.instruction = TurnInstruction::TurnSlightLeft;\n                    }\n                    else if (candidate.instruction == TurnInstruction::GoStraight &&\n                             has_obvious_with_same_name)\n                    {\n                        if (candidate.angle < obvious_with_same_name_angle)\n                            candidate.instruction = TurnInstruction::TurnSlightRight;\n                        else\n                            candidate.instruction = TurnInstruction::TurnSlightLeft;\n                    }\n                }\n            }\n        }\n    }\n    return turn_candidates;\n}\n\nstd::vector<EdgeBasedGraphFactory::TurnCandidate>\nEdgeBasedGraphFactory::getTurnCandidates(NodeID from_node, EdgeID via_eid)\n{\n    std::vector<TurnCandidate> turn_candidates;\n    const NodeID turn_node = m_node_based_graph->GetTarget(via_eid);\n    const NodeID only_restriction_to_node =\n        m_restriction_map->CheckForEmanatingIsOnlyTurn(from_node, turn_node);\n    const bool is_barrier_node = m_barrier_nodes.find(turn_node) != m_barrier_nodes.end();\n\n    for (const EdgeID onto_edge : m_node_based_graph->GetAdjacentEdgeRange(turn_node))\n    {\n        bool turn_is_valid = true;\n        if (m_node_based_graph->GetEdgeData(onto_edge).reversed)\n        {\n            turn_is_valid = false;\n        }\n        const NodeID to_node = m_node_based_graph->GetTarget(onto_edge);\n\n        if (turn_is_valid && (only_restriction_to_node != SPECIAL_NODEID) &&\n            (to_node != only_restriction_to_node))\n        {\n            // We are at an only_-restriction but not at the right turn.\n            ++restricted_turns_counter;\n            turn_is_valid = false;\n        }\n\n        if (turn_is_valid)\n        {\n            if (is_barrier_node)\n            {\n                if (from_node != to_node)\n                {\n                    ++skipped_barrier_turns_counter;\n                    turn_is_valid = false;\n                }\n            }\n            else\n            {\n                if (from_node == to_node && m_node_based_graph->GetOutDegree(turn_node) > 1)\n                {\n                    auto number_of_emmiting_bidirectional_edges = 0;\n                    for (auto edge : m_node_based_graph->GetAdjacentEdgeRange(turn_node))\n                    {\n                        auto target = m_node_based_graph->GetTarget(edge);\n                        auto reverse_edge = m_node_based_graph->FindEdge(target, turn_node);\n                        if (!m_node_based_graph->GetEdgeData(reverse_edge).reversed)\n                        {\n                            ++number_of_emmiting_bidirectional_edges;\n                        }\n                    }\n                    if (number_of_emmiting_bidirectional_edges > 1)\n                    {\n                        ++skipped_uturns_counter;\n                        turn_is_valid = false;\n                    }\n                }\n            }\n        }\n\n        // only add an edge if turn is not a U-turn except when it is\n        // at the end of a dead-end street\n        if (m_restriction_map->CheckIfTurnIsRestricted(from_node, turn_node, to_node) &&\n            (only_restriction_to_node == SPECIAL_NODEID) && (to_node != only_restriction_to_node))\n        {\n            // We are at an only_-restriction but not at the right turn.\n            ++restricted_turns_counter;\n            turn_is_valid = false;\n        }\n\n        // unpack first node of second segment if packed\n\n        const auto first_coordinate =\n            getRepresentativeCoordinate(from_node, turn_node, via_eid, INVERT);\n        const auto third_coordinate =\n            getRepresentativeCoordinate(turn_node, to_node, onto_edge, !INVERT);\n\n        const auto angle = util::coordinate_calculation::computeAngle(\n            first_coordinate, m_node_info_list[turn_node], third_coordinate);\n\n        const auto turn = AnalyzeTurn(from_node, via_eid, turn_node, onto_edge, to_node, angle);\n\n        auto confidence = getTurnConfidence(angle, turn);\n        if (!turn_is_valid)\n            confidence *= 0.8; // makes invalid turns more likely to be resolved in conflicts\n\n        turn_candidates.push_back({onto_edge, turn_is_valid, angle, turn, confidence});\n    }\n\n    const auto ByAngle = [](const TurnCandidate &first, const TurnCandidate second)\n    {\n        return first.angle < second.angle;\n    };\n    std::sort(std::begin(turn_candidates), std::end(turn_candidates), ByAngle);\n\n    const auto getLeft = [&](std::size_t index)\n    {\n        return (index + 1) % turn_candidates.size();\n    };\n\n    const auto getRight = [&](std::size_t index)\n    {\n        return (index + turn_candidates.size() - 1) % turn_candidates.size();\n    };\n\n    const auto isInvalidEquivalent = [&](std::size_t this_turn, std::size_t valid_turn)\n    {\n        if (!turn_candidates[valid_turn].valid || turn_candidates[this_turn].valid)\n            return false;\n\n        return angularDeviation(turn_candidates[this_turn].angle,\n                                turn_candidates[valid_turn].angle) < NARROW_TURN_ANGLE;\n    };\n\n    for (std::size_t index = 0; index < turn_candidates.size(); ++index)\n    {\n        if (isInvalidEquivalent(index, getRight(index)) ||\n            isInvalidEquivalent(index, getLeft(index)))\n        {\n            turn_candidates.erase(turn_candidates.begin() + index);\n            --index;\n        }\n    }\n    return turn_candidates;\n};\n\nint EdgeBasedGraphFactory::GetTurnPenalty(double angle, lua_State *lua_state) const\n{\n\n    if (speed_profile.has_turn_penalty_function)\n    {\n        try\n        {\n            // call lua profile to compute turn penalty\n            double penalty =\n                luabind::call_function<double>(lua_state, \"turn_function\", 180. - angle);\n            return static_cast<int>(penalty);\n        }\n        catch (const luabind::error &er)\n        {\n            util::SimpleLogger().Write(logWARNING) << er.what();\n        }\n    }\n    return 0;\n}\n\n// node_u -- (edge_1) --> node_v -- (edge_2) --> node_w\nTurnInstruction EdgeBasedGraphFactory::AnalyzeTurn(const NodeID node_u,\n                                                   const EdgeID edge1,\n                                                   const NodeID node_v,\n                                                   const EdgeID edge2,\n                                                   const NodeID node_w,\n                                                   const double angle) const\n{\n\n    const EdgeData &data1 = m_node_based_graph->GetEdgeData(edge1);\n    const EdgeData &data2 = m_node_based_graph->GetEdgeData(edge2);\n    if (node_u == node_w)\n    {\n        return TurnInstruction::UTurn;\n    }\n\n    // roundabouts need to be handled explicitely\n    if (data1.roundabout && data2.roundabout)\n    {\n        // Is a turn possible? If yes, we stay on the roundabout!\n        if (1 == m_node_based_graph->GetDirectedOutDegree(node_v))\n        {\n            // No turn possible.\n            return TurnInstruction::NoTurn;\n        }\n        return TurnInstruction::StayOnRoundAbout;\n    }\n    // Does turn start or end on roundabout?\n    if (data1.roundabout || data2.roundabout)\n    {\n        // We are entering the roundabout\n        if ((!data1.roundabout) && data2.roundabout)\n        {\n            return TurnInstruction::EnterRoundAbout;\n        }\n        // We are leaving the roundabout\n        if (data1.roundabout && (!data2.roundabout))\n        {\n            return TurnInstruction::LeaveRoundAbout;\n        }\n    }\n\n    // assign a designated turn angle instruction purely based on the angle\n    return getTurnDirection(angle);\n}\n\nQueryNode EdgeBasedGraphFactory::getRepresentativeCoordinate(const NodeID src,\n                                                             const NodeID tgt,\n                                                             const EdgeID via_eid,\n                                                             bool INVERTED) const\n{\n    if (m_compressed_edge_container.HasEntryForID(via_eid))\n    {\n        util::FixedPointCoordinate prev = util::FixedPointCoordinate(\n                                       m_node_info_list[INVERTED ? tgt : src].lat,\n                                       m_node_info_list[INVERTED ? tgt : src].lon),\n                                   cur;\n        // walk along the edge for the first 5 meters\n        const auto &geometry = m_compressed_edge_container.GetBucketReference(via_eid);\n        double dist = 0;\n        double this_dist = 0;\n        NodeID prev_id = INVERTED ? tgt : src;\n\n        const auto selectBestCandidate = [this](const NodeID current, const double current_distance,\n                                                const NodeID previous,\n                                                const double previous_distance)\n        {\n            if (current_distance < DESIRED_SEGMENT_LENGTH ||\n                current_distance - DESIRED_SEGMENT_LENGTH <\n                    DESIRED_SEGMENT_LENGTH - previous_distance ||\n                previous_distance < MINIMAL_SEGMENT_LENGTH)\n            {\n                return m_node_info_list[current];\n            }\n            else\n            {\n                return m_node_info_list[previous];\n            }\n        };\n\n        if (INVERTED)\n        {\n            for (auto itr = geometry.rbegin(), end = geometry.rend(); itr != end; ++itr)\n            {\n                const auto compressed_node = *itr;\n                cur = util::FixedPointCoordinate(m_node_info_list[compressed_node.first].lat,\n                                                 m_node_info_list[compressed_node.first].lon);\n                this_dist = util::coordinate_calculation::haversineDistance(prev, cur);\n                if (dist + this_dist > DESIRED_SEGMENT_LENGTH)\n                {\n                    return selectBestCandidate(compressed_node.first, dist + this_dist, prev_id,\n                                               dist);\n                }\n                dist += this_dist;\n                prev = cur;\n                prev_id = compressed_node.first;\n            }\n            cur = util::FixedPointCoordinate(m_node_info_list[src].lat, m_node_info_list[src].lon);\n            this_dist = util::coordinate_calculation::haversineDistance(prev, cur);\n            return selectBestCandidate(src, dist + this_dist, prev_id, dist);\n        }\n        else\n        {\n            for (auto itr = geometry.begin(), end = geometry.end(); itr != end; ++itr)\n            {\n                const auto compressed_node = *itr;\n                cur = util::FixedPointCoordinate(m_node_info_list[compressed_node.first].lat,\n                                                 m_node_info_list[compressed_node.first].lon);\n                this_dist = util::coordinate_calculation::haversineDistance(prev, cur);\n                if (dist + this_dist > DESIRED_SEGMENT_LENGTH)\n                {\n                    return selectBestCandidate(compressed_node.first, dist + this_dist, prev_id,\n                                               dist);\n                }\n                dist += this_dist;\n                prev = cur;\n                prev_id = compressed_node.first;\n            }\n            cur = util::FixedPointCoordinate(m_node_info_list[tgt].lat, m_node_info_list[tgt].lon);\n            this_dist = util::coordinate_calculation::haversineDistance(prev, cur);\n            return selectBestCandidate(tgt, dist + this_dist, prev_id, dist);\n        }\n    }\n    // default: If the edge is very short, or we do not have a compressed geometry\n    return m_node_info_list[INVERTED ? src : tgt];\n}\n} // namespace extractor\n} // namespace osrm\n", "idx": 2, "id": 15448, "msg": "", "proj": "Project-OSRM-osrm-backend", "lang": "cpp"}
{"patch": "@@ -159,7 +159,7 @@\n             if (options.delayOut == 0) {\n                 tipsy.hide();\n             } else {\n-                setTimeout(function() { if (tipsy.hoverState == 'out') tipsy.hide(); }, options.delayOut);\n+                setTimeout(function() { if (tipsy.hoverState == 'out' && !tipsy.hoverTooltip) tipsy.hide(); }, options.delayOut);\n             }\n         };\n         ", "y": 0, "oldf": "// tipsy, facebook style tooltips for jquery\n// version 1.0.0a\n// (c) 2008-2010 jason frame [jason@onehackoranother.com]\n// released under the MIT license\n\n(function($) {\n    \n    function maybeCall(thing, ctx) {\n        return (typeof thing == 'function') ? (thing.call(ctx)) : thing;\n    };\n    \n    function Tipsy(element, options) {\n        this.$element = $(element);\n        this.options = options;\n        this.enabled = true;\n        this.fixTitle();\n    };\n    \n    Tipsy.prototype = {\n        show: function() {\n            var title = this.getTitle();\n            if (title && this.enabled) {\n                var $tip = this.tip();\n                \n                $tip.find('.tipsy-inner')[this.options.html ? 'html' : 'text'](title);\n                $tip[0].className = 'tipsy '+this.options.cssClass; // reset classname in case of dynamic gravity\n                $tip.remove().css({top: 0, left: 0, visibility: 'hidden', display: 'block'}).prependTo(document.body);\n                \n                var pos = $.extend({}, this.$element.offset(), {\n                    width: this.$element[0].offsetWidth,\n                    height: this.$element[0].offsetHeight\n                });\n                \n                var actualWidth = $tip[0].offsetWidth,\n                    actualHeight = $tip[0].offsetHeight,\n                    gravity = maybeCall(this.options.gravity, this.$element[0]);\n                \n                var tp;\n                switch (gravity.charAt(0)) {\n                    case 'n':\n                        tp = {top: pos.top + pos.height + this.options.offset, left: pos.left + pos.width / 2 - actualWidth / 2};\n                        break;\n                    case 's':\n                        tp = {top: pos.top - actualHeight - this.options.offset, left: pos.left + pos.width / 2 - actualWidth / 2};\n                        break;\n                    case 'e':\n                        tp = {top: pos.top + pos.height / 2 - actualHeight / 2, left: pos.left - actualWidth - this.options.offset};\n                        break;\n                    case 'w':\n                        tp = {top: pos.top + pos.height / 2 - actualHeight / 2, left: pos.left + pos.width + this.options.offset};\n                        break;\n                }\n                \n                if (gravity.length == 2) {\n                    if (gravity.charAt(1) == 'w') {\n                        tp.left = pos.left + pos.width / 2 - 15;\n                    } else {\n                        tp.left = pos.left + pos.width / 2 - actualWidth + 15;\n                    }\n                }\n                \n                $tip.css(tp).addClass('tipsy-' + gravity);\n                $tip.find('.tipsy-arrow')[0].className = 'tipsy-arrow tipsy-arrow-' + gravity.charAt(0);\n                if (this.options.className) {\n                    $tip.addClass(maybeCall(this.options.className, this.$element[0]));\n                }\n                \n                if (this.options.fade) {\n                    $tip.stop().css({opacity: 0, display: 'block', visibility: 'visible'}).animate({opacity: this.options.opacity});\n                } else {\n                    $tip.css({visibility: 'visible', opacity: this.options.opacity});\n                }\n            }\n        },\n        \n        hide: function() {\n            if (this.options.fade) {\n                this.tip().stop().fadeOut(function() { $(this).remove(); });\n            } else {\n                this.tip().remove();\n            }\n        },\n        \n        fixTitle: function() {\n            var $e = this.$element;\n            if ($e.attr('title') || typeof($e.attr('original-title')) != 'string') {\n                $e.attr('original-title', $e.attr('title') || '').removeAttr('title');\n            }\n        },\n        \n        getTitle: function() {\n            var title, $e = this.$element, o = this.options;\n            this.fixTitle();\n            var title, o = this.options;\n            if (typeof o.title == 'string') {\n                title = $e.attr(o.title == 'title' ? 'original-title' : o.title);\n            } else if (typeof o.title == 'function') {\n                title = o.title.call($e[0]);\n            }\n            title = ('' + title).replace(/(^\\s*|\\s*$)/, \"\");\n            return title || o.fallback;\n        },\n        \n        tip: function() {\n            if (!this.$tip) {\n                this.$tip = $('<div class=\"tipsy\"></div>').html('<div class=\"tipsy-arrow\"></div><div class=\"tipsy-inner\"></div>');\n            }\n            return this.$tip;\n        },\n        \n        validate: function() {\n            if (!this.$element[0].parentNode) {\n                this.hide();\n                this.$element = null;\n                this.options = null;\n            }\n        },\n        \n        enable: function() { this.enabled = true; },\n        disable: function() { this.enabled = false; },\n        toggleEnabled: function() { this.enabled = !this.enabled; }\n    };\n    \n    $.fn.tipsy = function(options) {\n        \n        if (options === true) {\n            return this.data('tipsy');\n        } else if (typeof options == 'string') {\n            var tipsy = this.data('tipsy');\n            if (tipsy) tipsy[options]();\n            return this;\n        }\n        \n        options = $.extend({}, $.fn.tipsy.defaults, options);\n        \n        function get(ele) {\n            var tipsy = $.data(ele, 'tipsy');\n            if (!tipsy) {\n                tipsy = new Tipsy(ele, $.fn.tipsy.elementOptions(ele, options));\n                $.data(ele, 'tipsy', tipsy);\n            }\n            return tipsy;\n        }\n        \n        function enter() {\n            var tipsy = get(this);\n            tipsy.hoverState = 'in';\n            if (options.delayIn == 0) {\n                tipsy.show();\n            } else {\n                tipsy.fixTitle();\n                setTimeout(function() { if (tipsy.hoverState == 'in') tipsy.show(); }, options.delayIn);\n            }\n        };\n        \n        function leave() {\n            var tipsy = get(this);\n            tipsy.hoverState = 'out';\n            if (options.delayOut == 0) {\n                tipsy.hide();\n            } else {\n                setTimeout(function() { if (tipsy.hoverState == 'out') tipsy.hide(); }, options.delayOut);\n            }\n        };\n        \n        if (!options.live) this.each(function() { get(this); });\n        \n        if (options.trigger != 'manual') {\n            var binder   = options.live ? 'live' : 'bind',\n                eventIn  = options.trigger == 'hover' ? 'mouseenter' : 'focus',\n                eventOut = options.trigger == 'hover' ? 'mouseleave' : 'blur';\n            this[binder](eventIn, enter)[binder](eventOut, leave);\n        }\n        \n        return this;\n        \n    };\n    \n    $.fn.tipsy.defaults = {\n        className: null,\n        delayIn: 0,\n        delayOut: 0,\n        fade: false,\n        fallback: '',\n        gravity: 'n',\n        html: false,\n        live: false,\n        offset: 0,\n        opacity: 0.8,\n        title: 'title',\n        trigger: 'hover',\n\t\tcssClass: ''\n    };\n    \n    // Overwrite this method to provide options on a per-element basis.\n    // For example, you could store the gravity in a 'tipsy-gravity' attribute:\n    // return $.extend({}, options, {gravity: $(ele).attr('tipsy-gravity') || 'n' });\n    // (remember - do not modify 'options' in place!)\n    $.fn.tipsy.elementOptions = function(ele, options) {\n        return $.metadata ? $.extend({}, options, $(ele).metadata()) : options;\n    };\n    \n    $.fn.tipsy.autoNS = function() {\n        return $(this).offset().top > ($(document).scrollTop() + $(window).height() / 2) ? 's' : 'n';\n    };\n    \n    $.fn.tipsy.autoWE = function() {\n        return $(this).offset().left > ($(document).scrollLeft() + $(window).width() / 2) ? 'e' : 'w';\n    };\n    \n    /**\n     * yields a closure of the supplied parameters, producing a function that takes\n     * no arguments and is suitable for use as an autogravity function like so:\n     *\n     * @param margin (int) - distance from the viewable region edge that an\n     *        element should be before setting its tooltip's gravity to be away\n     *        from that edge.\n     * @param prefer (string, e.g. 'n', 'sw', 'w') - the direction to prefer\n     *        if there are no viewable region edges effecting the tooltip's\n     *        gravity. It will try to vary from this minimally, for example,\n     *        if 'sw' is preferred and an element is near the right viewable \n     *        region edge, but not the top edge, it will set the gravity for\n     *        that element's tooltip to be 'se', preserving the southern\n     *        component.\n     */\n     $.fn.tipsy.autoBounds = function(margin, prefer) {\n\t\treturn function() {\n\t\t\tvar dir = {ns: prefer[0], ew: (prefer.length > 1 ? prefer[1] : false)},\n\t\t\t    boundTop = $(document).scrollTop() + margin,\n\t\t\t    boundLeft = $(document).scrollLeft() + margin,\n\t\t\t    $this = $(this);\n\n\t\t\tif ($this.offset().top < boundTop) dir.ns = 'n';\n\t\t\tif ($this.offset().left < boundLeft) dir.ew = 'w';\n\t\t\tif ($(window).width() + $(document).scrollLeft() - $this.offset().left < margin) dir.ew = 'e';\n\t\t\tif ($(window).height() + $(document).scrollTop() - $this.offset().top < margin) dir.ns = 's';\n\n\t\t\treturn dir.ns + (dir.ew ? dir.ew : '');\n\t\t}\n\t};\n    \n})(jQuery);\n", "idx": 3, "id": 13321, "msg": "", "proj": "Countly-countly-server", "lang": "js"}
{"patch": "@@ -87,15 +87,12 @@ func TestInputRangeTestCounter(t *testing.T) {\n \tsdkErr = nil\n \n \tcheckpointed := sdk.Collect(ctx)\n-\tsum, err := batcher.records[0].Aggregator().(aggregator.Sum).Sum()\n-\trequire.Equal(t, int64(0), sum.AsInt64())\n-\trequire.Equal(t, 1, checkpointed)\n-\trequire.Nil(t, err)\n+\trequire.Equal(t, 0, checkpointed)\n \n \tbatcher.records = nil\n \tcounter.Add(ctx, 1)\n \tcheckpointed = sdk.Collect(ctx)\n-\tsum, err = batcher.records[0].Aggregator().(aggregator.Sum).Sum()\n+\tsum, err := batcher.records[0].Aggregator().(aggregator.Sum).Sum()\n \trequire.Equal(t, int64(1), sum.AsInt64())\n \trequire.Equal(t, 1, checkpointed)\n \trequire.Nil(t, err)", "y": 0, "oldf": "// Copyright The OpenTelemetry Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage metric_test\n\nimport (\n\t\"context\"\n\t\"fmt\"\n\t\"math\"\n\t\"strings\"\n\t\"testing\"\n\n\t\"github.com/stretchr/testify/require\"\n\n\t\"go.opentelemetry.io/otel/api/core\"\n\t\"go.opentelemetry.io/otel/api/key\"\n\t\"go.opentelemetry.io/otel/api/metric\"\n\texport \"go.opentelemetry.io/otel/sdk/export/metric\"\n\t\"go.opentelemetry.io/otel/sdk/export/metric/aggregator\"\n\tmetricsdk \"go.opentelemetry.io/otel/sdk/metric\"\n\t\"go.opentelemetry.io/otel/sdk/metric/aggregator/array\"\n\t\"go.opentelemetry.io/otel/sdk/metric/aggregator/sum\"\n\tbatchTest \"go.opentelemetry.io/otel/sdk/metric/batcher/test\"\n)\n\nvar Must = metric.Must\n\ntype correctnessBatcher struct {\n\tt *testing.T\n\n\trecords []export.Record\n}\n\nfunc (cb *correctnessBatcher) AggregatorFor(descriptor *metric.Descriptor) export.Aggregator {\n\tname := descriptor.Name()\n\tswitch {\n\tcase strings.HasSuffix(name, \".counter\"):\n\t\treturn sum.New()\n\tcase strings.HasSuffix(name, \".disabled\"):\n\t\treturn nil\n\tdefault:\n\t\treturn array.New()\n\t}\n}\n\nfunc (cb *correctnessBatcher) CheckpointSet() export.CheckpointSet {\n\tcb.t.Fatal(\"Should not be called\")\n\treturn nil\n}\n\nfunc (*correctnessBatcher) FinishedCollection() {\n}\n\nfunc (cb *correctnessBatcher) Process(_ context.Context, record export.Record) error {\n\tcb.records = append(cb.records, record)\n\treturn nil\n}\n\nfunc TestInputRangeTestCounter(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tvar sdkErr error\n\tsdk.SetErrorHandler(func(handleErr error) {\n\t\tsdkErr = handleErr\n\t})\n\n\tcounter := Must(meter).NewInt64Counter(\"name.counter\")\n\n\tcounter.Add(ctx, -1)\n\trequire.Equal(t, aggregator.ErrNegativeInput, sdkErr)\n\tsdkErr = nil\n\n\tcheckpointed := sdk.Collect(ctx)\n\tsum, err := batcher.records[0].Aggregator().(aggregator.Sum).Sum()\n\trequire.Equal(t, int64(0), sum.AsInt64())\n\trequire.Equal(t, 1, checkpointed)\n\trequire.Nil(t, err)\n\n\tbatcher.records = nil\n\tcounter.Add(ctx, 1)\n\tcheckpointed = sdk.Collect(ctx)\n\tsum, err = batcher.records[0].Aggregator().(aggregator.Sum).Sum()\n\trequire.Equal(t, int64(1), sum.AsInt64())\n\trequire.Equal(t, 1, checkpointed)\n\trequire.Nil(t, err)\n\trequire.Nil(t, sdkErr)\n}\n\nfunc TestInputRangeTestMeasure(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tvar sdkErr error\n\tsdk.SetErrorHandler(func(handleErr error) {\n\t\tsdkErr = handleErr\n\t})\n\n\tmeasure := Must(meter).NewFloat64Measure(\"name.measure\")\n\n\tmeasure.Record(ctx, math.NaN())\n\trequire.Equal(t, aggregator.ErrNaNInput, sdkErr)\n\tsdkErr = nil\n\n\tcheckpointed := sdk.Collect(ctx)\n\tcount, err := batcher.records[0].Aggregator().(aggregator.Distribution).Count()\n\trequire.Equal(t, int64(0), count)\n\trequire.Equal(t, 1, checkpointed)\n\trequire.Nil(t, err)\n\n\tmeasure.Record(ctx, 1)\n\tmeasure.Record(ctx, 2)\n\n\tbatcher.records = nil\n\tcheckpointed = sdk.Collect(ctx)\n\n\tcount, err = batcher.records[0].Aggregator().(aggregator.Distribution).Count()\n\trequire.Equal(t, int64(2), count)\n\trequire.Equal(t, 1, checkpointed)\n\trequire.Nil(t, sdkErr)\n\trequire.Nil(t, err)\n}\n\nfunc TestDisabledInstrument(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tmeasure := Must(meter).NewFloat64Measure(\"name.disabled\")\n\n\tmeasure.Record(ctx, -1)\n\tcheckpointed := sdk.Collect(ctx)\n\n\trequire.Equal(t, 0, checkpointed)\n\trequire.Equal(t, 0, len(batcher.records))\n}\n\nfunc TestRecordNaN(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tvar sdkErr error\n\tsdk.SetErrorHandler(func(handleErr error) {\n\t\tsdkErr = handleErr\n\t})\n\tc := Must(meter).NewFloat64Counter(\"sum.name\")\n\n\trequire.Nil(t, sdkErr)\n\tc.Add(ctx, math.NaN())\n\trequire.Error(t, sdkErr)\n}\n\nfunc TestSDKLabelsDeduplication(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tcounter := Must(meter).NewInt64Counter(\"counter\")\n\n\tconst (\n\t\tmaxKeys = 21\n\t\tkeySets = 2\n\t\trepeats = 3\n\t)\n\tvar keysA []core.Key\n\tvar keysB []core.Key\n\n\tfor i := 0; i < maxKeys; i++ {\n\t\tkeysA = append(keysA, core.Key(fmt.Sprintf(\"A%03d\", i)))\n\t\tkeysB = append(keysB, core.Key(fmt.Sprintf(\"B%03d\", i)))\n\t}\n\n\tvar allExpect [][]core.KeyValue\n\tfor numKeys := 0; numKeys < maxKeys; numKeys++ {\n\n\t\tvar kvsA []core.KeyValue\n\t\tvar kvsB []core.KeyValue\n\t\tfor r := 0; r < repeats; r++ {\n\t\t\tfor i := 0; i < numKeys; i++ {\n\t\t\t\tkvsA = append(kvsA, keysA[i].Int(r))\n\t\t\t\tkvsB = append(kvsB, keysB[i].Int(r))\n\t\t\t}\n\t\t}\n\n\t\tvar expectA []core.KeyValue\n\t\tvar expectB []core.KeyValue\n\t\tfor i := 0; i < numKeys; i++ {\n\t\t\texpectA = append(expectA, keysA[i].Int(repeats-1))\n\t\t\texpectB = append(expectB, keysB[i].Int(repeats-1))\n\t\t}\n\n\t\tcounter.Add(ctx, 1, kvsA...)\n\t\tcounter.Add(ctx, 1, kvsA...)\n\t\tallExpect = append(allExpect, expectA)\n\n\t\tif numKeys != 0 {\n\t\t\t// In this case A and B sets are the same.\n\t\t\tcounter.Add(ctx, 1, kvsB...)\n\t\t\tcounter.Add(ctx, 1, kvsB...)\n\t\t\tallExpect = append(allExpect, expectB)\n\t\t}\n\n\t}\n\n\tsdk.Collect(ctx)\n\n\tvar actual [][]core.KeyValue\n\tfor _, rec := range batcher.records {\n\t\tsum, _ := rec.Aggregator().(aggregator.Sum).Sum()\n\t\trequire.Equal(t, sum, core.NewInt64Number(2))\n\n\t\tkvs := export.IteratorToSlice(rec.Labels().Iter())\n\t\tactual = append(actual, kvs)\n\t}\n\n\trequire.ElementsMatch(t, allExpect, actual)\n}\n\nfunc TestDefaultLabelEncoder(t *testing.T) {\n\tencoder := export.NewDefaultLabelEncoder()\n\n\tencoded := encoder.Encode(export.LabelSlice([]core.KeyValue{key.String(\"A\", \"B\"), key.String(\"C\", \"D\")}).Iter())\n\trequire.Equal(t, `A=B,C=D`, encoded)\n\n\tencoded = encoder.Encode(export.LabelSlice([]core.KeyValue{key.String(\"A\", \"B,c=d\"), key.String(`C\\`, \"D\")}).Iter())\n\trequire.Equal(t, `A=B\\,c\\=d,C\\\\=D`, encoded)\n\n\tencoded = encoder.Encode(export.LabelSlice([]core.KeyValue{key.String(`\\`, `=`), key.String(`,`, `\\`)}).Iter())\n\trequire.Equal(t, `\\\\=\\=,\\,=\\\\`, encoded)\n\n\t// Note: the label encoder does not sort or de-dup values,\n\t// that is done in Labels(...).\n\tencoded = encoder.Encode(export.LabelSlice([]core.KeyValue{\n\t\tkey.Int(\"I\", 1),\n\t\tkey.Uint(\"U\", 1),\n\t\tkey.Int32(\"I32\", 1),\n\t\tkey.Uint32(\"U32\", 1),\n\t\tkey.Int64(\"I64\", 1),\n\t\tkey.Uint64(\"U64\", 1),\n\t\tkey.Float64(\"F64\", 1),\n\t\tkey.Float64(\"F64\", 1),\n\t\tkey.String(\"S\", \"1\"),\n\t\tkey.Bool(\"B\", true),\n\t}).Iter())\n\trequire.Equal(t, \"I=1,U=1,I32=1,U32=1,I64=1,U64=1,F64=1,F64=1,S=1,B=true\", encoded)\n}\n\nfunc TestObserverCollection(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\t_ = Must(meter).RegisterFloat64Observer(\"float.observer\", func(result metric.Float64ObserverResult) {\n\t\tresult.Observe(1, key.String(\"A\", \"B\"))\n\t\t// last value wins\n\t\tresult.Observe(-1, key.String(\"A\", \"B\"))\n\t\tresult.Observe(-1, key.String(\"C\", \"D\"))\n\t})\n\t_ = Must(meter).RegisterInt64Observer(\"int.observer\", func(result metric.Int64ObserverResult) {\n\t\tresult.Observe(-1, key.String(\"A\", \"B\"))\n\t\tresult.Observe(1)\n\t\t// last value wins\n\t\tresult.Observe(1, key.String(\"A\", \"B\"))\n\t\tresult.Observe(1)\n\t})\n\t_ = Must(meter).RegisterInt64Observer(\"empty.observer\", func(result metric.Int64ObserverResult) {\n\t})\n\n\tcollected := sdk.Collect(ctx)\n\n\trequire.Equal(t, 4, collected)\n\trequire.Equal(t, 4, len(batcher.records))\n\n\tout := batchTest.NewOutput(export.NewDefaultLabelEncoder())\n\tfor _, rec := range batcher.records {\n\t\t_ = out.AddTo(rec)\n\t}\n\trequire.EqualValues(t, map[string]float64{\n\t\t\"float.observer/A=B\": -1,\n\t\t\"float.observer/C=D\": -1,\n\t\t\"int.observer/\":      1,\n\t\t\"int.observer/A=B\":   1,\n\t}, out.Map)\n}\n\nfunc TestRecordBatch(t *testing.T) {\n\tctx := context.Background()\n\tbatcher := &correctnessBatcher{\n\t\tt: t,\n\t}\n\n\tsdk := metricsdk.New(batcher)\n\tmeter := metric.WrapMeterImpl(sdk, \"test\")\n\n\tcounter1 := Must(meter).NewInt64Counter(\"int64.counter\")\n\tcounter2 := Must(meter).NewFloat64Counter(\"float64.counter\")\n\tmeasure1 := Must(meter).NewInt64Measure(\"int64.measure\")\n\tmeasure2 := Must(meter).NewFloat64Measure(\"float64.measure\")\n\n\tsdk.RecordBatch(\n\t\tctx,\n\t\t[]core.KeyValue{\n\t\t\tkey.String(\"A\", \"B\"),\n\t\t\tkey.String(\"C\", \"D\"),\n\t\t},\n\t\tcounter1.Measurement(1),\n\t\tcounter2.Measurement(2),\n\t\tmeasure1.Measurement(3),\n\t\tmeasure2.Measurement(4),\n\t)\n\n\tsdk.Collect(ctx)\n\n\tout := batchTest.NewOutput(export.NewDefaultLabelEncoder())\n\tfor _, rec := range batcher.records {\n\t\t_ = out.AddTo(rec)\n\t}\n\trequire.EqualValues(t, map[string]float64{\n\t\t\"int64.counter/A=B,C=D\":   1,\n\t\t\"float64.counter/A=B,C=D\": 2,\n\t\t\"int64.measure/A=B,C=D\":   3,\n\t\t\"float64.measure/A=B,C=D\": 4,\n\t}, out.Map)\n}\n", "idx": 3, "id": 11928, "msg": "", "proj": "open-telemetry-opentelemetry-go", "lang": "go"}
{"patch": "@@ -507,16 +507,44 @@ func (s *Server) Stop() error {\n // OnStartupComplete lists the sites served by this server\n // and any relevant information, assuming caddy.Quiet == false.\n func (s *Server) OnStartupComplete() {\n+\n \tif caddy.Quiet {\n \t\treturn\n \t}\n+\n+\ts.OutputSiteInfo()\n+\n+}\n+\n+func (s *Server) OutputSiteInfo() {\n+\n+\tsitesByPort := make(map[string][]SiteConfig)\n \tfor _, site := range s.sites {\n-\t\toutput := site.Addr.String()\n-\t\tif caddy.IsLoopback(s.Address()) && !caddy.IsLoopback(site.Addr.Host) {\n-\t\t\toutput += \" (only accessible on this machine)\"\n+\t\t// Group Sites together by port before outputing\n+\t\tsitesArray := sitesByPort[\"port \"+site.Port()]\n+\t\tsitesArray = append(sitesArray, *site)\n+\t\tsitesByPort[\"port \"+site.Port()] = sitesArray\n+\t}\n+\n+\tfmt.Println(\" \")\n+\tfor key, asiteType := range sitesByPort {\n+\t\tsite := asiteType[0]\n+\t\tscheme := \"HTTP\"\n+\t\tif site.TLS.Enabled {\n+\t\t\tscheme = \"HTTPS\"\n+\t\t}\n+\n+\t\tfmt.Printf(\"Serving %s on \"+key+\" \\n\", scheme)\n+\n+\t\tfor _, site = range asiteType {\n+\t\t\toutput := site.Addr.String()\n+\t\t\tif caddy.IsLoopback(s.Address()) && !caddy.IsLoopback(site.Addr.Host) {\n+\t\t\t\toutput += \" (only accessible on this machine)\"\n+\t\t\t}\n+\t\t\tfmt.Println(output)\n+\t\t\tlog.Println(output)\n \t\t}\n-\t\tfmt.Println(output)\n-\t\tlog.Println(output)\n+\t\tfmt.Println(\" \")\n \t}\n }\n ", "y": 1, "oldf": "// Copyright 2015 Light Code Labs, LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package httpserver implements an HTTP server on top of Caddy.\npackage httpserver\n\nimport (\n\t\"context\"\n\t\"crypto/tls\"\n\t\"errors\"\n\t\"fmt\"\n\t\"log\"\n\t\"net\"\n\t\"net/http\"\n\t\"net/url\"\n\t\"os\"\n\t\"path\"\n\t\"path/filepath\"\n\t\"runtime\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/lucas-clemente/quic-go/h2quic\"\n\t\"github.com/mholt/caddy\"\n\t\"github.com/mholt/caddy/caddyhttp/staticfiles\"\n\t\"github.com/mholt/caddy/caddytls\"\n\t\"github.com/mholt/caddy/telemetry\"\n)\n\n// Server is the HTTP server implementation.\ntype Server struct {\n\tServer      *http.Server\n\tquicServer  *h2quic.Server\n\tlistener    net.Listener\n\tlistenerMu  sync.Mutex\n\tsites       []*SiteConfig\n\tconnTimeout time.Duration // max time to wait for a connection before force stop\n\ttlsGovChan  chan struct{} // close to stop the TLS maintenance goroutine\n\tvhosts      *vhostTrie\n}\n\n// ensure it satisfies the interface\nvar _ caddy.GracefulServer = new(Server)\n\nvar defaultALPN = []string{\"h2\", \"http/1.1\"}\n\n// makeTLSConfig extracts TLS settings from each site config to\n// build a tls.Config usable in Caddy HTTP servers. The returned\n// config will be nil if TLS is disabled for these sites.\nfunc makeTLSConfig(group []*SiteConfig) (*tls.Config, error) {\n\tvar tlsConfigs []*caddytls.Config\n\tfor i := range group {\n\t\tif HTTP2 && len(group[i].TLS.ALPN) == 0 {\n\t\t\t// if no application-level protocol was configured up to now,\n\t\t\t// default to HTTP/2, then HTTP/1.1 if necessary\n\t\t\tgroup[i].TLS.ALPN = defaultALPN\n\t\t}\n\t\ttlsConfigs = append(tlsConfigs, group[i].TLS)\n\t}\n\treturn caddytls.MakeTLSConfig(tlsConfigs)\n}\n\nfunc getFallbacks(sites []*SiteConfig) []string {\n\tfallbacks := []string{}\n\tfor _, sc := range sites {\n\t\tif sc.FallbackSite {\n\t\t\tfallbacks = append(fallbacks, sc.Addr.Host)\n\t\t}\n\t}\n\treturn fallbacks\n}\n\n// NewServer creates a new Server instance that will listen on addr\n// and will serve the sites configured in group.\nfunc NewServer(addr string, group []*SiteConfig) (*Server, error) {\n\ts := &Server{\n\t\tServer:      makeHTTPServerWithTimeouts(addr, group),\n\t\tvhosts:      newVHostTrie(),\n\t\tsites:       group,\n\t\tconnTimeout: GracefulTimeout,\n\t}\n\ts.vhosts.fallbackHosts = append(s.vhosts.fallbackHosts, getFallbacks(group)...)\n\ts.Server = makeHTTPServerWithHeaderLimit(s.Server, group)\n\ts.Server.Handler = s // this is weird, but whatever\n\n\t// extract TLS settings from each site config to build\n\t// a tls.Config, which will not be nil if TLS is enabled\n\ttlsConfig, err := makeTLSConfig(group)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\ts.Server.TLSConfig = tlsConfig\n\n\t// if TLS is enabled, make sure we prepare the Server accordingly\n\tif s.Server.TLSConfig != nil {\n\t\t// enable QUIC if desired (requires HTTP/2)\n\t\tif HTTP2 && QUIC {\n\t\t\ts.quicServer = &h2quic.Server{Server: s.Server}\n\t\t\ts.Server.Handler = s.wrapWithSvcHeaders(s.Server.Handler)\n\t\t}\n\n\t\t// wrap the HTTP handler with a handler that does MITM detection\n\t\ttlsh := &tlsHandler{next: s.Server.Handler}\n\t\ts.Server.Handler = tlsh // this needs to be the \"outer\" handler when Serve() is called, for type assertion\n\n\t\t// when Serve() creates the TLS listener later, that listener should\n\t\t// be adding a reference the ClientHello info to a map; this callback\n\t\t// will be sure to clear out that entry when the connection closes.\n\t\ts.Server.ConnState = func(c net.Conn, cs http.ConnState) {\n\t\t\t// when a connection closes or is hijacked, delete its entry\n\t\t\t// in the map, because we are done with it.\n\t\t\tif tlsh.listener != nil {\n\t\t\t\tif cs == http.StateHijacked || cs == http.StateClosed {\n\t\t\t\t\ttlsh.listener.helloInfosMu.Lock()\n\t\t\t\t\tdelete(tlsh.listener.helloInfos, c.RemoteAddr().String())\n\t\t\t\t\ttlsh.listener.helloInfosMu.Unlock()\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// As of Go 1.7, if the Server's TLSConfig is not nil, HTTP/2 is enabled only\n\t\t// if TLSConfig.NextProtos includes the string \"h2\"\n\t\tif HTTP2 && len(s.Server.TLSConfig.NextProtos) == 0 {\n\t\t\t// some experimenting shows that this NextProtos must have at least\n\t\t\t// one value that overlaps with the NextProtos of any other tls.Config\n\t\t\t// that is returned from GetConfigForClient; if there is no overlap,\n\t\t\t// the connection will fail (as of Go 1.8, Feb. 2017).\n\t\t\ts.Server.TLSConfig.NextProtos = defaultALPN\n\t\t}\n\t}\n\n\t// Compile custom middleware for every site (enables virtual hosting)\n\tfor _, site := range group {\n\t\tstack := Handler(staticfiles.FileServer{Root: http.Dir(site.Root), Hide: site.HiddenFiles, IndexPages: site.IndexPages})\n\t\tfor i := len(site.middleware) - 1; i >= 0; i-- {\n\t\t\tstack = site.middleware[i](stack)\n\t\t}\n\t\tsite.middlewareChain = stack\n\t\ts.vhosts.Insert(site.Addr.VHost(), site)\n\t}\n\n\treturn s, nil\n}\n\n// makeHTTPServerWithHeaderLimit apply minimum header limit within a group to given http.Server\nfunc makeHTTPServerWithHeaderLimit(s *http.Server, group []*SiteConfig) *http.Server {\n\tvar min int64\n\tfor _, cfg := range group {\n\t\tlimit := cfg.Limits.MaxRequestHeaderSize\n\t\tif limit == 0 {\n\t\t\tcontinue\n\t\t}\n\n\t\t// not set yet\n\t\tif min == 0 {\n\t\t\tmin = limit\n\t\t}\n\n\t\t// find a better one\n\t\tif limit < min {\n\t\t\tmin = limit\n\t\t}\n\t}\n\n\tif min > 0 {\n\t\ts.MaxHeaderBytes = int(min)\n\t}\n\treturn s\n}\n\n// makeHTTPServerWithTimeouts makes an http.Server from the group of\n// configs in a way that configures timeouts (or, if not set, it uses\n// the default timeouts) by combining the configuration of each\n// SiteConfig in the group. (Timeouts are important for mitigating\n// slowloris attacks.)\nfunc makeHTTPServerWithTimeouts(addr string, group []*SiteConfig) *http.Server {\n\t// find the minimum duration configured for each timeout\n\tvar min Timeouts\n\tfor _, cfg := range group {\n\t\tif cfg.Timeouts.ReadTimeoutSet &&\n\t\t\t(!min.ReadTimeoutSet || cfg.Timeouts.ReadTimeout < min.ReadTimeout) {\n\t\t\tmin.ReadTimeoutSet = true\n\t\t\tmin.ReadTimeout = cfg.Timeouts.ReadTimeout\n\t\t}\n\t\tif cfg.Timeouts.ReadHeaderTimeoutSet &&\n\t\t\t(!min.ReadHeaderTimeoutSet || cfg.Timeouts.ReadHeaderTimeout < min.ReadHeaderTimeout) {\n\t\t\tmin.ReadHeaderTimeoutSet = true\n\t\t\tmin.ReadHeaderTimeout = cfg.Timeouts.ReadHeaderTimeout\n\t\t}\n\t\tif cfg.Timeouts.WriteTimeoutSet &&\n\t\t\t(!min.WriteTimeoutSet || cfg.Timeouts.WriteTimeout < min.WriteTimeout) {\n\t\t\tmin.WriteTimeoutSet = true\n\t\t\tmin.WriteTimeout = cfg.Timeouts.WriteTimeout\n\t\t}\n\t\tif cfg.Timeouts.IdleTimeoutSet &&\n\t\t\t(!min.IdleTimeoutSet || cfg.Timeouts.IdleTimeout < min.IdleTimeout) {\n\t\t\tmin.IdleTimeoutSet = true\n\t\t\tmin.IdleTimeout = cfg.Timeouts.IdleTimeout\n\t\t}\n\t}\n\n\t// for the values that were not set, use defaults\n\tif !min.ReadTimeoutSet {\n\t\tmin.ReadTimeout = defaultTimeouts.ReadTimeout\n\t}\n\tif !min.ReadHeaderTimeoutSet {\n\t\tmin.ReadHeaderTimeout = defaultTimeouts.ReadHeaderTimeout\n\t}\n\tif !min.WriteTimeoutSet {\n\t\tmin.WriteTimeout = defaultTimeouts.WriteTimeout\n\t}\n\tif !min.IdleTimeoutSet {\n\t\tmin.IdleTimeout = defaultTimeouts.IdleTimeout\n\t}\n\n\t// set the final values on the server and return it\n\treturn &http.Server{\n\t\tAddr:              addr,\n\t\tReadTimeout:       min.ReadTimeout,\n\t\tReadHeaderTimeout: min.ReadHeaderTimeout,\n\t\tWriteTimeout:      min.WriteTimeout,\n\t\tIdleTimeout:       min.IdleTimeout,\n\t}\n}\n\nfunc (s *Server) wrapWithSvcHeaders(previousHandler http.Handler) http.HandlerFunc {\n\treturn func(w http.ResponseWriter, r *http.Request) {\n\t\ts.quicServer.SetQuicHeaders(w.Header())\n\t\tpreviousHandler.ServeHTTP(w, r)\n\t}\n}\n\n// Listen creates an active listener for s that can be\n// used to serve requests.\nfunc (s *Server) Listen() (net.Listener, error) {\n\tif s.Server == nil {\n\t\treturn nil, fmt.Errorf(\"Server field is nil\")\n\t}\n\n\tln, err := net.Listen(\"tcp\", s.Server.Addr)\n\tif err != nil {\n\t\tvar succeeded bool\n\t\tif runtime.GOOS == \"windows\" {\n\t\t\t// Windows has been known to keep sockets open even after closing the listeners.\n\t\t\t// Tests reveal this error case easily because they call Start() then Stop()\n\t\t\t// in succession. TODO: Better way to handle this? And why limit this to Windows?\n\t\t\tfor i := 0; i < 20; i++ {\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\tln, err = net.Listen(\"tcp\", s.Server.Addr)\n\t\t\t\tif err == nil {\n\t\t\t\t\tsucceeded = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif !succeeded {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif tcpLn, ok := ln.(*net.TCPListener); ok {\n\t\tln = tcpKeepAliveListener{TCPListener: tcpLn}\n\t}\n\n\tcln := s.WrapListener(ln)\n\n\t// Very important to return a concrete caddy.Listener\n\t// implementation for graceful restarts.\n\treturn cln.(caddy.Listener), nil\n}\n\n// WrapListener wraps ln in the listener middlewares configured\n// for this server.\nfunc (s *Server) WrapListener(ln net.Listener) net.Listener {\n\tif ln == nil {\n\t\treturn nil\n\t}\n\tcln := ln.(caddy.Listener)\n\tfor _, site := range s.sites {\n\t\tfor _, m := range site.listenerMiddleware {\n\t\t\tcln = m(cln)\n\t\t}\n\t}\n\treturn cln\n}\n\n// ListenPacket creates udp connection for QUIC if it is enabled,\nfunc (s *Server) ListenPacket() (net.PacketConn, error) {\n\tif QUIC {\n\t\tudpAddr, err := net.ResolveUDPAddr(\"udp\", s.Server.Addr)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\treturn net.ListenUDP(\"udp\", udpAddr)\n\t}\n\treturn nil, nil\n}\n\n// Serve serves requests on ln. It blocks until ln is closed.\nfunc (s *Server) Serve(ln net.Listener) error {\n\ts.listenerMu.Lock()\n\ts.listener = ln\n\ts.listenerMu.Unlock()\n\n\tif s.Server.TLSConfig != nil {\n\t\t// Create TLS listener - note that we do not replace s.listener\n\t\t// with this TLS listener; tls.listener is unexported and does\n\t\t// not implement the File() method we need for graceful restarts\n\t\t// on POSIX systems.\n\t\t// TODO: Is this ^ still relevant anymore? Maybe we can now that it's a net.Listener...\n\t\tln = newTLSListener(ln, s.Server.TLSConfig)\n\t\tif handler, ok := s.Server.Handler.(*tlsHandler); ok {\n\t\t\thandler.listener = ln.(*tlsHelloListener)\n\t\t}\n\n\t\t// Rotate TLS session ticket keys\n\t\ts.tlsGovChan = caddytls.RotateSessionTicketKeys(s.Server.TLSConfig)\n\t}\n\n\terr := s.Server.Serve(ln)\n\tif err == http.ErrServerClosed {\n\t\terr = nil // not an error worth reporting since closing a server is intentional\n\t}\n\tif s.quicServer != nil {\n\t\ts.quicServer.Close()\n\t}\n\treturn err\n}\n\n// ServePacket serves QUIC requests on pc until it is closed.\nfunc (s *Server) ServePacket(pc net.PacketConn) error {\n\tif s.quicServer != nil {\n\t\terr := s.quicServer.Serve(pc.(*net.UDPConn))\n\t\treturn fmt.Errorf(\"serving QUIC connections: %v\", err)\n\t}\n\treturn nil\n}\n\n// ServeHTTP is the entry point of all HTTP requests.\nfunc (s *Server) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\tdefer func() {\n\t\t// We absolutely need to be sure we stay alive up here,\n\t\t// even though, in theory, the errors middleware does this.\n\t\tif rec := recover(); rec != nil {\n\t\t\tlog.Printf(\"[PANIC] %v\", rec)\n\t\t\tDefaultErrorFunc(w, r, http.StatusInternalServerError)\n\t\t}\n\t}()\n\n\t// record the User-Agent string (with a cap on its length to mitigate attacks)\n\tua := r.Header.Get(\"User-Agent\")\n\tif len(ua) > 512 {\n\t\tua = ua[:512]\n\t}\n\tuaHash := telemetry.FastHash([]byte(ua)) // this is a normalized field\n\tgo telemetry.SetNested(\"http_user_agent\", uaHash, ua)\n\tgo telemetry.AppendUnique(\"http_user_agent_count\", uaHash)\n\tgo telemetry.Increment(\"http_request_count\")\n\n\t// copy the original, unchanged URL into the context\n\t// so it can be referenced by middlewares\n\turlCopy := *r.URL\n\tif r.URL.User != nil {\n\t\tuserInfo := new(url.Userinfo)\n\t\t*userInfo = *r.URL.User\n\t\turlCopy.User = userInfo\n\t}\n\tc := context.WithValue(r.Context(), OriginalURLCtxKey, urlCopy)\n\tr = r.WithContext(c)\n\n\t// Setup a replacer for the request that keeps track of placeholder\n\t// values across plugins.\n\treplacer := NewReplacer(r, nil, \"\")\n\tc = context.WithValue(r.Context(), ReplacerCtxKey, replacer)\n\tr = r.WithContext(c)\n\n\tw.Header().Set(\"Server\", caddy.AppName)\n\n\tstatus, _ := s.serveHTTP(w, r)\n\n\t// Fallback error response in case error handling wasn't chained in\n\tif status >= 400 {\n\t\tDefaultErrorFunc(w, r, status)\n\t}\n}\n\nfunc (s *Server) serveHTTP(w http.ResponseWriter, r *http.Request) (int, error) {\n\t// strip out the port because it's not used in virtual\n\t// hosting; the port is irrelevant because each listener\n\t// is on a different port.\n\thostname, _, err := net.SplitHostPort(r.Host)\n\tif err != nil {\n\t\thostname = r.Host\n\t}\n\n\t// look up the virtualhost; if no match, serve error\n\tvhost, pathPrefix := s.vhosts.Match(hostname + r.URL.Path)\n\tc := context.WithValue(r.Context(), caddy.CtxKey(\"path_prefix\"), pathPrefix)\n\tr = r.WithContext(c)\n\n\tif vhost == nil {\n\t\t// check for ACME challenge even if vhost is nil;\n\t\t// could be a new host coming online soon - choose any\n\t\t// vhost's cert manager configuration, I guess\n\t\tif len(s.sites) > 0 && s.sites[0].TLS.Manager.HandleHTTPChallenge(w, r) {\n\t\t\treturn 0, nil\n\t\t}\n\n\t\t// otherwise, log the error and write a message to the client\n\t\tremoteHost, _, err := net.SplitHostPort(r.RemoteAddr)\n\t\tif err != nil {\n\t\t\tremoteHost = r.RemoteAddr\n\t\t}\n\t\tWriteSiteNotFound(w, r) // don't add headers outside of this function (http.forwardproxy)\n\t\tlog.Printf(\"[INFO] %s - No such site at %s (Remote: %s, Referer: %s)\",\n\t\t\thostname, s.Server.Addr, remoteHost, r.Header.Get(\"Referer\"))\n\t\treturn 0, nil\n\t}\n\n\t// we still check for ACME challenge if the vhost exists,\n\t// because the HTTP challenge might be disabled by its config\n\tif vhost.TLS.Manager.HandleHTTPChallenge(w, r) {\n\t\treturn 0, nil\n\t}\n\n\t// trim the path portion of the site address from the beginning of\n\t// the URL path, so a request to example.com/foo/blog on the site\n\t// defined as example.com/foo appears as /blog instead of /foo/blog.\n\tif pathPrefix != \"/\" {\n\t\tr.URL = trimPathPrefix(r.URL, pathPrefix)\n\t}\n\n\t// enforce strict host matching, which ensures that the SNI\n\t// value (if any), matches the Host header; essential for\n\t// sites that rely on TLS ClientAuth sharing a port with\n\t// sites that do not - if mismatched, close the connection\n\tif vhost.StrictHostMatching && r.TLS != nil &&\n\t\tstrings.ToLower(r.TLS.ServerName) != strings.ToLower(hostname) {\n\t\tr.Close = true\n\t\tlog.Printf(\"[ERROR] %s - strict host matching: SNI (%s) and HTTP Host (%s) values differ\",\n\t\t\tvhost.Addr, r.TLS.ServerName, hostname)\n\t\treturn http.StatusForbidden, nil\n\t}\n\n\treturn vhost.middlewareChain.ServeHTTP(w, r)\n}\n\nfunc trimPathPrefix(u *url.URL, prefix string) *url.URL {\n\t// We need to use URL.EscapedPath() when trimming the pathPrefix as\n\t// URL.Path is ambiguous about / or %2f - see docs. See #1927\n\ttrimmedPath := strings.TrimPrefix(u.EscapedPath(), prefix)\n\tif !strings.HasPrefix(trimmedPath, \"/\") {\n\t\ttrimmedPath = \"/\" + trimmedPath\n\t}\n\t// After trimming path reconstruct uri string with Query before parsing\n\ttrimmedURI := trimmedPath\n\tif u.RawQuery != \"\" || u.ForceQuery == true {\n\t\ttrimmedURI = trimmedPath + \"?\" + u.RawQuery\n\t}\n\tif u.Fragment != \"\" {\n\t\ttrimmedURI = trimmedURI + \"#\" + u.Fragment\n\t}\n\ttrimmedURL, err := url.Parse(trimmedURI)\n\tif err != nil {\n\t\tlog.Printf(\"[ERROR] Unable to parse trimmed URL %s: %v\", trimmedURI, err)\n\t\treturn u\n\t}\n\treturn trimmedURL\n}\n\n// Address returns the address s was assigned to listen on.\nfunc (s *Server) Address() string {\n\treturn s.Server.Addr\n}\n\n// Stop stops s gracefully (or forcefully after timeout) and\n// closes its listener.\nfunc (s *Server) Stop() error {\n\tctx, cancel := context.WithTimeout(context.Background(), s.connTimeout)\n\tdefer cancel()\n\n\terr := s.Server.Shutdown(ctx)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// signal any TLS governor goroutines to exit\n\tif s.tlsGovChan != nil {\n\t\tclose(s.tlsGovChan)\n\t}\n\n\treturn nil\n}\n\n// OnStartupComplete lists the sites served by this server\n// and any relevant information, assuming caddy.Quiet == false.\nfunc (s *Server) OnStartupComplete() {\n\tif caddy.Quiet {\n\t\treturn\n\t}\n\tfor _, site := range s.sites {\n\t\toutput := site.Addr.String()\n\t\tif caddy.IsLoopback(s.Address()) && !caddy.IsLoopback(site.Addr.Host) {\n\t\t\toutput += \" (only accessible on this machine)\"\n\t\t}\n\t\tfmt.Println(output)\n\t\tlog.Println(output)\n\t}\n}\n\n// defaultTimeouts stores the default timeout values to use\n// if left unset by user configuration. NOTE: Most default\n// timeouts are disabled (see issues #1464 and #1733).\nvar defaultTimeouts = Timeouts{IdleTimeout: 5 * time.Minute}\n\n// tcpKeepAliveListener sets TCP keep-alive timeouts on accepted\n// connections. It's used by ListenAndServe and ListenAndServeTLS so\n// dead TCP connections (e.g. closing laptop mid-download) eventually\n// go away.\n//\n// Borrowed from the Go standard library.\ntype tcpKeepAliveListener struct {\n\t*net.TCPListener\n}\n\n// Accept accepts the connection with a keep-alive enabled.\nfunc (ln tcpKeepAliveListener) Accept() (c net.Conn, err error) {\n\ttc, err := ln.AcceptTCP()\n\tif err != nil {\n\t\treturn\n\t}\n\ttc.SetKeepAlive(true)\n\ttc.SetKeepAlivePeriod(3 * time.Minute)\n\treturn tc, nil\n}\n\n// File implements caddy.Listener; it returns the underlying file of the listener.\nfunc (ln tcpKeepAliveListener) File() (*os.File, error) {\n\treturn ln.TCPListener.File()\n}\n\n// ErrMaxBytesExceeded is the error returned by MaxBytesReader\n// when the request body exceeds the limit imposed\nvar ErrMaxBytesExceeded = errors.New(\"http: request body too large\")\n\n// DefaultErrorFunc responds to an HTTP request with a simple description\n// of the specified HTTP status code.\nfunc DefaultErrorFunc(w http.ResponseWriter, r *http.Request, status int) {\n\tWriteTextResponse(w, status, fmt.Sprintf(\"%d %s\\n\", status, http.StatusText(status)))\n}\n\nconst httpStatusMisdirectedRequest = 421 // RFC 7540, 9.1.2\n\n// WriteSiteNotFound writes appropriate error code to w, signaling that\n// requested host is not served by Caddy on a given port.\nfunc WriteSiteNotFound(w http.ResponseWriter, r *http.Request) {\n\tstatus := http.StatusNotFound\n\tif r.ProtoMajor >= 2 {\n\t\t// TODO: use http.StatusMisdirectedRequest when it gets defined\n\t\tstatus = httpStatusMisdirectedRequest\n\t}\n\tWriteTextResponse(w, status, fmt.Sprintf(\"%d Site %s is not served on this interface\\n\", status, r.Host))\n}\n\n// WriteTextResponse writes body with code status to w. The body will\n// be interpreted as plain text.\nfunc WriteTextResponse(w http.ResponseWriter, status int, body string) {\n\tw.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n\tw.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n\tw.WriteHeader(status)\n\tw.Write([]byte(body))\n}\n\n// SafePath joins siteRoot and reqPath and converts it to a path that can\n// be used to access a path on the local disk. It ensures the path does\n// not traverse outside of the site root.\n//\n// If opening a file, use http.Dir instead.\nfunc SafePath(siteRoot, reqPath string) string {\n\treqPath = filepath.ToSlash(reqPath)\n\treqPath = strings.Replace(reqPath, \"\\x00\", \"\", -1) // NOTE: Go 1.9 checks for null bytes in the syscall package\n\tif siteRoot == \"\" {\n\t\tsiteRoot = \".\"\n\t}\n\treturn filepath.Join(siteRoot, filepath.FromSlash(path.Clean(\"/\"+reqPath)))\n}\n\n// OriginalURLCtxKey is the key for accessing the original, incoming URL on an HTTP request.\nconst OriginalURLCtxKey = caddy.CtxKey(\"original_url\")\n", "idx": 1, "id": 13063, "msg": "Remove extra empty lines; these are very small, simple lines of code, so the extra lines look, well, empty. :)", "proj": "caddyserver-caddy", "lang": "go"}
{"patch": "@@ -35,7 +35,7 @@ PairLJClass2::PairLJClass2(LAMMPS *lmp) : Pair(lmp)\n {\n   respa_enable = 1;\n   writedata = 1;\n-  centroidstressflag = 1;\n+  centroidstressflag = CENTROID_SAME;\n }\n \n /* ---------------------------------------------------------------------- */", "y": 1, "oldf": "/* ----------------------------------------------------------------------\n   LAMMPS - Large-scale Atomic/Molecular Massively Parallel Simulator\n   https://lammps.sandia.gov/, Sandia National Laboratories\n   Steve Plimpton, sjplimp@sandia.gov\n   Copyright (2003) Sandia Corporation.  Under the terms of Contract\n   DE-AC04-94AL85000 with Sandia Corporation, the U.S. Government retains\n   certain rights in this software.  This software is distributed under\n   the GNU General Public License.\n   See the README file in the top-level LAMMPS directory.\n------------------------------------------------------------------------- */\n\n#include \"pair_lj_class2.h\"\n\n#include <cmath>\n#include <cstring>\n#include \"atom.h\"\n#include \"comm.h\"\n#include \"force.h\"\n#include \"neighbor.h\"\n#include \"neigh_list.h\"\n#include \"neigh_request.h\"\n#include \"update.h\"\n#include \"respa.h\"\n#include \"math_const.h\"\n#include \"memory.h\"\n#include \"error.h\"\n\n\nusing namespace LAMMPS_NS;\nusing namespace MathConst;\n\n/* ---------------------------------------------------------------------- */\n\nPairLJClass2::PairLJClass2(LAMMPS *lmp) : Pair(lmp)\n{\n  respa_enable = 1;\n  writedata = 1;\n  centroidstressflag = 1;\n}\n\n/* ---------------------------------------------------------------------- */\n\nPairLJClass2::~PairLJClass2()\n{\n  if (copymode) return;\n\n  if (allocated) {\n    memory->destroy(setflag);\n    memory->destroy(cutsq);\n\n    memory->destroy(cut);\n    memory->destroy(epsilon);\n    memory->destroy(sigma);\n    memory->destroy(lj1);\n    memory->destroy(lj2);\n    memory->destroy(lj3);\n    memory->destroy(lj4);\n    memory->destroy(offset);\n  }\n}\n\n/* ---------------------------------------------------------------------- */\n\nvoid PairLJClass2::compute(int eflag, int vflag)\n{\n  int i,j,ii,jj,inum,jnum,itype,jtype;\n  double xtmp,ytmp,ztmp,delx,dely,delz,evdwl,fpair;\n  double rsq,rinv,r2inv,r3inv,r6inv,forcelj,factor_lj;\n  int *ilist,*jlist,*numneigh,**firstneigh;\n\n  evdwl = 0.0;\n  ev_init(eflag,vflag);\n\n  double **x = atom->x;\n  double **f = atom->f;\n  int *type = atom->type;\n  int nlocal = atom->nlocal;\n  double *special_lj = force->special_lj;\n  int newton_pair = force->newton_pair;\n\n  inum = list->inum;\n  ilist = list->ilist;\n  numneigh = list->numneigh;\n  firstneigh = list->firstneigh;\n\n  // loop over neighbors of my atoms\n\n  for (ii = 0; ii < inum; ii++) {\n    i = ilist[ii];\n    xtmp = x[i][0];\n    ytmp = x[i][1];\n    ztmp = x[i][2];\n    itype = type[i];\n    jlist = firstneigh[i];\n    jnum = numneigh[i];\n\n    for (jj = 0; jj < jnum; jj++) {\n      j = jlist[jj];\n      factor_lj = special_lj[sbmask(j)];\n      j &= NEIGHMASK;\n\n      delx = xtmp - x[j][0];\n      dely = ytmp - x[j][1];\n      delz = ztmp - x[j][2];\n      rsq = delx*delx + dely*dely + delz*delz;\n      jtype = type[j];\n\n      if (rsq < cutsq[itype][jtype]) {\n        r2inv = 1.0/rsq;\n        rinv = sqrt(r2inv);\n        r3inv = r2inv*rinv;\n        r6inv = r3inv*r3inv;\n        forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n        fpair = factor_lj*forcelj*r2inv;\n\n        f[i][0] += delx*fpair;\n        f[i][1] += dely*fpair;\n        f[i][2] += delz*fpair;\n        if (newton_pair || j < nlocal) {\n          f[j][0] -= delx*fpair;\n          f[j][1] -= dely*fpair;\n          f[j][2] -= delz*fpair;\n        }\n\n        if (eflag) {\n          evdwl = r6inv*(lj3[itype][jtype]*r3inv-lj4[itype][jtype]) -\n            offset[itype][jtype];\n          evdwl *= factor_lj;\n        }\n\n        if (evflag) ev_tally(i,j,nlocal,newton_pair,\n                             evdwl,0.0,fpair,delx,dely,delz);\n      }\n    }\n  }\n\n  if (vflag_fdotr) virial_fdotr_compute();\n}\n\n/* ----------------------------------------------------------------------\n*/\n\nvoid PairLJClass2::compute_inner()\n{\n  int i,j,ii,jj,inum,jnum,itype,jtype;\n  double xtmp,ytmp,ztmp,delx,dely,delz,fpair;\n  double rsq,rinv,r2inv,r3inv,r6inv,forcelj,factor_lj,rsw;\n  int *ilist,*jlist,*numneigh,**firstneigh;\n\n  double **x = atom->x;\n  double **f = atom->f;\n  int *type = atom->type;\n  int nlocal = atom->nlocal;\n  double *special_lj = force->special_lj;\n  int newton_pair = force->newton_pair;\n\n  inum = list->inum_inner;\n  ilist = list->ilist_inner;\n  numneigh = list->numneigh_inner;\n  firstneigh = list->firstneigh_inner;\n\n  double cut_out_on = cut_respa[0];\n  double cut_out_off = cut_respa[1];\n\n  double cut_out_diff = cut_out_off - cut_out_on;\n  double cut_out_on_sq = cut_out_on*cut_out_on;\n  double cut_out_off_sq = cut_out_off*cut_out_off;\n\n  // loop over neighbors of my atoms\n\n  for (ii = 0; ii < inum; ii++) {\n    i = ilist[ii];\n    xtmp = x[i][0];\n    ytmp = x[i][1];\n    ztmp = x[i][2];\n    itype = type[i];\n    jlist = firstneigh[i];\n    jnum = numneigh[i];\n\n    for (jj = 0; jj < jnum; jj++) {\n      j = jlist[jj];\n      factor_lj = special_lj[sbmask(j)];\n      j &= NEIGHMASK;\n\n      delx = xtmp - x[j][0];\n      dely = ytmp - x[j][1];\n      delz = ztmp - x[j][2];\n      rsq = delx*delx + dely*dely + delz*delz;\n\n      if (rsq < cut_out_off_sq) {\n        r2inv = 1.0/rsq;\n                rinv = sqrt(r2inv);\n                r3inv = r2inv*rinv;\n        r6inv = r3inv*r3inv;\n        jtype = type[j];\n        forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n        fpair = factor_lj*forcelj*r2inv;\n        if (rsq > cut_out_on_sq) {\n          rsw = (sqrt(rsq) - cut_out_on)/cut_out_diff;\n          fpair *= 1.0 - rsw*rsw*(3.0 - 2.0*rsw);\n        }\n\n        f[i][0] += delx*fpair;\n        f[i][1] += dely*fpair;\n        f[i][2] += delz*fpair;\n        if (newton_pair || j < nlocal) {\n          f[j][0] -= delx*fpair;\n          f[j][1] -= dely*fpair;\n          f[j][2] -= delz*fpair;\n        }\n      }\n    }\n  }\n}\n\n/* ---------------------------------------------------------------------- */\n\nvoid PairLJClass2::compute_middle()\n{\n  int i,j,ii,jj,inum,jnum,itype,jtype;\n  double xtmp,ytmp,ztmp,delx,dely,delz,fpair;\n  double rsq,rinv,r2inv,r3inv,r6inv,forcelj,factor_lj,rsw;\n  int *ilist,*jlist,*numneigh,**firstneigh;\n\n  double **x = atom->x;\n  double **f = atom->f;\n  int *type = atom->type;\n  int nlocal = atom->nlocal;\n  double *special_lj = force->special_lj;\n  int newton_pair = force->newton_pair;\n\n  inum = list->inum_middle;\n  ilist = list->ilist_middle;\n  numneigh = list->numneigh_middle;\n  firstneigh = list->firstneigh_middle;\n\n  double cut_in_off = cut_respa[0];\n  double cut_in_on = cut_respa[1];\n  double cut_out_on = cut_respa[2];\n  double cut_out_off = cut_respa[3];\n\n  double cut_in_diff = cut_in_on - cut_in_off;\n  double cut_out_diff = cut_out_off - cut_out_on;\n  double cut_in_off_sq = cut_in_off*cut_in_off;\n  double cut_in_on_sq = cut_in_on*cut_in_on;\n  double cut_out_on_sq = cut_out_on*cut_out_on;\n  double cut_out_off_sq = cut_out_off*cut_out_off;\n\n  // loop over neighbors of my atoms\n\n  for (ii = 0; ii < inum; ii++) {\n    i = ilist[ii];\n    xtmp = x[i][0];\n    ytmp = x[i][1];\n    ztmp = x[i][2];\n    itype = type[i];\n    jlist = firstneigh[i];\n    jnum = numneigh[i];\n\n    for (jj = 0; jj < jnum; jj++) {\n      j = jlist[jj];\n      factor_lj = special_lj[sbmask(j)];\n      j &= NEIGHMASK;\n\n      delx = xtmp - x[j][0];\n      dely = ytmp - x[j][1];\n      delz = ztmp - x[j][2];\n      rsq = delx*delx + dely*dely + delz*delz;\n\n      if (rsq < cut_out_off_sq && rsq > cut_in_off_sq) {\n        r2inv = 1.0/rsq;\n                rinv = sqrt(r2inv);\n                r3inv = r2inv*rinv;\n        r6inv = r3inv*r3inv;\n        jtype = type[j];\n        forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n        fpair = factor_lj*forcelj*r2inv;\n        if (rsq < cut_in_on_sq) {\n          rsw = (sqrt(rsq) - cut_in_off)/cut_in_diff;\n          fpair *= rsw*rsw*(3.0 - 2.0*rsw);\n        }\n        if (rsq > cut_out_on_sq) {\n          rsw = (sqrt(rsq) - cut_out_on)/cut_out_diff;\n          fpair *= 1.0 + rsw*rsw*(2.0*rsw - 3.0);\n        }\n\n        f[i][0] += delx*fpair;\n        f[i][1] += dely*fpair;\n        f[i][2] += delz*fpair;\n        if (newton_pair || j < nlocal) {\n          f[j][0] -= delx*fpair;\n          f[j][1] -= dely*fpair;\n          f[j][2] -= delz*fpair;\n        }\n      }\n    }\n  }\n}\n\n/* ---------------------------------------------------------------------- */\n\nvoid PairLJClass2::compute_outer(int eflag, int vflag)\n{\n  int i,j,ii,jj,inum,jnum,itype,jtype;\n  double xtmp,ytmp,ztmp,delx,dely,delz,evdwl,fpair;\n  double rsq,rinv,r2inv,r3inv,r6inv,forcelj,factor_lj,rsw;\n  int *ilist,*jlist,*numneigh,**firstneigh;\n\n  evdwl = 0.0;\n  ev_init(eflag,vflag);\n\n  double **x = atom->x;\n  double **f = atom->f;\n  int *type = atom->type;\n  int nlocal = atom->nlocal;\n  double *special_lj = force->special_lj;\n  int newton_pair = force->newton_pair;\n\n  inum = list->inum;\n  ilist = list->ilist;\n  numneigh = list->numneigh;\n  firstneigh = list->firstneigh;\n\n  double cut_in_off = cut_respa[2];\n  double cut_in_on = cut_respa[3];\n\n  double cut_in_diff = cut_in_on - cut_in_off;\n  double cut_in_off_sq = cut_in_off*cut_in_off;\n  double cut_in_on_sq = cut_in_on*cut_in_on;\n\n  // loop over neighbors of my atoms\n\n  for (ii = 0; ii < inum; ii++) {\n    i = ilist[ii];\n    xtmp = x[i][0];\n    ytmp = x[i][1];\n    ztmp = x[i][2];\n    itype = type[i];\n    jlist = firstneigh[i];\n    jnum = numneigh[i];\n\n    for (jj = 0; jj < jnum; jj++) {\n      j = jlist[jj];\n      factor_lj = special_lj[sbmask(j)];\n      j &= NEIGHMASK;\n\n      delx = xtmp - x[j][0];\n      dely = ytmp - x[j][1];\n      delz = ztmp - x[j][2];\n      rsq = delx*delx + dely*dely + delz*delz;\n      jtype = type[j];\n\n      if (rsq < cutsq[itype][jtype]) {\n        if (rsq > cut_in_off_sq) {\n          r2inv = 1.0/rsq;\n                  rinv = sqrt(r2inv);\n                  r3inv = r2inv*rinv;\n          r6inv = r3inv*r3inv;\n          forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n          fpair = factor_lj*forcelj*r2inv;\n          if (rsq < cut_in_on_sq) {\n            rsw = (sqrt(rsq) - cut_in_off)/cut_in_diff;\n            fpair *= rsw*rsw*(3.0 - 2.0*rsw);\n          }\n\n          f[i][0] += delx*fpair;\n          f[i][1] += dely*fpair;\n          f[i][2] += delz*fpair;\n          if (newton_pair || j < nlocal) {\n            f[j][0] -= delx*fpair;\n            f[j][1] -= dely*fpair;\n            f[j][2] -= delz*fpair;\n          }\n        }\n\n        if (eflag) {\n          r2inv = 1.0/rsq;\n                  rinv = sqrt(r2inv);\n                  r3inv = r2inv*rinv;\n          r6inv = r3inv*r3inv;\n          evdwl = r6inv*(lj3[itype][jtype]*r3inv-lj4[itype][jtype]) -\n            offset[itype][jtype];\n          evdwl *= factor_lj;\n        }\n\n        if (vflag) {\n          if (rsq <= cut_in_off_sq) {\n            r2inv = 1.0/rsq;\n                        rinv = sqrt(r2inv);\n                        r3inv = r2inv*rinv;\n            r6inv = r3inv*r3inv;\n            forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n            fpair = factor_lj*forcelj*r2inv;\n          } else if (rsq < cut_in_on_sq)\n            fpair = factor_lj*forcelj*r2inv;\n        }\n\n        if (evflag) ev_tally(i,j,nlocal,newton_pair,\n                             evdwl,0.0,fpair,delx,dely,delz);\n      }\n    }\n  }\n}\n/* ----------------------------------------------------------------------\n   allocate all arrays\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::allocate()\n{\n  allocated = 1;\n  int n = atom->ntypes;\n\n  memory->create(setflag,n+1,n+1,\"pair:setflag\");\n  for (int i = 1; i <= n; i++)\n    for (int j = i; j <= n; j++)\n      setflag[i][j] = 0;\n\n  memory->create(cutsq,n+1,n+1,\"pair:cutsq\");\n\n  memory->create(cut,n+1,n+1,\"pair:cut\");\n  memory->create(epsilon,n+1,n+1,\"pair:epsilon\");\n  memory->create(sigma,n+1,n+1,\"pair:sigma\");\n  memory->create(lj1,n+1,n+1,\"pair:lj1\");\n  memory->create(lj2,n+1,n+1,\"pair:lj2\");\n  memory->create(lj3,n+1,n+1,\"pair:lj3\");\n  memory->create(lj4,n+1,n+1,\"pair:lj4\");\n  memory->create(offset,n+1,n+1,\"pair:offset\");\n}\n\n/* ----------------------------------------------------------------------\n   global settings\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::settings(int narg, char **arg)\n{\n  if (narg != 1) error->all(FLERR,\"Illegal pair_style command\");\n\n  cut_global = utils::numeric(FLERR,arg[0],false,lmp);\n\n  // reset cutoffs that have been explicitly set\n\n  if (allocated) {\n    int i,j;\n    for (i = 1; i <= atom->ntypes; i++)\n      for (j = i; j <= atom->ntypes; j++)\n        if (setflag[i][j]) cut[i][j] = cut_global;\n  }\n}\n\n/* ----------------------------------------------------------------------\n   set coeffs for one or more type pairs\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::coeff(int narg, char **arg)\n{\n  if (narg < 4 || narg > 5) error->all(FLERR,\"Incorrect args for pair coefficients\");\n  if (!allocated) allocate();\n\n  int ilo,ihi,jlo,jhi;\n  utils::bounds(FLERR,arg[0],1,atom->ntypes,ilo,ihi,error);\n  utils::bounds(FLERR,arg[1],1,atom->ntypes,jlo,jhi,error);\n\n  double epsilon_one = utils::numeric(FLERR,arg[2],false,lmp);\n  double sigma_one = utils::numeric(FLERR,arg[3],false,lmp);\n\n  double cut_one = cut_global;\n  if (narg == 5) cut_one = utils::numeric(FLERR,arg[4],false,lmp);\n\n  int count = 0;\n  for (int i = ilo; i <= ihi; i++) {\n    for (int j = MAX(jlo,i); j <= jhi; j++) {\n      epsilon[i][j] = epsilon_one;\n      sigma[i][j] = sigma_one;\n      cut[i][j] = cut_one;\n      setflag[i][j] = 1;\n      count++;\n    }\n  }\n\n  if (count == 0) error->all(FLERR,\"Incorrect args for pair coefficients\");\n}\n\n/* ----------------------------------------------------------------------\n   init specific to this pair style\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::init_style()\n{\n  // request regular or rRESPA neighbor list\n\n  int irequest;\n  int respa = 0;\n\n  if (update->whichflag == 1 && strstr(update->integrate_style,\"respa\")) {\n    if (((Respa *) update->integrate)->level_inner >= 0) respa = 1;\n    if (((Respa *) update->integrate)->level_middle >= 0) respa = 2;\n  }\n\n  irequest = neighbor->request(this,instance_me);\n\n  if (respa >= 1) {\n    neighbor->requests[irequest]->respaouter = 1;\n    neighbor->requests[irequest]->respainner = 1;\n  }\n  if (respa == 2) neighbor->requests[irequest]->respamiddle = 1;\n\n  // set rRESPA cutoffs\n\n  if (strstr(update->integrate_style,\"respa\") &&\n      ((Respa *) update->integrate)->level_inner >= 0)\n    cut_respa = ((Respa *) update->integrate)->cutoff;\n  else cut_respa = nullptr;\n}\n\n/* ----------------------------------------------------------------------\n   init for one type pair i,j and corresponding j,i\n------------------------------------------------------------------------- */\n\ndouble PairLJClass2::init_one(int i, int j)\n{\n  // always mix epsilon,sigma via sixthpower rules\n  // mix distance via user-defined rule\n\n  if (setflag[i][j] == 0) {\n    epsilon[i][j] = 2.0 * sqrt(epsilon[i][i]*epsilon[j][j]) *\n      pow(sigma[i][i],3.0) * pow(sigma[j][j],3.0) /\n      (pow(sigma[i][i],6.0) + pow(sigma[j][j],6.0));\n    sigma[i][j] =\n      pow((0.5 * (pow(sigma[i][i],6.0) + pow(sigma[j][j],6.0))),1.0/6.0);\n    cut[i][j] = mix_distance(cut[i][i],cut[j][j]);\n  }\n\n  lj1[i][j] = 18.0 * epsilon[i][j] * pow(sigma[i][j],9.0);\n  lj2[i][j] = 18.0 * epsilon[i][j] * pow(sigma[i][j],6.0);\n  lj3[i][j] = 2.0 * epsilon[i][j] * pow(sigma[i][j],9.0);\n  lj4[i][j] = 3.0 * epsilon[i][j] * pow(sigma[i][j],6.0);\n\n  if (offset_flag && (cut[i][j] > 0.0)) {\n    double ratio = sigma[i][j] / cut[i][j];\n    offset[i][j] = epsilon[i][j] * (2.0*pow(ratio,9.0) - 3.0*pow(ratio,6.0));\n  } else offset[i][j] = 0.0;\n\n  lj1[j][i] = lj1[i][j];\n  lj2[j][i] = lj2[i][j];\n  lj3[j][i] = lj3[i][j];\n  lj4[j][i] = lj4[i][j];\n  offset[j][i] = offset[i][j];\n\n  // check interior rRESPA cutoff\n\n  if (cut_respa && cut[i][j] < cut_respa[3])\n    error->all(FLERR,\"Pair cutoff < Respa interior cutoff\");\n\n  // compute I,J contribution to long-range tail correction\n  // count total # of atoms of type I and J via Allreduce\n\n  if (tail_flag) {\n    int *type = atom->type;\n    int nlocal = atom->nlocal;\n\n    double count[2],all[2];\n    count[0] = count[1] = 0.0;\n    for (int k = 0; k < nlocal; k++) {\n      if (type[k] == i) count[0] += 1.0;\n      if (type[k] == j) count[1] += 1.0;\n    }\n    MPI_Allreduce(count,all,2,MPI_DOUBLE,MPI_SUM,world);\n\n    double sig3 = sigma[i][j]*sigma[i][j]*sigma[i][j];\n    double sig6 = sig3*sig3;\n    double rc3 = cut[i][j]*cut[i][j]*cut[i][j];\n    double rc6 = rc3*rc3;\n    etail_ij = 2.0*MY_PI*all[0]*all[1]*epsilon[i][j] *\n      sig6 * (sig3 - 3.0*rc3) / (3.0*rc6);\n    ptail_ij = 2.0*MY_PI*all[0]*all[1]*epsilon[i][j] *\n      sig6 * (sig3 - 2.0*rc3) / rc6;\n  }\n\n  return cut[i][j];\n}\n\n/* ----------------------------------------------------------------------\n   proc 0 writes to restart file\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::write_restart(FILE *fp)\n{\n  write_restart_settings(fp);\n\n  int i,j;\n  for (i = 1; i <= atom->ntypes; i++)\n    for (j = i; j <= atom->ntypes; j++) {\n      fwrite(&setflag[i][j],sizeof(int),1,fp);\n      if (setflag[i][j]) {\n        fwrite(&epsilon[i][j],sizeof(double),1,fp);\n        fwrite(&sigma[i][j],sizeof(double),1,fp);\n        fwrite(&cut[i][j],sizeof(double),1,fp);\n      }\n    }\n}\n\n/* ----------------------------------------------------------------------\n   proc 0 reads from restart file, bcasts\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::read_restart(FILE *fp)\n{\n  read_restart_settings(fp);\n  allocate();\n\n  int i,j;\n  int me = comm->me;\n  for (i = 1; i <= atom->ntypes; i++)\n    for (j = i; j <= atom->ntypes; j++) {\n      if (me == 0) utils::sfread(FLERR,&setflag[i][j],sizeof(int),1,fp,nullptr,error);\n      MPI_Bcast(&setflag[i][j],1,MPI_INT,0,world);\n      if (setflag[i][j]) {\n        if (me == 0) {\n          utils::sfread(FLERR,&epsilon[i][j],sizeof(double),1,fp,nullptr,error);\n          utils::sfread(FLERR,&sigma[i][j],sizeof(double),1,fp,nullptr,error);\n          utils::sfread(FLERR,&cut[i][j],sizeof(double),1,fp,nullptr,error);\n        }\n        MPI_Bcast(&epsilon[i][j],1,MPI_DOUBLE,0,world);\n        MPI_Bcast(&sigma[i][j],1,MPI_DOUBLE,0,world);\n        MPI_Bcast(&cut[i][j],1,MPI_DOUBLE,0,world);\n      }\n    }\n}\n\n/* ----------------------------------------------------------------------\n   proc 0 writes to restart file\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::write_restart_settings(FILE *fp)\n{\n  fwrite(&cut_global,sizeof(double),1,fp);\n  fwrite(&offset_flag,sizeof(int),1,fp);\n  fwrite(&mix_flag,sizeof(int),1,fp);\n  fwrite(&tail_flag,sizeof(int),1,fp);\n}\n\n/* ----------------------------------------------------------------------\n   proc 0 reads from restart file, bcasts\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::read_restart_settings(FILE *fp)\n{\n  int me = comm->me;\n  if (me == 0) {\n    utils::sfread(FLERR,&cut_global,sizeof(double),1,fp,nullptr,error);\n    utils::sfread(FLERR,&offset_flag,sizeof(int),1,fp,nullptr,error);\n    utils::sfread(FLERR,&mix_flag,sizeof(int),1,fp,nullptr,error);\n    utils::sfread(FLERR,&tail_flag,sizeof(int),1,fp,nullptr,error);\n  }\n  MPI_Bcast(&cut_global,1,MPI_DOUBLE,0,world);\n  MPI_Bcast(&offset_flag,1,MPI_INT,0,world);\n  MPI_Bcast(&mix_flag,1,MPI_INT,0,world);\n  MPI_Bcast(&tail_flag,1,MPI_INT,0,world);\n}\n\n\n/* ----------------------------------------------------------------------\n   proc 0 writes to data file\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::write_data(FILE *fp)\n{\n  for (int i = 1; i <= atom->ntypes; i++)\n    fprintf(fp,\"%d %g %g\\n\",i,epsilon[i][i],sigma[i][i]);\n}\n\n/* ----------------------------------------------------------------------\n   proc 0 writes all pairs to data file\n------------------------------------------------------------------------- */\n\nvoid PairLJClass2::write_data_all(FILE *fp)\n{\n  for (int i = 1; i <= atom->ntypes; i++)\n    for (int j = i; j <= atom->ntypes; j++)\n      fprintf(fp,\"%d %d %g %g %g\\n\",i,j,epsilon[i][j],sigma[i][j],cut[i][j]);\n}\n\n/* ---------------------------------------------------------------------- */\n\ndouble PairLJClass2::single(int /*i*/, int /*j*/, int itype, int jtype, double rsq,\n                            double /*factor_coul*/, double factor_lj,\n                            double &fforce)\n{\n  double r2inv,rinv,r3inv,r6inv,forcelj,philj;\n\n  r2inv = 1.0/rsq;\n  rinv = sqrt(r2inv);\n  r3inv = r2inv*rinv;\n  r6inv = r3inv*r3inv;\n  forcelj = r6inv * (lj1[itype][jtype]*r3inv - lj2[itype][jtype]);\n  fforce = factor_lj*forcelj*r2inv;\n\n  philj = r6inv*(lj3[itype][jtype]*r3inv-lj4[itype][jtype]) -\n    offset[itype][jtype];\n  return factor_lj*philj;\n}\n\n/* ---------------------------------------------------------------------- */\n\nvoid *PairLJClass2::extract(const char *str, int &dim)\n{\n  dim = 2;\n  if (strcmp(str,\"epsilon\") == 0) return (void *) epsilon;\n  if (strcmp(str,\"sigma\") == 0) return (void *) sigma;\n  return nullptr;\n}\n", "idx": 1, "id": 29483, "msg": "this is the default - the point of changing the default was to remove this lines from all the true pairwise pair styles", "proj": "lammps-lammps", "lang": "cpp"}
{"patch": "@@ -3129,6 +3129,14 @@ func (s *Server) jsConsumerCreate(sub *subscription, c *client, a *Account, subj\n \t// Make sure we have sane defaults.\n \tsetConsumerConfigDefaults(&req.Config)\n \n+\t// Check if we have a BackOff defined that MaxDeliver is within range etc.\n+\tif lbo := len(req.Config.BackOff); lbo > 0 && req.Config.MaxDeliver <= lbo {\n+\t\terr := errors.New(\"max deliver required to be > length backoff values\")\n+\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n+\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n+\t\treturn\n+\t}\n+\n \t// Determine if we should proceed here when we are in clustered mode.\n \tif s.JetStreamIsClustered() {\n \t\tif req.Config.Direct {", "y": 1, "oldf": "// Copyright 2020-2022 The NATS Authors\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n// http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage server\n\nimport (\n\t\"bytes\"\n\t\"encoding/json\"\n\t\"errors\"\n\t\"fmt\"\n\t\"io/ioutil\"\n\t\"math/rand\"\n\t\"os\"\n\t\"path\"\n\t\"sort\"\n\t\"strconv\"\n\t\"strings\"\n\t\"sync/atomic\"\n\t\"time\"\n\t\"unicode\"\n\n\t\"github.com/nats-io/nuid\"\n)\n\n// Request API subjects for JetStream.\nconst (\n\t// All API endpoints.\n\tjsAllAPI = \"$JS.API.>\"\n\n\t// For constructing JetStream domain prefixes.\n\tjsDomainAPI = \"$JS.%s.API.>\"\n\n\tJSApiPrefix = \"$JS.API\"\n\n\t// JSApiAccountInfo is for obtaining general information about JetStream for this account.\n\t// Will return JSON response.\n\tJSApiAccountInfo = \"$JS.API.INFO\"\n\n\t// JSApiTemplateCreate is the endpoint to create new stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateCreate  = \"$JS.API.STREAM.TEMPLATE.CREATE.*\"\n\tJSApiTemplateCreateT = \"$JS.API.STREAM.TEMPLATE.CREATE.%s\"\n\n\t// JSApiTemplates is the endpoint to list all stream template names for this account.\n\t// Will return JSON response.\n\tJSApiTemplates = \"$JS.API.STREAM.TEMPLATE.NAMES\"\n\n\t// JSApiTemplateInfo is for obtaining general information about a named stream template.\n\t// Will return JSON response.\n\tJSApiTemplateInfo  = \"$JS.API.STREAM.TEMPLATE.INFO.*\"\n\tJSApiTemplateInfoT = \"$JS.API.STREAM.TEMPLATE.INFO.%s\"\n\n\t// JSApiTemplateDelete is the endpoint to delete stream templates.\n\t// Will return JSON response.\n\tJSApiTemplateDelete  = \"$JS.API.STREAM.TEMPLATE.DELETE.*\"\n\tJSApiTemplateDeleteT = \"$JS.API.STREAM.TEMPLATE.DELETE.%s\"\n\n\t// JSApiStreamCreate is the endpoint to create new streams.\n\t// Will return JSON response.\n\tJSApiStreamCreate  = \"$JS.API.STREAM.CREATE.*\"\n\tJSApiStreamCreateT = \"$JS.API.STREAM.CREATE.%s\"\n\n\t// JSApiStreamUpdate is the endpoint to update existing streams.\n\t// Will return JSON response.\n\tJSApiStreamUpdate  = \"$JS.API.STREAM.UPDATE.*\"\n\tJSApiStreamUpdateT = \"$JS.API.STREAM.UPDATE.%s\"\n\n\t// JSApiStreams is the endpoint to list all stream names for this account.\n\t// Will return JSON response.\n\tJSApiStreams = \"$JS.API.STREAM.NAMES\"\n\t// JSApiStreamList is the endpoint that will return all detailed stream information\n\tJSApiStreamList = \"$JS.API.STREAM.LIST\"\n\n\t// JSApiStreamInfo is for obtaining general information about a named stream.\n\t// Will return JSON response.\n\tJSApiStreamInfo  = \"$JS.API.STREAM.INFO.*\"\n\tJSApiStreamInfoT = \"$JS.API.STREAM.INFO.%s\"\n\n\t// JSApiStreamDelete is the endpoint to delete streams.\n\t// Will return JSON response.\n\tJSApiStreamDelete  = \"$JS.API.STREAM.DELETE.*\"\n\tJSApiStreamDeleteT = \"$JS.API.STREAM.DELETE.%s\"\n\n\t// JSApiStreamPurge is the endpoint to purge streams.\n\t// Will return JSON response.\n\tJSApiStreamPurge  = \"$JS.API.STREAM.PURGE.*\"\n\tJSApiStreamPurgeT = \"$JS.API.STREAM.PURGE.%s\"\n\n\t// JSApiStreamSnapshot is the endpoint to snapshot streams.\n\t// Will return a stream of chunks with a nil chunk as EOF to\n\t// the deliver subject. Caller should respond to each chunk\n\t// with a nil body response for ack flow.\n\tJSApiStreamSnapshot  = \"$JS.API.STREAM.SNAPSHOT.*\"\n\tJSApiStreamSnapshotT = \"$JS.API.STREAM.SNAPSHOT.%s\"\n\n\t// JSApiStreamRestore is the endpoint to restore a stream from a snapshot.\n\t// Caller should respond to each chunk with a nil body response.\n\tJSApiStreamRestore  = \"$JS.API.STREAM.RESTORE.*\"\n\tJSApiStreamRestoreT = \"$JS.API.STREAM.RESTORE.%s\"\n\n\t// JSApiMsgDelete is the endpoint to delete messages from a stream.\n\t// Will return JSON response.\n\tJSApiMsgDelete  = \"$JS.API.STREAM.MSG.DELETE.*\"\n\tJSApiMsgDeleteT = \"$JS.API.STREAM.MSG.DELETE.%s\"\n\n\t// JSApiMsgGet is the template for direct requests for a message by its stream sequence number.\n\t// Will return JSON response.\n\tJSApiMsgGet  = \"$JS.API.STREAM.MSG.GET.*\"\n\tJSApiMsgGetT = \"$JS.API.STREAM.MSG.GET.%s\"\n\n\t// JSApiConsumerCreate is the endpoint to create ephemeral consumers for streams.\n\t// Will return JSON response.\n\tJSApiConsumerCreate  = \"$JS.API.CONSUMER.CREATE.*\"\n\tJSApiConsumerCreateT = \"$JS.API.CONSUMER.CREATE.%s\"\n\n\t// JSApiDurableCreate is the endpoint to create durable consumers for streams.\n\t// You need to include the stream and consumer name in the subject.\n\tJSApiDurableCreate  = \"$JS.API.CONSUMER.DURABLE.CREATE.*.*\"\n\tJSApiDurableCreateT = \"$JS.API.CONSUMER.DURABLE.CREATE.%s.%s\"\n\n\t// JSApiConsumers is the endpoint to list all consumer names for the stream.\n\t// Will return JSON response.\n\tJSApiConsumers  = \"$JS.API.CONSUMER.NAMES.*\"\n\tJSApiConsumersT = \"$JS.API.CONSUMER.NAMES.%s\"\n\n\t// JSApiConsumerList is the endpoint that will return all detailed consumer information\n\tJSApiConsumerList = \"$JS.API.CONSUMER.LIST.*\"\n\n\t// JSApiConsumerInfo is for obtaining general information about a consumer.\n\t// Will return JSON response.\n\tJSApiConsumerInfo  = \"$JS.API.CONSUMER.INFO.*.*\"\n\tJSApiConsumerInfoT = \"$JS.API.CONSUMER.INFO.%s.%s\"\n\n\t// JSApiConsumerDelete is the endpoint to delete consumers.\n\t// Will return JSON response.\n\tJSApiConsumerDelete  = \"$JS.API.CONSUMER.DELETE.*.*\"\n\tJSApiConsumerDeleteT = \"$JS.API.CONSUMER.DELETE.%s.%s\"\n\n\t// JSApiRequestNextT is the prefix for the request next message(s) for a consumer in worker/pull mode.\n\tJSApiRequestNextT = \"$JS.API.CONSUMER.MSG.NEXT.%s.%s\"\n\n\t// jsRequestNextPre\n\tjsRequestNextPre = \"$JS.API.CONSUMER.MSG.NEXT.\"\n\n\t// For snapshots and restores. The ack will have additional tokens.\n\tjsSnapshotAckT    = \"$JS.SNAPSHOT.ACK.%s.%s\"\n\tjsRestoreDeliverT = \"$JS.SNAPSHOT.RESTORE.%s.%s\"\n\n\t// JSApiStreamRemovePeer is the endpoint to remove a peer from a clustered stream and its consumers.\n\t// Will return JSON response.\n\tJSApiStreamRemovePeer  = \"$JS.API.STREAM.PEER.REMOVE.*\"\n\tJSApiStreamRemovePeerT = \"$JS.API.STREAM.PEER.REMOVE.%s\"\n\n\t// JSApiStreamLeaderStepDown is the endpoint to have stream leader stepdown.\n\t// Will return JSON response.\n\tJSApiStreamLeaderStepDown  = \"$JS.API.STREAM.LEADER.STEPDOWN.*\"\n\tJSApiStreamLeaderStepDownT = \"$JS.API.STREAM.LEADER.STEPDOWN.%s\"\n\n\t// JSApiConsumerLeaderStepDown is the endpoint to have consumer leader stepdown.\n\t// Will return JSON response.\n\tJSApiConsumerLeaderStepDown  = \"$JS.API.CONSUMER.LEADER.STEPDOWN.*.*\"\n\tJSApiConsumerLeaderStepDownT = \"$JS.API.CONSUMER.LEADER.STEPDOWN.%s.%s\"\n\n\t// JSApiLeaderStepDown is the endpoint to have our metaleader stepdown.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiLeaderStepDown = \"$JS.API.META.LEADER.STEPDOWN\"\n\n\t// JSApiRemoveServer is the endpoint to remove a peer server from the cluster.\n\t// Only works from system account.\n\t// Will return JSON response.\n\tJSApiRemoveServer = \"$JS.API.SERVER.REMOVE\"\n\n\t// jsAckT is the template for the ack message stream coming back from a consumer\n\t// when they ACK/NAK, etc a message.\n\tjsAckT   = \"$JS.ACK.%s.%s\"\n\tjsAckPre = \"$JS.ACK.\"\n\n\t// jsFlowControl is for flow control subjects.\n\tjsFlowControlPre = \"$JS.FC.\"\n\t// jsFlowControl is for FC responses.\n\tjsFlowControl = \"$JS.FC.%s.%s.*\"\n\n\t// JSAdvisoryPrefix is a prefix for all JetStream advisories.\n\tJSAdvisoryPrefix = \"$JS.EVENT.ADVISORY\"\n\n\t// JSMetricPrefix is a prefix for all JetStream metrics.\n\tJSMetricPrefix = \"$JS.EVENT.METRIC\"\n\n\t// JSMetricConsumerAckPre is a metric containing ack latency.\n\tJSMetricConsumerAckPre = \"$JS.EVENT.METRIC.CONSUMER.ACK\"\n\n\t// JSAdvisoryConsumerMaxDeliveryExceedPre is a notification published when a message exceeds its delivery threshold.\n\tJSAdvisoryConsumerMaxDeliveryExceedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MAX_DELIVERIES\"\n\n\t// JSAdvisoryConsumerMsgTerminatedPre is a notification published when a message has been terminated.\n\tJSAdvisoryConsumerMsgTerminatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.MSG_TERMINATED\"\n\n\t// JSAdvisoryStreamCreatedPre notification that a stream was created.\n\tJSAdvisoryStreamCreatedPre = \"$JS.EVENT.ADVISORY.STREAM.CREATED\"\n\n\t// JSAdvisoryStreamDeletedPre notification that a stream was deleted.\n\tJSAdvisoryStreamDeletedPre = \"$JS.EVENT.ADVISORY.STREAM.DELETED\"\n\n\t// JSAdvisoryStreamUpdatedPre notification that a stream was updated.\n\tJSAdvisoryStreamUpdatedPre = \"$JS.EVENT.ADVISORY.STREAM.UPDATED\"\n\n\t// JSAdvisoryConsumerCreatedPre notification that a template created.\n\tJSAdvisoryConsumerCreatedPre = \"$JS.EVENT.ADVISORY.CONSUMER.CREATED\"\n\n\t// JSAdvisoryConsumerDeletedPre notification that a template deleted.\n\tJSAdvisoryConsumerDeletedPre = \"$JS.EVENT.ADVISORY.CONSUMER.DELETED\"\n\n\t// JSAdvisoryStreamSnapshotCreatePre notification that a snapshot was created.\n\tJSAdvisoryStreamSnapshotCreatePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_CREATE\"\n\n\t// JSAdvisoryStreamSnapshotCompletePre notification that a snapshot was completed.\n\tJSAdvisoryStreamSnapshotCompletePre = \"$JS.EVENT.ADVISORY.STREAM.SNAPSHOT_COMPLETE\"\n\n\t// JSAdvisoryStreamRestoreCreatePre notification that a restore was start.\n\tJSAdvisoryStreamRestoreCreatePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_CREATE\"\n\n\t// JSAdvisoryStreamRestoreCompletePre notification that a restore was completed.\n\tJSAdvisoryStreamRestoreCompletePre = \"$JS.EVENT.ADVISORY.STREAM.RESTORE_COMPLETE\"\n\n\t// JSAdvisoryStreamLeaderElectedPre notification that a replicated stream has elected a leader.\n\tJSAdvisoryStreamLeaderElectedPre = \"$JS.EVENT.ADVISORY.STREAM.LEADER_ELECTED\"\n\n\t// JSAdvisoryStreamQuorumLostPre notification that a stream and its consumers are stalled.\n\tJSAdvisoryStreamQuorumLostPre = \"$JS.EVENT.ADVISORY.STREAM.QUORUM_LOST\"\n\n\t// JSAdvisoryConsumerLeaderElectedPre notification that a replicated consumer has elected a leader.\n\tJSAdvisoryConsumerLeaderElectedPre = \"$JS.EVENT.ADVISORY.CONSUMER.LEADER_ELECTED\"\n\n\t// JSAdvisoryConsumerQuorumLostPre notification that a consumer is stalled.\n\tJSAdvisoryConsumerQuorumLostPre = \"$JS.EVENT.ADVISORY.CONSUMER.QUORUM_LOST\"\n\n\t// JSAdvisoryServerOutOfStorage notification that a server has no more storage.\n\tJSAdvisoryServerOutOfStorage = \"$JS.EVENT.ADVISORY.SERVER.OUT_OF_STORAGE\"\n\n\t// JSAdvisoryServerRemoved notification that a server has been removed from the system.\n\tJSAdvisoryServerRemoved = \"$JS.EVENT.ADVISORY.SERVER.REMOVED\"\n\n\t// JSAuditAdvisory is a notification about JetStream API access.\n\t// FIXME - Add in details about who..\n\tJSAuditAdvisory = \"$JS.EVENT.ADVISORY.API\"\n)\n\nvar denyAllJs = []string{jscAllSubj, raftAllSubj, jsAllAPI}\n\n// JSMaxDescription is the maximum description length for streams and consumers.\nconst JSMaxDescriptionLen = 4 * 1024\n\n// JSMaxNameLen is the maximum name lengths for streams, consumers and templates.\nconst JSMaxNameLen = 256\n\n// Responses for API calls.\n\n// ApiResponse is a standard response from the JetStream JSON API\ntype ApiResponse struct {\n\tType  string    `json:\"type\"`\n\tError *ApiError `json:\"error,omitempty\"`\n}\n\n// ToError checks if the response has a error and if it does converts it to an error avoiding\n// the pitfalls described by https://yourbasic.org/golang/gotcha-why-nil-error-not-equal-nil/\nfunc (r *ApiResponse) ToError() error {\n\tif r.Error == nil {\n\t\treturn nil\n\t}\n\n\treturn r.Error\n}\n\nconst JSApiOverloadedType = \"io.nats.jetstream.api.v1.system_overloaded\"\n\n// ApiPaged includes variables used to create paged responses from the JSON API\ntype ApiPaged struct {\n\tTotal  int `json:\"total\"`\n\tOffset int `json:\"offset\"`\n\tLimit  int `json:\"limit\"`\n}\n\n// ApiPagedRequest includes parameters allowing specific pages to be requests from APIs responding with ApiPaged\ntype ApiPagedRequest struct {\n\tOffset int `json:\"offset\"`\n}\n\n// JSApiAccountInfoResponse reports back information on jetstream for this account.\ntype JSApiAccountInfoResponse struct {\n\tApiResponse\n\t*JetStreamAccountStats\n}\n\nconst JSApiAccountInfoResponseType = \"io.nats.jetstream.api.v1.account_info_response\"\n\n// JSApiStreamCreateResponse stream creation.\ntype JSApiStreamCreateResponse struct {\n\tApiResponse\n\t*StreamInfo\n\tDidCreate bool `json:\"did_create,omitempty\"`\n}\n\nconst JSApiStreamCreateResponseType = \"io.nats.jetstream.api.v1.stream_create_response\"\n\n// JSApiStreamDeleteResponse stream removal.\ntype JSApiStreamDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamDeleteResponseType = \"io.nats.jetstream.api.v1.stream_delete_response\"\n\ntype JSApiStreamInfoRequest struct {\n\tDeletedDetails bool `json:\"deleted_details,omitempty\"`\n}\n\ntype JSApiStreamInfoResponse struct {\n\tApiResponse\n\t*StreamInfo\n}\n\nconst JSApiStreamInfoResponseType = \"io.nats.jetstream.api.v1.stream_info_response\"\n\n// JSApiNamesLimit is the maximum entries we will return for streams or consumers lists.\n// TODO(dlc) - with header or request support could request chunked response.\nconst JSApiNamesLimit = 1024\nconst JSApiListLimit = 256\n\ntype JSApiStreamNamesRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamNamesResponse list of streams.\n// A nil request is valid and means all streams.\ntype JSApiStreamNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []string `json:\"streams\"`\n}\n\nconst JSApiStreamNamesResponseType = \"io.nats.jetstream.api.v1.stream_names_response\"\n\ntype JSApiStreamListRequest struct {\n\tApiPagedRequest\n\t// These are filters that can be applied to the list.\n\tSubject string `json:\"subject,omitempty\"`\n}\n\n// JSApiStreamListResponse list of detailed stream information.\n// A nil request is valid and means all streams.\ntype JSApiStreamListResponse struct {\n\tApiResponse\n\tApiPaged\n\tStreams []*StreamInfo `json:\"streams\"`\n\tMissing []string      `json:\"missing,omitempty\"`\n}\n\nconst JSApiStreamListResponseType = \"io.nats.jetstream.api.v1.stream_list_response\"\n\n// JSApiStreamPurgeRequest is optional request information to the purge API.\n// Subject will filter the purge request to only messages that match the subject, which can have wildcards.\n// Sequence will purge up to but not including this sequence and can be combined with subject filtering.\n// Keep will specify how many messages to keep. This can also be combined with subject filtering.\n// Note that Sequence and Keep are mutually exclusive, so both can not be set at the same time.\ntype JSApiStreamPurgeRequest struct {\n\t// Purge up to but not including sequence.\n\tSequence uint64 `json:\"seq,omitempty\"`\n\t// Subject to match against messages for the purge command.\n\tSubject string `json:\"filter,omitempty\"`\n\t// Number of messages to keep.\n\tKeep uint64 `json:\"keep,omitempty\"`\n}\n\ntype JSApiStreamPurgeResponse struct {\n\tApiResponse\n\tSuccess bool   `json:\"success,omitempty\"`\n\tPurged  uint64 `json:\"purged\"`\n}\n\nconst JSApiStreamPurgeResponseType = \"io.nats.jetstream.api.v1.stream_purge_response\"\n\n// JSApiStreamUpdateResponse for updating a stream.\ntype JSApiStreamUpdateResponse struct {\n\tApiResponse\n\t*StreamInfo\n}\n\nconst JSApiStreamUpdateResponseType = \"io.nats.jetstream.api.v1.stream_update_response\"\n\n// JSApiMsgDeleteRequest delete message request.\ntype JSApiMsgDeleteRequest struct {\n\tSeq     uint64 `json:\"seq\"`\n\tNoErase bool   `json:\"no_erase,omitempty\"`\n}\n\ntype JSApiMsgDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMsgDeleteResponseType = \"io.nats.jetstream.api.v1.stream_msg_delete_response\"\n\ntype JSApiStreamSnapshotRequest struct {\n\t// Subject to deliver the chunks to for the snapshot.\n\tDeliverSubject string `json:\"deliver_subject\"`\n\t// Do not include consumers in the snapshot.\n\tNoConsumers bool `json:\"no_consumers,omitempty\"`\n\t// Optional chunk size preference.\n\t// Best to just let server select.\n\tChunkSize int `json:\"chunk_size,omitempty\"`\n\t// Check all message's checksums prior to snapshot.\n\tCheckMsgs bool `json:\"jsck,omitempty\"`\n}\n\n// JSApiStreamSnapshotResponse is the direct response to the snapshot request.\ntype JSApiStreamSnapshotResponse struct {\n\tApiResponse\n\t// Configuration of the given stream.\n\tConfig *StreamConfig `json:\"config,omitempty\"`\n\t// Current State for the given stream.\n\tState *StreamState `json:\"state,omitempty\"`\n}\n\nconst JSApiStreamSnapshotResponseType = \"io.nats.jetstream.api.v1.stream_snapshot_response\"\n\n// JSApiStreamRestoreRequest is the required restore request.\ntype JSApiStreamRestoreRequest struct {\n\t// Configuration of the given stream.\n\tConfig StreamConfig `json:\"config\"`\n\t// Current State for the given stream.\n\tState StreamState `json:\"state\"`\n}\n\n// JSApiStreamRestoreResponse is the direct response to the restore request.\ntype JSApiStreamRestoreResponse struct {\n\tApiResponse\n\t// Subject to deliver the chunks to for the snapshot restore.\n\tDeliverSubject string `json:\"deliver_subject\"`\n}\n\nconst JSApiStreamRestoreResponseType = \"io.nats.jetstream.api.v1.stream_restore_response\"\n\n// JSApiStreamRemovePeerRequest is the required remove peer request.\ntype JSApiStreamRemovePeerRequest struct {\n\t// Server name of the peer to be removed.\n\tPeer string `json:\"peer\"`\n}\n\n// JSApiStreamRemovePeerResponse is the response to a remove peer request.\ntype JSApiStreamRemovePeerResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamRemovePeerResponseType = \"io.nats.jetstream.api.v1.stream_remove_peer_response\"\n\n// JSApiStreamLeaderStepDownResponse is the response to a leader stepdown request.\ntype JSApiStreamLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.stream_leader_stepdown_response\"\n\n// JSApiConsumerLeaderStepDownResponse is the response to a consumer leader stepdown request.\ntype JSApiConsumerLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.consumer_leader_stepdown_response\"\n\n// JSApiLeaderStepdownRequest allows placement control over the meta leader placement.\ntype JSApiLeaderStepdownRequest struct {\n\tPlacement *Placement `json:\"placement,omitempty\"`\n}\n\n// JSApiLeaderStepDownResponse is the response to a meta leader stepdown request.\ntype JSApiLeaderStepDownResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiLeaderStepDownResponseType = \"io.nats.jetstream.api.v1.meta_leader_stepdown_response\"\n\n// JSApiMetaServerRemoveRequest will remove a peer from the meta group.\ntype JSApiMetaServerRemoveRequest struct {\n\t// Server name of the peer to be removed.\n\tServer string `json:\"peer\"`\n}\n\n// JSApiMetaServerRemoveResponse is the response to a peer removal request in the meta group.\ntype JSApiMetaServerRemoveResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiMetaServerRemoveResponseType = \"io.nats.jetstream.api.v1.meta_server_remove_response\"\n\n// JSApiMsgGetRequest get a message request.\ntype JSApiMsgGetRequest struct {\n\tSeq     uint64 `json:\"seq,omitempty\"`\n\tLastFor string `json:\"last_by_subj,omitempty\"`\n}\n\ntype JSApiMsgGetResponse struct {\n\tApiResponse\n\tMessage *StoredMsg `json:\"message,omitempty\"`\n}\n\nconst JSApiMsgGetResponseType = \"io.nats.jetstream.api.v1.stream_msg_get_response\"\n\n// JSWaitQueueDefaultMax is the default max number of outstanding requests for pull consumers.\nconst JSWaitQueueDefaultMax = 512\n\ntype JSApiConsumerCreateResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerCreateResponseType = \"io.nats.jetstream.api.v1.consumer_create_response\"\n\ntype JSApiConsumerDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiConsumerDeleteResponseType = \"io.nats.jetstream.api.v1.consumer_delete_response\"\n\ntype JSApiConsumerInfoResponse struct {\n\tApiResponse\n\t*ConsumerInfo\n}\n\nconst JSApiConsumerInfoResponseType = \"io.nats.jetstream.api.v1.consumer_info_response\"\n\ntype JSApiConsumersRequest struct {\n\tApiPagedRequest\n}\n\ntype JSApiConsumerNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []string `json:\"consumers\"`\n}\n\nconst JSApiConsumerNamesResponseType = \"io.nats.jetstream.api.v1.consumer_names_response\"\n\ntype JSApiConsumerListResponse struct {\n\tApiResponse\n\tApiPaged\n\tConsumers []*ConsumerInfo `json:\"consumers\"`\n\tMissing   []string        `json:\"missing,omitempty\"`\n}\n\nconst JSApiConsumerListResponseType = \"io.nats.jetstream.api.v1.consumer_list_response\"\n\n// JSApiConsumerGetNextRequest is for getting next messages for pull based consumers.\ntype JSApiConsumerGetNextRequest struct {\n\tExpires time.Duration `json:\"expires,omitempty\"`\n\tBatch   int           `json:\"batch,omitempty\"`\n\tNoWait  bool          `json:\"no_wait,omitempty\"`\n}\n\n// JSApiStreamTemplateCreateResponse for creating templates.\ntype JSApiStreamTemplateCreateResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateCreateResponseType = \"io.nats.jetstream.api.v1.stream_template_create_response\"\n\ntype JSApiStreamTemplateDeleteResponse struct {\n\tApiResponse\n\tSuccess bool `json:\"success,omitempty\"`\n}\n\nconst JSApiStreamTemplateDeleteResponseType = \"io.nats.jetstream.api.v1.stream_template_delete_response\"\n\n// JSApiStreamTemplateInfoResponse for information about stream templates.\ntype JSApiStreamTemplateInfoResponse struct {\n\tApiResponse\n\t*StreamTemplateInfo\n}\n\nconst JSApiStreamTemplateInfoResponseType = \"io.nats.jetstream.api.v1.stream_template_info_response\"\n\ntype JSApiStreamTemplatesRequest struct {\n\tApiPagedRequest\n}\n\n// JSApiStreamTemplateNamesResponse list of templates\ntype JSApiStreamTemplateNamesResponse struct {\n\tApiResponse\n\tApiPaged\n\tTemplates []string `json:\"streams\"`\n}\n\nconst JSApiStreamTemplateNamesResponseType = \"io.nats.jetstream.api.v1.stream_template_names_response\"\n\n// Default max API calls outstanding.\nconst defaultMaxJSApiOut = int64(4096)\n\n// Max API calls outstanding.\nvar maxJSApiOut = defaultMaxJSApiOut\n\nfunc (js *jetStream) apiDispatch(sub *subscription, c *client, acc *Account, subject, reply string, rmsg []byte) {\n\tjs.mu.RLock()\n\ts, rr := js.srv, js.apiSubs.Match(subject)\n\tjs.mu.RUnlock()\n\n\thdr, _ := c.msgParts(rmsg)\n\tif len(getHeader(ClientInfoHdr, hdr)) == 0 {\n\t\t// Check if this is the system account. We will let these through for the account info only.\n\t\tif s.SystemAccount() != acc || subject != JSApiAccountInfo {\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Shortcircuit.\n\tif len(rr.psubs)+len(rr.qsubs) == 0 {\n\t\treturn\n\t}\n\n\t// We should only have psubs and only 1 per result.\n\t// FIXME(dlc) - Should we respond here with NoResponders or error?\n\tif len(rr.psubs) != 1 {\n\t\ts.Warnf(\"Malformed JetStream API Request: [%s] %q\", subject, rmsg)\n\t\treturn\n\t}\n\tjsub := rr.psubs[0]\n\n\t// If this is directly from a client connection ok to do in place.\n\tif c.kind != ROUTER && c.kind != GATEWAY {\n\t\tjsub.icb(sub, c, acc, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\t// If we are here we have received this request over a non client connection.\n\t// We need to make sure not to block. We will spin a Go routine per but also make\n\t// sure we do not have too many outstanding.\n\tif apiOut := atomic.AddInt64(&js.apiInflight, 1); apiOut > maxJSApiOut {\n\t\tatomic.AddInt64(&js.apiInflight, -1)\n\t\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\t\tif err == nil {\n\t\t\tresp := &ApiResponse{Type: JSApiOverloadedType, Error: NewJSInsufficientResourcesError()}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t} else {\n\t\t\ts.Warnf(badAPIRequestT, rmsg)\n\t\t}\n\t\ts.Warnf(\"JetStream API limit exceeded: %d calls outstanding\", apiOut)\n\t\treturn\n\t}\n\n\t// If we are here we can properly dispatch this API call.\n\t// Copy the message and the client. Client for the pubArgs\n\t// but note the JSAPI only uses the hdr index to piece apart\n\t// the header from the msg body. No other references are needed.\n\t// FIXME(dlc) - Should cleanup eventually and make sending\n\t// and receiving internal messages more formal.\n\trmsg = copyBytes(rmsg)\n\tclient := &client{srv: s, kind: JETSTREAM}\n\tclient.pa = c.pa\n\n\t// Dispatch the API call to its own Go routine.\n\tgo func() {\n\t\tjsub.icb(sub, client, acc, subject, reply, rmsg)\n\t\tatomic.AddInt64(&js.apiInflight, -1)\n\t}()\n}\n\nfunc (s *Server) setJetStreamExportSubs() error {\n\tjs := s.getJetStream()\n\tif js == nil {\n\t\treturn NewJSNotEnabledError()\n\t}\n\n\t// This is the catch all now for all JetStream API calls.\n\tif _, err := s.sysSubscribe(jsAllAPI, js.apiDispatch); err != nil {\n\t\treturn err\n\t}\n\n\tif err := s.SystemAccount().AddServiceExport(jsAllAPI, nil); err != nil {\n\t\ts.Warnf(\"Error setting up jetstream service exports: %v\", err)\n\t\treturn err\n\t}\n\n\t// API handles themselves.\n\tpairs := []struct {\n\t\tsubject string\n\t\thandler msgHandler\n\t}{\n\t\t{JSApiAccountInfo, s.jsAccountInfoRequest},\n\t\t{JSApiTemplateCreate, s.jsTemplateCreateRequest},\n\t\t{JSApiTemplates, s.jsTemplateNamesRequest},\n\t\t{JSApiTemplateInfo, s.jsTemplateInfoRequest},\n\t\t{JSApiTemplateDelete, s.jsTemplateDeleteRequest},\n\t\t{JSApiStreamCreate, s.jsStreamCreateRequest},\n\t\t{JSApiStreamUpdate, s.jsStreamUpdateRequest},\n\t\t{JSApiStreams, s.jsStreamNamesRequest},\n\t\t{JSApiStreamList, s.jsStreamListRequest},\n\t\t{JSApiStreamInfo, s.jsStreamInfoRequest},\n\t\t{JSApiStreamDelete, s.jsStreamDeleteRequest},\n\t\t{JSApiStreamPurge, s.jsStreamPurgeRequest},\n\t\t{JSApiStreamSnapshot, s.jsStreamSnapshotRequest},\n\t\t{JSApiStreamRestore, s.jsStreamRestoreRequest},\n\t\t{JSApiStreamRemovePeer, s.jsStreamRemovePeerRequest},\n\t\t{JSApiStreamLeaderStepDown, s.jsStreamLeaderStepDownRequest},\n\t\t{JSApiConsumerLeaderStepDown, s.jsConsumerLeaderStepDownRequest},\n\t\t{JSApiMsgDelete, s.jsMsgDeleteRequest},\n\t\t{JSApiMsgGet, s.jsMsgGetRequest},\n\t\t{JSApiConsumerCreate, s.jsConsumerCreateRequest},\n\t\t{JSApiDurableCreate, s.jsDurableCreateRequest},\n\t\t{JSApiConsumers, s.jsConsumerNamesRequest},\n\t\t{JSApiConsumerList, s.jsConsumerListRequest},\n\t\t{JSApiConsumerInfo, s.jsConsumerInfoRequest},\n\t\t{JSApiConsumerDelete, s.jsConsumerDeleteRequest},\n\t}\n\n\tjs.mu.Lock()\n\tdefer js.mu.Unlock()\n\n\tfor _, p := range pairs {\n\t\tsub := &subscription{subject: []byte(p.subject), icb: p.handler}\n\t\tif err := js.apiSubs.Insert(sub); err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc (s *Server) sendAPIResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPI()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nfunc (s *Server) sendAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string) {\n\tacc.trackAPIErr()\n\tif reply != _EMPTY_ {\n\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t}\n\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n}\n\nconst errRespDelay = 500 * time.Millisecond\n\nfunc (s *Server) sendDelayedAPIErrResponse(ci *ClientInfo, acc *Account, subject, reply, request, response string, rg *raftGroup) {\n\tvar quitCh <-chan struct{}\n\tif rg != nil && rg.node != nil {\n\t\tquitCh = rg.node.QuitC()\n\t}\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tselect {\n\t\tcase <-quitCh:\n\t\tcase <-s.quitCh:\n\t\tcase <-time.After(errRespDelay):\n\t\t\tacc.trackAPIErr()\n\t\t\tif reply != _EMPTY_ {\n\t\t\t\ts.sendInternalAccountMsg(nil, reply, response)\n\t\t\t}\n\t\t\ts.sendJetStreamAPIAuditAdvisory(ci, acc, subject, request, response)\n\t\t}\n\t})\n}\n\nfunc (s *Server) getRequestInfo(c *client, raw []byte) (pci *ClientInfo, acc *Account, hdr, msg []byte, err error) {\n\thdr, msg = c.msgParts(raw)\n\tvar ci ClientInfo\n\n\tif len(hdr) > 0 {\n\t\tif err := json.Unmarshal(getHeader(ClientInfoHdr, hdr), &ci); err != nil {\n\t\t\treturn nil, nil, nil, nil, err\n\t\t}\n\t}\n\n\tif ci.Service != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Service)\n\t} else if ci.Account != _EMPTY_ {\n\t\tacc, _ = s.LookupAccount(ci.Account)\n\t} else {\n\t\t// Direct $SYS access.\n\t\tacc = c.acc\n\t\tif acc == nil {\n\t\t\tacc = s.SystemAccount()\n\t\t}\n\t}\n\tif acc == nil {\n\t\treturn nil, nil, nil, nil, ErrMissingAccount\n\t}\n\treturn &ci, acc, hdr, msg, nil\n}\n\nfunc (a *Account) trackAPI() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.mu.Lock()\n\t\tjsa.usage.api++\n\t\tjsa.apiTotal++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tatomic.AddInt64(&jsa.js.apiTotal, 1)\n\t\tjsa.mu.Unlock()\n\t}\n}\n\nfunc (a *Account) trackAPIErr() {\n\ta.mu.RLock()\n\tjsa := a.js\n\ta.mu.RUnlock()\n\tif jsa != nil {\n\t\tjsa.mu.Lock()\n\t\tjsa.usage.api++\n\t\tjsa.apiTotal++\n\t\tjsa.usage.err++\n\t\tjsa.apiErrors++\n\t\tjsa.sendClusterUsageUpdate()\n\t\tatomic.AddInt64(&jsa.js.apiTotal, 1)\n\t\tatomic.AddInt64(&jsa.js.apiErrors, 1)\n\t\tjsa.mu.Unlock()\n\t}\n}\n\nconst badAPIRequestT = \"Malformed JetStream API Request: %q\"\n\n// Helper function to check on JetStream being enabled but also on status of leafnodes\n// If the local account is not enabled but does have leafnode connectivity we will not\n// want to error immediately and let the other side decide.\nfunc (a *Account) checkJetStream() (enabled, shouldError bool) {\n\ta.mu.RLock()\n\tdefer a.mu.RUnlock()\n\treturn a.js != nil, a.nleafs+a.nrleafs == 0\n}\n\n// Request for current usage and limits for this account.\nfunc (s *Server) jsAccountInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiAccountInfoResponse{ApiResponse: ApiResponse{Type: JSApiAccountInfoResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif !doErr {\n\t\t\treturn\n\t\t}\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t} else {\n\t\tstats := acc.JetStreamUsage()\n\t\tresp.JetStreamAccountStats = &stats\n\t}\n\tb, err := json.Marshal(resp)\n\tif err != nil {\n\t\treturn\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), string(b))\n}\n\n// Helpers for token extraction.\nfunc templateNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\nfunc streamNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 5)\n}\n\nfunc consumerNameFromSubject(subject string) string {\n\treturn tokenAt(subject, 6)\n}\n\n// Request to create a new template.\nfunc (s *Server) jsTemplateCreateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateCreateResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar cfg StreamTemplateConfig\n\tif err := json.Unmarshal(msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\ttemplateName := templateNameFromSubject(subject)\n\tif templateName != cfg.Name {\n\t\tresp.Error = NewJSTemplateNameNotMatchSubjectError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tt, err := acc.addStreamTemplate(&cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\ttcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: tcfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all template names.\nfunc (s *Server) jsTemplateNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateNamesResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Not supported for now.\n\tif s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterUnSupportFeatureError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamTemplatesRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tts := acc.templates()\n\tsort.Slice(ts, func(i, j int) bool {\n\t\treturn strings.Compare(ts[i].StreamTemplateConfig.Name, ts[j].StreamTemplateConfig.Name) < 0\n\t})\n\n\ttcnt := len(ts)\n\tif offset > tcnt {\n\t\toffset = tcnt\n\t}\n\n\tfor _, t := range ts[offset:] {\n\t\tt.mu.Lock()\n\t\tname := t.Name\n\t\tt.mu.Unlock()\n\t\tresp.Templates = append(resp.Templates, name)\n\t\tif len(resp.Templates) >= JSApiNamesLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = tcnt\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\tif resp.Templates == nil {\n\t\tresp.Templates = []string{}\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream template.\nfunc (s *Server) jsTemplateInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateInfoResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\tt, err := acc.lookupStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tt.mu.Lock()\n\tcfg := t.StreamTemplateConfig.deepCopy()\n\tstreams := t.streams\n\tif streams == nil {\n\t\tstreams = []string{}\n\t}\n\tt.mu.Unlock()\n\n\tresp.StreamTemplateInfo = &StreamTemplateInfo{Config: cfg, Streams: streams}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a stream template.\nfunc (s *Server) jsTemplateDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamTemplateDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamTemplateDeleteResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tname := templateNameFromSubject(subject)\n\terr = acc.deleteStreamTemplate(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamTemplateDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc (s *Server) jsonResponse(v interface{}) string {\n\tb, err := json.Marshal(v)\n\tif err != nil {\n\t\ts.Warnf(\"Problem marshaling JSON for JetStream API:\", err)\n\t\treturn \"\"\n\t}\n\treturn string(b)\n}\n\n// Request to create a stream.\nfunc (s *Server) jsStreamCreateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tvar cfg StreamConfig\n\tif err := json.Unmarshal(msg, &cfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\thasStream := func(streamName string) (bool, int32, []string) {\n\t\tvar exists bool\n\t\tvar maxMsgSize int32\n\t\tvar subs []string\n\t\tif s.JetStreamIsClustered() {\n\t\t\tif js, _ := s.getJetStreamCluster(); js != nil {\n\t\t\t\tjs.mu.RLock()\n\t\t\t\tif sa := js.streamAssignment(acc.Name, streamName); sa != nil {\n\t\t\t\t\tmaxMsgSize = sa.Config.MaxMsgSize\n\t\t\t\t\tsubs = sa.Config.Subjects\n\t\t\t\t\texists = true\n\t\t\t\t}\n\t\t\t\tjs.mu.RUnlock()\n\t\t\t}\n\t\t} else if mset, err := acc.lookupStream(streamName); err == nil {\n\t\t\tmaxMsgSize = mset.cfg.MaxMsgSize\n\t\t\tsubs = mset.cfg.Subjects\n\t\t\texists = true\n\t\t}\n\t\treturn exists, maxMsgSize, subs\n\t}\n\n\tvar streamSubs []string\n\tvar deliveryPrefixes []string\n\tvar apiPrefixes []string\n\n\t// Do some pre-checking for mirror config to avoid cycles in clustered mode.\n\tif cfg.Mirror != nil {\n\t\tif len(cfg.Subjects) > 0 {\n\t\t\tresp.Error = NewJSMirrorWithSubjectsError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif len(cfg.Sources) > 0 {\n\t\t\tresp.Error = NewJSMirrorWithSourcesError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.FilterSubject != _EMPTY_ {\n\t\t\tresp.Error = NewJSMirrorWithSubjectFiltersError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.OptStartSeq > 0 && cfg.Mirror.OptStartTime != nil {\n\t\t\tresp.Error = NewJSMirrorWithStartSeqAndTimeError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Duplicates != time.Duration(0) {\n\t\t\tresp.Error = &ApiError{Code: 400, Description: \"stream mirrors do not make use of a de-duplication window\"}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// We do not require other stream to exist anymore, but if we can see it check payloads.\n\t\texists, maxMsgSize, subs := hasStream(cfg.Mirror.Name)\n\t\tif len(subs) > 0 {\n\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t}\n\t\tif exists && cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\tresp.Error = NewJSMirrorMaxMessageSizeTooBigError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif cfg.Mirror.External != nil {\n\t\t\tif cfg.Mirror.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, cfg.Mirror.External.DeliverPrefix)\n\t\t\t}\n\t\t\tif cfg.Mirror.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, cfg.Mirror.External.ApiPrefix)\n\t\t\t}\n\t\t}\n\t}\n\tif len(cfg.Sources) > 0 {\n\t\tfor _, src := range cfg.Sources {\n\t\t\tif src.External == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\texists, maxMsgSize, subs := hasStream(src.Name)\n\t\t\tif len(subs) > 0 {\n\t\t\t\tstreamSubs = append(streamSubs, subs...)\n\t\t\t}\n\t\t\tif src.External.DeliverPrefix != _EMPTY_ {\n\t\t\t\tdeliveryPrefixes = append(deliveryPrefixes, src.External.DeliverPrefix)\n\t\t\t}\n\t\t\tif src.External.ApiPrefix != _EMPTY_ {\n\t\t\t\tapiPrefixes = append(apiPrefixes, src.External.ApiPrefix)\n\t\t\t}\n\t\t\tif exists && cfg.MaxMsgSize > 0 && maxMsgSize > 0 && cfg.MaxMsgSize < maxMsgSize {\n\t\t\t\tresp.Error = NewJSSourceMaxMessageSizeTooBigError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// check prefix overlap with subjects\n\tfor _, pfx := range deliveryPrefixes {\n\t\tif !IsValidPublishSubject(pfx) {\n\t\t\tresp.Error = NewJSStreamInvalidExternalDeliverySubjError(pfx)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tfor _, sub := range streamSubs {\n\t\t\tif SubjectsCollide(sub, fmt.Sprintf(\"%s.%s\", pfx, sub)) {\n\t\t\t\tresp.Error = NewJSStreamExternalDelPrefixOverlapsError(pfx, sub)\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\t// check if api prefixes overlap\n\tfor _, apiPfx := range apiPrefixes {\n\t\tif !IsValidPublishSubject(apiPfx) {\n\t\t\tresp.Error = &ApiError{Code: 400, Description: fmt.Sprintf(\"stream external api prefix %q must be a valid subject without wildcards\", apiPfx)}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif SubjectsCollide(apiPfx, JSApiPrefix) {\n\t\t\tresp.Error = NewJSStreamExternalApiOverlapError(apiPfx, JSApiPrefix)\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\t// Check for MaxBytes required.\n\tif acc.maxBytesRequired() && cfg.MaxBytes <= 0 {\n\t\tresp.Error = NewJSStreamMaxBytesRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Hand off to cluster for processing.\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRequest(ci, acc, subject, reply, rmsg, &cfg)\n\t\treturn\n\t}\n\n\tmset, err := acc.addStream(&cfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config()}\n\tresp.DidCreate = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to update a stream.\nfunc (s *Server) jsStreamUpdateRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamUpdateResponse{ApiResponse: ApiResponse{Type: JSApiStreamUpdateResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tvar ncfg StreamConfig\n\tif err := json.Unmarshal(msg, &ncfg); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tcfg, err := checkStreamCfg(&ncfg)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamInvalidConfigError(err)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tif streamName != cfg.Name {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamUpdateRequest(ci, acc, subject, reply, rmsg, &cfg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := mset.update(&cfg); err != nil {\n\t\tresp.Error = NewJSStreamUpdateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs, _ := s.getJetStreamCluster()\n\n\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config(), Cluster: js.clusterInfo(mset.raftGroup())}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all stream names.\nfunc (s *Server) jsStreamNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamNamesResponse{ApiResponse: ApiResponse{Type: JSApiStreamNamesResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamNamesRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar numStreams int\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tfor stream, sa := range cc.streams[acc.Name] {\n\t\t\tif IsNatsErr(sa.err, JSClusterNotAssignedErr) {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif filter != _EMPTY_ {\n\t\t\t\t// These could not have subjects auto-filled in since they are raw and unprocessed.\n\t\t\t\tif len(sa.Config.Subjects) == 0 {\n\t\t\t\t\tif SubjectsCollide(filter, sa.Config.Name) {\n\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t}\n\t\t\t\t} else {\n\t\t\t\t\tfor _, subj := range sa.Config.Subjects {\n\t\t\t\t\t\tif SubjectsCollide(filter, subj) {\n\t\t\t\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tresp.Streams = append(resp.Streams, stream)\n\t\t\t}\n\t\t}\n\t\tjs.mu.RUnlock()\n\t\tif len(resp.Streams) > 1 {\n\t\t\tsort.Slice(resp.Streams, func(i, j int) bool { return strings.Compare(resp.Streams[i], resp.Streams[j]) < 0 })\n\t\t}\n\t\tnumStreams = len(resp.Streams)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t\tresp.Streams = resp.Streams[:offset]\n\t\t}\n\t} else {\n\t\tmsets := acc.filteredStreams(filter)\n\t\t// Since we page results order matters.\n\t\tif len(msets) > 1 {\n\t\t\tsort.Slice(msets, func(i, j int) bool {\n\t\t\t\treturn strings.Compare(msets[i].cfg.Name, msets[j].cfg.Name) < 0\n\t\t\t})\n\t\t}\n\n\t\tnumStreams = len(msets)\n\t\tif offset > numStreams {\n\t\t\toffset = numStreams\n\t\t}\n\n\t\tfor _, mset := range msets[offset:] {\n\t\t\tresp.Streams = append(resp.Streams, mset.cfg.Name)\n\t\t\tif len(resp.Streams) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numStreams\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed stream info.\n// TODO(dlc) - combine with above long term\nfunc (s *Server) jsStreamListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiStreamListResponseType},\n\t\tStreams:     []*StreamInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tvar filter string\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamListRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t\tif req.Subject != _EMPTY_ {\n\t\t\tfilter = req.Subject\n\t\t}\n\t}\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\t// Need to copy these off before sending..\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() { s.jsClusteredStreamListRequest(acc, ci, filter, offset, subject, reply, msg) })\n\t\treturn\n\t}\n\n\t// TODO(dlc) - Maybe hold these results for large results that we expect to be paged.\n\t// TODO(dlc) - If this list is long maybe do this in a Go routine?\n\tvar msets []*stream\n\tif filter == _EMPTY_ {\n\t\tmsets = acc.streams()\n\t} else {\n\t\tmsets = acc.filteredStreams(filter)\n\t}\n\n\tsort.Slice(msets, func(i, j int) bool {\n\t\treturn strings.Compare(msets[i].cfg.Name, msets[j].cfg.Name) < 0\n\t})\n\n\tscnt := len(msets)\n\tif offset > scnt {\n\t\toffset = scnt\n\t}\n\n\tfor _, mset := range msets[offset:] {\n\t\tresp.Streams = append(resp.Streams, &StreamInfo{\n\t\t\tCreated: mset.createdTime(),\n\t\t\tState:   mset.state(),\n\t\t\tConfig:  mset.config(),\n\t\t\tMirror:  mset.mirrorInfo(),\n\t\t\tSources: mset.sourcesInfo()},\n\t\t)\n\t\tif len(resp.Streams) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = scnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about a stream.\nfunc (s *Server) jsStreamInfoRequest(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, hdr, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamInfoResponse{ApiResponse: ApiResponse{Type: JSApiStreamInfoResponseType}}\n\n\t// If someone creates a duplicate stream that is identical we will get this request forwarded to us.\n\t// Make sure the response type is for a create call.\n\tif rt := getHeader(JSResponseType, hdr); len(rt) > 0 && string(rt) == jsCreateResponse {\n\t\tresp.ApiResponse.Type = JSApiStreamCreateResponseType\n\t}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, streamName)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tisLeaderless := js.isGroupLeaderless(sa.Group)\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(streamName) && !isLeaderless {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), sa.Group)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar details bool\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamInfoRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tdetails = req.DeletedDetails\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tconfig := mset.config()\n\n\tjs, _ := s.getJetStreamCluster()\n\n\tresp.StreamInfo = &StreamInfo{\n\t\tCreated: mset.createdTime(),\n\t\tState:   mset.stateWithDetail(details),\n\t\tConfig:  config,\n\t\tDomain:  s.getOpts().JetStreamDomain,\n\t\tCluster: js.clusterInfo(mset.raftGroup()),\n\t}\n\tif mset.isMirror() {\n\t\tresp.StreamInfo.Mirror = mset.mirrorInfo()\n\t} else if mset.hasSources() {\n\t\tresp.StreamInfo.Sources = mset.sourcesInfo()\n\t}\n\n\t// Check for out of band catchups.\n\tif mset.hasCatchupPeers() {\n\t\tmset.checkClusterInfo(resp.StreamInfo)\n\t}\n\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a stream leader stepdown.\nfunc (s *Server) jsStreamLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiStreamLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(sa.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\tif !acc.JetStreamIsStreamLeader(name) {\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(name)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Call actual stepdown.\n\tif mset != nil {\n\t\tif node := mset.raftNode(); node != nil {\n\t\t\tnode.StepDown()\n\t\t}\n\t}\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have a consumer leader stepdown.\nfunc (s *Server) jsConsumerLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiConsumerLeaderStepDownResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tstream := tokenAt(subject, 6)\n\tconsumer := tokenAt(subject, 7)\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\tjs.mu.RUnlock()\n\n\tif isLeader && sa == nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t} else if sa == nil {\n\t\treturn\n\t}\n\tvar ca *consumerAssignment\n\tif sa.consumers != nil {\n\t\tca = sa.consumers[consumer]\n\t}\n\tif ca == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\t// Check to see if we are a member of the group and if the group has no leader.\n\tif js.isGroupLeaderless(ca.Group) {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif !acc.JetStreamIsConsumerLeader(stream, consumer) {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\to := mset.lookupConsumer(consumer)\n\tif o == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Call actual stepdown.\n\to.raftNode().StepDown()\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to remove a peer from a clustered stream.\nfunc (s *Server) jsStreamRemovePeerRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\t// Have extra token for this one.\n\tname := tokenAt(subject, 6)\n\n\tvar resp = JSApiStreamRemovePeerResponse{ApiResponse: ApiResponse{Type: JSApiStreamRemovePeerResponseType}}\n\n\t// If we are not in clustered mode this is a failed request.\n\tif !s.JetStreamIsClustered() {\n\t\tresp.Error = NewJSClusterRequiredError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we are clustered. See if we are the stream leader in order to proceed.\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil {\n\t\treturn\n\t}\n\tif js.isLeaderless() {\n\t\tresp.Error = NewJSClusterNotAvailError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tjs.mu.RLock()\n\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, name)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are meta leader.\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRemovePeerRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif req.Peer == _EMPTY_ {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif sa == nil {\n\t\t// No stream present.\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check to see if we are a member of the group and if the group has no leader.\n\t// Peers here is a server name, convert to node name.\n\tnodeName := string(getHash(req.Peer))\n\n\tjs.mu.RLock()\n\trg := sa.Group\n\tisMember := rg.isMember(nodeName)\n\tjs.mu.RUnlock()\n\n\t// Make sure we are a member.\n\tif !isMember {\n\t\tresp.Error = NewJSClusterPeerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are here we have a valid peer member set for removal.\n\tif !js.removePeerFromStream(sa, nodeName) {\n\t\tresp.Error = NewJSPeerRemapError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to have the metaleader remove a peer from the system.\nfunc (s *Server) jsLeaderServerRemoveRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar resp = JSApiMetaServerRemoveResponse{ApiResponse: ApiResponse{Type: JSApiMetaServerRemoveResponseType}}\n\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiMetaServerRemoveRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar found string\n\tjs.mu.RLock()\n\tfor _, p := range cc.meta.Peers() {\n\t\tsi, ok := s.nodeToInfo.Load(p.ID)\n\t\tif ok && si.(nodeInfo).name == req.Server {\n\t\t\tfound = p.ID\n\t\t\tbreak\n\t\t}\n\t}\n\tjs.mu.RUnlock()\n\n\tif found == _EMPTY_ {\n\t\tresp.Error = NewJSClusterServerNotMemberError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// So we have a valid peer.\n\tjs.mu.Lock()\n\tcc.meta.ProposeRemovePeer(found)\n\tjs.mu.Unlock()\n\n\tresp.Success = true\n\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n}\n\n// Request to have the meta leader stepdown.\n// These will only be received the the meta leaders, so less checking needed.\nfunc (s *Server) jsLeaderStepDownRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tjs, cc := s.getJetStreamCluster()\n\tif js == nil || cc == nil || cc.meta == nil {\n\t\treturn\n\t}\n\n\t// Extra checks here but only leader is listening.\n\tjs.mu.RLock()\n\tisLeader := cc.isLeader()\n\tjs.mu.RUnlock()\n\n\tif !isLeader {\n\t\treturn\n\t}\n\n\tvar preferredLeader string\n\tvar resp = JSApiLeaderStepDownResponse{ApiResponse: ApiResponse{Type: JSApiLeaderStepDownResponseType}}\n\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiLeaderStepdownRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif len(req.Placement.Tags) > 0 {\n\t\t\t// Tags currently not supported.\n\t\t\tresp.Error = NewJSClusterTagsError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tcn := req.Placement.Cluster\n\t\tvar peers []string\n\t\tourID := cc.meta.ID()\n\t\tfor _, p := range cc.meta.Peers() {\n\t\t\tif si, ok := s.nodeToInfo.Load(p.ID); ok && si != nil {\n\t\t\t\tif ni := si.(nodeInfo); ni.offline || ni.cluster != cn || p.ID == ourID {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tpeers = append(peers, p.ID)\n\t\t\t}\n\t\t}\n\t\tif len(peers) == 0 {\n\t\t\tresp.Error = NewJSClusterNoPeersError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Randomize and select.\n\t\tif len(peers) > 1 {\n\t\t\trand.Shuffle(len(peers), func(i, j int) { peers[i], peers[j] = peers[j], peers[i] })\n\t\t}\n\t\tpreferredLeader = peers[0]\n\t}\n\n\t// Call actual stepdown.\n\terr = cc.meta.StepDown(preferredLeader)\n\tif err != nil {\n\t\tresp.Error = NewJSRaftGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\nfunc isEmptyRequest(req []byte) bool {\n\tif len(req) == 0 {\n\t\treturn true\n\t}\n\tif bytes.Equal(req, []byte(\"{}\")) {\n\t\treturn true\n\t}\n\t// If we are here we didn't get our simple match, but still could be valid.\n\tvar v interface{}\n\tif err := json.Unmarshal(req, &v); err != nil {\n\t\treturn false\n\t}\n\tvm, ok := v.(map[string]interface{})\n\tif !ok {\n\t\treturn false\n\t}\n\treturn len(vm) == 0\n}\n\n// Request to delete a stream.\nfunc (s *Server) jsStreamDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamDeleteResponse{ApiResponse: ApiResponse{Type: JSApiStreamDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\n\t// Clustered.\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamDeleteRequest(ci, acc, stream, subject, reply, msg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif err := mset.delete(); err != nil {\n\t\tresp.Error = NewJSStreamDeleteError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete a message.\n// This expects a stream sequence number as the msg body.\nfunc (s *Server) jsMsgDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgDeleteResponse{ApiResponse: ApiResponse{Type: JSApiMsgDeleteResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgDeleteRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyDelete {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(errors.New(\"message delete not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredMsgDeleteRequest(ci, acc, mset, stream, subject, reply, &req, rmsg)\n\t\treturn\n\t}\n\n\tvar removed bool\n\tif req.NoErase {\n\t\tremoved, err = mset.removeMsg(req.Seq)\n\t} else {\n\t\tremoved, err = mset.eraseMsg(req.Seq)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSStreamMsgDeleteFailedError(err, Unless(err))\n\t} else if !removed {\n\t\tresp.Error = NewJSSequenceNotFoundError(req.Seq)\n\t} else {\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to get a raw stream message.\nfunc (s *Server) jsMsgGetRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := tokenAt(subject, 6)\n\n\tvar resp = JSApiMsgGetResponse{ApiResponse: ApiResponse{Type: JSApiMsgGetResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tvar req JSApiMsgGetRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Check that we do not have both options set.\n\tif req.Seq > 0 && req.LastFor != _EMPTY_ || req.Seq == 0 && req.LastFor == _EMPTY_ {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar subj string\n\tvar hdr []byte\n\tvar data []byte\n\tvar ts int64\n\tseq := req.Seq\n\n\tif req.Seq > 0 {\n\t\tsubj, hdr, data, ts, err = mset.store.LoadMsg(req.Seq)\n\t} else {\n\t\tsubj, seq, hdr, data, ts, err = mset.store.LoadLastMsg(req.LastFor)\n\t}\n\tif err != nil {\n\t\tresp.Error = NewJSNoMessageFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Message = &StoredMsg{\n\t\tSubject:  subj,\n\t\tSequence: seq,\n\t\tHeader:   hdr,\n\t\tData:     data,\n\t\tTime:     time.Unix(0, ts).UTC(),\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to purge a stream.\nfunc (s *Server) jsStreamPurgeRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tvar resp = JSApiStreamPurgeResponse{ApiResponse: ApiResponse{Type: JSApiStreamPurgeResponseType}}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the stream is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa := cc.isLeader(), js.streamAssignment(acc.Name, stream)\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && sa == nil {\n\t\t\t// We can't find the stream, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// No stream present.\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if sa == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(sa.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the stream assigned and a leader, so only the stream leader should answer.\n\t\tif !acc.JetStreamIsStreamLeader(stream) {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar purgeRequest *JSApiStreamPurgeRequest\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiStreamPurgeRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif req.Sequence > 0 && req.Keep > 0 {\n\t\t\tresp.Error = NewJSBadRequestError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tpurgeRequest = &req\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.Sealed {\n\t\tresp.Error = NewJSStreamSealedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif mset.cfg.DenyPurge {\n\t\tresp.Error = NewJSStreamPurgeFailedError(errors.New(\"stream purge not permitted\"))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamPurgeRequest(ci, acc, mset, stream, subject, reply, rmsg, purgeRequest)\n\t\treturn\n\t}\n\n\tpurged, err := mset.purge(purgeRequest)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t} else {\n\t\tresp.Purged = purged\n\t\tresp.Success = true\n\t}\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to restore a stream.\nfunc (s *Server) jsStreamRestoreRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamIsLeader() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tvar req JSApiStreamRestoreRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tstream := streamNameFromSubject(subject)\n\n\tif stream != req.Config.Name && req.Config.Name == _EMPTY_ {\n\t\treq.Config.Name = stream\n\t}\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredStreamRestoreRequest(ci, acc, &req, stream, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tif _, err := acc.lookupStream(stream); err == nil {\n\t\tresp.Error = NewJSStreamNameExistError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\ts.processStreamRestore(ci, acc, &req.Config, subject, reply, string(msg))\n}\n\nfunc (s *Server) processStreamRestore(ci *ClientInfo, acc *Account, cfg *StreamConfig, subject, reply, msg string) <-chan error {\n\tjs := s.getJetStream()\n\n\tvar resp = JSApiStreamRestoreResponse{ApiResponse: ApiResponse{Type: JSApiStreamRestoreResponseType}}\n\n\tsnapDir := path.Join(js.config.StoreDir, snapStagingDir)\n\tif _, err := os.Stat(snapDir); os.IsNotExist(err) {\n\t\tif err := os.MkdirAll(snapDir, defaultDirPerms); err != nil {\n\t\t\tresp.Error = &ApiError{Code: 503, Description: \"JetStream unable to create temp storage for restore\"}\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn nil\n\t\t}\n\t}\n\n\ttfile, err := ioutil.TempFile(snapDir, \"js-restore-\")\n\tif err != nil {\n\t\tresp.Error = NewJSTempStorageFailedError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\tstreamName := cfg.Name\n\ts.Noticef(\"Starting restore for stream '%s > %s'\", acc.Name, streamName)\n\n\tstart := time.Now().UTC()\n\tdomain := s.getOpts().JetStreamDomain\n\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCreatePre+\".\"+streamName, &JSRestoreCreateAdvisory{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSRestoreCreateAdvisoryType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: start,\n\t\t},\n\t\tStream: streamName,\n\t\tClient: ci,\n\t\tDomain: domain,\n\t})\n\n\t// Create our internal subscription to accept the snapshot.\n\trestoreSubj := fmt.Sprintf(jsRestoreDeliverT, streamName, nuid.Next())\n\n\ttype result struct {\n\t\terr   error\n\t\treply string\n\t}\n\n\t// For signaling to upper layers.\n\tresultCh := make(chan result, 1)\n\tactiveQ := newIPQueue() // of int\n\n\tvar total int\n\n\t// FIXM(dlc) - Probably take out of network path eventually due to disk I/O?\n\tprocessChunk := func(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n\t\t// We require reply subjects to communicate back failures, flow etc. If they do not have one log and cancel.\n\t\tif reply == _EMPTY_ {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' requires reply subject for each chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Account client messages have \\r\\n on end. This is an error.\n\t\tif len(msg) < LEN_CR_LF {\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t\tresultCh <- result{\n\t\t\t\tfmt.Errorf(\"restore for stream '%s > %s' received short chunk\", acc.Name, streamName),\n\t\t\t\treply,\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\t// Adjust.\n\t\tmsg = msg[:len(msg)-LEN_CR_LF]\n\n\t\t// This means we are complete with our transfer from the client.\n\t\tif len(msg) == 0 {\n\t\t\ts.Debugf(\"Finished staging restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\tresultCh <- result{err, reply}\n\t\t\treturn\n\t\t}\n\n\t\t// We track total and check on server limits.\n\t\t// TODO(dlc) - We could check apriori and cancel initial request if we know it won't fit.\n\t\ttotal += len(msg)\n\t\tif js.wouldExceedLimits(FileStorage, total) {\n\t\t\ts.resourcesExeededError()\n\t\t\tresultCh <- result{NewJSInsufficientResourcesError(), reply}\n\t\t\treturn\n\t\t}\n\n\t\t// Append chunk to temp file. Mark as issue if we encounter an error.\n\t\tif n, err := tfile.Write(msg); n != len(msg) || err != nil {\n\t\t\tresultCh <- result{err, reply}\n\t\t\tif reply != _EMPTY_ {\n\t\t\t\ts.sendInternalAccountMsg(acc, reply, \"-ERR 'storage failure during restore'\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\tactiveQ.push(len(msg))\n\n\t\ts.sendInternalAccountMsg(acc, reply, nil)\n\t}\n\n\tsub, err := acc.subscribeInternal(restoreSubj, processChunk)\n\tif err != nil {\n\t\ttfile.Close()\n\t\tos.Remove(tfile.Name())\n\t\tresp.Error = NewJSRestoreSubscribeFailedError(err, restoreSubj)\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, msg, s.jsonResponse(&resp))\n\t\treturn nil\n\t}\n\n\t// Mark the subject so the end user knows where to send the snapshot chunks.\n\tresp.DeliverSubject = restoreSubj\n\ts.sendAPIResponse(ci, acc, subject, reply, msg, s.jsonResponse(resp))\n\n\tdoneCh := make(chan error, 1)\n\n\t// Monitor the progress from another Go routine.\n\ts.startGoRoutine(func() {\n\t\tdefer s.grWG.Done()\n\t\tdefer func() {\n\t\t\ttfile.Close()\n\t\t\tos.Remove(tfile.Name())\n\t\t\tsub.client.processUnsub(sub.sid)\n\t\t}()\n\n\t\tconst activityInterval = 5 * time.Second\n\t\tnotActive := time.NewTimer(activityInterval)\n\t\tdefer notActive.Stop()\n\n\t\ttotal := 0\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase result := <-resultCh:\n\t\t\t\terr := result.err\n\t\t\t\tvar mset *stream\n\n\t\t\t\t// If we staged properly go ahead and do restore now.\n\t\t\t\tif err == nil {\n\t\t\t\t\ts.Debugf(\"Finalizing restore for stream '%s > %s'\", acc.Name, streamName)\n\t\t\t\t\ttfile.Seek(0, 0)\n\t\t\t\t\tmset, err = acc.RestoreStream(cfg, tfile)\n\t\t\t\t} else {\n\t\t\t\t\terrStr := err.Error()\n\t\t\t\t\ttmp := []rune(errStr)\n\t\t\t\t\ttmp[0] = unicode.ToUpper(tmp[0])\n\t\t\t\t\ts.Warnf(errStr)\n\t\t\t\t}\n\n\t\t\t\tend := time.Now().UTC()\n\n\t\t\t\t// TODO(rip) - Should this have the error code in it??\n\t\t\t\ts.publishAdvisory(acc, JSAdvisoryStreamRestoreCompletePre+\".\"+streamName, &JSRestoreCompleteAdvisory{\n\t\t\t\t\tTypedEvent: TypedEvent{\n\t\t\t\t\t\tType: JSRestoreCompleteAdvisoryType,\n\t\t\t\t\t\tID:   nuid.Next(),\n\t\t\t\t\t\tTime: end,\n\t\t\t\t\t},\n\t\t\t\t\tStream: streamName,\n\t\t\t\t\tStart:  start,\n\t\t\t\t\tEnd:    end,\n\t\t\t\t\tBytes:  int64(total),\n\t\t\t\t\tClient: ci,\n\t\t\t\t\tDomain: domain,\n\t\t\t\t})\n\n\t\t\t\tvar resp = JSApiStreamCreateResponse{ApiResponse: ApiResponse{Type: JSApiStreamCreateResponseType}}\n\n\t\t\t\tif err != nil {\n\t\t\t\t\tresp.Error = NewJSStreamRestoreError(err, Unless(err))\n\t\t\t\t\ts.Warnf(\"Restore failed for %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), streamName, acc.Name, end.Sub(start))\n\t\t\t\t} else {\n\t\t\t\t\tresp.StreamInfo = &StreamInfo{Created: mset.createdTime(), State: mset.state(), Config: mset.config()}\n\t\t\t\t\ts.Noticef(\"Completed restore of %s for stream '%s > %s' in %v\",\n\t\t\t\t\t\tfriendlyBytes(int64(total)), streamName, acc.Name, end.Sub(start))\n\t\t\t\t}\n\n\t\t\t\t// On the last EOF, send back the stream info or error status.\n\t\t\t\ts.sendInternalAccountMsg(acc, result.reply, s.jsonResponse(&resp))\n\t\t\t\t// Signal to the upper layers.\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\tcase <-activeQ.ch:\n\t\t\t\tn := activeQ.popOne().(int)\n\t\t\t\ttotal += n\n\t\t\t\tnotActive.Reset(activityInterval)\n\t\t\tcase <-notActive.C:\n\t\t\t\terr := fmt.Errorf(\"restore for stream '%s > %s' is stalled\", acc, streamName)\n\t\t\t\tdoneCh <- err\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t})\n\n\treturn doneCh\n}\n\n// Process a snapshot request.\nfunc (s *Server) jsStreamSnapshotRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tsmsg := string(msg)\n\tstream := streamNameFromSubject(subject)\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() && !acc.JetStreamIsStreamLeader(stream) {\n\t\treturn\n\t}\n\n\tvar resp = JSApiStreamSnapshotResponse{ApiResponse: ApiResponse{Type: JSApiStreamSnapshotResponseType}}\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif isEmptyRequest(msg) {\n\t\tresp.Error = NewJSBadRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar req JSApiStreamSnapshotRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif !IsValidSubject(req.DeliverSubject) {\n\t\tresp.Error = NewJSSnapshotDeliverSubjectInvalidError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We will do the snapshot in a go routine as well since check msgs may\n\t// stall this go routine.\n\tgo func() {\n\t\tif req.CheckMsgs {\n\t\t\ts.Noticef(\"Starting health check and snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t} else {\n\t\t\ts.Noticef(\"Starting snapshot for stream '%s > %s'\", mset.jsa.account.Name, mset.name())\n\t\t}\n\n\t\tstart := time.Now().UTC()\n\n\t\tsr, err := mset.snapshot(0, req.CheckMsgs, !req.NoConsumers)\n\t\tif err != nil {\n\t\t\ts.Warnf(\"Snapshot of stream '%s > %s' failed: %v\", mset.jsa.account.Name, mset.name(), err)\n\t\t\tresp.Error = NewJSStreamSnapshotError(err, Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, smsg, s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tconfig := mset.config()\n\t\tresp.State = &sr.State\n\t\tresp.Config = &config\n\n\t\ts.sendAPIResponse(ci, acc, subject, reply, smsg, s.jsonResponse(resp))\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCreatePre+\".\"+mset.name(), &JSSnapshotCreateAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCreatedAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: time.Now().UTC(),\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tState:  sr.State,\n\t\t\tClient: ci,\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\t// Now do the real streaming.\n\t\ts.streamSnapshot(ci, acc, mset, sr, &req)\n\n\t\tend := time.Now().UTC()\n\n\t\ts.publishAdvisory(acc, JSAdvisoryStreamSnapshotCompletePre+\".\"+mset.name(), &JSSnapshotCompleteAdvisory{\n\t\t\tTypedEvent: TypedEvent{\n\t\t\t\tType: JSSnapshotCompleteAdvisoryType,\n\t\t\t\tID:   nuid.Next(),\n\t\t\t\tTime: end,\n\t\t\t},\n\t\t\tStream: mset.name(),\n\t\t\tStart:  start,\n\t\t\tEnd:    end,\n\t\t\tClient: ci,\n\t\t\tDomain: s.getOpts().JetStreamDomain,\n\t\t})\n\n\t\ts.Noticef(\"Completed snapshot of %s for stream '%s > %s' in %v\",\n\t\t\tfriendlyBytes(int64(sr.State.Bytes)),\n\t\t\tmset.jsa.account.Name,\n\t\t\tmset.name(),\n\t\t\tend.Sub(start))\n\t}()\n}\n\n// Default chunk size for now.\nconst defaultSnapshotChunkSize = 256 * 1024\nconst defaultSnapshotWindowSize = 32 * 1024 * 1024 // 32MB\n\n// streamSnapshot will stream out our snapshot to the reply subject.\nfunc (s *Server) streamSnapshot(ci *ClientInfo, acc *Account, mset *stream, sr *SnapshotResult, req *JSApiStreamSnapshotRequest) {\n\tchunkSize := req.ChunkSize\n\tif chunkSize == 0 {\n\t\tchunkSize = defaultSnapshotChunkSize\n\t}\n\t// Setup for the chunk stream.\n\treply := req.DeliverSubject\n\tr := sr.Reader\n\tdefer r.Close()\n\n\t// Check interest for the snapshot deliver subject.\n\tinch := make(chan bool, 1)\n\tacc.sl.RegisterNotification(req.DeliverSubject, inch)\n\tdefer acc.sl.ClearNotification(req.DeliverSubject, inch)\n\thasInterest := <-inch\n\tif !hasInterest {\n\t\t// Allow 2 seconds or so for interest to show up.\n\t\tselect {\n\t\tcase <-inch:\n\t\tcase <-time.After(2 * time.Second):\n\t\t}\n\t}\n\n\t// Create our ack flow handler.\n\t// This is very simple for now.\n\tacks := make(chan struct{}, 1)\n\tacks <- struct{}{}\n\n\t// Track bytes outstanding.\n\tvar out int32\n\n\t// We will place sequence number and size of chunk sent in the reply.\n\tackSubj := fmt.Sprintf(jsSnapshotAckT, mset.name(), nuid.Next())\n\tackSub, _ := mset.subscribeInternalUnlocked(ackSubj+\".>\", func(_ *subscription, _ *client, _ *Account, subject, _ string, _ []byte) {\n\t\tcs, _ := strconv.Atoi(tokenAt(subject, 6))\n\t\t// This is very crude and simple, but ok for now.\n\t\t// This only matters when sending multiple chunks.\n\t\tif atomic.AddInt32(&out, int32(-cs)) < defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase acks <- struct{}{}:\n\t\t\tdefault:\n\t\t\t}\n\t\t}\n\t})\n\tdefer mset.unsubscribeUnlocked(ackSub)\n\n\t// TODO(dlc) - Add in NATS-Chunked-Sequence header\n\n\tfor index := 1; ; index++ {\n\t\tchunk := make([]byte, chunkSize)\n\t\tn, err := r.Read(chunk)\n\t\tchunk = chunk[:n]\n\t\tif err != nil {\n\t\t\tif n > 0 {\n\t\t\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, chunk, nil, 0))\n\t\t\t}\n\t\t\tbreak\n\t\t}\n\n\t\t// Wait on acks for flow control if past our window size.\n\t\t// Wait up to 1ms for now if no acks received.\n\t\tif atomic.LoadInt32(&out) > defaultSnapshotWindowSize {\n\t\t\tselect {\n\t\t\tcase <-acks:\n\t\t\tcase <-inch: // Lost interest\n\t\t\t\tgoto done\n\t\t\tcase <-time.After(10 * time.Millisecond):\n\t\t\t}\n\t\t}\n\t\tackReply := fmt.Sprintf(\"%s.%d.%d\", ackSubj, len(chunk), index)\n\t\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, ackReply, nil, chunk, nil, 0))\n\t\tatomic.AddInt32(&out, int32(len(chunk)))\n\t}\ndone:\n\t// Send last EOF\n\t// TODO(dlc) - place hash in header\n\tmset.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, nil, nil, nil, 0))\n}\n\n// Request to create a durable consumer.\nfunc (s *Server) jsDurableCreateRequest(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\ts.jsConsumerCreate(sub, c, acc, subject, reply, msg, true)\n}\n\n// Request to create a consumer.\nfunc (s *Server) jsConsumerCreateRequest(sub *subscription, c *client, acc *Account, subject, reply string, msg []byte) {\n\ts.jsConsumerCreate(sub, c, acc, subject, reply, msg, false)\n}\n\nfunc (s *Server) jsConsumerCreate(sub *subscription, c *client, a *Account, subject, reply string, rmsg []byte, expectDurable bool) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerCreateResponse{ApiResponse: ApiResponse{Type: JSApiConsumerCreateResponseType}}\n\n\tvar streamName string\n\tif expectDurable {\n\t\tstreamName = tokenAt(subject, 6)\n\t} else {\n\t\tstreamName = tokenAt(subject, 5)\n\t}\n\n\tvar req CreateConsumerRequest\n\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\tresp.Error = NewJSInvalidJSONError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// We reject if flow control is set without heartbeats.\n\tif req.Config.FlowControl && req.Config.Heartbeat == 0 {\n\t\tresp.Error = NewJSConsumerWithFlowControlNeedsHeartbeatsError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// Make sure we have sane defaults.\n\tsetConsumerConfigDefaults(&req.Config)\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tif req.Config.Direct {\n\t\t\t// Check to see if we have this stream and are the stream leader.\n\t\t\tif !acc.JetStreamIsStreamLeader(streamName) {\n\t\t\t\treturn\n\t\t\t}\n\t\t} else {\n\t\t\tjs, cc := s.getJetStreamCluster()\n\t\t\tif js == nil || cc == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// Make sure we are meta leader.\n\t\t\tif !s.JetStreamIsLeader() {\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif streamName != req.Stream {\n\t\tresp.Error = NewJSStreamMismatchError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tif expectDurable {\n\t\tif numTokens(subject) != 7 {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotInSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Now check on requirements for durable request.\n\t\tif req.Config.Durable == _EMPTY_ {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotSetError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tconsumerName := tokenAt(subject, 7)\n\t\tif consumerName != req.Config.Durable {\n\t\t\tresp.Error = NewJSConsumerDurableNameNotMatchSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t} else {\n\t\tif numTokens(subject) != 5 {\n\t\t\tresp.Error = NewJSConsumerEphemeralWithDurableInSubjectError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tif req.Config.Durable != _EMPTY_ {\n\t\t\tresp.Error = NewJSConsumerEphemeralWithDurableNameError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t}\n\n\tif s.JetStreamIsClustered() && !req.Config.Direct {\n\t\ts.jsClusteredConsumerRequest(ci, acc, subject, reply, rmsg, req.Stream, &req.Config)\n\t\treturn\n\t}\n\n\tstream, err := acc.lookupStream(req.Stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\to, err := stream.addConsumer(&req.Config)\n\tif err != nil {\n\t\tresp.Error = NewJSConsumerCreateError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.ConsumerInfo = o.info()\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all consumer names.\nfunc (s *Server) jsConsumerNamesRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerNamesResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerNamesResponseType},\n\t\tConsumers:   []string{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tvar numConsumers int\n\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\t// TODO(dlc) - Debug or Warn?\n\t\t\treturn\n\t\t}\n\t\tjs.mu.RLock()\n\t\tsas := cc.streams[acc.Name]\n\t\tif sas == nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tsa := sas[streamName]\n\t\tif sa == nil || sa.err != nil {\n\t\t\tjs.mu.RUnlock()\n\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\tfor consumer := range sa.consumers {\n\t\t\tresp.Consumers = append(resp.Consumers, consumer)\n\t\t}\n\t\tif len(resp.Consumers) > 1 {\n\t\t\tsort.Slice(resp.Consumers, func(i, j int) bool { return strings.Compare(resp.Consumers[i], resp.Consumers[j]) < 0 })\n\t\t}\n\t\tnumConsumers = len(resp.Consumers)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t\tresp.Consumers = resp.Consumers[:offset]\n\t\t}\n\t\tjs.mu.RUnlock()\n\n\t} else {\n\t\tmset, err := acc.lookupStream(streamName)\n\t\tif err != nil {\n\t\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\tobs := mset.getPublicConsumers()\n\t\tsort.Slice(obs, func(i, j int) bool {\n\t\t\treturn strings.Compare(obs[i].name, obs[j].name) < 0\n\t\t})\n\n\t\tnumConsumers = len(obs)\n\t\tif offset > numConsumers {\n\t\t\toffset = numConsumers\n\t\t}\n\n\t\tfor _, o := range obs[offset:] {\n\t\t\tresp.Consumers = append(resp.Consumers, o.String())\n\t\t\tif len(resp.Consumers) >= JSApiNamesLimit {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\tresp.Total = numConsumers\n\tresp.Limit = JSApiNamesLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for the list of all detailed consumer information.\nfunc (s *Server) jsConsumerListRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerListResponse{\n\t\tApiResponse: ApiResponse{Type: JSApiConsumerListResponseType},\n\t\tConsumers:   []*ConsumerInfo{},\n\t}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\n\tvar offset int\n\tif !isEmptyRequest(msg) {\n\t\tvar req JSApiConsumersRequest\n\t\tif err := json.Unmarshal(msg, &req); err != nil {\n\t\t\tresp.Error = NewJSInvalidJSONError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\toffset = req.Offset\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\n\t// Clustered mode will invoke a scatter and gather.\n\tif s.JetStreamIsClustered() {\n\t\tmsg = copyBytes(msg)\n\t\ts.startGoRoutine(func() {\n\t\t\ts.jsClusteredConsumerListRequest(acc, ci, offset, streamName, subject, reply, msg)\n\t\t})\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.getPublicConsumers()\n\tsort.Slice(obs, func(i, j int) bool {\n\t\treturn strings.Compare(obs[i].name, obs[j].name) < 0\n\t})\n\n\tocnt := len(obs)\n\tif offset > ocnt {\n\t\toffset = ocnt\n\t}\n\n\tfor _, o := range obs[offset:] {\n\t\tresp.Consumers = append(resp.Consumers, o.info())\n\t\tif len(resp.Consumers) >= JSApiListLimit {\n\t\t\tbreak\n\t\t}\n\t}\n\tresp.Total = ocnt\n\tresp.Limit = JSApiListLimit\n\tresp.Offset = offset\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request for information about an consumer.\nfunc (s *Server) jsConsumerInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tstreamName := streamNameFromSubject(subject)\n\tconsumerName := consumerNameFromSubject(subject)\n\n\tvar resp = JSApiConsumerInfoResponse{ApiResponse: ApiResponse{Type: JSApiConsumerInfoResponseType}}\n\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\t// If we are in clustered mode we need to be the stream leader to proceed.\n\tif s.JetStreamIsClustered() {\n\t\t// Check to make sure the consumer is assigned.\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\n\t\tjs.mu.RLock()\n\t\tisLeader, sa, ca := cc.isLeader(), js.streamAssignment(acc.Name, streamName), js.consumerAssignment(acc.Name, streamName, consumerName)\n\t\tourID := cc.meta.ID()\n\t\tjs.mu.RUnlock()\n\n\t\tif isLeader && ca == nil {\n\t\t\t// We can't find the consumer, so mimic what would be the errors below.\n\t\t\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\t\t\tif doErr {\n\t\t\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif sa == nil {\n\t\t\t\tresp.Error = NewJSStreamNotFoundError()\n\t\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here the consumer is not present.\n\t\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t} else if ca == nil {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), nil)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\t// Check to see if we are a member of the group and if the group has no leader.\n\t\tif js.isGroupLeaderless(ca.Group) {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\n\t\t// We have the consumer assigned and a leader, so only the consumer leader should answer.\n\t\tif !acc.JetStreamIsConsumerLeader(streamName, consumerName) {\n\t\t\tif js.isLeaderless() {\n\t\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\t\t// Delaying an error response gives the leader a chance to respond before us\n\t\t\t\ts.sendDelayedAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp), ca.Group)\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif ca == nil {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// We have a consumer assignment.\n\t\t\tjs.mu.RLock()\n\t\t\tvar node RaftNode\n\t\t\tif rg := ca.Group; rg != nil && rg.node != nil && rg.isMember(ourID) {\n\t\t\t\tnode = rg.node\n\t\t\t}\n\t\t\tjs.mu.RUnlock()\n\t\t\t// Check if we should ignore all together.\n\t\t\tif node == nil {\n\t\t\t\t// We have been assigned and are pending.\n\t\t\t\tif ca.pending {\n\t\t\t\t\t// Send our config and defaults for state and no cluster info.\n\t\t\t\t\tresp.ConsumerInfo = &ConsumerInfo{\n\t\t\t\t\t\tStream:  ca.Stream,\n\t\t\t\t\t\tName:    ca.Name,\n\t\t\t\t\t\tCreated: ca.Created,\n\t\t\t\t\t\tConfig:  ca.Config,\n\t\t\t\t\t}\n\t\t\t\t\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t\tif node != nil && (node.GroupLeader() != _EMPTY_ || node.HadPreviousLeader()) {\n\t\t\t\treturn\n\t\t\t}\n\t\t\t// If we are here we are a member and this is just a new consumer that does not have a leader yet.\n\t\t\t// Will fall through and return what we have. All consumers can respond but this should be very rare\n\t\t\t// but makes more sense to clients when they try to create, get a consumer exists, and then do consumer info.\n\t\t}\n\t}\n\n\tif !acc.JetStreamEnabled() {\n\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(streamName)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumerName)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.ConsumerInfo = obs.info()\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// Request to delete an Consumer.\nfunc (s *Server) jsConsumerDeleteRequest(sub *subscription, c *client, _ *Account, subject, reply string, rmsg []byte) {\n\tif c == nil || !s.JetStreamEnabled() {\n\t\treturn\n\t}\n\tci, acc, _, msg, err := s.getRequestInfo(c, rmsg)\n\tif err != nil {\n\t\ts.Warnf(badAPIRequestT, msg)\n\t\treturn\n\t}\n\n\tvar resp = JSApiConsumerDeleteResponse{ApiResponse: ApiResponse{Type: JSApiConsumerDeleteResponseType}}\n\n\t// Determine if we should proceed here when we are in clustered mode.\n\tif s.JetStreamIsClustered() {\n\t\tjs, cc := s.getJetStreamCluster()\n\t\tif js == nil || cc == nil {\n\t\t\treturn\n\t\t}\n\t\tif js.isLeaderless() {\n\t\t\tresp.Error = NewJSClusterNotAvailError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t\treturn\n\t\t}\n\t\t// Make sure we are meta leader.\n\t\tif !s.JetStreamIsLeader() {\n\t\t\treturn\n\t\t}\n\t}\n\n\tif hasJS, doErr := acc.checkJetStream(); !hasJS {\n\t\tif doErr {\n\t\t\tresp.Error = NewJSNotEnabledForAccountError()\n\t\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\t}\n\t\treturn\n\t}\n\tif !isEmptyRequest(msg) {\n\t\tresp.Error = NewJSNotEmptyRequestError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tstream := streamNameFromSubject(subject)\n\tconsumer := consumerNameFromSubject(subject)\n\n\tif s.JetStreamIsClustered() {\n\t\ts.jsClusteredConsumerDeleteRequest(ci, acc, stream, consumer, subject, reply, rmsg)\n\t\treturn\n\t}\n\n\tmset, err := acc.lookupStream(stream)\n\tif err != nil {\n\t\tresp.Error = NewJSStreamNotFoundError(Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\n\tobs := mset.lookupConsumer(consumer)\n\tif obs == nil {\n\t\tresp.Error = NewJSConsumerNotFoundError()\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tif err := obs.delete(); err != nil {\n\t\tresp.Error = NewJSStreamGeneralError(err, Unless(err))\n\t\ts.sendAPIErrResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(&resp))\n\t\treturn\n\t}\n\tresp.Success = true\n\ts.sendAPIResponse(ci, acc, subject, reply, string(msg), s.jsonResponse(resp))\n}\n\n// sendJetStreamAPIAuditAdvisor will send the audit event for a given event.\nfunc (s *Server) sendJetStreamAPIAuditAdvisory(ci *ClientInfo, acc *Account, subject, request, response string) {\n\ts.publishAdvisory(acc, JSAuditAdvisory, JSAPIAudit{\n\t\tTypedEvent: TypedEvent{\n\t\t\tType: JSAPIAuditType,\n\t\t\tID:   nuid.Next(),\n\t\t\tTime: time.Now().UTC(),\n\t\t},\n\t\tServer:   s.Name(),\n\t\tClient:   ci,\n\t\tSubject:  subject,\n\t\tRequest:  request,\n\t\tResponse: response,\n\t\tDomain:   s.getOpts().JetStreamDomain,\n\t})\n}\n", "idx": 1, "id": 14413, "msg": "Remove this, and then have `resp.Error = NewJS...` that you will get adding the new error.", "proj": "nats-io-nats-server", "lang": "go"}
{"patch": "@@ -77,7 +77,7 @@ func (c *createCommand) AppendFlags(f *flag.FlagSet) {\n \tf.Var(&c.selectors, \"selector\", \"A colon-delimited type:value selector. Can be used more than once\")\n \tf.Var(&c.federatesWith, \"federatesWith\", \"SPIFFE ID of a trust domain to federate with. Can be used more than once\")\n \tf.BoolVar(&c.node, \"node\", false, \"If set, this entry will be applied to matching nodes rather than workloads\")\n-\tf.BoolVar(&c.admin, \"admin\", false, \"If set, the SPIFFE ID in this entry will be granted access to the Registration API\")\n+\tf.BoolVar(&c.admin, \"admin\", false, \"If set, the SPIFFE ID in this entry will be granted access to the Server APIs\")\n \tf.BoolVar(&c.downstream, \"downstream\", false, \"A boolean value that, when set, indicates that the entry describes a downstream SPIRE server\")\n \tf.Int64Var(&c.entryExpiry, \"entryExpiry\", 0, \"An expiry, from epoch in seconds, for the resulting registration entry to be pruned\")\n \tf.Var(&c.dnsNames, \"dns\", \"A DNS name that will be included in SVIDs issued based on this entry, where appropriate. Can be used more than once\")", "y": 1, "oldf": "package entry\n\nimport (\n\t\"errors\"\n\t\"flag\"\n\n\t\"github.com/mitchellh/cli\"\n\t\"github.com/spiffe/spire/cmd/spire-server/util\"\n\tcommon_cli \"github.com/spiffe/spire/pkg/common/cli\"\n\t\"github.com/spiffe/spire/pkg/common/idutil\"\n\t\"github.com/spiffe/spire/proto/spire/api/server/entry/v1\"\n\t\"github.com/spiffe/spire/proto/spire/types\"\n\t\"google.golang.org/grpc/codes\"\n\n\t\"golang.org/x/net/context\"\n)\n\n// NewCreateCommand creates a new \"create\" subcommand for \"entry\" command.\nfunc NewCreateCommand() cli.Command {\n\treturn newCreateCommand(common_cli.DefaultEnv)\n}\n\nfunc newCreateCommand(env *common_cli.Env) cli.Command {\n\treturn util.AdaptCommand(env, new(createCommand))\n}\n\ntype createCommand struct {\n\t// Path to an optional data file. If set, other\n\t// opts will be ignored.\n\tpath string\n\n\t// Type and value are delimited by a colon (:)\n\t// ex. \"unix:uid:1000\" or \"spiffe_id:spiffe://example.org/foo\"\n\tselectors StringsFlag\n\n\t// Workload parent spiffeID\n\tparentID string\n\n\t// Workload spiffeID\n\tspiffeID string\n\n\t// TTL for certificates issued to this workload\n\tttl int\n\n\t// List of SPIFFE IDs of trust domains the registration entry is federated with\n\tfederatesWith StringsFlag\n\n\t// Whether or not the registration entry is for an \"admin\" workload\n\tadmin bool\n\n\t// Whether or not the entry is for a downstream SPIRE server\n\tdownstream bool\n\n\t// Whether or not the entry represents a node or group of nodes\n\tnode bool\n\n\t// Expiry of entry\n\tentryExpiry int64\n\n\t// DNSNames entries for SVIDs based on this entry\n\tdnsNames StringsFlag\n}\n\nfunc (*createCommand) Name() string {\n\treturn \"entry create\"\n}\n\nfunc (*createCommand) Synopsis() string {\n\treturn \"Creates registration entries\"\n}\n\nfunc (c *createCommand) AppendFlags(f *flag.FlagSet) {\n\tf.StringVar(&c.parentID, \"parentID\", \"\", \"The SPIFFE ID of this record's parent\")\n\tf.StringVar(&c.spiffeID, \"spiffeID\", \"\", \"The SPIFFE ID that this record represents\")\n\tf.IntVar(&c.ttl, \"ttl\", 0, \"The lifetime, in seconds, for SVIDs issued based on this registration entry\")\n\tf.StringVar(&c.path, \"data\", \"\", \"Path to a file containing registration JSON (optional). If set to '-', read the JSON from stdin.\")\n\tf.Var(&c.selectors, \"selector\", \"A colon-delimited type:value selector. Can be used more than once\")\n\tf.Var(&c.federatesWith, \"federatesWith\", \"SPIFFE ID of a trust domain to federate with. Can be used more than once\")\n\tf.BoolVar(&c.node, \"node\", false, \"If set, this entry will be applied to matching nodes rather than workloads\")\n\tf.BoolVar(&c.admin, \"admin\", false, \"If set, the SPIFFE ID in this entry will be granted access to the Registration API\")\n\tf.BoolVar(&c.downstream, \"downstream\", false, \"A boolean value that, when set, indicates that the entry describes a downstream SPIRE server\")\n\tf.Int64Var(&c.entryExpiry, \"entryExpiry\", 0, \"An expiry, from epoch in seconds, for the resulting registration entry to be pruned\")\n\tf.Var(&c.dnsNames, \"dns\", \"A DNS name that will be included in SVIDs issued based on this entry, where appropriate. Can be used more than once\")\n}\n\nfunc (c *createCommand) Run(ctx context.Context, env *common_cli.Env, serverClient util.ServerClient) error {\n\tif err := c.validate(); err != nil {\n\t\treturn err\n\t}\n\n\tvar entries []*types.Entry\n\tvar err error\n\tif c.path != \"\" {\n\t\tentries, err = parseFile(c.path)\n\t} else {\n\t\tentries, err = c.parseConfig()\n\t}\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tsucceeded, failed, err := createEntries(ctx, serverClient.NewEntryClient(), entries)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Print entries that succeeded to be created\n\tfor _, r := range succeeded {\n\t\tprintEntry(r.Entry, env.Printf)\n\t}\n\n\t// Print entries that failed to be created\n\tfor _, r := range failed {\n\t\tenv.ErrPrintf(\"Failed to create the following entry (code: %s, msg: %q):\\n\",\n\t\t\tcodes.Code(r.Status.Code),\n\t\t\tr.Status.Message)\n\t\tprintEntry(r.Entry, env.ErrPrintf)\n\t}\n\n\tif len(failed) > 0 {\n\t\treturn errors.New(\"failed to create one or more entries\")\n\t}\n\n\treturn nil\n}\n\n// validate performs basic validation, even on fields that we\n// have defaults defined for.\nfunc (c *createCommand) validate() (err error) {\n\t// If a path is set, we have all we need\n\tif c.path != \"\" {\n\t\treturn nil\n\t}\n\n\tif len(c.selectors) < 1 {\n\t\treturn errors.New(\"at least one selector is required\")\n\t}\n\n\tif c.node && len(c.federatesWith) > 0 {\n\t\treturn errors.New(\"node entries can not federate\")\n\t}\n\n\tif c.parentID == \"\" && !c.node {\n\t\treturn errors.New(\"a parent ID is required if the node flag is not set\")\n\t}\n\n\tif c.spiffeID == \"\" {\n\t\treturn errors.New(\"a SPIFFE ID is required\")\n\t}\n\n\tif c.ttl < 0 {\n\t\treturn errors.New(\"a positive TTL is required\")\n\t}\n\n\t// make sure all SPIFFE ID's are well formed\n\tc.spiffeID, err = idutil.NormalizeSpiffeID(c.spiffeID, idutil.AllowAny())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tif c.parentID != \"\" {\n\t\tc.parentID, err = idutil.NormalizeSpiffeID(c.parentID, idutil.AllowAny())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\tfor i := range c.federatesWith {\n\t\tc.federatesWith[i], err = idutil.NormalizeSpiffeID(c.federatesWith[i], idutil.AllowAny())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// parseConfig builds a registration entry from the given config\nfunc (c *createCommand) parseConfig() ([]*types.Entry, error) {\n\tspiffeID, err := idStringToProto(c.spiffeID)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tparentID, err := getParentID(c, spiffeID.TrustDomain)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\te := &types.Entry{\n\t\tParentId:   parentID,\n\t\tSpiffeId:   spiffeID,\n\t\tTtl:        int32(c.ttl),\n\t\tDownstream: c.downstream,\n\t\tExpiresAt:  c.entryExpiry,\n\t\tDnsNames:   c.dnsNames,\n\t}\n\n\tselectors := []*types.Selector{}\n\tfor _, s := range c.selectors {\n\t\tcs, err := parseSelector(s)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tselectors = append(selectors, cs)\n\t}\n\n\te.Selectors = selectors\n\te.FederatesWith = c.federatesWith\n\te.Admin = c.admin\n\treturn []*types.Entry{e}, nil\n}\n\nfunc createEntries(ctx context.Context, c entry.EntryClient, entries []*types.Entry) (succeeded, failed []*entry.BatchCreateEntryResponse_Result, err error) {\n\tresp, err := c.BatchCreateEntry(ctx, &entry.BatchCreateEntryRequest{Entries: entries})\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tfor i, r := range resp.Results {\n\t\tswitch r.Status.Code {\n\t\tcase int32(codes.OK):\n\t\t\tsucceeded = append(succeeded, r)\n\t\tdefault:\n\t\t\t// The Entry API does not include in the results the entries that\n\t\t\t// failed to be created, so we populate them from the request data.\n\t\t\tr.Entry = entries[i]\n\t\t\tfailed = append(failed, r)\n\t\t}\n\t}\n\n\treturn succeeded, failed, nil\n}\n\nfunc getParentID(config *createCommand, td string) (*types.SPIFFEID, error) {\n\t// If the node flag is set, then set the Parent ID to the server's expected SPIFFE ID\n\tif config.node {\n\t\treturn &types.SPIFFEID{\n\t\t\tTrustDomain: td,\n\t\t\tPath:        idutil.ServerIDPath,\n\t\t}, nil\n\t}\n\treturn idStringToProto(config.parentID)\n}\n", "idx": 1, "id": 15658, "msg": "non-blocking nit: lowercase `server` when not preceded by `SPIRE`. I might also be a little more descriptive? `... to SPIRE Server's management APIs` ?", "proj": "spiffe-spire", "lang": "go"}
{"patch": "@@ -71,7 +71,7 @@ var ruleTestData = []TableEntry{\n \t\t\"-m icmp6 --icmpv6-type 10/12\"),\n \n \tEntry(\"Dest net\", 4,\n-\t\tproto.Rule{DstNet: \"10.0.0.0/16\"},\n+\t\tproto.Rule{DstNet: []string{\"10.0.0.0/16\"}},\n \t\t\"--destination 10.0.0.0/16\"),\n \tEntry(\"Dest IP set\", 4,\n \t\tproto.Rule{DstIpSetIds: []string{\"ipsetid1\"}},", "y": 0, "oldf": "// Copyright (c) 2016-2017 Tigera, Inc. All rights reserved.\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage rules_test\n\nimport (\n\t. \"github.com/projectcalico/felix/rules\"\n\n\t. \"github.com/onsi/ginkgo\"\n\t. \"github.com/onsi/ginkgo/extensions/table\"\n\t. \"github.com/onsi/gomega\"\n\n\t\"github.com/projectcalico/felix/ipsets\"\n\t\"github.com/projectcalico/felix/iptables\"\n\t\"github.com/projectcalico/felix/proto\"\n)\n\nvar ruleTestData = []TableEntry{\n\tEntry(\"Empty rule\", 4, proto.Rule{}, \"\"),\n\n\t// Non-negated matches...\n\n\tEntry(\"Protocol name\", 4,\n\t\tproto.Rule{Protocol: &proto.Protocol{NumberOrName: &proto.Protocol_Name{\"tcp\"}}},\n\t\t\"-p tcp\"),\n\tEntry(\"Protocol num\", 4,\n\t\tproto.Rule{Protocol: &proto.Protocol{NumberOrName: &proto.Protocol_Number{8}}},\n\t\t\"-p 8\"),\n\n\tEntry(\"Source net\", 4,\n\t\tproto.Rule{SrcNet: \"10.0.0.0/16\"},\n\t\t\"--source 10.0.0.0/16\"),\n\tEntry(\"Source IP set\", 4,\n\t\tproto.Rule{SrcIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set --match-set cali4-ipsetid1 src\"),\n\tEntry(\"Source IP sets\", 4,\n\t\tproto.Rule{SrcIpSetIds: []string{\"ipsetid1\", \"ipsetid2\"}},\n\t\t\"-m set --match-set cali4-ipsetid1 src -m set --match-set cali4-ipsetid2 src\"),\n\tEntry(\"Source ports\", 4,\n\t\tproto.Rule{SrcPorts: []*proto.PortRange{{First: 10, Last: 12}}},\n\t\t\"-m multiport --source-ports 10:12\"),\n\tEntry(\"Source ports (multiple)\", 4,\n\t\tproto.Rule{SrcPorts: []*proto.PortRange{\n\t\t\t{First: 10, Last: 12},\n\t\t\t{First: 20, Last: 30},\n\t\t\t{First: 8080, Last: 8080},\n\t\t}},\n\t\t\"-m multiport --source-ports 10:12,20:30,8080\"),\n\tEntry(\"ICMP\", 4,\n\t\tproto.Rule{Icmp: &proto.Rule_IcmpType{IcmpType: 10}},\n\t\t\"-m icmp --icmp-type 10\"),\n\tEntry(\"ICMP with code\", 4,\n\t\tproto.Rule{Icmp: &proto.Rule_IcmpTypeCode{IcmpTypeCode: &proto.IcmpTypeAndCode{Type: 10, Code: 12}}},\n\t\t\"-m icmp --icmp-type 10/12\"),\n\tEntry(\"ICMP\", 6,\n\t\tproto.Rule{Icmp: &proto.Rule_IcmpType{IcmpType: 10}},\n\t\t\"-m icmp6 --icmpv6-type 10\"),\n\tEntry(\"ICMP with code\", 6,\n\t\tproto.Rule{Icmp: &proto.Rule_IcmpTypeCode{IcmpTypeCode: &proto.IcmpTypeAndCode{Type: 10, Code: 12}}},\n\t\t\"-m icmp6 --icmpv6-type 10/12\"),\n\n\tEntry(\"Dest net\", 4,\n\t\tproto.Rule{DstNet: \"10.0.0.0/16\"},\n\t\t\"--destination 10.0.0.0/16\"),\n\tEntry(\"Dest IP set\", 4,\n\t\tproto.Rule{DstIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set --match-set cali4-ipsetid1 dst\"),\n\tEntry(\"Dest IP sets\", 4,\n\t\tproto.Rule{DstIpSetIds: []string{\"ipsetid1\", \"ipsetid2\"}},\n\t\t\"-m set --match-set cali4-ipsetid1 dst -m set --match-set cali4-ipsetid2 dst\"),\n\tEntry(\"Dest ports\", 4,\n\t\tproto.Rule{DstPorts: []*proto.PortRange{{First: 10, Last: 12}}},\n\t\t\"-m multiport --destination-ports 10:12\"),\n\tEntry(\"Dest ports (multiple)\", 4,\n\t\tproto.Rule{DstPorts: []*proto.PortRange{\n\t\t\t{First: 10, Last: 12},\n\t\t\t{First: 20, Last: 30},\n\t\t\t{First: 8080, Last: 8080},\n\t\t}},\n\t\t\"-m multiport --destination-ports 10:12,20:30,8080\"),\n\n\t// Negated matches...\n\n\tEntry(\"Protocol name\", 4,\n\t\tproto.Rule{NotProtocol: &proto.Protocol{NumberOrName: &proto.Protocol_Name{\"tcp\"}}},\n\t\t\"! -p tcp\"),\n\tEntry(\"Protocol num\", 4,\n\t\tproto.Rule{NotProtocol: &proto.Protocol{NumberOrName: &proto.Protocol_Number{8}}},\n\t\t\"! -p 8\"),\n\n\tEntry(\"Source net\", 4,\n\t\tproto.Rule{NotSrcNet: \"10.0.0.0/16\"},\n\t\t\"! --source 10.0.0.0/16\"),\n\tEntry(\"Source IP set\", 4,\n\t\tproto.Rule{NotSrcIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set ! --match-set cali4-ipsetid1 src\"),\n\tEntry(\"Source IP set v6\", 6,\n\t\tproto.Rule{NotSrcIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set ! --match-set cali6-ipsetid1 src\"),\n\tEntry(\"Source IP sets\", 4,\n\t\tproto.Rule{NotSrcIpSetIds: []string{\"ipsetid1\", \"ipsetid2\"}},\n\t\t\"-m set ! --match-set cali4-ipsetid1 src -m set ! --match-set cali4-ipsetid2 src\"),\n\tEntry(\"Source ports\", 4,\n\t\tproto.Rule{NotSrcPorts: []*proto.PortRange{{First: 10, Last: 12}}},\n\t\t\"-m multiport ! --source-ports 10:12\"),\n\tEntry(\"Source ports (multiple)\", 4,\n\t\tproto.Rule{NotSrcPorts: []*proto.PortRange{\n\t\t\t{First: 10, Last: 12},\n\t\t\t{First: 20, Last: 30},\n\t\t\t{First: 8080, Last: 8080},\n\t\t}},\n\t\t\"-m multiport ! --source-ports 10:12,20:30,8080\"),\n\tEntry(\"Source ports (>15) should be broken into blocks\", 4,\n\t\tproto.Rule{NotSrcPorts: []*proto.PortRange{\n\t\t\t{First: 1, Last: 2},\n\t\t\t{First: 3, Last: 4},\n\t\t\t{First: 5, Last: 6},\n\t\t\t{First: 7, Last: 8},\n\t\t\t{First: 9, Last: 10},\n\t\t\t{First: 11, Last: 12},\n\t\t\t{First: 13, Last: 14},\n\t\t\t{First: 15, Last: 16},\n\t\t}},\n\t\t\"-m multiport ! --source-ports 1:2,3:4,5:6,7:8,9:10,11:12,13:14 -m multiport ! --source-ports 15:16\"),\n\tEntry(\"ICMP\", 4,\n\t\tproto.Rule{NotIcmp: &proto.Rule_NotIcmpType{NotIcmpType: 10}},\n\t\t\"-m icmp ! --icmp-type 10\"),\n\tEntry(\"ICMP with code\", 4,\n\t\tproto.Rule{NotIcmp: &proto.Rule_NotIcmpTypeCode{NotIcmpTypeCode: &proto.IcmpTypeAndCode{Type: 10, Code: 12}}},\n\t\t\"-m icmp ! --icmp-type 10/12\"),\n\tEntry(\"ICMP\", 6,\n\t\tproto.Rule{NotIcmp: &proto.Rule_NotIcmpType{NotIcmpType: 10}},\n\t\t\"-m icmp6 ! --icmpv6-type 10\"),\n\tEntry(\"ICMP with code\", 6,\n\t\tproto.Rule{NotIcmp: &proto.Rule_NotIcmpTypeCode{NotIcmpTypeCode: &proto.IcmpTypeAndCode{Type: 10, Code: 12}}},\n\t\t\"-m icmp6 ! --icmpv6-type 10/12\"),\n\n\tEntry(\"Dest net\", 4,\n\t\tproto.Rule{NotDstNet: \"10.0.0.0/16\"},\n\t\t\"! --destination 10.0.0.0/16\"),\n\tEntry(\"Dest IP set\", 4,\n\t\tproto.Rule{NotDstIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set ! --match-set cali4-ipsetid1 dst\"),\n\tEntry(\"Dest IP set\", 6,\n\t\tproto.Rule{NotDstIpSetIds: []string{\"ipsetid1\"}},\n\t\t\"-m set ! --match-set cali6-ipsetid1 dst\"),\n\tEntry(\"Dest IP sets\", 4,\n\t\tproto.Rule{NotDstIpSetIds: []string{\"ipsetid1\", \"ipsetid2\"}},\n\t\t\"-m set ! --match-set cali4-ipsetid1 dst -m set ! --match-set cali4-ipsetid2 dst\"),\n\tEntry(\"Dest ports\", 4,\n\t\tproto.Rule{NotDstPorts: []*proto.PortRange{{First: 10, Last: 12}}},\n\t\t\"-m multiport ! --destination-ports 10:12\"),\n\tEntry(\"Dest ports (>15) should be broken into blocks\", 4,\n\t\tproto.Rule{NotDstPorts: []*proto.PortRange{\n\t\t\t{First: 1, Last: 2},\n\t\t\t{First: 3, Last: 4},\n\t\t\t{First: 5, Last: 6},\n\t\t\t{First: 7, Last: 8},\n\t\t\t{First: 9, Last: 10},\n\t\t\t{First: 11, Last: 12},\n\t\t\t{First: 13, Last: 14},\n\t\t\t{First: 15, Last: 16},\n\t\t}},\n\t\t\"-m multiport ! --destination-ports 1:2,3:4,5:6,7:8,9:10,11:12,13:14 -m multiport ! --destination-ports 15:16\"),\n\tEntry(\"Dest ports (multiple)\", 4,\n\t\tproto.Rule{NotDstPorts: []*proto.PortRange{\n\t\t\t{First: 10, Last: 12},\n\t\t\t{First: 20, Last: 30},\n\t\t\t{First: 8080, Last: 8080},\n\t\t}},\n\t\t\"-m multiport ! --destination-ports 10:12,20:30,8080\"),\n}\n\nvar _ = Describe(\"Protobuf rule to iptables rule conversion\", func() {\n\tvar rrConfigNormal = Config{\n\t\tIPIPEnabled:        true,\n\t\tIPIPTunnelAddress:  nil,\n\t\tIPSetConfigV4:      ipsets.NewIPVersionConfig(ipsets.IPFamilyV4, \"cali\", nil, nil),\n\t\tIPSetConfigV6:      ipsets.NewIPVersionConfig(ipsets.IPFamilyV6, \"cali\", nil, nil),\n\t\tIptablesMarkAccept: 0x8,\n\t\tIptablesMarkPass:   0x10,\n\t\tIptablesLogPrefix:  \"calico-packet\",\n\t}\n\n\tDescribeTable(\n\t\t\"Allow rules should be correctly rendered\",\n\t\tfunc(ipVer int, in proto.Rule, expMatch string) {\n\t\t\trenderer := NewRenderer(rrConfigNormal)\n\t\t\trules := renderer.ProtoRuleToIptablesRules(&in, uint8(ipVer))\n\t\t\t// For allow, should be one match rule that sets the mark, then one that reads the\n\t\t\t// mark and returns.\n\t\t\tExpect(len(rules)).To(Equal(2))\n\t\t\tExpect(rules[0].Match.Render()).To(Equal(expMatch))\n\t\t\tExpect(rules[0].Action).To(Equal(iptables.SetMarkAction{Mark: 0x8}))\n\t\t\tExpect(rules[1]).To(Equal(iptables.Rule{\n\t\t\t\tMatch:  iptables.Match().MarkSet(0x8),\n\t\t\t\tAction: iptables.ReturnAction{},\n\t\t\t}))\n\n\t\t\t// Explicit allow should be treated the same as empty.\n\t\t\tin.Action = \"allow\"\n\t\t\trules2 := renderer.ProtoRuleToIptablesRules(&in, uint8(ipVer))\n\t\t\tExpect(rules2).To(Equal(rules))\n\t\t},\n\t\truleTestData...,\n\t)\n\n\tDescribeTable(\n\t\t\"pass rules should be correctly rendered\",\n\t\tfunc(ipVer int, in proto.Rule, expMatch string) {\n\t\t\tfor _, action := range []string{\"next-tier\", \"pass\"} {\n\t\t\t\tBy(\"Rendering for action \" + action)\n\t\t\t\trenderer := NewRenderer(rrConfigNormal)\n\t\t\t\tin.Action = action\n\t\t\t\trules := renderer.ProtoRuleToIptablesRules(&in, uint8(ipVer))\n\t\t\t\t// For pass, should be one match rule that sets the mark, then one\n\t\t\t\t// that reads the mark and returns.\n\t\t\t\tExpect(len(rules)).To(Equal(2))\n\t\t\t\tExpect(rules[0].Match.Render()).To(Equal(expMatch))\n\t\t\t\tExpect(rules[0].Action).To(Equal(iptables.SetMarkAction{Mark: 0x10}))\n\t\t\t\tExpect(rules[1]).To(Equal(iptables.Rule{\n\t\t\t\t\tMatch:  iptables.Match().MarkSet(0x10),\n\t\t\t\t\tAction: iptables.ReturnAction{},\n\t\t\t\t}))\n\t\t\t}\n\t\t},\n\t\truleTestData...,\n\t)\n\n\tDescribeTable(\n\t\t\"Log rules should be correctly rendered\",\n\t\tfunc(ipVer int, in proto.Rule, expMatch string) {\n\t\t\trenderer := NewRenderer(rrConfigNormal)\n\t\t\tlogRule := in\n\t\t\tlogRule.Action = \"log\"\n\t\t\trules := renderer.ProtoRuleToIptablesRules(&logRule, uint8(ipVer))\n\t\t\t// For deny, should be one match rule that just does the DROP.\n\t\t\tExpect(len(rules)).To(Equal(1))\n\t\t\tExpect(rules[0].Match.Render()).To(Equal(expMatch))\n\t\t\tExpect(rules[0].Action).To(Equal(iptables.LogAction{Prefix: \"calico-packet\"}))\n\t\t},\n\t\truleTestData...,\n\t)\n\n\tDescribeTable(\n\t\t\"Log rules should be correctly rendered with non-default prefix\",\n\t\tfunc(ipVer int, in proto.Rule, expMatch string) {\n\t\t\trrConfigPrefix := rrConfigNormal\n\t\t\trrConfigPrefix.IptablesLogPrefix = \"foobar\"\n\t\t\trenderer := NewRenderer(rrConfigPrefix)\n\t\t\tlogRule := in\n\t\t\tlogRule.Action = \"log\"\n\t\t\trules := renderer.ProtoRuleToIptablesRules(&logRule, uint8(ipVer))\n\t\t\t// For deny, should be one match rule that just does the DROP.\n\t\t\tExpect(len(rules)).To(Equal(1))\n\t\t\tExpect(rules[0].Match.Render()).To(Equal(expMatch))\n\t\t\tExpect(rules[0].Action).To(Equal(iptables.LogAction{Prefix: \"foobar\"}))\n\t\t},\n\t\truleTestData...,\n\t)\n\n\tDescribeTable(\n\t\t\"Deny rules should be correctly rendered\",\n\t\tfunc(ipVer int, in proto.Rule, expMatch string) {\n\t\t\trenderer := NewRenderer(rrConfigNormal)\n\t\t\tdenyRule := in\n\t\t\tdenyRule.Action = \"deny\"\n\t\t\trules := renderer.ProtoRuleToIptablesRules(&denyRule, uint8(ipVer))\n\t\t\t// For deny, should be one match rule that just does the DROP.\n\t\t\tExpect(len(rules)).To(Equal(1))\n\t\t\tExpect(rules[0].Match.Render()).To(Equal(expMatch))\n\t\t\tExpect(rules[0].Action).To(Equal(iptables.DropAction{}))\n\t\t},\n\t\truleTestData...,\n\t)\n\n\tvar renderer *DefaultRuleRenderer\n\tBeforeEach(func() {\n\t\trenderer = NewRenderer(rrConfigNormal).(*DefaultRuleRenderer)\n\t})\n\n\tIt(\"should skip rules of incorrect IP version\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{IpVersion: 4}}, 6)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed source CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{SrcNet: \"10.0.0.1\"}}, 6)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed source CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{SrcNet: \"feed::beef\"}}, 4)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed dest CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{DstNet: \"10.0.0.1\"}}, 6)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed dest CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{DstNet: \"feed::beef\"}}, 4)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed negated source CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{NotSrcNet: \"10.0.0.1\"}}, 6)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed negated source CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{NotSrcNet: \"feed::beef\"}}, 4)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed negated dest CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{NotDstNet: \"10.0.0.1\"}}, 6)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"should skip with mixed negated dest CIDR matches\", func() {\n\t\trules := renderer.ProtoRulesToIptablesRules([]*proto.Rule{{NotDstNet: \"feed::beef\"}}, 4)\n\t\tExpect(rules).To(BeEmpty())\n\t})\n\n\tIt(\"Should correctly render the cross-product of the source/dest ports\", func() {\n\t\tsrcPorts := []*proto.PortRange{\n\t\t\t{First: 1, Last: 2},\n\t\t\t{First: 3, Last: 4},\n\t\t\t{First: 5, Last: 6},\n\t\t\t{First: 7, Last: 8},\n\t\t\t{First: 9, Last: 10},\n\t\t\t{First: 11, Last: 12},\n\t\t\t{First: 13, Last: 14},\n\t\t\t{First: 15, Last: 16},\n\t\t}\n\t\tdstPorts := []*proto.PortRange{\n\t\t\t{First: 101, Last: 102},\n\t\t\t{First: 103, Last: 104},\n\t\t\t{First: 105, Last: 106},\n\t\t\t{First: 107, Last: 108},\n\t\t\t{First: 109, Last: 1010},\n\t\t\t{First: 1011, Last: 1012},\n\t\t\t{First: 1013, Last: 1014},\n\t\t\t{First: 1015, Last: 1016},\n\t\t}\n\t\trule := proto.Rule{\n\t\t\tProtocol: &proto.Protocol{NumberOrName: &proto.Protocol_Name{\"tcp\"}},\n\t\t\tSrcPorts: srcPorts,\n\t\t\tDstPorts: dstPorts,\n\t\t}\n\t\trenderer := NewRenderer(rrConfigNormal)\n\t\trules := renderer.ProtoRuleToIptablesRules(&rule, uint8(4))\n\t\tExpect(rules).To(Equal([]iptables.Rule{\n\t\t\t{\n\t\t\t\tMatch: iptables.Match().Protocol(\"tcp\").\n\t\t\t\t\tSourcePortRanges(srcPorts[:7]).\n\t\t\t\t\tDestPortRanges(dstPorts[:7]),\n\t\t\t\tAction: iptables.SetMarkAction{Mark: 0x8},\n\t\t\t},\n\t\t\t{Match: iptables.Match().MarkSet(0x8), Action: iptables.ReturnAction{}},\n\t\t\t{\n\t\t\t\tMatch: iptables.Match().Protocol(\"tcp\").\n\t\t\t\t\tSourcePortRanges(srcPorts[:7]).\n\t\t\t\t\tDestPortRanges(dstPorts[7:8]),\n\t\t\t\tAction: iptables.SetMarkAction{Mark: 0x8},\n\t\t\t},\n\t\t\t{Match: iptables.Match().MarkSet(0x8), Action: iptables.ReturnAction{}},\n\t\t\t{\n\t\t\t\tMatch: iptables.Match().Protocol(\"tcp\").\n\t\t\t\t\tSourcePortRanges(srcPorts[7:8]).\n\t\t\t\t\tDestPortRanges(dstPorts[:7]),\n\t\t\t\tAction: iptables.SetMarkAction{Mark: 0x8},\n\t\t\t},\n\t\t\t{Match: iptables.Match().MarkSet(0x8), Action: iptables.ReturnAction{}},\n\t\t\t{\n\t\t\t\tMatch: iptables.Match().Protocol(\"tcp\").\n\t\t\t\t\tSourcePortRanges(srcPorts[7:8]).\n\t\t\t\t\tDestPortRanges(dstPorts[7:8]),\n\t\t\t\tAction: iptables.SetMarkAction{Mark: 0x8},\n\t\t\t},\n\t\t\t{Match: iptables.Match().MarkSet(0x8), Action: iptables.ReturnAction{}},\n\t\t}))\n\t})\n})\n\nvar _ = DescribeTable(\"Port split tests\",\n\tfunc(in []*proto.PortRange, expected [][]*proto.PortRange) {\n\t\tExpect(SplitPortList(in)).To(Equal(expected))\n\t},\n\tEntry(\"nil input\", ([]*proto.PortRange)(nil), [][]*proto.PortRange{{}}),\n\tEntry(\"empty input\", []*proto.PortRange{}, [][]*proto.PortRange{{}}),\n\tEntry(\"single input\", []*proto.PortRange{{First: 1, Last: 1}}, [][]*proto.PortRange{{{First: 1, Last: 1}}}),\n\tEntry(\"range input\", []*proto.PortRange{{First: 1, Last: 10}}, [][]*proto.PortRange{{{First: 1, Last: 10}}}),\n\tEntry(\"exactly 15 single ports should give exactly one split\", []*proto.PortRange{\n\t\t{First: 1, Last: 1},\n\t\t{First: 2, Last: 2},\n\t\t{First: 3, Last: 3},\n\t\t{First: 4, Last: 4},\n\t\t{First: 5, Last: 5},\n\t\t{First: 6, Last: 6},\n\t\t{First: 7, Last: 7},\n\t\t{First: 8, Last: 8},\n\t\t{First: 9, Last: 9},\n\t\t{First: 10, Last: 10},\n\t\t{First: 11, Last: 11},\n\t\t{First: 12, Last: 12},\n\t\t{First: 13, Last: 13},\n\t\t{First: 14, Last: 14},\n\t\t{First: 15, Last: 15},\n\t}, [][]*proto.PortRange{{\n\t\t{First: 1, Last: 1},\n\t\t{First: 2, Last: 2},\n\t\t{First: 3, Last: 3},\n\t\t{First: 4, Last: 4},\n\t\t{First: 5, Last: 5},\n\t\t{First: 6, Last: 6},\n\t\t{First: 7, Last: 7},\n\t\t{First: 8, Last: 8},\n\t\t{First: 9, Last: 9},\n\t\t{First: 10, Last: 10},\n\t\t{First: 11, Last: 11},\n\t\t{First: 12, Last: 12},\n\t\t{First: 13, Last: 13},\n\t\t{First: 14, Last: 14},\n\t\t{First: 15, Last: 15},\n\t}}),\n\tEntry(\"exactly 16 single ports should give exactly tow splits\", []*proto.PortRange{\n\t\t{First: 1, Last: 1},\n\t\t{First: 2, Last: 2},\n\t\t{First: 3, Last: 3},\n\t\t{First: 4, Last: 4},\n\t\t{First: 5, Last: 5},\n\t\t{First: 6, Last: 6},\n\t\t{First: 7, Last: 7},\n\t\t{First: 8, Last: 8},\n\t\t{First: 9, Last: 9},\n\t\t{First: 10, Last: 10},\n\t\t{First: 11, Last: 11},\n\t\t{First: 12, Last: 12},\n\t\t{First: 13, Last: 13},\n\t\t{First: 14, Last: 14},\n\t\t{First: 15, Last: 15},\n\t\t{First: 16, Last: 16},\n\t}, [][]*proto.PortRange{{\n\t\t{First: 1, Last: 1},\n\t\t{First: 2, Last: 2},\n\t\t{First: 3, Last: 3},\n\t\t{First: 4, Last: 4},\n\t\t{First: 5, Last: 5},\n\t\t{First: 6, Last: 6},\n\t\t{First: 7, Last: 7},\n\t\t{First: 8, Last: 8},\n\t\t{First: 9, Last: 9},\n\t\t{First: 10, Last: 10},\n\t\t{First: 11, Last: 11},\n\t\t{First: 12, Last: 12},\n\t\t{First: 13, Last: 13},\n\t\t{First: 14, Last: 14},\n\t\t{First: 15, Last: 15},\n\t}, {\n\t\t{First: 16, Last: 16},\n\t}}),\n\tEntry(\"port ranges should count for 2 single ports\", []*proto.PortRange{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t\t{First: 15, Last: 15},\n\t}, [][]*proto.PortRange{{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t\t{First: 15, Last: 15},\n\t}}),\n\tEntry(\"port range straggling 15-16 should be put in second group\", []*proto.PortRange{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t\t{First: 15, Last: 16},\n\t}, [][]*proto.PortRange{{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t}, {\n\t\t{First: 15, Last: 16},\n\t}}),\n\tEntry(\"further splits should be made in correct place\", []*proto.PortRange{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t\t{First: 15, Last: 16},\n\t\t{First: 21, Last: 22},\n\t\t{First: 23, Last: 24},\n\t\t{First: 23, Last: 26},\n\t\t{First: 27, Last: 28},\n\t\t{First: 29, Last: 210},\n\t\t{First: 211, Last: 212},\n\t\t{First: 213, Last: 214},\n\t\t{First: 215, Last: 216},\n\t}, [][]*proto.PortRange{{\n\t\t{First: 1, Last: 2},\n\t\t{First: 3, Last: 4},\n\t\t{First: 5, Last: 6},\n\t\t{First: 7, Last: 8},\n\t\t{First: 9, Last: 10},\n\t\t{First: 11, Last: 12},\n\t\t{First: 13, Last: 14},\n\t}, {\n\t\t{First: 15, Last: 16},\n\t\t{First: 21, Last: 22},\n\t\t{First: 23, Last: 24},\n\t\t{First: 23, Last: 26},\n\t\t{First: 27, Last: 28},\n\t\t{First: 29, Last: 210},\n\t\t{First: 211, Last: 212},\n\t}, {\n\t\t{First: 213, Last: 214},\n\t\t{First: 215, Last: 216},\n\t}}),\n)\n", "idx": 2, "id": 15447, "msg": "", "proj": "projectcalico-felix", "lang": "go"}
{"patch": "@@ -686,7 +686,7 @@ func (api *Server) getBlockMeta(blkHash string) (*iotexapi.GetBlockMetasResponse\n \tblockMeta := &iotextypes.BlockMeta{\n \t\tHash:             blkHash,\n \t\tHeight:           blk.Height(),\n-\t\tTimestamp:        blkHeaderPb.GetCore().GetTimestamp().GetSeconds(),\n+\t\tTimestamp:        blkHeaderPb.GetCore().GetTimestamp(),\n \t\tNumActions:       int64(len(blk.Actions)),\n \t\tProducerAddress:  blk.ProducerAddress(),\n \t\tTransferAmount:   transferAmount.String(),", "y": 0, "oldf": "// Copyright (c) 2019 IoTeX\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage api\n\nimport (\n\t\"context\"\n\t\"encoding/hex\"\n\t\"math/big\"\n\t\"net\"\n\t\"strconv\"\n\n\t\"github.com/golang/protobuf/proto\"\n\tgrpc_prometheus \"github.com/grpc-ecosystem/go-grpc-prometheus\"\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/zap\"\n\t\"google.golang.org/grpc\"\n\t\"google.golang.org/grpc/codes\"\n\t\"google.golang.org/grpc/reflection\"\n\t\"google.golang.org/grpc/status\"\n\n\t\"github.com/iotexproject/iotex-core/action\"\n\t\"github.com/iotexproject/iotex-core/action/protocol\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/poll\"\n\t\"github.com/iotexproject/iotex-core/action/protocol/rolldpos\"\n\t\"github.com/iotexproject/iotex-core/actpool\"\n\t\"github.com/iotexproject/iotex-core/address\"\n\t\"github.com/iotexproject/iotex-core/blockchain\"\n\t\"github.com/iotexproject/iotex-core/blockchain/block\"\n\t\"github.com/iotexproject/iotex-core/config\"\n\t\"github.com/iotexproject/iotex-core/dispatcher\"\n\t\"github.com/iotexproject/iotex-core/gasstation\"\n\t\"github.com/iotexproject/iotex-core/indexservice\"\n\t\"github.com/iotexproject/iotex-core/pkg/hash\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n\t\"github.com/iotexproject/iotex-core/pkg/util/byteutil\"\n\t\"github.com/iotexproject/iotex-core/pkg/version\"\n\t\"github.com/iotexproject/iotex-core/protogen/iotexapi\"\n\t\"github.com/iotexproject/iotex-core/protogen/iotextypes\"\n\t\"github.com/iotexproject/iotex-core/state\"\n)\n\nvar (\n\t// ErrInternalServer indicates the internal server error\n\tErrInternalServer = errors.New(\"internal server error\")\n\t// ErrReceipt indicates the error of receipt\n\tErrReceipt = errors.New(\"invalid receipt\")\n\t// ErrAction indicates the error of action\n\tErrAction = errors.New(\"invalid action\")\n)\n\n// BroadcastOutbound sends a broadcast message to the whole network\ntype BroadcastOutbound func(ctx context.Context, chainID uint32, msg proto.Message) error\n\n// Config represents the config to setup api\ntype Config struct {\n\tbroadcastHandler BroadcastOutbound\n}\n\n// Option is the option to override the api config\ntype Option func(cfg *Config) error\n\n// WithBroadcastOutbound is the option to broadcast msg outbound\nfunc WithBroadcastOutbound(broadcastHandler BroadcastOutbound) Option {\n\treturn func(cfg *Config) error {\n\t\tcfg.broadcastHandler = broadcastHandler\n\t\treturn nil\n\t}\n}\n\n// Server provides api for user to query blockchain data\ntype Server struct {\n\tbc               blockchain.Blockchain\n\tdp               dispatcher.Dispatcher\n\tap               actpool.ActPool\n\tgs               *gasstation.GasStation\n\tbroadcastHandler BroadcastOutbound\n\tcfg              config.API\n\tidx              *indexservice.Server\n\tregistry         *protocol.Registry\n\tgrpcserver       *grpc.Server\n}\n\n// NewServer creates a new server\nfunc NewServer(\n\tcfg config.API,\n\tchain blockchain.Blockchain,\n\tdispatcher dispatcher.Dispatcher,\n\tactPool actpool.ActPool,\n\tidx *indexservice.Server,\n\tregistry *protocol.Registry,\n\topts ...Option,\n) (*Server, error) {\n\tapiCfg := Config{}\n\tfor _, opt := range opts {\n\t\tif err := opt(&apiCfg); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\n\tif cfg == (config.API{}) {\n\t\tlog.L().Warn(\"API server is not configured.\")\n\t\tcfg = config.Default.API\n\t}\n\n\tsvr := &Server{\n\t\tbc:               chain,\n\t\tdp:               dispatcher,\n\t\tap:               actPool,\n\t\tbroadcastHandler: apiCfg.broadcastHandler,\n\t\tcfg:              cfg,\n\t\tidx:              idx,\n\t\tregistry:         registry,\n\t\tgs:               gasstation.NewGasStation(chain, cfg),\n\t}\n\n\tsvr.grpcserver = grpc.NewServer(\n\t\tgrpc.StreamInterceptor(grpc_prometheus.StreamServerInterceptor),\n\t\tgrpc.UnaryInterceptor(grpc_prometheus.UnaryServerInterceptor),\n\t)\n\tiotexapi.RegisterAPIServiceServer(svr.grpcserver, svr)\n\tgrpc_prometheus.Register(svr.grpcserver)\n\treflection.Register(svr.grpcserver)\n\n\treturn svr, nil\n}\n\n// GetAccount returns the metadata of an account\nfunc (api *Server) GetAccount(ctx context.Context, in *iotexapi.GetAccountRequest) (*iotexapi.GetAccountResponse, error) {\n\tstate, err := api.bc.StateByAddr(in.Address)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tpendingNonce, err := api.ap.GetPendingNonce(in.Address)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\tnumActions, err := api.bc.GetActionCountByAddress(in.Address)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\taccountMeta := &iotextypes.AccountMeta{\n\t\tAddress:      in.Address,\n\t\tBalance:      state.Balance.String(),\n\t\tNonce:        state.Nonce,\n\t\tPendingNonce: pendingNonce,\n\t\tNumActions:   numActions,\n\t}\n\treturn &iotexapi.GetAccountResponse{AccountMeta: accountMeta}, nil\n}\n\n// GetActions returns actions\nfunc (api *Server) GetActions(ctx context.Context, in *iotexapi.GetActionsRequest) (*iotexapi.GetActionsResponse, error) {\n\tswitch {\n\tcase in.GetByIndex() != nil:\n\t\trequest := in.GetByIndex()\n\t\treturn api.getActions(request.Start, request.Count)\n\tcase in.GetByHash() != nil:\n\t\trequest := in.GetByHash()\n\t\treturn api.getSingleAction(request.ActionHash, request.CheckPending)\n\tcase in.GetByAddr() != nil:\n\t\trequest := in.GetByAddr()\n\t\treturn api.getActionsByAddress(request.Address, request.Start, request.Count)\n\tcase in.GetUnconfirmedByAddr() != nil:\n\t\trequest := in.GetUnconfirmedByAddr()\n\t\treturn api.getUnconfirmedActionsByAddress(request.Address, request.Start, request.Count)\n\tcase in.GetByBlk() != nil:\n\t\trequest := in.GetByBlk()\n\t\treturn api.getActionsByBlock(request.BlkHash, request.Start, request.Count)\n\tdefault:\n\t\treturn nil, nil\n\t}\n}\n\n// GetBlockMetas returns block metadata\nfunc (api *Server) GetBlockMetas(ctx context.Context, in *iotexapi.GetBlockMetasRequest) (*iotexapi.GetBlockMetasResponse, error) {\n\tswitch {\n\tcase in.GetByIndex() != nil:\n\t\trequest := in.GetByIndex()\n\t\treturn api.getBlockMetas(request.Start, request.Count)\n\tcase in.GetByHash() != nil:\n\t\trequest := in.GetByHash()\n\t\treturn api.getBlockMeta(request.BlkHash)\n\tdefault:\n\t\treturn nil, nil\n\t}\n}\n\n// GetChainMeta returns blockchain metadata\nfunc (api *Server) GetChainMeta(ctx context.Context, in *iotexapi.GetChainMetaRequest) (*iotexapi.GetChainMetaResponse, error) {\n\ttipHeight := api.bc.TipHeight()\n\tif tipHeight == 0 {\n\t\treturn &iotexapi.GetChainMetaResponse{\n\t\t\tChainMeta: &iotextypes.ChainMeta{\n\t\t\t\tEpoch: &iotextypes.EpochData{},\n\t\t\t},\n\t\t}, nil\n\t}\n\ttotalActions, err := api.bc.GetTotalActions()\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\n\tblockLimit := int64(api.cfg.TpsWindow)\n\tif blockLimit <= 0 {\n\t\treturn nil, status.Errorf(codes.Internal, \"block limit is %d\", blockLimit)\n\t}\n\n\t// avoid genesis block\n\tif int64(tipHeight) < blockLimit {\n\t\tblockLimit = int64(tipHeight)\n\t}\n\tr, err := api.getBlockMetas(tipHeight-uint64(blockLimit)+1, uint64(blockLimit))\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tblks := r.BlkMetas\n\n\tif len(blks) == 0 {\n\t\treturn nil, status.Error(codes.NotFound, \"get 0 blocks! not able to calculate aps\")\n\t}\n\n\tvar numActions int64\n\tfor _, blk := range blks {\n\t\tnumActions += blk.NumActions\n\t}\n\n\tp, ok := api.registry.Find(rolldpos.ProtocolID)\n\tif !ok {\n\t\treturn nil, status.Error(codes.Internal, \"rolldpos protocol is not registered\")\n\t}\n\trp, ok := p.(*rolldpos.Protocol)\n\tif !ok {\n\t\treturn nil, status.Error(codes.Internal, \"fail to cast rolldpos protocol\")\n\t}\n\tepochNum := rp.GetEpochNum(tipHeight)\n\tepochHeight := rp.GetEpochHeight(epochNum)\n\tgravityChainStartHeight, err := api.getGravityChainStartHeight(epochHeight)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\n\ttimeDuration := blks[len(blks)-1].Timestamp - blks[0].Timestamp\n\t// if time duration is less than 1 second, we set it to be 1 second\n\tif timeDuration < 1 {\n\t\ttimeDuration = 1\n\t}\n\n\ttps := numActions / timeDuration\n\n\tchainMeta := &iotextypes.ChainMeta{\n\t\tHeight: tipHeight,\n\t\tEpoch: &iotextypes.EpochData{\n\t\t\tNum:                     epochNum,\n\t\t\tHeight:                  epochHeight,\n\t\t\tGravityChainStartHeight: gravityChainStartHeight,\n\t\t},\n\t\tNumActions: int64(totalActions),\n\t\tTps:        tps,\n\t}\n\n\treturn &iotexapi.GetChainMetaResponse{ChainMeta: chainMeta}, nil\n}\n\n// GetServerMeta gets the server metadata\nfunc (api *Server) GetServerMeta(ctx context.Context,\n\tin *iotexapi.GetServerMetaRequest) (*iotexapi.GetServerMetaResponse, error) {\n\treturn &iotexapi.GetServerMetaResponse{ServerMeta: &iotextypes.ServerMeta{\n\t\tPackageVersion:  version.PackageVersion,\n\t\tPackageCommitID: version.PackageCommitID,\n\t\tGitStatus:       version.GitStatus,\n\t\tGoVersion:       version.GoVersion,\n\t\tBuildTime:       version.BuildTime,\n\t}}, nil\n}\n\n// SendAction is the API to send an action to blockchain.\nfunc (api *Server) SendAction(ctx context.Context, in *iotexapi.SendActionRequest) (res *iotexapi.SendActionResponse, err error) {\n\tlog.L().Debug(\"receive send action request\")\n\n\t// broadcast to the network\n\tif err = api.broadcastHandler(context.Background(), api.bc.ChainID(), in.Action); err != nil {\n\t\tlog.L().Warn(\"Failed to broadcast SendAction request.\", zap.Error(err))\n\t}\n\t// send to actpool via dispatcher\n\tapi.dp.HandleBroadcast(context.Background(), api.bc.ChainID(), in.Action)\n\n\treturn &iotexapi.SendActionResponse{}, nil\n}\n\n// GetReceiptByAction gets receipt with corresponding action hash\nfunc (api *Server) GetReceiptByAction(ctx context.Context, in *iotexapi.GetReceiptByActionRequest) (*iotexapi.GetReceiptByActionResponse, error) {\n\tactHash, err := toHash256(in.ActionHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\treceipt, err := api.bc.GetReceiptByActionHash(actHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tblkHash, err := api.bc.GetBlockHashByActionHash(actHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\treturn &iotexapi.GetReceiptByActionResponse{\n\t\tReceiptInfo: &iotexapi.ReceiptInfo{\n\t\t\tReceipt: receipt.ConvertToReceiptPb(),\n\t\t\tBlkHash: hex.EncodeToString(blkHash[:]),\n\t\t},\n\t}, nil\n}\n\n// ReadContract reads the state in a contract address specified by the slot\nfunc (api *Server) ReadContract(ctx context.Context, in *iotexapi.ReadContractRequest) (*iotexapi.ReadContractResponse, error) {\n\tlog.L().Debug(\"receive read smart contract request\")\n\n\tselp := &action.SealedEnvelope{}\n\tif err := selp.LoadProto(in.Action); err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\tsc, ok := selp.Action().(*action.Execution)\n\tif !ok {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"not an execution\")\n\t}\n\n\tcallerAddr, err := address.FromBytes(selp.SrcPubkey().Hash())\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\n\tretval, receipt, err := api.bc.ExecuteContractRead(callerAddr, sc)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\treturn &iotexapi.ReadContractResponse{\n\t\tData:    hex.EncodeToString(retval),\n\t\tReceipt: receipt.ConvertToReceiptPb(),\n\t}, nil\n}\n\n// ReadState reads state on blockchain\nfunc (api *Server) ReadState(ctx context.Context, in *iotexapi.ReadStateRequest) (*iotexapi.ReadStateResponse, error) {\n\tres, err := api.readState(ctx, in)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\treturn res, nil\n}\n\n// SuggestGasPrice suggests gas price\nfunc (api *Server) SuggestGasPrice(ctx context.Context, in *iotexapi.SuggestGasPriceRequest) (*iotexapi.SuggestGasPriceResponse, error) {\n\tsuggestPrice, err := api.gs.SuggestGasPrice()\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\treturn &iotexapi.SuggestGasPriceResponse{GasPrice: suggestPrice}, nil\n}\n\n// EstimateGasForAction estimates gas for action\nfunc (api *Server) EstimateGasForAction(ctx context.Context, in *iotexapi.EstimateGasForActionRequest) (*iotexapi.EstimateGasForActionResponse, error) {\n\testimateGas, err := api.gs.EstimateGasForAction(in.Action)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\treturn &iotexapi.EstimateGasForActionResponse{Gas: estimateGas}, nil\n}\n\n// GetEpochMeta gets epoch metadata\nfunc (api *Server) GetEpochMeta(\n\tctx context.Context,\n\tin *iotexapi.GetEpochMetaRequest,\n) (*iotexapi.GetEpochMetaResponse, error) {\n\tif in.EpochNumber < 1 {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"epoch number cannot be less than one\")\n\t}\n\tp, ok := api.registry.Find(rolldpos.ProtocolID)\n\tif !ok {\n\t\treturn nil, status.Error(codes.Internal, \"rolldpos protocol is not registered\")\n\t}\n\trp, ok := p.(*rolldpos.Protocol)\n\tif !ok {\n\t\treturn nil, status.Error(codes.Internal, \"fail to cast rolldpos protocol\")\n\t}\n\tepochHeight := rp.GetEpochHeight(in.EpochNumber)\n\tgravityChainStartHeight, err := api.getGravityChainStartHeight(epochHeight)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tepochData := &iotextypes.EpochData{\n\t\tNum:                     in.EpochNumber,\n\t\tHeight:                  epochHeight,\n\t\tGravityChainStartHeight: gravityChainStartHeight,\n\t}\n\n\tnumBlks, produce, err := api.bc.ProductivityByEpoch(in.EpochNumber)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\n\treadStateRequest := &iotexapi.ReadStateRequest{\n\t\tProtocolID: []byte(poll.ProtocolID),\n\t\tMethodName: []byte(\"BlockProducersByEpoch\"),\n\t\tArguments:  [][]byte{byteutil.Uint64ToBytes(in.EpochNumber)},\n\t}\n\tres, err := api.readState(context.Background(), readStateRequest)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tvar BlockProducers state.CandidateList\n\tif err := BlockProducers.Deserialize(res.Data); err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\n\tvar blockProducersInfo []*iotexapi.BlockProducerInfo\n\tfor _, bp := range BlockProducers {\n\t\tvar active bool\n\t\tvar blockProduction uint64\n\t\tif production, ok := produce[bp.Address]; ok {\n\t\t\tactive = true\n\t\t\tblockProduction = production\n\t\t}\n\t\tblockProducersInfo = append(blockProducersInfo, &iotexapi.BlockProducerInfo{\n\t\t\tAddress:    bp.Address,\n\t\t\tVotes:      bp.Votes.String(),\n\t\t\tActive:     active,\n\t\t\tProduction: blockProduction,\n\t\t})\n\t}\n\n\treturn &iotexapi.GetEpochMetaResponse{\n\t\tEpochData:          epochData,\n\t\tTotalBlocks:        numBlks,\n\t\tBlockProducersInfo: blockProducersInfo,\n\t}, nil\n}\n\n// Start starts the API server\nfunc (api *Server) Start() error {\n\tportStr := \":\" + strconv.Itoa(api.cfg.Port)\n\tlis, err := net.Listen(\"tcp\", portStr)\n\tif err != nil {\n\t\tlog.L().Error(\"API server failed to listen.\", zap.Error(err))\n\t\treturn errors.Wrap(err, \"API server failed to listen\")\n\t}\n\tlog.L().Info(\"API server is listening.\", zap.String(\"addr\", lis.Addr().String()))\n\n\tgo func() {\n\t\tif err := api.grpcserver.Serve(lis); err != nil {\n\t\t\tlog.L().Fatal(\"Node failed to serve.\", zap.Error(err))\n\t\t}\n\t}()\n\treturn nil\n}\n\n// Stop stops the API server\nfunc (api *Server) Stop() error {\n\tapi.grpcserver.Stop()\n\tlog.L().Info(\"API server stops.\")\n\treturn nil\n}\n\nfunc (api *Server) readState(ctx context.Context, in *iotexapi.ReadStateRequest) (*iotexapi.ReadStateResponse, error) {\n\tp, ok := api.registry.Find(string(in.ProtocolID))\n\tif !ok {\n\t\treturn nil, status.Errorf(codes.Internal, \"protocol %s isn't registered\", string(in.ProtocolID))\n\t}\n\t// TODO: need to complete the context\n\tctx = protocol.WithRunActionsCtx(ctx, protocol.RunActionsCtx{\n\t\tBlockHeight: api.bc.TipHeight(),\n\t\tRegistry:    api.registry,\n\t})\n\tws, err := api.bc.GetFactory().NewWorkingSet()\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Internal, err.Error())\n\t}\n\tdata, err := p.ReadState(ctx, ws, in.MethodName, in.Arguments...)\n\t// TODO: need to distinguish user error and system error\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tout := iotexapi.ReadStateResponse{\n\t\tData: data,\n\t}\n\treturn &out, nil\n}\n\n// GetActions returns actions within the range\nfunc (api *Server) getActions(start uint64, count uint64) (*iotexapi.GetActionsResponse, error) {\n\tvar res []*iotexapi.ActionInfo\n\tvar actionCount uint64\n\n\ttipHeight := api.bc.TipHeight()\n\tfor height := 1; height <= int(tipHeight); height++ {\n\t\tblk, err := api.bc.GetBlockByHeight(uint64(height))\n\t\tif err != nil {\n\t\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t\t}\n\t\tselps := blk.Actions\n\t\tfor i := 0; i < len(selps); i++ {\n\t\t\tactionCount++\n\n\t\t\tif actionCount <= start {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tif uint64(len(res)) >= count {\n\t\t\t\treturn &iotexapi.GetActionsResponse{ActionInfo: res}, nil\n\t\t\t}\n\t\t\tact, err := api.convertToAction(selps[i], true)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tres = append(res, act)\n\t\t}\n\t}\n\n\treturn &iotexapi.GetActionsResponse{ActionInfo: res}, nil\n}\n\n// getSingleAction returns action by action hash\nfunc (api *Server) getSingleAction(actionHash string, checkPending bool) (*iotexapi.GetActionsResponse, error) {\n\tactHash, err := toHash256(actionHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\tact, err := api.getAction(actHash, checkPending)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.Unavailable, err.Error())\n\t}\n\treturn &iotexapi.GetActionsResponse{ActionInfo: []*iotexapi.ActionInfo{act}}, nil\n}\n\n// getActionsByAddress returns all actions associated with an address\nfunc (api *Server) getActionsByAddress(address string, start uint64, count uint64) (*iotexapi.GetActionsResponse, error) {\n\tvar res []*iotexapi.ActionInfo\n\tactions, err := api.getTotalActionsByAddress(address)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\tvar actionCount uint64\n\tfor i := 0; i < len(actions); i++ {\n\t\tactionCount++\n\n\t\tif actionCount <= start {\n\t\t\tcontinue\n\t\t}\n\n\t\tif uint64(len(res)) >= count {\n\t\t\tbreak\n\t\t}\n\n\t\tact, err := api.getAction(actions[i], false)\n\t\tif err != nil {\n\t\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t\t}\n\n\t\tres = append(res, act)\n\t}\n\n\treturn &iotexapi.GetActionsResponse{ActionInfo: res}, nil\n}\n\n// getUnconfirmedActionsByAddress returns all unconfirmed actions in actpool associated with an address\nfunc (api *Server) getUnconfirmedActionsByAddress(address string, start uint64, count uint64) (*iotexapi.GetActionsResponse, error) {\n\tvar res []*iotexapi.ActionInfo\n\tvar actionCount uint64\n\n\tselps := api.ap.GetUnconfirmedActs(address)\n\tfor i := 0; i < len(selps); i++ {\n\t\tactionCount++\n\n\t\tif actionCount <= start {\n\t\t\tcontinue\n\t\t}\n\n\t\tif uint64(len(res)) >= count {\n\t\t\tbreak\n\t\t}\n\t\tact, err := api.convertToAction(selps[i], false)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tres = append(res, act)\n\t}\n\n\treturn &iotexapi.GetActionsResponse{ActionInfo: res}, nil\n}\n\n// getActionsByBlock returns all actions in a block\nfunc (api *Server) getActionsByBlock(blkHash string, start uint64, count uint64) (*iotexapi.GetActionsResponse, error) {\n\tvar res []*iotexapi.ActionInfo\n\thash, err := toHash256(blkHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\n\tblk, err := api.bc.GetBlockByHash(hash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\n\tselps := blk.Actions\n\tvar actionCount uint64\n\tfor i := 0; i < len(selps); i++ {\n\t\tactionCount++\n\n\t\tif actionCount <= start {\n\t\t\tcontinue\n\t\t}\n\n\t\tif uint64(len(res)) >= count {\n\t\t\tbreak\n\t\t}\n\n\t\tact, err := api.convertToAction(selps[i], true)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tres = append(res, act)\n\t}\n\treturn &iotexapi.GetActionsResponse{ActionInfo: res}, nil\n}\n\n// getBlockMetas gets block within the height range\nfunc (api *Server) getBlockMetas(start uint64, number uint64) (*iotexapi.GetBlockMetasResponse, error) {\n\ttipHeight := api.bc.TipHeight()\n\tif start > tipHeight {\n\t\treturn nil, status.Error(codes.InvalidArgument, \"start height should not exceed tip height\")\n\t}\n\tvar res []*iotextypes.BlockMeta\n\tfor height := int(start); height <= int(tipHeight); height++ {\n\t\tif uint64(len(res)) >= number {\n\t\t\tbreak\n\t\t}\n\t\tblk, err := api.bc.GetBlockByHeight(uint64(height))\n\t\tif err != nil {\n\t\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t\t}\n\t\tblockHeaderPb := blk.ConvertToBlockHeaderPb()\n\n\t\thash := blk.HashBlock()\n\t\ttxRoot := blk.TxRoot()\n\t\treceiptRoot := blk.ReceiptRoot()\n\t\tdeltaStateDigest := blk.DeltaStateDigest()\n\t\ttransferAmount := getTranferAmountInBlock(blk)\n\n\t\tblockMeta := &iotextypes.BlockMeta{\n\t\t\tHash:             hex.EncodeToString(hash[:]),\n\t\t\tHeight:           blk.Height(),\n\t\t\tTimestamp:        blockHeaderPb.GetCore().GetTimestamp().GetSeconds(),\n\t\t\tNumActions:       int64(len(blk.Actions)),\n\t\t\tProducerAddress:  blk.ProducerAddress(),\n\t\t\tTransferAmount:   transferAmount.String(),\n\t\t\tTxRoot:           hex.EncodeToString(txRoot[:]),\n\t\t\tReceiptRoot:      hex.EncodeToString(receiptRoot[:]),\n\t\t\tDeltaStateDigest: hex.EncodeToString(deltaStateDigest[:]),\n\t\t}\n\n\t\tres = append(res, blockMeta)\n\t}\n\n\treturn &iotexapi.GetBlockMetasResponse{BlkMetas: res}, nil\n}\n\n// getBlockMeta returns block by block hash\nfunc (api *Server) getBlockMeta(blkHash string) (*iotexapi.GetBlockMetasResponse, error) {\n\thash, err := toHash256(blkHash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.InvalidArgument, err.Error())\n\t}\n\n\tblk, err := api.bc.GetBlockByHash(hash)\n\tif err != nil {\n\t\treturn nil, status.Error(codes.NotFound, err.Error())\n\t}\n\n\tblkHeaderPb := blk.ConvertToBlockHeaderPb()\n\ttxRoot := blk.TxRoot()\n\treceiptRoot := blk.ReceiptRoot()\n\tdeltaStateDigest := blk.DeltaStateDigest()\n\ttransferAmount := getTranferAmountInBlock(blk)\n\n\tblockMeta := &iotextypes.BlockMeta{\n\t\tHash:             blkHash,\n\t\tHeight:           blk.Height(),\n\t\tTimestamp:        blkHeaderPb.GetCore().GetTimestamp().GetSeconds(),\n\t\tNumActions:       int64(len(blk.Actions)),\n\t\tProducerAddress:  blk.ProducerAddress(),\n\t\tTransferAmount:   transferAmount.String(),\n\t\tTxRoot:           hex.EncodeToString(txRoot[:]),\n\t\tReceiptRoot:      hex.EncodeToString(receiptRoot[:]),\n\t\tDeltaStateDigest: hex.EncodeToString(deltaStateDigest[:]),\n\t}\n\n\treturn &iotexapi.GetBlockMetasResponse{BlkMetas: []*iotextypes.BlockMeta{blockMeta}}, nil\n}\n\nfunc (api *Server) getGravityChainStartHeight(epochHeight uint64) (uint64, error) {\n\tgravityChainStartHeight := epochHeight\n\tif _, ok := api.registry.Find(poll.ProtocolID); ok {\n\t\treadStateRequest := &iotexapi.ReadStateRequest{\n\t\t\tProtocolID: []byte(poll.ProtocolID),\n\t\t\tMethodName: []byte(\"GetGravityChainStartHeight\"),\n\t\t\tArguments:  [][]byte{byteutil.Uint64ToBytes(epochHeight)},\n\t\t}\n\t\tres, err := api.readState(context.Background(), readStateRequest)\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tgravityChainStartHeight = byteutil.BytesToUint64(res.GetData())\n\t}\n\treturn gravityChainStartHeight, nil\n}\n\nfunc (api *Server) convertToAction(selp action.SealedEnvelope, pullBlkHash bool) (*iotexapi.ActionInfo, error) {\n\tactHash := selp.Hash()\n\tblkHash := hash.ZeroHash256\n\tvar err error\n\tif pullBlkHash {\n\t\tif blkHash, err = api.bc.GetBlockHashByActionHash(actHash); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t}\n\treturn &iotexapi.ActionInfo{\n\t\tAction:  selp.Proto(),\n\t\tActHash: hex.EncodeToString(actHash[:]),\n\t\tBlkHash: hex.EncodeToString(blkHash[:]),\n\t}, nil\n}\n\nfunc (api *Server) getAction(actHash hash.Hash256, checkPending bool) (*iotexapi.ActionInfo, error) {\n\tvar selp action.SealedEnvelope\n\tvar err error\n\tif selp, err = api.bc.GetActionByActionHash(actHash); err != nil {\n\t\tif checkPending {\n\t\t\t// Try to fetch pending action from actpool\n\t\t\tselp, err = api.ap.GetActionByHash(actHash)\n\t\t}\n\t}\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn api.convertToAction(selp, !checkPending)\n}\n\nfunc (api *Server) getTotalActionsByAddress(address string) ([]hash.Hash256, error) {\n\tvar actions []hash.Hash256\n\tif api.cfg.UseRDS {\n\t\tactionHistory, err := api.idx.Indexer().GetIndexHistory(config.IndexAction, address)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tactions = append(actions, actionHistory...)\n\t} else {\n\t\tactionsFromAddress, err := api.bc.GetActionsFromAddress(address)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tactionsToAddress, err := api.bc.GetActionsToAddress(address)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\n\t\tactionsFromAddress = append(actionsFromAddress, actionsToAddress...)\n\t\tactions = append(actions, actionsFromAddress...)\n\t}\n\treturn actions, nil\n}\n\nfunc toHash256(hashString string) (hash.Hash256, error) {\n\tbytes, err := hex.DecodeString(hashString)\n\tif err != nil {\n\t\treturn hash.ZeroHash256, err\n\t}\n\tvar hash hash.Hash256\n\tcopy(hash[:], bytes)\n\treturn hash, nil\n}\n\nfunc getTranferAmountInBlock(blk *block.Block) *big.Int {\n\ttotalAmount := big.NewInt(0)\n\tfor _, selp := range blk.Actions {\n\t\ttransfer, ok := selp.Action().(*action.Transfer)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\ttotalAmount.Add(totalAmount, transfer.Amount())\n\t}\n\treturn totalAmount\n}\n", "idx": 5, "id": 17306, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go"}
{"patch": "@@ -24,10 +24,10 @@ type ID string\n \n // Session structure holds all required information about current session between service consumer and provider\n type Session struct {\n-\tID              ID\n-\tConfig          ServiceConfiguration\n-\tConsumerID      identity.Identity\n-\tDestroyCallback DestroyCallback\n+\tID         ID\n+\tConfig     ServiceConfiguration\n+\tConsumerID identity.Identity\n+\tStop       chan struct{}\n }\n \n // ServiceConfiguration defines service configuration from underlying transport mechanism to be passed to remote party", "y": 1, "oldf": "/*\n * Copyright (C) 2017 The \"MysteriumNetwork/node\" Authors.\n *\n * This program is free software: you can redistribute it and/or modify\n * it under the terms of the GNU General Public License as published by\n * the Free Software Foundation, either version 3 of the License, or\n * (at your option) any later version.\n *\n * This program is distributed in the hope that it will be useful,\n * but WITHOUT ANY WARRANTY; without even the implied warranty of\n * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n * GNU General Public License for more details.\n *\n * You should have received a copy of the GNU General Public License\n * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n */\n\npackage session\n\nimport \"github.com/mysteriumnetwork/node/identity\"\n\n// ID represents session id type\ntype ID string\n\n// Session structure holds all required information about current session between service consumer and provider\ntype Session struct {\n\tID              ID\n\tConfig          ServiceConfiguration\n\tConsumerID      identity.Identity\n\tDestroyCallback DestroyCallback\n}\n\n// ServiceConfiguration defines service configuration from underlying transport mechanism to be passed to remote party\n// should be serializable to json format\ntype ServiceConfiguration interface{}\n", "idx": 1, "id": 13285, "msg": "Naming. it's not a channel to stop something, it's a channel to notify. Better name would be \"Done\", \"Gone\", \"Closed\" or smth like that.", "proj": "mysteriumnetwork-node", "lang": "go"}
{"patch": "@@ -259,7 +259,7 @@ class CLI(object):\n             self.handle_exception(exc)\n         finally:\n             try:\n-                for fname in url_shorthands + jmx_shorthands:\n+                for fname in url_shorthands + jmx_shorthands + jtl_shorthands:\n                     os.remove(fname)\n                 self.engine.post_process()\n             except BaseException as exc:", "y": 0, "oldf": "#! /usr/bin/env python\n\"\"\"\nCopyright 2015 BlazeMeter Inc.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\"\"\"\nimport copy\nimport logging\nimport os\nimport platform\nimport shutil\nimport signal\nimport sys\nimport tempfile\nimport traceback\nfrom logging import Formatter\nfrom optparse import OptionParser, Option\nfrom tempfile import NamedTemporaryFile\n\nimport yaml\nfrom colorlog import ColoredFormatter\n\nimport bzt\nfrom bzt import ManualShutdown, NormalShutdown, RCProvider, AutomatedShutdown\nfrom bzt import TaurusException, ToolError\nfrom bzt import TaurusInternalException, TaurusConfigError, TaurusNetworkError\nfrom bzt.engine import Engine, Configuration, ScenarioExecutor\nfrom bzt.engine import SETTINGS\nfrom bzt.linter import ConfigurationLinter\nfrom bzt.six import HTTPError, string_types, get_stacktrace, integer_types\nfrom bzt.utils import run_once, is_int, BetterDict, get_full_path, is_url\n\n\nclass CLI(object):\n    \"\"\"\n    'cli' means 'tool' in hebrew, did you know that?\n\n    :param options: OptionParser parsed parameters\n    \"\"\"\n    console_handler = logging.StreamHandler(sys.stdout)\n\n    CLI_SETTINGS = \"cli\"\n\n    def __init__(self, options):\n        self.signal_count = 0\n        self.options = options\n        self.setup_logging(options)\n        self.log = logging.getLogger('')\n        self.log.info(\"Taurus CLI Tool v%s\", bzt.VERSION)\n        self.log.debug(\"Command-line options: %s\", self.options)\n        self.log.debug(\"Python: %s %s\", platform.python_implementation(), platform.python_version())\n        self.log.debug(\"OS: %s\", platform.uname())\n        self.engine = Engine(self.log)\n        self.exit_code = 0\n\n    @staticmethod\n    @run_once\n    def setup_logging(options):\n        \"\"\"\n        Setting up console and file logging, colored if possible\n\n        :param options: OptionParser parsed options\n        \"\"\"\n        colors = {\n            'WARNING': 'yellow',\n            'ERROR': 'red',\n            'CRITICAL': 'bold_red',\n        }\n        fmt_file = Formatter(\"[%(asctime)s %(levelname)s %(name)s] %(message)s\")\n        if sys.stdout.isatty():\n            fmt_verbose = ColoredFormatter(\"%(log_color)s[%(asctime)s %(levelname)s %(name)s] %(message)s\",\n                                           log_colors=colors)\n            fmt_regular = ColoredFormatter(\"%(log_color)s%(asctime)s %(levelname)s: %(message)s\",\n                                           \"%H:%M:%S\", log_colors=colors)\n        else:\n            fmt_verbose = Formatter(\"[%(asctime)s %(levelname)s %(name)s] %(message)s\")\n            fmt_regular = Formatter(\"%(asctime)s %(levelname)s: %(message)s\", \"%H:%M:%S\")\n\n        logger = logging.getLogger('')\n        logger.setLevel(logging.DEBUG)\n\n        # log everything to file\n        if options.log is None:\n            tf = tempfile.NamedTemporaryFile(prefix=\"bzt_\", suffix=\".log\", delete=False)\n            tf.close()\n            os.chmod(tf.name, 0o644)\n            options.log = tf.name\n\n        if options.log:\n            file_handler = logging.FileHandler(options.log)\n            file_handler.setLevel(logging.DEBUG)\n            file_handler.setFormatter(fmt_file)\n            logger.addHandler(file_handler)\n\n        # log something to console\n        if options.verbose:\n            CLI.console_handler.setLevel(logging.DEBUG)\n            CLI.console_handler.setFormatter(fmt_verbose)\n        elif options.quiet:\n            CLI.console_handler.setLevel(logging.WARNING)\n            CLI.console_handler.setFormatter(fmt_regular)\n        else:\n            CLI.console_handler.setLevel(logging.INFO)\n            CLI.console_handler.setFormatter(fmt_regular)\n\n        logger.addHandler(CLI.console_handler)\n\n        logging.getLogger(\"requests\").setLevel(logging.WARNING)  # misplaced?\n\n    def __close_log(self):\n        \"\"\"\n        Close log handlers\n        :return:\n        \"\"\"\n        if self.options.log:\n            # need to finalize the logger before finishing\n            for handler in self.log.handlers:\n                if issubclass(handler.__class__, logging.FileHandler):\n                    self.log.debug(\"Closing log handler: %s\", handler.baseFilename)\n                    handler.close()\n                    self.log.handlers.remove(handler)\n\n    def __move_log_to_artifacts(self):\n        \"\"\"\n        Close log handlers, copy log to artifacts dir, recreate file handlers\n        :return:\n        \"\"\"\n        if self.options.log:\n            for handler in self.log.handlers:\n                if issubclass(handler.__class__, logging.FileHandler):\n                    self.log.debug(\"Closing log handler: %s\", handler.baseFilename)\n                    handler.close()\n                    self.log.handlers.remove(handler)\n\n            if os.path.exists(self.options.log):\n                self.engine.existing_artifact(self.options.log, move=True, target_filename=\"bzt.log\")\n            self.options.log = os.path.join(self.engine.artifacts_dir, \"bzt.log\")\n\n            file_handler = logging.FileHandler(self.options.log)\n            file_handler.setLevel(logging.DEBUG)\n            file_handler.setFormatter(Formatter(\"[%(asctime)s %(levelname)s %(name)s] %(message)s\"))\n\n            self.log.addHandler(file_handler)\n            self.log.debug(\"Switched writing logs to %s\", self.options.log)\n\n    def __configure(self, configs):\n        self.log.info(\"Starting with configs: %s\", configs)\n\n        if self.options.no_system_configs is None:\n            self.options.no_system_configs = False\n\n        bzt_rc = os.path.expanduser(os.path.join('~', \".bzt-rc\"))\n        if os.path.exists(bzt_rc):\n            self.log.debug(\"Using personal config: %s\" % bzt_rc)\n        else:\n            self.log.debug(\"Adding personal config: %s\", bzt_rc)\n            self.log.info(\"No personal config found, creating one at %s\", bzt_rc)\n            shutil.copy(os.path.join(get_full_path(__file__, step_up=1), 'resources', 'base-bzt-rc.yml'), bzt_rc)\n\n        merged_config = self.engine.configure([bzt_rc] + configs, not self.options.no_system_configs)\n\n        # apply aliases\n        for alias in self.options.aliases:\n            cli_aliases = self.engine.config.get('cli-aliases')\n            keys = sorted(cli_aliases.keys())\n            err = TaurusConfigError(\"'%s' not found in aliases. Available aliases are: %s\" % (alias, \", \".join(keys)))\n            self.engine.config.merge(cli_aliases.get(alias, err))\n\n        if self.options.option:\n            overrider = ConfigOverrider(self.log)\n            overrider.apply_overrides(self.options.option, self.engine.config)\n\n        if self.__is_verbose():\n            CLI.console_handler.setLevel(logging.DEBUG)\n        self.engine.create_artifacts_dir(configs, merged_config)\n        self.engine.default_cwd = os.getcwd()\n        self.engine.eval_env()  # yacky, I don't like having it here, but how to apply it after aliases and artif dir?\n\n    def __is_verbose(self):\n        settings = self.engine.config.get(SETTINGS, force_set=True)\n        settings.get('verbose', bool(self.options.verbose))  # respect value from config\n        if self.options.verbose:  # force verbosity if cmdline asked for it\n            settings['verbose'] = True\n\n        return settings.get('verbose', False)\n\n    def __lint_config(self):\n        settings = self.engine.config.get(CLI.CLI_SETTINGS).get(\"linter\")\n        self.log.debug(\"Linting config\")\n        self.warn_on_unfamiliar_fields = settings.get(\"warn-on-unfamiliar-fields\", True)\n        config_copy = copy.deepcopy(self.engine.config)\n        ignored_warnings = settings.get(\"ignored-warnings\", [])\n        self.linter = ConfigurationLinter(config_copy, ignored_warnings, self.log)\n        self.linter.register_checkers()\n        self.linter.lint()\n        warnings = self.linter.get_warnings()\n        for warning in warnings:\n            self.log.warning(str(warning))\n\n        if settings.get(\"lint-and-exit\", False):\n            if warnings:\n                raise TaurusConfigError(\"Errors were found in the configuration\")\n            else:\n                raise NormalShutdown(\"Linting has finished, no errors were found\")\n\n    def _level_down_logging(self):\n        target = logging.DEBUG if self.__is_verbose() else logging.INFO\n        for handler in self.log.handlers:\n            if issubclass(handler.__class__, logging.FileHandler):\n                if handler.level != target:\n                    msg = \"Leveling down log file verbosity to %s, use -v option to have DEBUG messages enabled\"\n                    self.log.debug(msg, logging.getLevelName(target))\n                    handler.setLevel(target)\n\n    def _level_up_logging(self):\n        for handler in self.log.handlers:\n            if issubclass(handler.__class__, logging.FileHandler):\n                if handler.level != logging.DEBUG:\n                    handler.setLevel(logging.DEBUG)\n                    self.log.debug(\"Leveled up log file verbosity\")\n\n    def perform(self, configs):\n        \"\"\"\n        Run the tool\n\n        :type configs: list\n        :return: integer exit code\n        \"\"\"\n        url_shorthands = []\n        jmx_shorthands = []\n        try:\n            url_shorthands = self.__get_url_shorthands(configs)\n            configs.extend(url_shorthands)\n\n            jmx_shorthands = self.__get_jmx_shorthands(configs)\n            configs.extend(jmx_shorthands)\n\n            if not self.engine.config.get(SETTINGS).get('verbose', False, force_set=True):\n                self.engine.logging_level_down = self._level_down_logging\n                self.engine.logging_level_up = self._level_up_logging\n\n            self.__configure(configs)\n            self.__move_log_to_artifacts()\n            self.__lint_config()\n\n            self.engine.prepare()\n            self.engine.run()\n        except BaseException as exc:\n            self.handle_exception(exc)\n        finally:\n            try:\n                for fname in url_shorthands + jmx_shorthands:\n                    os.remove(fname)\n                self.engine.post_process()\n            except BaseException as exc:\n                self.handle_exception(exc)\n\n        self.log.info(\"Artifacts dir: %s\", self.engine.artifacts_dir)\n        if self.engine.artifacts_dir is None:\n            self.log.info(\"Log file: %s\", self.options.log)\n\n        if self.exit_code:\n            self.log.warning(\"Done performing with code: %s\", self.exit_code)\n        else:\n            self.log.info(\"Done performing with code: %s\", self.exit_code)\n\n        self.__close_log()\n\n        return self.exit_code\n\n    def handle_exception(self, exc):\n        log_level = {'info': logging.DEBUG, 'http': logging.DEBUG, 'default': logging.DEBUG}\n        if not self.exit_code:  # only fist exception goes to the screen\n            log_level['info'] = logging.WARNING\n            log_level['http'] = logging.ERROR\n            log_level['default'] = logging.ERROR\n            if isinstance(exc, RCProvider):\n                self.exit_code = exc.get_rc()\n            else:\n                self.exit_code = 1\n\n        if isinstance(exc, KeyboardInterrupt):\n            self.__handle_keyboard_interrupt(exc, log_level)\n            log_level['default'] = logging.DEBUG\n        elif isinstance(exc, TaurusException):\n            self.__handle_taurus_exception(exc, log_level['default'])\n            log_level['default'] = logging.DEBUG\n        elif isinstance(exc, HTTPError):\n            msg = \"Response from %s: [%s] %s %s\" % (exc.geturl(), exc.code, exc.reason, exc.read())\n            self.log.log(log_level['http'], msg)\n            log_level['default'] = logging.DEBUG\n\n        self.log.log(log_level['default'], \"%s: %s\\n%s\", type(exc).__name__, exc, get_stacktrace(exc))\n\n    def __handle_keyboard_interrupt(self, exc, log_level):\n        if isinstance(exc, ManualShutdown):\n            self.log.log(log_level['info'], \"Interrupted by user\")\n        elif isinstance(exc, AutomatedShutdown):\n            self.log.log(log_level['info'], \"Automated shutdown\")\n        elif isinstance(exc, NormalShutdown):\n            self.log.log(logging.DEBUG, \"Shutting down by request from code\")\n        elif isinstance(exc, KeyboardInterrupt):\n            self.log.log(log_level['info'], \"Keyboard interrupt\")\n        else:\n            msg = \"Non-KeyboardInterrupt exception %s: %s\\n%s\"\n            raise ValueError(msg % (type(exc), exc, get_stacktrace(exc)))\n\n    def __handle_taurus_exception(self, exc, log_level):\n        if isinstance(exc, TaurusConfigError):\n            self.log.log(log_level, \"Config Error: %s\", exc)\n        elif isinstance(exc, TaurusInternalException):\n            self.log.log(log_level, \"Internal Error: %s\", exc)\n        elif isinstance(exc, ToolError):\n            self.log.log(log_level, \"Child Process Error: %s\", exc)\n            if exc.diagnostics is not None:\n                for line in exc.diagnostics:\n                    self.log.log(log_level, line)\n        elif isinstance(exc, TaurusNetworkError):\n            self.log.log(log_level, \"Network Error: %s\", exc)\n        else:\n            self.log.log(log_level, \"Generic Taurus Error: %s\", exc)\n\n    def __get_jmx_shorthands(self, configs):\n        \"\"\"\n        Generate json file with execution, executor and scenario settings\n        :type configs: list\n        :return: list\n        \"\"\"\n        jmxes = []\n        for filename in configs[:]:\n            if filename.lower().endswith(\".jmx\"):\n                jmxes.append(filename)\n                configs.remove(filename)\n\n        if jmxes:\n            self.log.debug(\"Adding JMX shorthand config for: %s\", jmxes)\n            fds = NamedTemporaryFile(prefix=\"jmx_\", suffix=\".json\")\n            fname = fds.name\n            fds.close()\n\n            config = Configuration()\n\n            for jmx_file in jmxes:\n                piece = {\"executor\": \"jmeter\", \"scenario\": {\"script\": jmx_file}}\n                config.get(ScenarioExecutor.EXEC, [], force_set=True).append(piece)  # Does it brake single execution?\n\n            config.dump(fname, Configuration.JSON)\n\n            return [fname]\n        else:\n            return []\n\n    def __get_url_shorthands(self, configs):\n        \"\"\"\n        :type configs: list\n        :return: list\n        \"\"\"\n        urls = []\n        for candidate in configs[:]:\n            if is_url(candidate):\n                urls.append(candidate)\n                configs.remove(candidate)\n\n        if urls:\n            self.log.debug(\"Adding HTTP shorthand config for: %s\", urls)\n            config_fds = NamedTemporaryFile(prefix=\"http_\", suffix=\".yml\")\n            fname = config_fds.name\n            config_fds.close()\n\n            config = Configuration.from_dict({\n                \"execution\": [{\n                    \"concurrency\": \"${__tstFeedback(Throughput_Limiter,1,${__P(concurrencyCap,1)},2)}\",\n                    \"hold-for\": \"2m\",\n                    \"throughput\": \"${__P(throughput,600)}\",\n                    \"scenario\": \"linear-growth\",\n                }],\n                \"scenarios\": {\n                    \"linear-growth\": {\n                        \"retrieve-resources\": False,\n                        \"timeout\": \"5s\",\n                        \"keepalive\": False,\n                        \"requests\": [{\n                            \"action\": \"pause\",\n                            \"pause-duration\": 0,\n                            \"jsr223\": [{\n                                \"language\": \"javascript\",\n                                \"execute\": \"before\",\n                                \"script-text\": \"\"\"\nvar startTime = parseInt(props.get(\"startTime\"));\nif (!startTime) {\n    startTime = Math.floor((new Date()).getTime() / 1000);\n    props.put(\"startTime\", startTime);\n} else {\n    var now = Math.floor((new Date()).getTime() / 1000);\n    var offset = now - startTime;\n    if (offset < 60) {\n        var targetOffset = Math.max(offset * 10, 10);\n        props.put(\"throughput\", targetOffset.toString());\n    }\n}\"\"\"\n                            }]\n                        }] + urls,\n                    }\n                },\n                \"modules\": {\n                    \"jmeter\": {\n                        \"properties\": {\n                            \"throughput\": 1,\n                            \"concurrencyCap\": 500,\n                        },\n                    }\n                }\n            })\n            config.dump(fname, Configuration.JSON)\n            return [fname]\n        else:\n            return []\n\n\nclass ConfigOverrider(object):\n    def __init__(self, logger):\n        \"\"\"\n        :type logger: logging.Logger\n        \"\"\"\n        super(ConfigOverrider, self).__init__()\n        self.log = logger.getChild(self.__class__.__name__)\n\n    def apply_overrides(self, options, dest):\n        \"\"\"\n        Apply overrides\n        :type options: list[str]\n        :type dest: BetterDict\n        \"\"\"\n        for option in options:\n            name = option[:option.index('=')]\n            value = option[option.index('=') + 1:]\n            try:\n                self.__apply_single_override(dest, name, value)\n            except BaseException:\n                self.log.debug(\"Failed override: %s\", traceback.format_exc())\n                self.log.error(\"Failed to apply override %s=%s\", name, value)\n                raise\n\n        dest.dump()\n\n    def __apply_single_override(self, dest, name, value):\n        \"\"\"\n        Apply single override\n        :type name: str\n        :type value: str\n        \"\"\"\n        self.log.debug(\"Applying %s=%s\", name, value)\n        parts = [(int(x) if is_int(x) else x) for x in name.split(\".\")]\n        pointer = dest\n        for index, part in enumerate(parts[:-1]):\n            self.__ensure_list_capacity(pointer, part, parts[index + 1])\n\n            if isinstance(part, integer_types):\n                if part < 0:\n                    if isinstance(parts[index + 1], integer_types):\n                        pointer.append([])\n                    else:\n                        pointer.append(BetterDict())\n                    pointer = pointer[-1]\n                else:\n                    pointer = pointer[part]\n            elif isinstance(parts[index + 1], integer_types) and isinstance(pointer, dict):\n                pointer = pointer.get(part, [], force_set=True)\n            else:\n                pointer = pointer.get(part, force_set=True)\n        self.__ensure_list_capacity(pointer, parts[-1])\n        self.log.debug(\"Applying: [%s]=%s\", parts[-1], value)\n        if isinstance(parts[-1], string_types) and parts[-1][0] == '^':\n            item = parts[-1][1:]\n\n            if isinstance(pointer, list):\n                item = int(item)\n                if -len(pointer) <= item < len(pointer):\n                    del pointer[item]\n                else:\n                    self.log.debug(\"No value to delete: %s\", item)\n            elif isinstance(pointer, dict):\n                if item in pointer:\n                    del pointer[item]\n                else:\n                    self.log.debug(\"No value to delete: %s\", item)\n            else:\n                raise ValueError(\"Cannot handle override %s in non-iterable type %s\" % (item, pointer))\n\n        else:\n            parsed_value = self.__parse_override_value(value)\n            self.log.debug(\"Parsed override value: %r -> %r (%s)\", value, parsed_value, type(parsed_value))\n            if isinstance(parsed_value, dict):\n                parsed_value = BetterDict.from_dict(parsed_value)\n            if isinstance(pointer, list) and parts[-1] < 0:\n                pointer.append(parsed_value)\n            else:\n                pointer[parts[-1]] = parsed_value\n\n    @staticmethod\n    def __parse_override_value(override):\n        try:\n            return yaml.load(override)\n        except BaseException:\n            return override\n\n    def __ensure_list_capacity(self, pointer, part, next_part=None):\n        \"\"\"\n        Extend pointer list to hold additional item\n        :type pointer: list\n        :type part: int\n        \"\"\"\n        if isinstance(pointer, list) and isinstance(part, integer_types):\n            while len(pointer) <= part:\n                self.log.debug(\"Len %s less than %s\", len(pointer), part)\n                if isinstance(next_part, integer_types):\n                    pointer.append([])\n                else:\n                    pointer.append(BetterDict())\n\n\nclass OptionParserWithAliases(OptionParser, object):\n    \"\"\"\n    Decorator that processes short opts as aliases\n    \"\"\"\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        super(OptionParserWithAliases, self).__init__(\n            usage=usage, option_list=option_list,\n            option_class=option_class, version=version,\n            conflict_handler=conflict_handler, description=description, formatter=formatter,\n            add_help_option=add_help_option, prog=prog, epilog=epilog)\n        self.aliases = []\n\n    def _process_short_opts(self, rargs, values):\n        if rargs[0].startswith('-') and len(rargs[0]) > 2:\n            self.aliases.append(rargs.pop(0)[1:])\n        else:\n            return OptionParser._process_short_opts(self, rargs, values)\n\n    def parse_args(self, args=None, values=None):\n        res = OptionParser.parse_args(self, args, values)\n        res[0].aliases = self.aliases\n        return res\n\n\ndef get_option_parser():\n    usage = \"Usage: bzt [options] [configs] [-aliases]\"\n    dsc = \"BlazeMeter Taurus Tool v%s, the configuration-driven test running engine\" % bzt.VERSION\n    parser = OptionParserWithAliases(usage=usage, description=dsc, prog=\"bzt\")\n    parser.add_option('-l', '--log', action='store', default=None,\n                      help=\"Log file location\")\n    parser.add_option('-o', '--option', action='append',\n                      help=\"Override option in config\")\n    parser.add_option('-q', '--quiet', action='store_true',\n                      help=\"Only errors and warnings printed to console\")\n    parser.add_option('-v', '--verbose', action='store_true',\n                      help=\"Prints all logging messages to console\")\n    parser.add_option('-n', '--no-system-configs', action='store_true',\n                      help=\"Skip system and user config files\")\n    return parser\n\n\ndef signal_handler(sig, frame):\n    \"\"\"\n    required for non-tty python runs to interrupt\n    :param frame:\n    :param sig:\n    \"\"\"\n    del sig, frame\n    raise ManualShutdown()\n\n\ndef main():\n    \"\"\"\n    This function is used as entrypoint by setuptools\n    \"\"\"\n    parser = get_option_parser()\n\n    parsed_options, parsed_configs = parser.parse_args()\n\n    executor = CLI(parsed_options)\n\n    try:\n        code = executor.perform(parsed_configs)\n    except BaseException as exc_top:\n        logging.error(\"%s: %s\", type(exc_top).__name__, exc_top)\n        logging.debug(\"Exception: %s\", traceback.format_exc())\n        code = 1\n\n    exit(code)\n\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, signal_handler)\n    signal.signal(signal.SIGTERM, signal_handler)\n    main()\n", "idx": 12, "id": 15107, "msg": "", "proj": "Blazemeter-taurus", "lang": "py"}
{"patch": "@@ -917,6 +917,7 @@ Check the dag for consistency, and emit errors if input dependencies, etc are mi\n \n static int makeflow_check(struct dag *d)\n {\n+\tstruct stat buf;\n \tstruct dag_node *n;\n \tstruct dag_file *f;\n \tint error = 0;", "y": 0, "oldf": "/*\nCopyright (C) 2008- The University of Notre Dame\nThis software is distributed under the GNU General Public License.\nSee the file COPYING for details.\n*/\n\n#include \"auth_all.h\"\n#include \"auth_ticket.h\"\n#include \"batch_job.h\"\n#include \"cctools.h\"\n#include \"copy_stream.h\"\n#include \"debug.h\"\n#include \"getopt_aux.h\"\n#include \"hash_table.h\"\n#include \"int_sizes.h\"\n#include \"itable.h\"\n#include \"link.h\"\n#include \"list.h\"\n#include \"load_average.h\"\n#include \"macros.h\"\n#include \"path.h\"\n#include \"random.h\"\n#include \"rmonitor.h\"\n#include \"stringtools.h\"\n#include \"work_queue.h\"\n#include \"work_queue_catalog.h\"\n#include \"xxmalloc.h\"\n\n#include \"dag.h\"\n#include \"dag_visitors.h\"\n#include \"makeflow_summary.h\"\n#include \"makeflow_log.h\"\n#include \"makeflow_gc.h\"\n#include \"parser.h\"\n\n#include <fcntl.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n\n#include <errno.h>\n#include <signal.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n/*\nCode organization notes:\n\n- The modules dag/dag_node/dag_file etc contain the data structures that\nrepresent the dag structure by itself.  Functions named dag_*() create\nand manipulate those data structures, but do not execute the dag itself.\nThese are shared between makeflow and other tools that read and manipulate\nthe dag, like makeflow_viz, makeflow_linker, and so forth.\n\n- The modules makeflow/makeflow_log/makeflow_gc etc contain the functions\nthat execute the dag by invoking batch operations, processing the log, etc.\nThese are all functions named makeflow_*() to distinguish them from dag_*().\n\n- The separation between dag structure and execution state is imperfect,\nbecause some of the execution state (note states, node counts, etc)\nis stored in struct dag and struct dag_node.  Perhaps this can be improved.\n\n- All operations on files should use the batch_fs_*() functions, rather\nthan invoking Unix I/O directly.  This is because some batch systems\n(Hadoop, Confuga, etc) also include the storage where the files to be\naccessed are located.\n\n- APIs like work_queue_* should be indirectly accessed by setting options\nin Batch Job using batch_queue_set_option. See batch_job_work_queue.c for\nan example.\n*/\n\n#define DEFAULT_MONITOR_LOG_FORMAT \"resource-rule-%06.6d\"\n\n#define CONTAINER_SH \"docker.wrapper.sh\"\n\n#define MAX_REMOTE_JOBS_DEFAULT 100\n\ntypedef enum {\n\tCONTAINER_MODE_NONE,\n\tCONTAINER_MODE_DOCKER,\n\t// CONTAINER_MODE_ROCKET etc\n} container_mode_t;\n\nstatic sig_atomic_t makeflow_abort_flag = 0;\nstatic int makeflow_failed_flag = 0;\nstatic int makeflow_submit_timeout = 3600;\nstatic int makeflow_retry_flag = 0;\nstatic int makeflow_retry_max = MAX_REMOTE_JOBS_DEFAULT;\n\nstatic makeflow_gc_method_t makeflow_gc_method = MAKEFLOW_GC_NONE;\nstatic int makeflow_gc_param = -1;\nstatic int makeflow_gc_barrier = 1;\nstatic double makeflow_gc_task_ratio = 0.05;\n\nstatic batch_queue_type_t batch_queue_type = BATCH_QUEUE_TYPE_LOCAL;\nstatic struct batch_queue *local_queue = 0;\nstatic struct batch_queue *remote_queue = 0;\n\nstatic int local_jobs_max = 1;\nstatic int remote_jobs_max = 100;\n\nstatic char *project = NULL;\nstatic int port = 0;\nstatic int output_len_check = 0;\n\nstatic int cache_mode = 1;\n\nstatic char *monitor_exe  = \"resource_monitor_cctools\";\n\nstatic int monitor_mode = 0;\nstatic int monitor_enable_time_series = 0;\nstatic int monitor_enable_list_files  = 0;\n\nstatic char *monitor_limits_name = NULL;\nstatic int   monitor_interval = 1;\t// in seconds\nstatic char *monitor_log_format = NULL;\nstatic char *monitor_log_dir = NULL;\n\nstatic container_mode_t container_mode = CONTAINER_MODE_NONE;\nstatic char *container_image = NULL;\nstatic char *image_tar = NULL;\n\n/* wait upto this many seconds for an output file of a succesfull task\n * to appear on the local filesystem (e.g, to deal with NFS\n * semantics. . */\nstatic int file_creation_patience_wait_time = 0;\n\n/* Write a verbose transaction log with SYMBOL tags.\n * SYMBOLs are category labels (SYMBOLs should be deprecated\n * once weaver/pbui tools are updated.) */\nstatic int log_verbose_mode = 0;\n\nstatic char *wrapper_command = 0;\nstatic struct list *wrapper_input_files = 0;\nstatic struct list *wrapper_output_files = 0;\n\nstatic void makeflow_wrapper_add_command( const char *cmd )\n{\n\tif(!wrapper_command) {\n\t\twrapper_command = strdup(cmd);\n\t} else {\n\t\twrapper_command = string_wrap_command(wrapper_command,cmd);\n\t}\n}\n\nstatic void makeflow_wrapper_add_input_file( const char *file )\n{\n\tif(!wrapper_input_files) wrapper_input_files = list_create();\n\tlist_push_tail(wrapper_input_files,dag_file_create(file));\n}\n\nstatic void makeflow_wrapper_add_output_file( const char *file )\n{\n\tif(!wrapper_output_files) wrapper_output_files = list_create();\n\tlist_push_tail(wrapper_output_files,dag_file_create(file));\n}\n\n/*\nAbort the dag by removing all batch jobs from all queues.\n*/\n\nstatic void makeflow_abort_all(struct dag *d)\n{\n\tUINT64_T jobid;\n\tstruct dag_node *n;\n\n\tprintf(\"got abort signal...\\n\");\n\n\titable_firstkey(d->local_job_table);\n\twhile(itable_nextkey(d->local_job_table, &jobid, (void **) &n)) {\n\t\tprintf(\"aborting local job %\" PRIu64 \"\\n\", jobid);\n\t\tbatch_job_remove(local_queue, jobid);\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_ABORTED);\n\t}\n\n\titable_firstkey(d->remote_job_table);\n\twhile(itable_nextkey(d->remote_job_table, &jobid, (void **) &n)) {\n\t\tprintf(\"aborting remote job %\" PRIu64 \"\\n\", jobid);\n\t\tbatch_job_remove(remote_queue, jobid);\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_ABORTED);\n\t}\n}\n\n/*\nClean a specific file, while emitting an appropriate message.\n*/\n\nstatic void makeflow_file_clean(const char *filename, int silent)\n{\n\tif(!filename)\n\t\treturn;\n\n\tif(batch_fs_unlink(remote_queue, filename) == 0) {\n\t\tif(!silent)\n\t\t\tprintf(\"deleted path %s\\n\", filename);\n\t} else if(errno == ENOENT) {\n\t\t// say nothing\n\t} else if(!silent) {\n\t\tfprintf(stderr, \"couldn't delete %s: %s\\n\", filename, strerror(errno));\n\t}\n}\n\n/*\nFor a given dag node, export all variables into the environment.\nThis is currently only used when cleaning a makeflow recurisvely,\nand would be better handled by invoking batch_job_local.\n*/\n\nstatic void makeflow_node_export_variables( struct dag *d, struct dag_node *n )\n{\n\tstruct nvpair *nv = dag_node_env_create(d,n);\n\tif(nv) {\n\t\tnvpair_export(nv);\n\t\tnvpair_delete(nv);\n\t}\n}\n\n/*\nClean a node of a dag by cleaning all of its output dependencies,\nor by cleaning it recursively if it is a Makeflow itself.\n*/\n\nstatic void makeflow_node_clean(struct dag *d, struct dag_node *n)\n{\n\tstruct dag_file *f;\n\tlist_first_item(n->target_files);\n\twhile((f = list_next_item(n->target_files))) {\n\t\tmakeflow_file_clean(f->filename, 0);\n\t\thash_table_remove(d->completed_files, f->filename);\n\t}\n\n\t/* If the node is a Makeflow job, then we should recursively call the\n\t * clean operation on it. */\n\tif(n->nested_job) {\n\t\tchar *command = xxmalloc(sizeof(char) * (strlen(n->command) + 4));\n\t\tsprintf(command, \"%s -c\", n->command);\n\n\t\t/* XXX this should use the batch job interface for consistency */\n\t\tmakeflow_node_export_variables(d, n);\n\t\tsystem(command);\n\t\tfree(command);\n\t}\n}\n\n/*\nClean the entire dag by cleaning all nodes.\n*/\n\nstatic void makeflow_clean(struct dag *d)\n{\n\tstruct dag_node *n;\n\tfor(n = d->nodes; n; n = n->next)\n\t\tmakeflow_node_clean(d, n);\n}\n\nstatic void makeflow_node_force_rerun(struct itable *rerun_table, struct dag *d, struct dag_node *n);\n\n/*\nDecide whether to rerun a node based on batch and file system status.\n*/\n\nvoid makeflow_node_decide_rerun(struct itable *rerun_table, struct dag *d, struct dag_node *n)\n{\n\tstruct stat filestat;\n\tstruct dag_file *f;\n\n\tif(itable_lookup(rerun_table, n->nodeid))\n\t\treturn;\n\n\t// Below are a bunch of situations when a node has to be rerun.\n\n\t// If a job was submitted to Condor, then just reconnect to it.\n\tif(n->state == DAG_NODE_STATE_RUNNING && !(n->local_job && local_queue) && batch_queue_type == BATCH_QUEUE_TYPE_CONDOR) {\n\t\t// Reconnect the Condor jobs\n\t\tfprintf(stderr, \"rule still running: %s\\n\", n->command);\n\t\titable_insert(d->remote_job_table, n->jobid, n);\n\n\t\t// Otherwise, we cannot reconnect to the job, so rerun it\n\t} else if(n->state == DAG_NODE_STATE_RUNNING || n->state == DAG_NODE_STATE_FAILED || n->state == DAG_NODE_STATE_ABORTED) {\n\t\tfprintf(stderr, \"will retry failed rule: %s\\n\", n->command);\n\t\tgoto rerun;\n\t}\n\t// Rerun if an input file has been updated since the last execution.\n\tlist_first_item(n->source_files);\n\twhile((f = list_next_item(n->source_files))) {\n\t\tif(batch_fs_stat(remote_queue, f->filename, &filestat) >= 0) {\n\t\t\tif(S_ISDIR(filestat.st_mode))\n\t\t\t\tcontinue;\n\t\t\tif(difftime(filestat.st_mtime, n->previous_completion) > 0) {\n\t\t\t\tgoto rerun;\t// rerun this node\n\t\t\t}\n\t\t} else {\n\t\t\tif(!f->created_by) {\n\t\t\t\tfprintf(stderr, \"makeflow: input file %s does not exist and is not created by any rule.\\n\", f->filename);\n\t\t\t\texit(1);\n\t\t\t} else {\n\t\t\t\t/* If input file is missing, but node completed and file was garbage, then avoid rerunning. */\n\t\t\t\tif(n->state == DAG_NODE_STATE_COMPLETE && set_lookup(d->collect_table, f)) {\n\t\t\t\t\tcontinue;\n\t\t\t\t}\n\t\t\t\tgoto rerun;\n\t\t\t}\n\t\t}\n\t}\n\n\t// Rerun if an output file is missing.\n\tlist_first_item(n->target_files);\n\twhile((f = list_next_item(n->target_files))) {\n\t\tif(batch_fs_stat(remote_queue, f->filename, &filestat) < 0) {\n\t\t\t/* If output file is missing, but node completed and file was garbage, then avoid rerunning. */\n\t\t\tif(n->state == DAG_NODE_STATE_COMPLETE && set_lookup(d->collect_table, f)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\t\t\tgoto rerun;\n\t\t}\n\t}\n\n\t// Do not rerun this node\n\treturn;\n\n\t  rerun:\n\tmakeflow_node_force_rerun(rerun_table, d, n);\n}\n\n/*\nReset all state to cause a node to be re-run.\n*/\n\nvoid makeflow_node_force_rerun(struct itable *rerun_table, struct dag *d, struct dag_node *n)\n{\n\tstruct dag_node *p;\n\tstruct dag_file *f1;\n\tstruct dag_file *f2;\n\tint child_node_found;\n\n\tif(itable_lookup(rerun_table, n->nodeid))\n\t\treturn;\n\n\t// Mark this node as having been rerun already\n\titable_insert(rerun_table, n->nodeid, n);\n\n\t// Remove running batch jobs\n\tif(n->state == DAG_NODE_STATE_RUNNING) {\n\t\tif(n->local_job && local_queue) {\n\t\t\tbatch_job_remove(local_queue, n->jobid);\n\t\t\titable_remove(d->local_job_table, n->jobid);\n\t\t} else {\n\t\t\tbatch_job_remove(remote_queue, n->jobid);\n\t\t\titable_remove(d->remote_job_table, n->jobid);\n\t\t}\n\t}\n\t// Clean up things associated with this node\n\tmakeflow_node_clean(d, n);\n\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_WAITING);\n\n\t// For each parent node, rerun it if input file was garbage collected\n\tlist_first_item(n->source_files);\n\twhile((f1 = list_next_item(n->source_files))) {\n\t\tif(!set_lookup(d->collect_table, f1))\n\t\t\tcontinue;\n\n\t\tp = f1->created_by;\n\t\tif(p) {\n\t\t\tmakeflow_node_force_rerun(rerun_table, d, p);\n\t\t\tf1->ref_count += 1;\n\t\t}\n\t}\n\n\t// For each child node, rerun it\n\tlist_first_item(n->target_files);\n\twhile((f1 = list_next_item(n->target_files))) {\n\t\tfor(p = d->nodes; p; p = p->next) {\n\t\t\tchild_node_found = 0;\n\n\t\t\tlist_first_item(p->source_files);\n\t\t\twhile((f2 = list_next_item(n->source_files))) {\n\t\t\t\tif(!strcmp(f1->filename, f2->filename)) {\n\t\t\t\t\tchild_node_found = 1;\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\t\t\t}\n\t\t\tif(child_node_found) {\n\t\t\t\tmakeflow_node_force_rerun(rerun_table, d, p);\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic void makeflow_prepare_nested_jobs(struct dag *d)\n{\n\t/* Update nested jobs with appropriate number of local jobs (total\n\t * local jobs max / maximum number of concurrent nests). */\n\tint dag_nested_width = dag_width(d, 1);\n\tint update_dag_nests = 1;\n\tchar *s = getenv(\"MAKEFLOW_UPDATE_NESTED_JOBS\");\n\tif(s)\n\t\tupdate_dag_nests = atoi(s);\n\n\tif(dag_nested_width > 0 && update_dag_nests) {\n\t\tdag_nested_width = MIN(dag_nested_width, local_jobs_max);\n\t\tstruct dag_node *n;\n\t\tfor(n = d->nodes; n; n = n->next) {\n\t\t\tif(n->nested_job && ((n->local_job && local_queue) || batch_queue_type == BATCH_QUEUE_TYPE_LOCAL)) {\n\t\t\t\tchar *command = xxmalloc(strlen(n->command) + 20);\n\t\t\t\tsprintf(command, \"%s -j %d\", n->command, local_jobs_max / dag_nested_width);\n\t\t\t\tfree((char *) n->command);\n\t\t\t\tn->command = command;\n\t\t\t}\n\t\t}\n\t}\n}\n\nstatic char *monitor_log_name(char *dirname, int nodeid)\n{\n\tchar *name = string_format(monitor_log_format, nodeid);\n\tchar *path = string_format(\"%s/%s\", dirname, name);\n\tfree(name);\n\n\treturn path;\n}\n\n/*\nPrepare a node for monitoring by wrapping the command and attaching the\nappropriate input and output dependencies.\n*/\n\nstatic int makeflow_prepare_for_monitoring(struct dag *d)\n{\n\tstruct dag_node *n;\n\n\tfor(n = d->nodes; n; n = n->next)\n\t{\n\t\tchar *log_name_prefix = monitor_log_name(monitor_log_dir, n->nodeid);\n\t\tchar *log_name;\n\n\t\tdag_node_add_source_file(n, monitor_exe, NULL);\n\n\t\tlog_name = string_format(\"%s.summary\", log_name_prefix);\n\t\tdag_node_add_target_file(n, log_name, NULL);\n\t\tfree(log_name);\n\n\t\tif(monitor_enable_time_series)\n\t\t{\n\t\t\tlog_name = string_format(\"%s.series\", log_name_prefix);\n\t\t\tdag_node_add_target_file(n, log_name, NULL);\n\t\t\tfree(log_name);\n\t\t}\n\n\t\tif(monitor_enable_list_files)\n\t\t{\n\t\t\tlog_name = string_format(\"%s.files\", log_name_prefix);\n\t\t\tdag_node_add_target_file(n, log_name, NULL);\n\t\t\tfree(log_name);\n\t\t}\n\n\t\tfree(log_name_prefix);\n\t}\n\n\treturn 1;\n}\n\n/*\nWraps a given command with the appropriate resource monitor string.\nReturns a newly allocated string that must be freed.\n*/\n\nstatic char *makeflow_node_rmonitor_wrap_command( struct dag_node *n, const char *command )\n{\n\tchar *log_name_prefix = monitor_log_name(monitor_log_dir, n->nodeid);\n\tchar *limits_str = dag_node_resources_wrap_as_rmonitor_options(n);\n\tchar *extra_options = string_format(\"%s -V '%-15s%s'\",\n\t\t\tlimits_str ? limits_str : \"\",\n\t\t\t\"category:\",\n\t\t\tn->category->label);\n\n\tlog_name_prefix     = monitor_log_name(monitor_log_dir, n->nodeid);\n\n\tchar * result = resource_monitor_rewrite_command(command,\n\t\t\tmonitor_exe,\n\t\t\tlog_name_prefix,\n\t\t\tmonitor_limits_name,\n\t\t\textra_options,\n\t\t\t1,                           /* summaries always enabled */\n\t\t\tmonitor_enable_time_series,\n\t\t\tmonitor_enable_list_files);\n\n\tfree(log_name_prefix);\n\tfree(extra_options);\n\tfree(limits_str);\n\n\treturn result;\n}\n\n\n/*\nReplace instances of %% in a string with the string 'replace'.\nTo escape this behavior, %%%% becomes %%.\n(Backslash it not used as the escape, as it would interfere with shell escapes.)\nThis function works like realloc: the string str must be created by malloc\nand may be freed and reallocated.  Therefore, always invoke it like this:\nx = replace_percents(x,replace);\n*/\n\nstatic char * replace_percents( char *str, const char *replace )\n{\n\t/* Common case: do nothing if no percents. */\n\tif(!strchr(str,'%')) return str;\n\n\tbuffer_t buffer;\n\tbuffer_init(&buffer);\n\n\tchar *s;\n\tfor(s=str;*s;s++) {\n\t\tif(*s=='%' && *(s+1)=='%' ) {\n\t\t\tif( *(s+2)=='%' && *(s+3)=='%') {\n\t\t\t\tbuffer_putlstring(&buffer,\"%%\",2);\n\t\t\t\ts+=3;\n\t\t\t} else {\n\t\t\t\tbuffer_putstring(&buffer,replace);\n\t\t\t\ts++;\n\t\t\t}\n\t\t} else {\n\t\t\tbuffer_putlstring(&buffer,s,1);\n\t\t}\n\t}\n\n\tchar *result;\n\tbuffer_dup(&buffer,&result);\n\tbuffer_free(&buffer);\n\n\tfree(str);\n\n\treturn result;\n}\n\n/*\nGiven a file, return the string that identifies it appropriately\nfor the given batch system, combining the local and remote name\nand making substitutions according to the node.\n*/\n\nstatic char * makeflow_file_format( struct dag_node *n, struct dag_file *f, struct batch_queue *queue )\n{\n\tconst char *remotename = dag_node_get_remote_name(n, f->filename);\n\tif(!remotename) remotename = f->filename;\n\n\tswitch (batch_queue_get_type(queue)) {\n\t\tcase BATCH_QUEUE_TYPE_WORK_QUEUE:\n\t\t\treturn string_format(\"%s=%s,\", f->filename, remotename);\n\t\tcase BATCH_QUEUE_TYPE_CONDOR:\n\t\t\treturn string_format(\"%s,\", remotename);\n\t\tdefault:\n\t\t\treturn string_format(\"%s,\", f->filename);\n\t}\n}\n\n/*\nGiven a list of files, add the files to the given string.\nReturns the original string, realloced if necessary\n*/\n\nstatic char * makeflow_file_list_format( struct dag_node *node, char *file_str, struct list *file_list, struct batch_queue *queue )\n{\n\tstruct dag_file *file;\n\n\tif(!file_str) file_str = strdup(\"\");\n\n\tif(!file_list) return file_str;\n\n\tlist_first_item(file_list);\n\twhile((file=list_next_item(file_list))) {\n\t\tchar *f = makeflow_file_format(node,file,queue);\n\t\tfile_str = string_combine(file_str,f);\n\t\tfree(f);\n\t}\n\n\treturn file_str;\n}\n\n/*\nSubmit one fully formed job, retrying failures up to the makeflow_submit_timeout.\nThis is necessary because busy batch systems occasionally do not accept a job submission.\n*/\n\nstatic batch_job_id_t makeflow_node_submit_retry( struct batch_queue *queue, const char *command, const char *input_files, const char *output_files, struct nvpair *envlist )\n{\n\ttime_t stoptime = time(0) + makeflow_submit_timeout;\n\tint waittime = 1;\n\tbatch_job_id_t jobid = 0;\n\n\t/* Display the fully elaborated command, just like Make does. */\n\tprintf(\"submitting job: %s\\n\", command);\n\n\twhile(1) {\n\t\tjobid = batch_job_submit(queue, command, input_files, output_files, envlist );\n\t\tif(jobid >= 0) {\n\t\t\tprintf(\"submitted job %\"PRIbjid\"\\n\", jobid);\n\t\t\treturn jobid;\n\t\t}\n\n\t\tfprintf(stderr, \"couldn't submit batch job, still trying...\\n\");\n\n\t\tif(makeflow_abort_flag) break;\n\n\t\tif(time(0) > stoptime) {\n\t\t\tfprintf(stderr, \"unable to submit job after %d seconds!\\n\", makeflow_submit_timeout);\n\t\t\tbreak;\n\t\t}\n\n\t\tsleep(waittime);\n\t\twaittime *= 2;\n\t\tif(waittime > 60) waittime = 60;\n\t}\n\n\treturn 0;\n}\n\nstatic void makeflow_create_docker_sh()\n{\n\tFILE *wrapper_fn;\n\n\twrapper_fn = fopen(CONTAINER_SH, \"w\");\n\n\tif (image_tar == NULL) {\n\n\t\tfprintf(wrapper_fn, \"#!/bin/sh\\n\\\ncurr_dir=`pwd`\\n\\\ndefault_dir=/root/worker\\n\\\nflock /tmp/lockfile /usr/bin/docker pull %s\\n\\\ndocker run --rm -m 1g -v $curr_dir:$default_dir -w $default_dir %s \\\"$@\\\"\\n\", container_image, container_image);\n\n\t} else {\n\n\t\tfprintf(wrapper_fn, \"#!/bin/sh\\n\\\ncurr_dir=`pwd`\\n\\\ndefault_dir=/root/worker\\n\\\nflock /tmp/lockfile /usr/bin/docker load < %s\\n\\\ndocker run --rm -m 1g -v $curr_dir:$default_dir -w $default_dir %s \\\"$@\\\"\\n\", image_tar, container_image);\n\n\t\tmakeflow_wrapper_add_input_file(image_tar);\n\t}\n\n\tfclose(wrapper_fn);\n\n\tchmod(CONTAINER_SH, 0755);\n\n\tmakeflow_wrapper_add_input_file(CONTAINER_SH);\n}\n\n/*\nSubmit a node to the appropriate batch system, after materializing\nthe necessary list of input and output files, and applying all\nwrappers and options.\n*/\n\nstatic void makeflow_node_submit(struct dag *d, struct dag_node *n)\n{\n\tstruct batch_queue *queue;\n\n\tif(n->local_job && local_queue) {\n\t\tqueue = local_queue;\n\t} else {\n\t\tqueue = remote_queue;\n\t}\n\n\t/* Create strings for all the files mentioned by this node. */\n\tchar *input_files = makeflow_file_list_format(n,0,n->source_files,queue);\n\tchar *output_files = makeflow_file_list_format(n,0,n->target_files,queue);\n\n\t/* Add the wrapper input and output files to the strings. */\n\t/* This function may realloc input_files and output_files. */\n\tinput_files = makeflow_file_list_format(n,input_files,wrapper_input_files,queue);\n\toutput_files = makeflow_file_list_format(n,output_files,wrapper_output_files,queue);\n\t/* Apply the wrapper(s) to the command, if it is (they are) enabled. */\n\tchar *command = string_wrap_command(n->command, wrapper_command);\n\n\t/* Wrap the command with the resource monitor, if it is enabled. */\n\tif(monitor_mode) {\n\t\tchar *newcommand = makeflow_node_rmonitor_wrap_command(n,command);\n\t\tfree(command);\n\t\tcommand = newcommand;\n\t}\n\n\t/* Before setting the batch job options (stored in the \"BATCH_OPTIONS\"\n\t * variable), we must save the previous global queue value, and then\n\t * restore it after we submit. */\n\tstruct dag_variable_lookup_set s = { d, n->category, n, NULL };\n\tchar *batch_options_env    = dag_variable_lookup_string(\"BATCH_OPTIONS\", &s);\n\tchar *batch_submit_options = dag_node_resources_wrap_options(n, batch_options_env, batch_queue_get_type(queue));\n\tchar *old_batch_submit_options = NULL;\n\n\tfree(batch_options_env);\n\tif(batch_submit_options) {\n\t\tdebug(D_MAKEFLOW_RUN, \"Batch options: %s\\n\", batch_submit_options);\n\t\tif(batch_queue_get_option(queue, \"batch-options\"))\n\t\t\told_batch_submit_options = xxstrdup(batch_queue_get_option(queue, \"batch-options\"));\n\t\tbatch_queue_set_option(queue, \"batch-options\", batch_submit_options);\n\t\tfree(batch_submit_options);\n\t}\n\n\t/* Generate the environment vars specific to this node. */\n\tstruct nvpair *envlist = dag_node_env_create(d,n);\n\n\t/*\n\tJust before execution, replace double-percents with the nodeid.\n\tThis is used for substituting in the nodeid into a wrapper command or file.\n\t*/\n\tchar *nodeid = string_format(\"%d\",n->nodeid);\n\tcommand = replace_percents(command,nodeid);\n\tinput_files = replace_percents(input_files,nodeid);\n\toutput_files = replace_percents(output_files,nodeid);\n\tfree(nodeid);\n\n\t/* Now submit the actual job, retrying failures as needed. */\n\tn->jobid = makeflow_node_submit_retry(queue,command,input_files,output_files,envlist);\n\n\t/* Restore old batch job options. */\n\tif(old_batch_submit_options) {\n\t\tbatch_queue_set_option(queue, \"batch-options\", old_batch_submit_options);\n\t\tfree(old_batch_submit_options);\n\t}\n\n\t/* Update all of the necessary data structures. */\n\tif(n->jobid >= 0) {\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_RUNNING);\n\t\tif(n->local_job && local_queue) {\n\t\t\titable_insert(d->local_job_table, n->jobid, n);\n\t\t} else {\n\t\t\titable_insert(d->remote_job_table, n->jobid, n);\n\t\t}\n\t} else {\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_FAILED);\n\t\tmakeflow_failed_flag = 1;\n\t}\n\n\tfree(command);\n\tfree(input_files);\n\tfree(output_files);\n\tnvpair_delete(envlist);\n}\n\nstatic int makeflow_node_ready(struct dag *d, struct dag_node *n)\n{\n\tstruct dag_file *f;\n\n\tif(n->state != DAG_NODE_STATE_WAITING)\n\t\treturn 0;\n\n\tif(n->local_job && local_queue) {\n\t\tif(dag_local_jobs_running(d) >= local_jobs_max)\n\t\t\treturn 0;\n\t} else {\n\t\tif(dag_remote_jobs_running(d) >= remote_jobs_max)\n\t\t\treturn 0;\n\t}\n\n\tlist_first_item(n->source_files);\n\twhile((f = list_next_item(n->source_files))) {\n\t\tif(hash_table_lookup(d->completed_files, f->filename)) {\n\t\t\tcontinue;\n\t\t} else {\n\t\t\treturn 0;\n\t\t}\n\t}\n\n\treturn 1;\n}\n\n/*\nFind all jobs ready to be run, then submit them.\n*/\n\nstatic void makeflow_dispatch_ready_jobs(struct dag *d)\n{\n\tstruct dag_node *n;\n\n\tfor(n = d->nodes; n; n = n->next) {\n\n\t\tif(dag_remote_jobs_running(d) >= remote_jobs_max && dag_local_jobs_running(d) >= local_jobs_max)\n\t\t\tbreak;\n\n\t\tif(makeflow_node_ready(d, n)) {\n\t\t\tmakeflow_node_submit(d, n);\n\t\t}\n\t}\n}\n\n/*\nCheck the the indicated file was created and log, error, or retry as appropriate.\n*/\n\nint makeflow_node_check_file_was_created(struct dag_node *n, struct dag_file *f)\n{\n\tstruct stat buf;\n\tint file_created = 0;\n\n\tint64_t start_check = time(0);\n\n\twhile(!file_created) {\n\t\tif(batch_fs_stat(remote_queue, f->filename, &buf) < 0) {\n\t\t\tfprintf(stderr, \"%s did not create file %s\\n\", n->command, f->filename);\n\t\t}\n\t\telse if(output_len_check && buf.st_size <= 0) {\n\t\t\tdebug(D_MAKEFLOW_RUN, \"%s created a file of length %ld\\n\", n->command, (long) buf.st_size);\n\t\t}\n\t\telse {\n\t\t\t/* File was created and has length larger than zero. */\n\t\t\tdebug(D_MAKEFLOW_RUN, \"File %s created by rule %d.\\n\", f->filename, n->nodeid);\n\t\t\tfile_created = 1;\n\t\t\tbreak;\n\t\t}\n\n\t\tif(file_creation_patience_wait_time > 0 && time(0) - start_check < file_creation_patience_wait_time) {\n\t\t\t/* Failed to see the file. Sleep and try again. */\n\t\t\tdebug(D_MAKEFLOW_RUN, \"Checking again for file %s.\\n\", f->filename);\n\t\t\tsleep(1);\n\t\t} else {\n\t\t\t/* Failed was not seen by makeflow in the aloted tries. */\n\t\t\tdebug(D_MAKEFLOW_RUN, \"File %s was not created by rule %d.\\n\", f->filename, n->nodeid);\n\t\t\tfile_created = 0;\n\t\t\tbreak;\n\t\t}\n\t}\n\n\treturn file_created;\n}\n\n/*\nMark the given task as completing, using the batch_job_info completion structure provided by batch_job.\n*/\n\nstatic void makeflow_node_complete(struct dag *d, struct dag_node *n, struct batch_job_info *info)\n{\n\tstruct dag_file *f;\n\tint job_failed = 0;\n\n\tif(n->state != DAG_NODE_STATE_RUNNING)\n\t\treturn;\n\n\tif(info->exited_normally && info->exit_code == 0) {\n\t\tlist_first_item(n->target_files);\n\t\twhile((f = list_next_item(n->target_files))) {\n\t\t\tif(!makeflow_node_check_file_was_created(n, f))\n\t\t\t{\n\t\t\t\tjob_failed = 1;\n\t\t\t}\n\t\t}\n\t} else {\n\t\tif(info->exited_normally) {\n\t\t\tfprintf(stderr, \"%s failed with exit code %d\\n\", n->command, info->exit_code);\n\t\t} else {\n\t\t\tfprintf(stderr, \"%s crashed with signal %d (%s)\\n\", n->command, info->exit_signal, strsignal(info->exit_signal));\n\t\t}\n\t\tjob_failed = 1;\n\t}\n\n\tif(job_failed) {\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_FAILED);\n\t\tif(monitor_mode && info->exit_code == 147)\n\t\t{\n\t\t\tfprintf(stderr, \"\\nrule %d failed because it exceeded the resources limits.\\n\", n->nodeid);\n\t\t\tchar *log_name_prefix = monitor_log_name(monitor_log_dir, n->nodeid);\n\t\t\tchar *summary_name = string_format(\"%s.summary\", log_name_prefix);\n\t\t\tstruct rmsummary *s = rmsummary_parse_limits_exceeded(summary_name);\n\n\t\t\tif(s)\n\t\t\t{\n\t\t\t\trmsummary_print(stderr, s, NULL);\n\t\t\t\tfree(s);\n\t\t\t\tfprintf(stderr, \"\\n\");\n\t\t\t}\n\n\t\t\tfree(log_name_prefix);\n\t\t\tfree(summary_name);\n\n\t\t\tmakeflow_failed_flag = 1;\n\t\t}\n\t\telse if(makeflow_retry_flag || info->exit_code == 101) {\n\t\t\tn->failure_count++;\n\t\t\tif(n->failure_count > makeflow_retry_max) {\n\t\t\t\tfprintf(stderr, \"job %s failed too many times.\\n\", n->command);\n\t\t\t\tmakeflow_failed_flag = 1;\n\t\t\t} else {\n\t\t\t\tfprintf(stderr, \"will retry failed job %s\\n\", n->command);\n\t\t\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_WAITING);\n\t\t\t}\n\t\t}\n\t\telse\n\t\t{\n\t\t\tmakeflow_failed_flag = 1;\n\t\t}\n\t} else {\n\t\t/* Record which target files have been generated by this node. */\n\t\tlist_first_item(n->target_files);\n\t\twhile((f = list_next_item(n->target_files))) {\n\t\t\thash_table_insert(d->completed_files, f->filename, f->filename);\n\t\t}\n\n\t\t/* Mark source files that have been used by this node */\n\t\tlist_first_item(n->source_files);\n\t\twhile((f = list_next_item(n->source_files)))\n\t\t\tf->ref_count+= -1;\n\n\t\tset_first_element(d->collect_table);\n\t\twhile((f = set_next_element(d->collect_table))) {\n\t\t\tdebug(D_MAKEFLOW_RUN, \"%s: %d\\n\", f->filename, f->ref_count);\n\t\t}\n\n\t\tmakeflow_log_state_change(d, n, DAG_NODE_STATE_COMPLETE);\n\t}\n}\n\n/*\nCheck the dag for consistency, and emit errors if input dependencies, etc are missing.\n*/\n\nstatic int makeflow_check(struct dag *d)\n{\n\tstruct dag_node *n;\n\tstruct dag_file *f;\n\tint error = 0;\n\n\tdebug(D_MAKEFLOW_RUN, \"checking rules for consistency...\\n\");\n\n\tfor(n = d->nodes; n; n = n->next) {\n\t\tlist_first_item(n->source_files);\n\t\twhile((f = list_next_item(n->source_files))) {\n\t\t\tstruct stat buf;\n\n\t\t\tif(hash_table_lookup(d->completed_files, f->filename)) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif(batch_fs_stat(remote_queue, f->filename, &buf) >= 0) {\n\t\t\t\thash_table_insert(d->completed_files, f->filename, f->filename);\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif(f->created_by) {\n\t\t\t\tcontinue;\n\t\t\t}\n\n\t\t\tif(!error) {\n\t\t\t\tfprintf(stderr, \"makeflow: %s does not exist, and is not created by any rule.\\n\", f->filename);\n\t\t\t}\n\t\t\terror = 1;\n\t\t}\n\t}\n\n\tif(error) {\n\t\treturn 0;\n\t} else {\n\t\treturn 1;\n\t}\n}\n\n/*\nMain loop for running a makeflow: submit jobs, wait for completion, keep going until everything done.\n*/\n\nstatic void makeflow_run( struct dag *d )\n{\n\tstruct dag_node *n;\n\tbatch_job_id_t jobid;\n\tstruct batch_job_info info;\n\n\twhile(!makeflow_abort_flag) {\n\t\tmakeflow_dispatch_ready_jobs(d);\n\n\t\tif(dag_local_jobs_running(d)==0 && dag_remote_jobs_running(d)==0 )\n\t\t\tbreak;\n\n\t\tif(dag_remote_jobs_running(d)) {\n\t\t\tint tmp_timeout = 5;\n\t\t\tjobid = batch_job_wait_timeout(remote_queue, &info, time(0) + tmp_timeout);\n\t\t\tif(jobid > 0) {\n\t\t\t\tprintf(\"job %\"PRIbjid\" completed\\n\",jobid);\n\t\t\t\tdebug(D_MAKEFLOW_RUN, \"Job %\" PRIbjid \" has returned.\\n\", jobid);\n\t\t\t\tn = itable_remove(d->remote_job_table, jobid);\n\t\t\t\tif(n)\n\t\t\t\t\tmakeflow_node_complete(d, n, &info);\n\t\t\t}\n\t\t}\n\n\t\tif(dag_local_jobs_running(d)) {\n\t\t\ttime_t stoptime;\n\t\t\tint tmp_timeout = 5;\n\n\t\t\tif(dag_remote_jobs_running(d)) {\n\t\t\t\tstoptime = time(0);\n\t\t\t} else {\n\t\t\t\tstoptime = time(0) + tmp_timeout;\n\t\t\t}\n\n\t\t\tjobid = batch_job_wait_timeout(local_queue, &info, stoptime);\n\t\t\tif(jobid > 0) {\n\t\t\t\tdebug(D_MAKEFLOW_RUN, \"Job %\" PRIbjid \" has returned.\\n\", jobid);\n\t\t\t\tn = itable_remove(d->local_job_table, jobid);\n\t\t\t\tif(n)\n\t\t\t\t\tmakeflow_node_complete(d, n, &info);\n\t\t\t}\n\t\t}\n\n\t\t/* Rather than try to garbage collect after each time in this\n\t\t * wait loop, perform garbage collection after a proportional\n\t\t * amount of tasks have passed. */\n\t\tmakeflow_gc_barrier--;\n\t\tif(makeflow_gc_method != MAKEFLOW_GC_NONE && makeflow_gc_barrier == 0) {\n\t\t\tmakeflow_gc(d,makeflow_gc_method,makeflow_gc_param);\n\t\t\tmakeflow_gc_barrier = MAX(d->nodeid_counter * makeflow_gc_task_ratio, 1);\n\t\t}\n\t}\n\n\tif(makeflow_abort_flag) {\n\t\tmakeflow_abort_all(d);\n\t} else {\n\t\tif(!makeflow_failed_flag && makeflow_gc_method != MAKEFLOW_GC_NONE) {\n\t\t\tmakeflow_gc(d,MAKEFLOW_GC_FORCE,0);\n\t\t}\n\t}\n}\n\n/*\nSignal handler to catch abort signals.  Note that permissible actions in signal handlers are very limited, so we emit a message to the terminal and update a global variable noticed by makeflow_run.\n*/\n\nstatic void handle_abort(int sig)\n{\n\tstatic int abort_count_to_exit = 5;\n\n\tabort_count_to_exit -= 1;\n\tint fd = open(\"/dev/tty\", O_WRONLY);\n\tif (fd >= 0) {\n\t\tchar buf[256];\n\t\tsnprintf(buf, sizeof(buf), \"Received signal %d, will try to clean up remote resources. Send signal %d more times to force exit.\\n\", sig, abort_count_to_exit);\n\t\twrite(fd, buf, strlen(buf));\n\t\tclose(fd);\n\t}\n\tif (abort_count_to_exit == 1)\n\t\tsignal(sig, SIG_DFL);\n\tmakeflow_abort_flag = 1;\n}\n\n\n\n\n\n\nstatic void show_help_run(const char *cmd)\n{\n\tprintf(\"Use: %s [options] <dagfile>\\n\", cmd);\n\tprintf(\"Frequently used options:\\n\\n\");\n\tprintf(\" %-30s Clean up: remove logfile and all targets.\\n\", \"-c,--clean\");\n\tprintf(\" %-30s Batch system type: (default is local)\\n\", \"-T,--batch-type=<type>\");\n\tprintf(\" %-30s %s\\n\\n\", \"\", batch_queue_type_string());\n\tprintf(\"Other options are:\\n\");\n\tprintf(\" %-30s Advertise the master information to a catalog server.\\n\", \"-a,--advertise\");\n\tprintf(\" %-30s Disable the check for AFS. (experts only.)\\n\", \"-A,--disable-afs-check\");\n\tprintf(\" %-30s Add these options to all batch submit files.\\n\", \"-B,--batch-options=<options>\");\n\tprintf(\" %-30s Set catalog server to <catalog>. Format: HOSTNAME:PORT \\n\", \"-C,--catalog-server=<catalog>\");\n\tprintf(\" %-30s Enable debugging for this subsystem\\n\", \"-d,--debug=<subsystem>\");\n\tprintf(\" %-30s Write summary of workflow to this file upon success or failure.\\n\", \"-f,--summary-log=<file>\");\n\tprintf(\" %-30s Work Queue fast abort multiplier.           (default is deactivated)\\n\", \"-F,--wq-fast-abort=<#>\");\n\tprintf(\" %-30s Show this help screen.\\n\", \"-h,--help\");\n\tprintf(\" %-30s Max number of local jobs to run at once.    (default is # of cores)\\n\", \"-j,--max-local=<#>\");\n\tprintf(\" %-30s Max number of remote jobs to run at once.\\n\", \"-J,--max-remote=<#>\");\n\tprintf(\"                                                            (default %d for -Twq, %d otherwise.)\\n\", 10*MAX_REMOTE_JOBS_DEFAULT, MAX_REMOTE_JOBS_DEFAULT );\n\tprintf(\" %-30s Use this file for the makeflow log.         (default is X.makeflowlog)\\n\", \"-l,--makeflow-log=<logfile>\");\n\tprintf(\" %-30s Use this file for the batch system log.     (default is X.<type>log)\\n\", \"-L,--batch-log=<logfile>\");\n\tprintf(\" %-30s Send summary of workflow to this email address upon success or failure.\\n\", \"-m,--email=<email>\");\n\tprintf(\" %-30s Set the project name to <project>\\n\", \"-N,--project-name=<project>\");\n\tprintf(\" %-30s Send debugging to this file. (can also be :stderr, :stdout, :syslog, or :journal)\\n\", \"-o,--debug-file=<file>\");\n\tprintf(\" %-30s Rotate debug file once it reaches this size.\\n\", \"   --debug-rotate-max=<bytes>\");\n\tprintf(\" %-30s Password file for authenticating workers.\\n\", \"   --password\");\n\tprintf(\" %-30s Port number to use with Work Queue.       (default is %d, 0=arbitrary)\\n\", \"-p,--port=<port>\", WORK_QUEUE_DEFAULT_PORT);\n\tprintf(\" %-30s Priority. Higher the value, higher the priority.\\n\", \"-P,--priority=<integer>\");\n\tprintf(\" %-30s Automatically retry failed batch jobs up to %d times.\\n\", \"-R,--retry\", makeflow_retry_max);\n\tprintf(\" %-30s Automatically retry failed batch jobs up to n times.\\n\", \"-r,--retry-count=<n>\");\n\tprintf(\" %-30s Wait for output files to be created upto n seconds (e.g., to deal with NFS semantics).\\n\", \"   --wait-for-files-upto=<n>\");\n\tprintf(\" %-30s Time to retry failed batch job submission.  (default is %ds)\\n\", \"-S,--submission-timeout=<#>\", makeflow_submit_timeout);\n\tprintf(\" %-30s Work Queue keepalive timeout.               (default is %ds)\\n\", \"-t,--wq-keepalive-timeout=<#>\", WORK_QUEUE_DEFAULT_KEEPALIVE_TIMEOUT);\n\tprintf(\" %-30s Work Queue keepalive interval.              (default is %ds)\\n\", \"-u,--wq-keepalive-interval=<#>\", WORK_QUEUE_DEFAULT_KEEPALIVE_INTERVAL);\n\tprintf(\" %-30s Show version string\\n\", \"-v,--version\");\n\tprintf(\" %-30s Work Queue scheduling algorithm.            (time|files|fcfs)\\n\", \"-W,--wq-schedule=<mode>\");\n\tprintf(\" %-30s Working directory for the batch system.\\n\", \"   --working-dir=<dir|url>\");\n\tprintf(\" %-30s Wrap all commands with this prefix.\\n\", \"   --wrapper=<cmd>\");\n\tprintf(\" %-30s Wrapper command requires this input file.\\n\", \"   --wrapper-input=<cmd>\");\n\tprintf(\" %-30s Wrapper command produces this output file.\\n\", \"   --wrapper-output=<cmd>\");\n\tprintf(\" %-30s Change directory: chdir to enable executing the Makefile in other directory.\\n\", \"-X,--change-directory\");\n\tprintf(\" %-30s Force failure on zero-length output files \\n\", \"-z,--zero-length-error\");\n\tprintf(\" %-30s Select port at random and write it to this file.\\n\", \"-Z,--port-file=<file>\");\n\tprintf(\" %-30s Disable Work Queue caching.                 (default is false)\\n\", \"   --disable-wq-cache\");\n\tprintf(\" %-30s Add node id symbol tags in the makeflow log.        (default is false)\\n\", \"   --log-verbose\");\n\tprintf(\" %-30s Run each task with a container based on this docker image.\\n\", \"--docker=<image>\");\n\tprintf(\" %-30s Load docker image from the tar file.\\n\", \"--docker-tar=<tar file>\");\n\n\tprintf(\"\\n*Monitor Options:\\n\\n\");\n\tprintf(\" %-30s Enable the resource monitor, and write the monitor logs to <dir>.\\n\", \"-M,--monitor=<dir>\");\n\tprintf(\" %-30s Use <file> as value-pairs for resource limits.\\n\", \"   --monitor-limits=<file>\");\n\tprintf(\" %-30s Set monitor interval to <#> seconds.        (default is 1 second)\\n\", \"   --monitor-interval=<#>\");\n\tprintf(\" %-30s Enable monitor time series.                 (default is disabled)\\n\", \"   --monitor-with-time-series\");\n\tprintf(\" %-30s Enable monitoring of openened files.        (default is disabled)\\n\", \"   --monitor-with-opened-files\");\n\tprintf(\" %-30s Format for monitor logs.                    (default %s)\\n\", \"   --monitor-log-fmt=<fmt>\", DEFAULT_MONITOR_LOG_FORMAT);\n}\n\nint main(int argc, char *argv[])\n{\n\tint c;\n\trandom_init();\n\tdebug_config(argv[0]);\n\n\tcctools_version_debug((long) D_MAKEFLOW_RUN, argv[0]);\n\tconst char *dagfile;\n\tchar *change_dir = NULL;\n\tchar *batchlogfilename = NULL;\n\tconst char *batch_submit_options = getenv(\"BATCH_OPTIONS\");\n\tchar *catalog_host;\n\tint catalog_port;\n\tint clean_mode = 0;\n\tchar *email_summary_to = NULL;\n\tint explicit_remote_jobs_max = 0;\n\tint explicit_local_jobs_max = 0;\n\tchar *logfilename = NULL;\n\tint port_set = 0;\n\ttimestamp_t runtime = 0;\n\tint skip_afs_check = 0;\n\ttimestamp_t time_completed = 0;\n\tconst char *work_queue_keepalive_interval = NULL;\n\tconst char *work_queue_keepalive_timeout = NULL;\n\tconst char *work_queue_master_mode = \"standalone\";\n\tconst char *work_queue_port_file = NULL;\n\tconst char *priority = NULL;\n\tchar *work_queue_password = NULL;\n\tchar *wq_wait_queue_size = 0;\n\tint did_explicit_auth = 0;\n\tchar *chirp_tickets = NULL;\n\tchar *working_dir = NULL;\n\tchar *write_summary_to = NULL;\n\tchar *s;\n\n\ts = getenv(\"MAKEFLOW_BATCH_QUEUE_TYPE\");\n\tif(s) {\n\t\tbatch_queue_type = batch_queue_type_from_string(s);\n\t\tif(batch_queue_type == BATCH_QUEUE_TYPE_UNKNOWN) {\n\t\t\tfprintf(stderr, \"makeflow: unknown batch queue type: %s (from $MAKEFLOW_BATCH_QUEUE_TYPE)\\n\", s);\n\t\t\treturn 1;\n\t\t}\n\t}\n\n\ts = getenv(\"WORK_QUEUE_MASTER_MODE\");\n\tif(s) {\n\t\twork_queue_master_mode = s;\n\t}\n\n\ts = getenv(\"WORK_QUEUE_NAME\");\n\tif(s) {\n\t\tproject = xxstrdup(s);\n\t}\n\ts = getenv(\"WORK_QUEUE_FAST_ABORT_MULTIPLIER\");\n\tif(s) {\n\t\twq_option_fast_abort_multiplier = atof(s);\n\t}\n\n\tenum {\n\t\tLONG_OPT_AUTH = UCHAR_MAX+1,\n\t\tLONG_OPT_DEBUG_ROTATE_MAX,\n\t\tLONG_OPT_DISABLE_BATCH_CACHE,\n\t\tLONG_OPT_DOT_CONDENSE,\n\t\tLONG_OPT_FILE_CREATION_PATIENCE_WAIT_TIME,\n\t\tLONG_OPT_MONITOR,\n\t\tLONG_OPT_MONITOR_INTERVAL,\n\t\tLONG_OPT_MONITOR_LIMITS,\n\t\tLONG_OPT_MONITOR_LOG_NAME,\n\t\tLONG_OPT_MONITOR_OPENED_FILES,\n\t\tLONG_OPT_MONITOR_TIME_SERIES,\n\t\tLONG_OPT_PASSWORD,\n\t\tLONG_OPT_TICKETS,\n\t\tLONG_OPT_VERBOSE_PARSING,\n\t\tLONG_OPT_LOG_VERBOSE_MODE,\n\t\tLONG_OPT_WORKING_DIR,\n\t\tLONG_OPT_WQ_WAIT_FOR_WORKERS,\n\t\tLONG_OPT_WRAPPER,\n\t\tLONG_OPT_WRAPPER_INPUT,\n\t\tLONG_OPT_WRAPPER_OUTPUT,\n\t\tLONG_OPT_DOCKER,\n\t\tLONG_OPT_DOCKER_TAR\n\t};\n\n\tstatic const struct option long_options_run[] = {\n\t\t{\"advertise\", no_argument, 0, 'a'},\n\t\t{\"auth\", required_argument, 0, LONG_OPT_AUTH},\n\t\t{\"batch-log\", required_argument, 0, 'L'},\n\t\t{\"batch-options\", required_argument, 0, 'B'},\n\t\t{\"batch-type\", required_argument, 0, 'T'},\n\t\t{\"catalog-server\", required_argument, 0, 'C'},\n\t\t{\"clean\", no_argument, 0, 'c'},\n\t\t{\"debug\", required_argument, 0, 'd'},\n\t\t{\"debug-file\", required_argument, 0, 'o'},\n\t\t{\"debug-rotate-max\", required_argument, 0, LONG_OPT_DEBUG_ROTATE_MAX},\n\t\t{\"disable-afs-check\", no_argument, 0, 'A'},\n\t\t{\"disable-cache\", no_argument, 0, LONG_OPT_DISABLE_BATCH_CACHE},\n\t\t{\"email\", required_argument, 0, 'm'},\n\t\t{\"wait-for-files-upto\", required_argument, 0, LONG_OPT_FILE_CREATION_PATIENCE_WAIT_TIME},\n\t\t{\"help\", no_argument, 0, 'h'},\n\t\t{\"makeflow-log\", required_argument, 0, 'l'},\n\t\t{\"max-local\", required_argument, 0, 'j'},\n\t\t{\"max-remote\", required_argument, 0, 'J'},\n\t\t{\"monitor\", required_argument, 0, LONG_OPT_MONITOR},\n\t\t{\"monitor-interval\", required_argument, 0, LONG_OPT_MONITOR_INTERVAL},\n\t\t{\"monitor-limits\", required_argument,   0, LONG_OPT_MONITOR_LIMITS},\n\t\t{\"monitor-log-name\", required_argument, 0, LONG_OPT_MONITOR_LOG_NAME},\n\t\t{\"monitor-with-opened-files\", no_argument, 0, LONG_OPT_MONITOR_OPENED_FILES},\n\t\t{\"monitor-with-time-series\",  no_argument, 0, LONG_OPT_MONITOR_TIME_SERIES},\n\t\t{\"password\", required_argument, 0, LONG_OPT_PASSWORD},\n\t\t{\"port\", required_argument, 0, 'p'},\n\t\t{\"port-file\", required_argument, 0, 'Z'},\n\t\t{\"priority\", required_argument, 0, 'P'},\n\t\t{\"project-name\", required_argument, 0, 'N'},\n\t\t{\"retry\", no_argument, 0, 'R'},\n\t\t{\"retry-count\", required_argument, 0, 'r'},\n\t\t{\"show-output\", no_argument, 0, 'O'},\n\t\t{\"submission-timeout\", required_argument, 0, 'S'},\n\t\t{\"summary-log\", required_argument, 0, 'f'},\n\t\t{\"tickets\", required_argument, 0, LONG_OPT_TICKETS},\n\t\t{\"version\", no_argument, 0, 'v'},\n\t\t{\"log-verbose\", no_argument, 0, LONG_OPT_LOG_VERBOSE_MODE},\n\t\t{\"working-dir\", required_argument, 0, LONG_OPT_WORKING_DIR},\n\t\t{\"wq-estimate-capacity\", no_argument, 0, 'E'},\n\t\t{\"wq-fast-abort\", required_argument, 0, 'F'},\n\t\t{\"wq-keepalive-interval\", required_argument, 0, 'u'},\n\t\t{\"wq-keepalive-timeout\", required_argument, 0, 't'},\n\t\t{\"wq-schedule\", required_argument, 0, 'W'},\n\t\t{\"wq-wait-queue-size\", required_argument, 0, LONG_OPT_WQ_WAIT_FOR_WORKERS},\n\t\t{\"wrapper\", required_argument, 0, LONG_OPT_WRAPPER},\n\t\t{\"wrapper-input\", required_argument, 0, LONG_OPT_WRAPPER_INPUT},\n\t\t{\"wrapper-output\", required_argument, 0, LONG_OPT_WRAPPER_OUTPUT},\n\t\t{\"zero-length-error\", no_argument, 0, 'z'},\n\t\t{\"change-directory\", required_argument, 0, 'X'},\n\t\t{\"docker\", required_argument, 0, LONG_OPT_DOCKER},\n\t\t{\"docker-tar\", required_argument, 0, LONG_OPT_DOCKER_TAR},\n\t\t{0, 0, 0, 0}\n\t};\n\n\tstatic const char option_string_run[] = \"aAB:cC:d:Ef:F:g:G:hj:J:l:L:m:M:N:o:Op:P:r:RS:t:T:u:vW:X:zZ:\";\n\n\twhile((c = getopt_long(argc, argv, option_string_run, long_options_run, NULL)) >= 0) {\n\t\tswitch (c) {\n\t\t\tcase 'a':\n\t\t\t\twork_queue_master_mode = \"catalog\";\n\t\t\t\tbreak;\n\t\t\tcase 'A':\n\t\t\t\tskip_afs_check = 1;\n\t\t\t\tbreak;\n\t\t\tcase 'B':\n\t\t\t\tbatch_submit_options = optarg;\n\t\t\t\tbreak;\n\t\t\tcase 'c':\n\t\t\t\tclean_mode = 1;\n\t\t\t\tbreak;\n\t\t\tcase 'C':\n\t\t\t\tif(!work_queue_catalog_parse(optarg, &catalog_host, &catalog_port)) {\n\t\t\t\t\tfprintf(stderr, \"makeflow: catalog server should be given as HOSTNAME:PORT'.\\n\");\n\t\t\t\t\texit(1);\n\t\t\t\t}\n\t\t\t\tsetenv(\"CATALOG_HOST\", catalog_host, 1);\n\n\t\t\t\tchar *value = string_format(\"%d\", catalog_port);\n\t\t\t\tsetenv(\"CATALOG_PORT\", value, 1);\n\t\t\t\tfree(value);\n\t\t\t\tbreak;\n\t\t\tcase 'd':\n\t\t\t\tdebug_flags_set(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'E':\n\t\t\t\t// This option is deprecated. Capacity estimation is now on by default.\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_AUTH:\n\t\t\t\tif (!auth_register_byname(optarg))\n\t\t\t\t\tfatal(\"could not register authentication method `%s': %s\", optarg, strerror(errno));\n\t\t\t\tdid_explicit_auth = 1;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_TICKETS:\n\t\t\t\tchirp_tickets = strdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'f':\n\t\t\t\twrite_summary_to = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'F':\n\t\t\t\twq_option_fast_abort_multiplier = atof(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'g':\n\t\t\t\tif(strcasecmp(optarg, \"none\") == 0) {\n\t\t\t\t\tmakeflow_gc_method = MAKEFLOW_GC_NONE;\n\t\t\t\t} else if(strcasecmp(optarg, \"ref_count\") == 0) {\n\t\t\t\t\tmakeflow_gc_method = MAKEFLOW_GC_REF_COUNT;\n\t\t\t\t\tif(makeflow_gc_param < 0)\n\t\t\t\t\t\tmakeflow_gc_param = 16;\t/* Try to collect at most 16 files. */\n\t\t\t\t} else if(strcasecmp(optarg, \"on_demand\") == 0) {\n\t\t\t\t\tmakeflow_gc_method = MAKEFLOW_GC_ON_DEMAND;\n\t\t\t\t\tif(makeflow_gc_param < 0)\n\t\t\t\t\t\tmakeflow_gc_param = 1 << 14;\t/* Inode threshold of 2^14. */\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"makeflow: invalid garbage collection method: %s\\n\", optarg);\n\t\t\t\t\texit(1);\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 'G':\n\t\t\t\tmakeflow_gc_param = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_FILE_CREATION_PATIENCE_WAIT_TIME:\n\t\t\t\tfile_creation_patience_wait_time = MAX(0,atoi(optarg));\n\t\t\t\tbreak;\n\t\t\tcase 'h':\n\t\t\t\tshow_help_run(argv[0]);\n\t\t\t\treturn 0;\n\t\t\tcase 'j':\n\t\t\t\texplicit_local_jobs_max = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'J':\n\t\t\t\texplicit_remote_jobs_max = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'l':\n\t\t\t\tlogfilename = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'L':\n\t\t\t\tbatchlogfilename = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'm':\n\t\t\t\temail_summary_to = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tif(monitor_log_dir)\n\t\t\t\t\tfree(monitor_log_dir);\n\t\t\t\tmonitor_log_dir = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR_LIMITS:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tif(monitor_limits_name)\n\t\t\t\t\tfree(monitor_limits_name);\n\t\t\t\tmonitor_limits_name = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR_INTERVAL:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tmonitor_interval = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR_TIME_SERIES:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tmonitor_enable_time_series = 1;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR_OPENED_FILES:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tmonitor_enable_list_files = 1;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_MONITOR_LOG_NAME:\n\t\t\t\tmonitor_mode = 1;\n\t\t\t\tif(monitor_log_format)\n\t\t\t\t\tfree(monitor_log_format);\n\t\t\t\tmonitor_log_format = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'M':\n\t\t\tcase 'N':\n\t\t\t\tfree(project);\n\t\t\t\tproject = xxstrdup(optarg);\n\t\t\t\twork_queue_master_mode = \"catalog\";\n\t\t\t\tbreak;\n\t\t\tcase 'o':\n\t\t\t\tdebug_config_file(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'p':\n\t\t\t\tport_set = 1;\n\t\t\t\tport = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'P':\n\t\t\t\tpriority = optarg;\n\t\t\t\tbreak;\n\t\t\tcase 'r':\n\t\t\t\tmakeflow_retry_flag = 1;\n\t\t\t\tmakeflow_retry_max = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 'R':\n\t\t\t\tmakeflow_retry_flag = 1;\n\t\t\t\tbreak;\n\t\t\tcase 'S':\n\t\t\t\tmakeflow_submit_timeout = atoi(optarg);\n\t\t\t\tbreak;\n\t\t\tcase 't':\n\t\t\t\twork_queue_keepalive_timeout = optarg;\n\t\t\t\tbreak;\n\t\t\tcase 'T':\n\t\t\t\tbatch_queue_type = batch_queue_type_from_string(optarg);\n\t\t\t\tif(batch_queue_type == BATCH_QUEUE_TYPE_UNKNOWN) {\n\t\t\t\t\tfprintf(stderr, \"makeflow: unknown batch queue type: %s\\n\", optarg);\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 'u':\n\t\t\t\twork_queue_keepalive_interval = optarg;\n\t\t\t\tbreak;\n\t\t\tcase 'v':\n\t\t\t\tcctools_version_print(stdout, argv[0]);\n\t\t\t\treturn 0;\n\t\t\tcase 'W':\n\t\t\t\tif(!strcmp(optarg, \"files\")) {\n\t\t\t\t\twq_option_scheduler = WORK_QUEUE_SCHEDULE_FILES;\n\t\t\t\t} else if(!strcmp(optarg, \"time\")) {\n\t\t\t\t\twq_option_scheduler = WORK_QUEUE_SCHEDULE_TIME;\n\t\t\t\t} else if(!strcmp(optarg, \"fcfs\")) {\n\t\t\t\t\twq_option_scheduler = WORK_QUEUE_SCHEDULE_FCFS;\n\t\t\t\t} else {\n\t\t\t\t\tfprintf(stderr, \"makeflow: unknown scheduling mode %s\\n\", optarg);\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase 'z':\n\t\t\t\toutput_len_check = 1;\n\t\t\t\tbreak;\n\t\t\tcase 'Z':\n\t\t\t\twork_queue_port_file = optarg;\n\t\t\t\tport = 0;\n\t\t\t\tport_set = 1;\t//WQ is going to set the port, so we continue as if already set.\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_PASSWORD:\n\t\t\t\tif(copy_file_to_buffer(optarg, &work_queue_password, NULL) < 0) {\n\t\t\t\t\tfprintf(stderr, \"makeflow: couldn't open %s: %s\\n\", optarg, strerror(errno));\n\t\t\t\t\treturn 1;\n\t\t\t\t}\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_DISABLE_BATCH_CACHE:\n\t\t\t\tcache_mode = 0;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_WQ_WAIT_FOR_WORKERS:\n\t\t\t\twq_wait_queue_size = optarg;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_WORKING_DIR:\n\t\t\t\tfree(working_dir);\n\t\t\t\tworking_dir = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_DEBUG_ROTATE_MAX:\n\t\t\t\tdebug_config_file_size(string_metric_parse(optarg));\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_LOG_VERBOSE_MODE:\n\t\t\t\tlog_verbose_mode = 1;\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_WRAPPER:\n\t\t\t\tmakeflow_wrapper_add_command(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_WRAPPER_INPUT:\n\t\t\t\tmakeflow_wrapper_add_input_file(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_WRAPPER_OUTPUT:\n\t\t\t\tmakeflow_wrapper_add_output_file(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_DOCKER:\n\t\t\t\tcontainer_mode = CONTAINER_MODE_DOCKER;\n\t\t\t\tcontainer_image = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tcase LONG_OPT_DOCKER_TAR:\n\t\t\t\timage_tar = xxstrdup(optarg);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tshow_help_run(argv[0]);\n\t\t\t\treturn 1;\n\t\t\tcase 'X':\n\t\t\t\tchange_dir = optarg;\n\t\t\t\tbreak;\n\t\t}\n\t}\n\n\tif(!did_explicit_auth)\n\t\tauth_register_all();\n\tif(chirp_tickets) {\n\t\tauth_ticket_load(chirp_tickets);\n\t\tfree(chirp_tickets);\n\t} else {\n\t\tauth_ticket_load(NULL);\n\t}\n\n\tif((argc - optind) != 1) {\n\t\tint rv = access(\"./Makeflow\", R_OK);\n\t\tif(rv < 0) {\n\t\t\tfprintf(stderr, \"makeflow: No makeflow specified and file \\\"./Makeflow\\\" could not be found.\\n\");\n\t\t\tfprintf(stderr, \"makeflow: Run \\\"%s -h\\\" for help with options.\\n\", argv[0]);\n\t\t\treturn 1;\n\t\t}\n\n\t\tdagfile = \"./Makeflow\";\n\t} else {\n\t\tdagfile = argv[optind];\n\t}\n\n\tif(batch_queue_type == BATCH_QUEUE_TYPE_WORK_QUEUE) {\n\t\tif(strcmp(work_queue_master_mode, \"catalog\") == 0 && project == NULL) {\n\t\t\tfprintf(stderr, \"makeflow: Makeflow running in catalog mode. Please use '-N' option to specify the name of this project.\\n\");\n\t\t\tfprintf(stderr, \"makeflow: Run \\\"makeflow -h\\\" for help with options.\\n\");\n\t\t\treturn 1;\n\t\t}\n\t\t// Use Work Queue default port in standalone mode when port is not\n\t\t// specified with -p option. In Work Queue catalog mode, Work Queue\n\t\t// would choose an arbitrary port when port is not explicitly specified.\n\t\tif(!port_set && strcmp(work_queue_master_mode, \"standalone\") == 0) {\n\t\t\tport_set = 1;\n\t\t\tport = WORK_QUEUE_DEFAULT_PORT;\n\t\t}\n\n\t\tif(port_set) {\n\t\t\tchar *value;\n\t\t\tvalue = string_format(\"%d\", port);\n\t\t\tsetenv(\"WORK_QUEUE_PORT\", value, 1);\n\t\t\tfree(value);\n\t\t}\n\t}\n\n\tif(!logfilename)\n\t\tlogfilename = string_format(\"%s.makeflowlog\", dagfile);\n\n\tif(!batchlogfilename) {\n\t\tswitch (batch_queue_type) {\n\t\t\tcase BATCH_QUEUE_TYPE_CONDOR:\n\t\t\t\tbatchlogfilename = string_format(\"%s.condorlog\", dagfile);\n\t\t\t\tbreak;\n\t\t\tcase BATCH_QUEUE_TYPE_WORK_QUEUE:\n\t\t\t\tbatchlogfilename = string_format(\"%s.wqlog\", dagfile);\n\t\t\t\tbreak;\n\t\t\tdefault:\n\t\t\t\tbatchlogfilename = string_format(\"%s.batchlog\", dagfile);\n\t\t\t\tbreak;\n\t\t}\n\n\t\t// In clean mode, delete all existing log files\n\t\tif(clean_mode) {\n\t\t\tBUFFER_STACK_ABORT(B, PATH_MAX);\n\t\t\tbuffer_putfstring(B, \"%s.condorlog\", dagfile);\n\t\t\tunlink(buffer_tostring(B));\n\t\t\tbuffer_rewind(B, 0);\n\t\t\tbuffer_putfstring(B, \"%s.wqlog\", dagfile);\n\t\t\tunlink(buffer_tostring(B));\n\t\t\tbuffer_rewind(B, 0);\n\t\t\tbuffer_putfstring(B, \"%s.batchlog\", dagfile);\n\t\t\tunlink(buffer_tostring(B));\n\t\t}\n\t}\n\n\tif(monitor_mode) {\n\t\tif(!monitor_log_dir)\n\t\t\tfatal(\"Monitor mode was enabled, but a log output directory was not specified (use -M<dir>)\");\n\n\t\tmonitor_exe = resource_monitor_copy_to_wd(NULL);\n\n\t\tif(monitor_interval < 1)\n\t\t\tfatal(\"Monitoring interval should be non-negative.\");\n\n\t\tif(!monitor_log_format)\n\t\t\tmonitor_log_format = DEFAULT_MONITOR_LOG_FORMAT;\n\t}\n\n\tprintf(\"parsing %s...\\n\",dagfile);\n\tstruct dag *d = dag_from_file(dagfile);\n\tif(!d) {\n\t\tfatal(\"makeflow: couldn't load %s: %s\\n\", dagfile, strerror(errno));\n\t}\n\n\t// Makeflows running LOCAL batch type have only one queue that behaves as if remote\n\t// This forces -J vs -j to behave correctly\n\tif(batch_queue_type == BATCH_QUEUE_TYPE_LOCAL) {\n\t\texplicit_remote_jobs_max = explicit_local_jobs_max;\n\t}\n\n\tif(explicit_local_jobs_max) {\n\t\tlocal_jobs_max = explicit_local_jobs_max;\n\t} else {\n\t\tlocal_jobs_max = load_average_get_cpus();\n\t}\n\n\tif(explicit_remote_jobs_max) {\n\t\tremote_jobs_max = explicit_remote_jobs_max;\n\t} else {\n\t\tif(batch_queue_type == BATCH_QUEUE_TYPE_LOCAL) {\n\t\t\tremote_jobs_max = load_average_get_cpus();\n\t\t} else if(batch_queue_type == BATCH_QUEUE_TYPE_WORK_QUEUE) {\n\t\t\tremote_jobs_max = 10 * MAX_REMOTE_JOBS_DEFAULT;\n\t\t} else {\n\t\t\tremote_jobs_max = MAX_REMOTE_JOBS_DEFAULT;\n\t\t}\n\t}\n\n\ts = getenv(\"MAKEFLOW_MAX_REMOTE_JOBS\");\n\tif(s) {\n\t\tremote_jobs_max = MIN(remote_jobs_max, atoi(s));\n\t}\n\n\ts = getenv(\"MAKEFLOW_MAX_LOCAL_JOBS\");\n\tif(s) {\n\t\tint n = atoi(s);\n\t\tlocal_jobs_max = MIN(local_jobs_max, n);\n\t\tif(batch_queue_type == BATCH_QUEUE_TYPE_LOCAL) {\n\t\t\tremote_jobs_max = MIN(local_jobs_max, n);\n\t\t}\n\t}\n\n\tif(monitor_mode && !makeflow_prepare_for_monitoring(d)) {\n\t\tfatal(\"Could not prepare for monitoring.\\n\");\n\t}\n\n\tremote_queue = batch_queue_create(batch_queue_type);\n\tif(!remote_queue) {\n\t\tfprintf(stderr, \"makeflow: couldn't create batch queue.\\n\");\n\t\tif(port != 0)\n\t\t\tfprintf(stderr, \"makeflow: perhaps port %d is already in use?\\n\", port);\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tbatch_queue_set_logfile(remote_queue, batchlogfilename);\n\tbatch_queue_set_option(remote_queue, \"batch-options\", batch_submit_options);\n\tbatch_queue_set_option(remote_queue, \"skip-afs-check\", skip_afs_check ? \"yes\" : \"no\");\n\tbatch_queue_set_option(remote_queue, \"password\", work_queue_password);\n\tbatch_queue_set_option(remote_queue, \"master-mode\", work_queue_master_mode);\n\tbatch_queue_set_option(remote_queue, \"name\", project);\n\tbatch_queue_set_option(remote_queue, \"priority\", priority);\n\tbatch_queue_set_option(remote_queue, \"estimate-capacity\", \"yes\"); // capacity estimation is on by default\n\tbatch_queue_set_option(remote_queue, \"keepalive-interval\", work_queue_keepalive_interval);\n\tbatch_queue_set_option(remote_queue, \"keepalive-timeout\", work_queue_keepalive_timeout);\n\tbatch_queue_set_option(remote_queue, \"caching\", cache_mode ? \"yes\" : \"no\");\n\tbatch_queue_set_option(remote_queue, \"wait-queue-size\", wq_wait_queue_size);\n\tbatch_queue_set_option(remote_queue, \"working-dir\", working_dir);\n\n\t/* Do not create a local queue for systems where local and remote are the same. */\n\n\tif(batch_queue_type == BATCH_QUEUE_TYPE_CHIRP ||\n\t   batch_queue_type == BATCH_QUEUE_TYPE_HADOOP ||\n\t   batch_queue_type == BATCH_QUEUE_TYPE_LOCAL) {\n\t\tlocal_queue = 0;\n\t} else {\n\t\tlocal_queue = batch_queue_create(BATCH_QUEUE_TYPE_LOCAL);\n\t\tif(!local_queue) {\n\t\t\tfatal(\"couldn't create local job queue.\");\n\t\t}\n\t}\n\n\t/* Remote storage modes do not (yet) support measuring storage for garbage collection. */\n\n\tif(batch_queue_type==BATCH_QUEUE_TYPE_CHIRP || batch_queue_type==BATCH_QUEUE_TYPE_HADOOP) {\n\t\tif(makeflow_gc_method == MAKEFLOW_GC_ON_DEMAND) {\n\t\t\tmakeflow_gc_method = MAKEFLOW_GC_REF_COUNT;\n\t\t}\n\t}\n\n\tif(makeflow_gc_method != MAKEFLOW_GC_NONE)\n\t\tmakeflow_gc_prepare(d);\n\n\tmakeflow_prepare_nested_jobs(d);\n\n\tif (change_dir)\n\t\tchdir(change_dir);\n\n\tif(clean_mode) {\n\t\tprintf(\"cleaning filesystem...\\n\");\n\t\tmakeflow_clean(d);\n\t\tunlink(logfilename);\n\t\tunlink(batchlogfilename);\n\t\texit(0);\n\t}\n\n\tprintf(\"checking %s for consistency...\\n\",dagfile);\n\tif(!makeflow_check(d)) {\n\t\texit(EXIT_FAILURE);\n\t}\n\n\tprintf(\"%s has %d rules.\\n\",dagfile,d->nodeid_counter);\n\n\tsetlinebuf(stdout);\n\tsetlinebuf(stderr);\n\n\tmakeflow_log_recover(d, logfilename, log_verbose_mode );\n\n\tprintf(\"starting workflow....\\n\");\n\n\tport = batch_queue_port(remote_queue);\n\tif(work_queue_port_file)\n\t\topts_write_port_file(work_queue_port_file, port);\n\tif(port > 0)\n\t\tprintf(\"listening for workers on port %d.\\n\", port);\n\n\tsignal(SIGINT, handle_abort);\n\tsignal(SIGQUIT, handle_abort);\n\tsignal(SIGTERM, handle_abort);\n\n\tmakeflow_log_started_event(d);\n\n\truntime = timestamp_get();\n\n\tif (container_mode == CONTAINER_MODE_DOCKER) {\n\n\t/* 1) create a global script for running docker container\n\t * 2) add this script to the global wrapper list\n\t * 3) reformat each task command\n\t */\n\n\t\tmakeflow_create_docker_sh();\n\t\tchar *global_cmd = string_format(\"sh %s\", CONTAINER_SH);\n\t\tmakeflow_wrapper_add_command(global_cmd);\n\t}\n\n\tmakeflow_run(d);\n\ttime_completed = timestamp_get();\n\truntime = time_completed - runtime;\n\n\tif(local_queue)\n\t\tbatch_queue_delete(local_queue);\n\tbatch_queue_delete(remote_queue);\n\n\tif(write_summary_to || email_summary_to)\n\t\tmakeflow_summary_create(d, write_summary_to, email_summary_to, runtime, time_completed, argc, argv, dagfile, remote_queue, makeflow_abort_flag, makeflow_failed_flag );\n\n\t/* XXX better to write created files to log, then delete those listed in log. */\n\n\tif (container_mode == CONTAINER_MODE_DOCKER) {\n\t\tchar *cmd = string_format(\"rm %s\", CONTAINER_SH);\n\t\tsystem(cmd);\n\t\tfree(cmd);\n\t}\n\n\tif(makeflow_abort_flag) {\n\t\tmakeflow_log_aborted_event(d);\n\t\tfprintf(stderr, \"workflow was aborted.\\n\");\n\t\texit(EXIT_FAILURE);\n\t} else if(makeflow_failed_flag) {\n\t\tmakeflow_log_failed_event(d);\n\t\tfprintf(stderr, \"workflow failed.\\n\");\n\t\texit(EXIT_FAILURE);\n\t} else {\n\t\tmakeflow_log_completed_event(d);\n\t\tprintf(\"nothing left to do.\\n\");\n\t\texit(EXIT_SUCCESS);\n\t}\n\n\treturn 0;\n}\n\n/* vim: set noexpandtab tabstop=4: */\n", "idx": 16, "id": 12047, "msg": "", "proj": "cooperative-computing-lab-cctools", "lang": "c"}
{"patch": "@@ -2108,10 +2108,10 @@ public class EditDatafilesPage implements java.io.Serializable {\n     \t\t\t\t// -----------------------------------------------------------\n     \t\t\t\tuploadWarningMessage = processUploadedFileList(datafiles);\n     \t\t\t}\n-    \t\t\tif(!uploadInProgress) {\n+    \t\t\tif(uploadInProgress.isFalse()) {\n     \t\t\t\tlogger.warning(\"Upload in progress cancelled\");\n     \t\t\t\tfor (DataFile newFile : datafiles) {\n-    \t\t\t\t\tdeleteTempFile(newFile);\n+    \t\t\t\t\tFileUtil.deleteTempFile(newFile, dataset, ingestService);\n     \t\t\t\t}\n     \t\t\t}\n     \t\t}", "y": 0, "oldf": "package edu.harvard.iq.dataverse;\n\nimport edu.harvard.iq.dataverse.provenance.ProvPopupFragmentBean;\nimport edu.harvard.iq.dataverse.api.AbstractApiBean;\nimport edu.harvard.iq.dataverse.authorization.AuthenticationServiceBean;\nimport edu.harvard.iq.dataverse.authorization.Permission;\nimport edu.harvard.iq.dataverse.authorization.users.AuthenticatedUser;\nimport edu.harvard.iq.dataverse.branding.BrandingUtil;\nimport edu.harvard.iq.dataverse.datasetutility.AddReplaceFileHelper;\nimport edu.harvard.iq.dataverse.datasetutility.FileSizeChecker;\nimport edu.harvard.iq.dataverse.datasetutility.FileReplaceException;\nimport edu.harvard.iq.dataverse.datasetutility.FileReplacePageHelper;\nimport edu.harvard.iq.dataverse.dataaccess.DataAccess;\nimport edu.harvard.iq.dataverse.dataaccess.DataAccessOption;\nimport edu.harvard.iq.dataverse.dataaccess.ImageThumbConverter;\nimport edu.harvard.iq.dataverse.dataaccess.S3AccessIO;\nimport edu.harvard.iq.dataverse.dataaccess.StorageIO;\nimport edu.harvard.iq.dataverse.datacapturemodule.DataCaptureModuleUtil;\nimport edu.harvard.iq.dataverse.datacapturemodule.ScriptRequestResponse;\nimport edu.harvard.iq.dataverse.dataset.DatasetThumbnail;\nimport edu.harvard.iq.dataverse.engine.command.Command;\nimport edu.harvard.iq.dataverse.engine.command.CommandContext;\nimport edu.harvard.iq.dataverse.engine.command.exception.CommandException;\nimport edu.harvard.iq.dataverse.engine.command.exception.IllegalCommandException;\nimport edu.harvard.iq.dataverse.engine.command.impl.RequestRsyncScriptCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.UpdateDatasetThumbnailCommand;\nimport edu.harvard.iq.dataverse.engine.command.impl.UpdateDatasetVersionCommand;\nimport edu.harvard.iq.dataverse.ingest.IngestRequest;\nimport edu.harvard.iq.dataverse.ingest.IngestServiceBean;\nimport edu.harvard.iq.dataverse.ingest.IngestUtil;\nimport edu.harvard.iq.dataverse.search.FileView;\nimport edu.harvard.iq.dataverse.search.IndexServiceBean;\nimport edu.harvard.iq.dataverse.settings.SettingsServiceBean;\nimport edu.harvard.iq.dataverse.util.FileUtil;\nimport edu.harvard.iq.dataverse.util.JsfHelper;\nimport edu.harvard.iq.dataverse.util.SystemConfig;\nimport edu.harvard.iq.dataverse.util.BundleUtil;\nimport edu.harvard.iq.dataverse.util.EjbUtil;\nimport static edu.harvard.iq.dataverse.util.JsfHelper.JH;\nimport static edu.harvard.iq.dataverse.util.StringUtil.isEmpty;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.io.InputStream;\nimport java.io.StringReader;\nimport java.nio.file.Files;\nimport java.nio.file.Path;\nimport java.nio.file.Paths;\nimport java.util.ArrayList;\nimport java.util.HashMap;\nimport java.util.Iterator;\nimport java.util.List;\nimport java.util.Map;\nimport java.util.logging.Logger;\nimport javax.ejb.EJB;\nimport javax.ejb.EJBException;\nimport javax.faces.application.FacesMessage;\nimport javax.faces.context.FacesContext;\nimport javax.faces.event.ActionEvent;\nimport javax.faces.view.ViewScoped;\nimport javax.inject.Inject;\nimport javax.inject.Named;\nimport org.primefaces.event.FileUploadEvent;\nimport org.primefaces.model.file.UploadedFile;\nimport javax.json.Json;\nimport javax.json.JsonObject;\nimport javax.json.JsonArray;\nimport javax.json.JsonReader;\nimport org.apache.commons.httpclient.HttpClient;\nimport org.apache.commons.io.IOUtils;\nimport org.apache.commons.httpclient.methods.GetMethod;\nimport java.text.DateFormat;\nimport java.util.Arrays;\nimport java.util.Set;\nimport java.util.logging.Level;\nimport javax.faces.event.AjaxBehaviorEvent;\nimport javax.faces.event.FacesEvent;\nimport javax.servlet.ServletOutputStream;\nimport javax.servlet.http.HttpServletResponse;\nimport org.apache.commons.lang.StringUtils;\nimport org.primefaces.PrimeFaces;\n\n/**\n *\n * @author Leonid Andreev\n */\n@ViewScoped\n@Named(\"EditDatafilesPage\")\npublic class EditDatafilesPage implements java.io.Serializable {\n\n    private static final Logger logger = Logger.getLogger(EditDatafilesPage.class.getCanonicalName());\n    private boolean uploadWarningMessageIsNotAnError;\n\n    public enum FileEditMode {\n\n        EDIT, UPLOAD, CREATE, SINGLE, SINGLE_REPLACE\n    };\n    \n    @EJB\n    DatasetServiceBean datasetService;\n    @EJB\n    DataverseServiceBean dataverseService;\n    @EJB\n    DatasetVersionServiceBean datasetVersionService;\n    @EJB\n    DataFileServiceBean datafileService;\n    @EJB\n    PermissionServiceBean permissionService;\n    @EJB\n    IngestServiceBean ingestService;\n    @EJB\n    EjbDataverseEngine commandEngine;\n    @Inject\n    DataverseSession session;\n    @EJB\n    UserNotificationServiceBean userNotificationService;\n    @EJB\n    SettingsServiceBean settingsService;\n    @EJB\n    AuthenticationServiceBean authService;\n    @EJB\n    SystemConfig systemConfig;\n    @EJB\n    DataverseLinkingServiceBean dvLinkingService;\n    @EJB\n    IndexServiceBean indexService;\n    @Inject\n    DataverseRequestServiceBean dvRequestService;\n    @Inject PermissionsWrapper permissionsWrapper;\n    @Inject FileDownloadHelper fileDownloadHelper;\n    @Inject ProvPopupFragmentBean provPopupFragmentBean;\n    @Inject\n    SettingsWrapper settingsWrapper;\n\n    private Dataset dataset = new Dataset();\n    \n    private FileReplacePageHelper fileReplacePageHelper;\n\n\n    private String selectedFileIdsString = null; \n    private FileEditMode mode; \n    private List<Long> selectedFileIdsList = new ArrayList<>(); \n    private List<FileMetadata> fileMetadatas = new ArrayList<>();;\n\n    \n    private Long ownerId;\n    private Long versionId;\n    private List<DataFile> newFiles = new ArrayList<>();\n    private List<DataFile> uploadedFiles = new ArrayList<>();\n    private DatasetVersion workingVersion;\n    private DatasetVersion clone;\n    private String dropBoxSelection = \"\";\n    private String displayCitation;\n    private boolean tabularDataTagsUpdated = false; \n    \n    private String persistentId;\n    \n    private String versionString = \"\";\n            \n    \n    private boolean saveEnabled = false; \n\n    // Used to store results of permissions checks\n    private final Map<String, Boolean> datasetPermissionMap = new HashMap<>(); // { Permission human_name : Boolean }\n\n    private Long maxFileUploadSizeInBytes = null;\n    private Integer multipleUploadFilesLimit = null; \n    \n    private final int NUMBER_OF_SCROLL_ROWS = 25;\n    \n    private DataFile singleFile = null;\n\n    public DataFile getSingleFile() {\n        return singleFile;\n    }\n\n    public void setSingleFile(DataFile singleFile) {\n        this.singleFile = singleFile;\n    }\n    \n    public String getSelectedFileIds() {\n        return selectedFileIdsString;\n    }\n    \n    public DataFile getFileToReplace(){\n        if (!this.isFileReplaceOperation()){\n            return null;\n        }\n        if (this.fileReplacePageHelper == null){\n            return null;\n        }\n        return this.fileReplacePageHelper.getFileToReplace();\n    }\n    \n    public void setSelectedFileIds(String selectedFileIds) {\n        selectedFileIdsString = selectedFileIds;\n    }\n    \n    public FileEditMode getMode() {\n        return mode;\n    }\n    \n    public void setMode(FileEditMode mode) {\n        this.mode = mode;\n    }\n    \n    public List<FileMetadata> getFileMetadatas() {\n        \n        // -------------------------------------\n        // Handle a Replace operation\n        //  - The List<FileMetadata> comes from a different source\n        // -------------------------------------\n        if (isFileReplaceOperation()){\n            if (fileReplacePageHelper.wasPhase1Successful()){\n                logger.fine(\"Replace: File metadatas 'list' of 1 from the fileReplacePageHelper.\");\n                return fileReplacePageHelper.getNewFileMetadatasBeforeSave();\n            } else {\n                logger.fine(\"Replace: replacement file not yet uploaded.\");\n                return null;\n            }            \n        }\n        \n        if (fileMetadatas != null) {\n            logger.fine(\"Returning a list of \"+fileMetadatas.size()+\" file metadatas.\");\n        } else {\n            logger.fine(\"File metadatas list hasn't been initialized yet.\");\n        }\n        // [experimental] \n        // this would be a way to hide any already-uploaded files from the page\n        // while a new upload is happening:\n        // (the uploadStarted button on the page needs the update=\"filesTable\"\n        // attribute added for this to work)\n        //if (uploadInProgress) {\n        //    return null; \n        //}\n        \n        return fileMetadatas;\n    }\n  \n    public void setFileMetadatas(List<FileMetadata> fileMetadatas) {\n        this.fileMetadatas = fileMetadatas;\n    }\n    \n    /* \n        The 2 methods below are for setting up the PrimeFaces:dataTabe component\n        used to display the uploaded files, or the files selected for editing. \n    \n        - isScrollable(): \n          this supplies the value of the component attribute \"scrollable\". \n          When we have more than NUMBER_OF_SCROLL_ROWS worth of files (currently\n          set to 25), we will add a scroller to the table, showing NUMBER_OF_SCROLL_ROWS\n          at a time; thus making the page a little bit more useable. \n          When there is fewer rows, however, the attribute needs to be set to \n          \"false\" - because otherwise some (idiosyncratic) amount of white space \n          is added to the bottom of the table, making the page look silly. \n    \n        - getScrollHeightPercentage():\n          this method calculates the *percentage* of the total length of the \n          list of files, such that the resulting table is always NUMBER_OF_SCROLL_ROWS \n          high. This is *the only way* to keep the number of files shown in the \n          table fixed as the size of the list grows! (the \"scrollRows\" attribute\n          of the p:dataTable component only applies when \"liveScroll=true\" is being\n          used). \n    */\n    \n    public boolean isScrollable() {\n        return !(fileMetadatas == null || fileMetadatas.size() <= NUMBER_OF_SCROLL_ROWS + 1);\n    }\n    \n    public String getScrollHeightPercentage() {\n        int perc; \n        if (fileMetadatas == null || fileMetadatas.size() < NUMBER_OF_SCROLL_ROWS) {\n            perc = 100;\n        } else {\n            perc = NUMBER_OF_SCROLL_ROWS * 100 / fileMetadatas.size();\n        }\n        \n        if (perc == 0) {\n            perc = 1;\n        } else if (perc > 100) {\n            perc = 100;\n        }\n        \n        logger.fine(\"scroll height percentage: \"+perc);\n        return perc + \"%\";\n    }\n    \n    /*\n        Any settings, such as the upload size limits, should be saved locally - \n        so that the db doesn't get hit repeatedly. (this setting is initialized \n        in the init() method)\n    \n        This may be \"null\", signifying unlimited download size.\n    */\n    \n    public Long getMaxFileUploadSizeInBytes() {\n        return this.maxFileUploadSizeInBytes;\n    }\n    \n    public String getHumanMaxFileUploadSizeInBytes() {\n        return FileSizeChecker.bytesToHumanReadable(this.maxFileUploadSizeInBytes);\n    }\n    \n    public boolean isUnlimitedUploadFileSize() {\n        \n        return this.maxFileUploadSizeInBytes == null;\n    }\n    \n    /*\n        The number of files the GUI user is allowed to upload in one batch, \n        via drag-and-drop, or through the file select dialog. Now configurable \n        in the Settings table. \n    */\n    public Integer getMaxNumberOfFiles() {\n        return this.multipleUploadFilesLimit;\n    }\n    /**\n     * Check Dataset related permissions\n     * \n     * @param permissionToCheck\n     * @return \n     */\n    public boolean doesSessionUserHaveDataSetPermission(Permission permissionToCheck){\n        if (permissionToCheck == null){\n            return false;\n        }\n               \n        String permName = permissionToCheck.getHumanName();\n       \n        // Has this check already been done? \n        // \n        if (this.datasetPermissionMap.containsKey(permName)){\n            // Yes, return previous answer\n            return this.datasetPermissionMap.get(permName);\n        }\n        \n        // Check the permission\n        //\n        boolean hasPermission = this.permissionService.userOn(this.session.getUser(), this.dataset).has(permissionToCheck);\n\n        // Save the permission\n        this.datasetPermissionMap.put(permName, hasPermission);\n        \n        // return true/false\n        return hasPermission;\n    }\n    \n    public void reset() {\n        // ?\n    }\n\n    public String getGlobalId() {\n        return persistentId;\n    }\n        \n    public String getPersistentId() {\n        return persistentId;\n    }\n\n    public void setPersistentId(String persistentId) {\n        this.persistentId = persistentId;\n    }\n\n    public String getDisplayCitation() {\n        //displayCitation = dataset.getCitation(false, workingVersion);\n        return displayCitation;\n    }\n\n    public void setDisplayCitation(String displayCitation) {\n        this.displayCitation = displayCitation;\n    }\n\n    public String getDropBoxSelection() {\n        return dropBoxSelection;\n    }\n\n    public String getDropBoxKey() {\n        // Site-specific DropBox application registration key is configured \n        // via a JVM option under glassfish.\n        //if (true)return \"some-test-key\";  // for debugging\n\n        String configuredDropBoxKey = System.getProperty(\"dataverse.dropbox.key\");\n        if (configuredDropBoxKey != null) {\n            return configuredDropBoxKey;\n        }\n        return \"\";\n    }\n\n    public void setDropBoxSelection(String dropBoxSelection) {\n        this.dropBoxSelection = dropBoxSelection;\n    }\n\n    public Dataset getDataset() {\n        return dataset;\n    }\n\n    public void setDataset(Dataset dataset) {\n        this.dataset = dataset;\n    }\n\n    public DatasetVersion getWorkingVersion() {\n        return workingVersion;\n    }\n\n    public Long getOwnerId() {\n        return ownerId;\n    }\n\n    public void setOwnerId(Long ownerId) {\n        this.ownerId = ownerId;\n    }\n\n    public Long getVersionId() {\n        return versionId;\n    }\n\n    public void setVersionId(Long versionId) {\n        this.versionId = versionId;\n    }\n\n    public String initCreateMode(String modeToken, DatasetVersion version, List<DataFile> newFilesList, List<FileMetadata> selectedFileMetadatasList) {\n        if (modeToken == null) {\n            logger.fine(\"Request to initialize Edit Files page with null token (aborting).\");\n            return null;\n        }\n        \n        if (!modeToken.equals(\"CREATE\")) {\n            logger.fine(\"Request to initialize Edit Files page with token \" + modeToken + \" (aborting).\");\n            return null; \n        }\n        \n        logger.fine(\"Initializing Edit Files page in CREATE mode;\");\n        \n        if (version == null) {\n            return permissionsWrapper.notFound();\n        }\n\n        workingVersion = version; \n        dataset = version.getDataset();\n        mode = FileEditMode.CREATE;\n        newFiles = newFilesList;\n        uploadedFiles = new ArrayList<>();\n        selectedFiles = selectedFileMetadatasList;\n        \n        this.maxFileUploadSizeInBytes = systemConfig.getMaxFileUploadSizeForStore(dataset.getOwner().getEffectiveStorageDriverId());\n        this.multipleUploadFilesLimit = systemConfig.getMultipleUploadFilesLimit();\n        \n        logger.fine(\"done\");\n        \n        saveEnabled = true;\n        return null; \n    }\n    \n\n    public String init() {\n        // default mode should be EDIT\n        if (mode == null) {\n            mode = FileEditMode.EDIT;\n        }\n        \n        newFiles = new ArrayList<>();\n        uploadedFiles = new ArrayList<>(); \n        \n        if (dataset.getId() != null){\n            // Set Working Version and Dataset by Datasaet Id and Version\n            //retrieveDatasetVersionResponse = datasetVersionService.retrieveDatasetVersionById(dataset.getId(), null);\n            dataset = datasetService.find(dataset.getId());\n            // Is the Dataset harvested? (because we don't allow editing of harvested \n            // files!)\n            if (dataset == null || dataset.isHarvested()) {\n                return permissionsWrapper.notFound();\n            }\n        } else {\n            // It could be better to show an error page of some sort, explaining\n            // that the dataset id is mandatory... But 404 will do for now.\n            return permissionsWrapper.notFound();\n        }\n        \n\n\n        this.maxFileUploadSizeInBytes = systemConfig.getMaxFileUploadSizeForStore(dataset.getOwner().getEffectiveStorageDriverId());\n        this.multipleUploadFilesLimit = systemConfig.getMultipleUploadFilesLimit();\n                       \n        workingVersion = dataset.getEditVersion();\n        clone = workingVersion.cloneDatasetVersion();\n        if (workingVersion == null || !workingVersion.isDraft()) {\n            // Sorry, we couldn't find/obtain a draft version for this dataset!\n            return permissionsWrapper.notFound();\n        }\n        \n        // Check if they have permission to modify this dataset: \n        \n        if (!permissionService.on(dataset).has(Permission.EditDataset)) {\n            return permissionsWrapper.notAuthorized();\n        }\n\n        // -------------------------------------------\n        //  Is this a file replacement operation?\n        // -------------------------------------------\n        if (mode == FileEditMode.SINGLE_REPLACE){\n            /*\n            http://localhost:8080/editdatafiles.xhtml?mode=SINGLE_REPLACE&datasetId=26&fid=726\n            */        \n            DataFile fileToReplace = loadFileToReplace();\n            if (fileToReplace == null){\n                return permissionsWrapper.notFound();\n            }\n            \n            //DataverseRequest dvRequest2 = createDataverseRequest(authUser);\n            AddReplaceFileHelper addReplaceFileHelper = new AddReplaceFileHelper(dvRequestService.getDataverseRequest(),\n                                                ingestService,\n                                                datasetService,\n                                                datafileService,\n                                                permissionService,\n                                                commandEngine,\n                                                systemConfig);\n                        \n            fileReplacePageHelper = new FileReplacePageHelper(addReplaceFileHelper,\n                                                dataset, \n                                                fileToReplace);\n\n            populateFileMetadatas();\n            singleFile = getFileToReplace();\n        }else if (mode == FileEditMode.EDIT || mode == FileEditMode.SINGLE) {\n\n            if (selectedFileIdsString != null) {\n                String[] ids = selectedFileIdsString.split(\",\");\n\n                for (String id : ids) {\n                    Long test = null;\n                    try {\n                        test = new Long(id);\n                    } catch (NumberFormatException nfe) {\n                        // do nothing...\n                        test = null;\n                    }\n                    if (test != null) {\n                        if (mode == FileEditMode.SINGLE) {\n                            singleFile = datafileService.find(test);\n                        }\n                        selectedFileIdsList.add(test);\n                    }\n                }\n            }\n\n            if (selectedFileIdsList.size() < 1) {\n                logger.fine(\"No numeric file ids supplied to the page, in the edit mode. Redirecting to the 404 page.\");\n                // If no valid file IDs specified, send them to the 404 page...\n                return permissionsWrapper.notFound();\n            }\n\n            logger.fine(\"The page is called with \" + selectedFileIdsList.size() + \" file ids.\");\n\n            populateFileMetadatas();\n            setUpRsync();\n            // and if no filemetadatas can be found for the specified file ids \n            // and version id - same deal, send them to the \"not found\" page. \n            // (at least for now; ideally, we probably want to show them a page \n            // with a more informative error message; something alonog the lines \n            // of - could not find the files for the ids specified; or, these \n            // datafiles are not present in the version specified, etc.\n            if (fileMetadatas.size() < 1) {\n                return permissionsWrapper.notFound();\n            }\n            \n            if (FileEditMode.SINGLE == mode){\n                if (fileMetadatas.get(0).getDatasetVersion().getId() != null){\n                    versionString = \"DRAFT\";\n                }\n            }    \n                       \n        }\n        \n        saveEnabled = true; \n        if (mode == FileEditMode.UPLOAD && workingVersion.getFileMetadatas().isEmpty() && rsyncUploadSupported())  {\n            setUpRsync();\n        }\n\n        if (mode == FileEditMode.UPLOAD) {\n            if (settingsWrapper.getUploadMethodsCount() == 1){\n                JH.addMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.message.uploadFiles.label\"), BundleUtil.getStringFromBundle(\"dataset.message.uploadFilesSingle.message\", Arrays.asList(systemConfig.getGuidesBaseUrl(), systemConfig.getGuidesVersion())));\n            } else if (settingsWrapper.getUploadMethodsCount() > 1) {\n                JH.addMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.message.uploadFiles.label\"), BundleUtil.getStringFromBundle(\"dataset.message.uploadFilesMultiple.message\", Arrays.asList(systemConfig.getGuidesBaseUrl(), systemConfig.getGuidesVersion())));\n            }\n            \n        }\n        \n        if (settingsService.isTrueForKey(SettingsServiceBean.Key.PublicInstall, false)){\n            JH.addMessage(FacesMessage.SEVERITY_WARN, getBundleString(\"dataset.message.publicInstall\"));\n        }   \n        \n        return null;\n    }\n    \n    \n    private void msg(String s){\n        System.out.println(s);\n    }\n    \n    /**\n     * For single file replacement, load the file to replace\n     * \n     * @return \n     */\n    private DataFile loadFileToReplace(){\n        \n        Map<String, String> params =FacesContext.getCurrentInstance().\n                                getExternalContext().getRequestParameterMap();\n        \n        if (params.containsKey(\"fid\")){\n            String fid = params.get(\"fid\");\n            if ((!fid.isEmpty()) && (StringUtils.isNumeric(fid))){\n                selectedFileIdsList.add(Long.parseLong(fid));\n                return datafileService.find(Long.parseLong(fid));\n            }\n        }\n        return null;\n        \n    } // loadFileToReplace\n    \n    private List<FileMetadata> selectedFiles; // = new ArrayList<>();\n\n    public List<FileMetadata> getSelectedFiles() {\n        return selectedFiles;\n    }\n\n    public void setSelectedFiles(List<FileMetadata> selectedFiles) {\n        this.selectedFiles = selectedFiles;\n    }\n    \n    private boolean selectAllFiles;\n\n    public boolean isSelectAllFiles() {\n        return selectAllFiles;\n    }\n\n    public void setSelectAllFiles(boolean selectAllFiles) {\n        this.selectAllFiles = selectAllFiles;\n    }\n    \n    public String getVersionString() {\n        return versionString;\n    }\n\n    public void setVersionString(String versionString) {\n        this.versionString = versionString;\n    }\n    \n    /*\n    public void toggleSelectedFiles(){\n        this.selectedFiles = new ArrayList<>();\n        if(this.selectAllFiles){\n            if (mode == FileEditMode.CREATE) {\n                for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n                    this.selectedFiles.add(fmd);\n                }\n            } else {\n                for (FileMetadata fmd : fileMetadatas) {\n                    this.selectedFiles.add(fmd);\n                }\n            }\n        }\n    }\n    */\n    /*\n    public String getSelectedFilesIdsString() {        \n        String downloadIdString = \"\";\n        for (FileMetadata fmd : this.selectedFiles){\n            if (!StringUtil.isEmpty(downloadIdString)) {\n                downloadIdString += \",\";\n            }\n            downloadIdString += fmd.getDataFile().getId();\n        }\n        return downloadIdString;\n      \n    }\n*/\n    /*\n    public void updateFileCounts(){\n        \n        setSelectedUnrestrictedFiles(new ArrayList<FileMetadata>());\n        setSelectedRestrictedFiles(new ArrayList<FileMetadata>());\n        for (FileMetadata fmd : this.selectedFiles){\n            if(fmd.isRestricted()){\n                getSelectedRestrictedFiles().add(fmd);\n            } else {\n                getSelectedUnrestrictedFiles().add(fmd);\n            }\n        }\n    }*/\n    \n    List<FileMetadata> previouslyRestrictedFiles = null;\n    \n    public boolean isShowAccessPopup() {\n        for (FileMetadata fmd : this.fileMetadatas) {\n            if (fmd.isRestricted()) {\n            \n                if (fmd.getDataFile().getId() == null) {\n                    // if this is a brand new file, it's definitely not \n                    // of a previously restricted kind!\n                    return true; \n                }\n            \n                if (previouslyRestrictedFiles != null) {\n                    boolean contains = false;\n                    for (FileMetadata fmp : previouslyRestrictedFiles) {\n                        // OK, we've already checked if it's a brand new file - \n                        // above. So we can safely assume that this datafile\n                        // has a valid db id... so it is safe to use the \n                        // equals() method:\n                        if (fmp.getDataFile().equals(fmd.getDataFile())) {\n                            contains = true;\n                            break;\n                        }\n                    }\n                    if (!contains) {\n                        return true;\n                    }\n                }\n            }\n        }\n        return false;\n    }\n    \n    public void setShowAccessPopup(boolean showAccessPopup) {} // dummy set method\n     \n    //This function was reverted to its pre-commands state as the current command\n    //requires editDataset privlidges. If a non-admin user with only createDataset privlidges\n    //attempts to restrict a datafile before the dataset is created, the operation\n    //fails silently. This is because they are only granted editDataset permissions\n    //for that scope after the creation is completed.  -Matthew 4.7.1\n    public void restrictFiles(boolean restricted) throws UnsupportedOperationException{\n        \n        //First make sure they've even selected file(s) to restrict.... \n        if (this.getSelectedFiles().isEmpty()) {\n            if (restricted) {\n                PrimeFaces.current().executeScript(\"PF('selectFilesForRestrictEditFilesPage').show()\");\n            } else {\n                PrimeFaces.current().executeScript(\"PF('selectFilesForUnRestrictEditFilesPage').show()\");\n            }\n            return;\n        }\n\n        // since we are restricted files, first set the previously restricted file list, so we can compare for\n        // determining whether to show the access popup\n        previouslyRestrictedFiles = new ArrayList<>();\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (fmd.isRestricted()) {\n                previouslyRestrictedFiles.add(fmd);\n            }\n        }\n        \n        String fileNames = null;\n        \n        for (FileMetadata fmd : this.getSelectedFiles()) {\n            if (restricted && !fmd.isRestricted()) {\n                // collect the names of the newly-restrticted files, \n                // to show in the success message:\n                if (fileNames == null) {\n                    fileNames = fmd.getLabel();\n                } else {\n                    fileNames = fileNames.concat(\", \" + fmd.getLabel());\n                }\n            }\n            fmd.setRestricted(restricted);\n            \n//            Command cmd;\n//            cmd = new RestrictFileCommand(fmd.getDataFile(), dvRequestService.getDataverseRequest(), restricted);\n//            commandEngine.submit(cmd);\n            \n                                  \n            if (workingVersion.isDraft() && !fmd.getDataFile().isReleased()) {\n                // We do not really need to check that the working version is \n                // a draft here - it must be a draft, if we've gotten this\n                // far. But just in case. -- L.A. 4.2.1\n                  fmd.getDataFile().setRestricted(restricted);              \n            }\n        }\n        if (fileNames != null) {\n            String successMessage = getBundleString(\"file.restricted.success\");\n            logger.fine(successMessage);\n            successMessage = successMessage.replace(\"{0}\", fileNames);\n            JsfHelper.addFlashMessage(successMessage);    \n        }\n    } \n\n    public void restrictFilesDP(boolean restricted) {\n        // since we are restricted files, first set the previously restricted file list, so we can compare for\n        // determinin whether to show the access popup\n        if (previouslyRestrictedFiles == null) {\n            previouslyRestrictedFiles = new ArrayList<>();\n            for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n                if (fmd.isRestricted()) {\n                    previouslyRestrictedFiles.add(fmd);\n                }\n            }\n        }        \n        \n        String fileNames = null;       \n        for (FileMetadata fmw : workingVersion.getFileMetadatas()) {\n            for (FileMetadata fmd : this.getSelectedFiles()) {\n                if (restricted && !fmw.isRestricted()) {\n                // collect the names of the newly-restrticted files, \n                    // to show in the success message:\n                    if (fileNames == null) {\n                        fileNames = fmd.getLabel();\n                    } else {\n                        fileNames = fileNames.concat(\", \" + fmd.getLabel());\n                    }\n                }\n                if (fmd.getDataFile().equals(fmw.getDataFile())) {\n                    fmw.setRestricted(restricted);\n                }\n            }\n        }\n        if (fileNames != null) {\n            String successMessage = getBundleString(\"file.restricted.success\");\n            logger.fine(successMessage);\n            successMessage = successMessage.replace(\"{0}\", fileNames);\n            JsfHelper.addFlashMessage(successMessage);    \n        }\n    } \n    \n    public int getRestrictedFileCount() {\n        int restrictedFileCount = 0;\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            if (fmd.isRestricted()) {\n                restrictedFileCount++;\n            }\n        }\n\n        return restrictedFileCount;\n    }\n\n    private List<FileMetadata> filesToBeDeleted = new ArrayList<>();\n\n    \n    public void deleteReplacementFile() throws FileReplaceException{\n        if (!isFileReplaceOperation()){\n            throw new FileReplaceException(\"Only use this for File Replace Operations\");            \n        }\n\n        if (!fileReplacePageHelper.wasPhase1Successful()){\n            throw new FileReplaceException(\"Should only be called if Phase 1 was successful\");                        \n        }\n        \n        fileReplacePageHelper.resetReplaceFileHelper();\n\n\n        \n        String successMessage = getBundleString(\"file.deleted.replacement.success\");\n        logger.fine(successMessage);\n        JsfHelper.addFlashMessage(successMessage);\n        \n    }\n    \n    \n    /**\n     * \n     * @param msgName - from the bundle e.g. \"file.deleted.success\"\n     * @return \n     */\n    private String getBundleString(String msgName){\n        \n       return BundleUtil.getStringFromBundle(msgName);\n    }\n    \n    // This deleteFilesCompleted method is used in editFilesFragment.xhtml\n    public void deleteFilesCompleted(){\n        \n    }\n        \n    public void deleteFiles() {\n        logger.fine(\"entering bulk file delete (EditDataFilesPage)\");\n        if (isFileReplaceOperation()){\n            try {\n                deleteReplacementFile();\n            } catch (FileReplaceException ex) {\n                Logger.getLogger(EditDatafilesPage.class.getName()).log(Level.SEVERE, null, ex);\n            }\n            return;\n        }\n        \n        String fileNames = null;\n        for (FileMetadata fmd : this.getSelectedFiles()) {\n                // collect the names of the files, \n            // to show in the success message:\n            if (fileNames == null) {\n                fileNames = fmd.getLabel();\n            } else {\n                fileNames = fileNames.concat(\", \" + fmd.getLabel());\n            }\n        }\n\n        for (FileMetadata markedForDelete : this.getSelectedFiles()) {\n            logger.fine(\"delete requested on file \"+markedForDelete.getLabel());\n            logger.fine(\"file metadata id: \"+markedForDelete.getId());\n            logger.fine(\"datafile id: \"+markedForDelete.getDataFile().getId());\n            logger.fine(\"page is in edit mode \"+mode.name());\n            \n                // has this filemetadata been saved already? (or is it a brand new\n                // filemetadata, created as part of a brand new version, created when \n                // the user clicked 'delete', that hasn't been saved in the db yet?)\n                if (markedForDelete.getId() != null) {\n                    logger.fine(\"this is a filemetadata from an existing draft version\");\n                // so all we remove is the file from the fileMetadatas (from the\n                // file metadatas attached to the editVersion, and from the\n                // display list of file metadatas that are being edited)\n                // and let the delete be handled in the command (by adding it to the\n                // filesToBeDeleted list):\n\n                    dataset.getEditVersion().getFileMetadatas().remove(markedForDelete);\n                    fileMetadatas.remove(markedForDelete);\n                    filesToBeDeleted.add(markedForDelete);\n                } else {\n                    logger.fine(\"this is a brand-new (unsaved) filemetadata\");\n                    // ok, this is a brand-new DRAFT version. \n\n                // if (mode != FileEditMode.CREATE) {\n                // If the bean is in the 'CREATE' mode, the page is using\n                // dataset.getEditVersion().getFileMetadatas() directly,\n                // so there's no need to delete this meta from the local\n                // fileMetadatas list. (but doing both just adds a no-op and won't cause an\n                // error)\n\n                    // 1. delete the filemetadata from the local display list: \n                removeFileMetadataFromList(fileMetadatas, markedForDelete);\n                    // 2. delete the filemetadata from the version: \n                removeFileMetadataFromList(dataset.getEditVersion().getFileMetadatas(), markedForDelete);\n                        }\n\n\n            if (markedForDelete.getDataFile().getId() == null) {\n                logger.fine(\"this is a brand new file.\");\n                // the file was just added during this step, so in addition to \n                // removing it from the fileMetadatas lists (above), we also remove it from\n                // the newFiles list and the dataset's files, so it never gets saved.\n                \n                removeDataFileFromList(dataset.getFiles(), markedForDelete.getDataFile());\n                removeDataFileFromList(newFiles, markedForDelete.getDataFile());\n                deleteTempFile(markedForDelete.getDataFile());\n                // Also remove checksum from the list of newly uploaded checksums (perhaps odd\n                // to delete and then try uploading the same file again, but it seems like it\n                // should be allowed/the checksum list is part of the state to clean-up\n                checksumMapNew.remove(markedForDelete.getDataFile().getChecksumValue());\n                    \n                        }\n                    }\n        if (fileNames != null) {\n            String successMessage = getBundleString(\"file.deleted.success\");\n            logger.fine(successMessage);\n            successMessage = successMessage.replace(\"{0}\", fileNames);\n            JsfHelper.addFlashMessage(successMessage);\n                    }\n                }\n                \n    private void deleteTempFile(DataFile dataFile) {\n    \t// Before we remove the file from the list and forget about \n    \t// it:\n    \t// The physical uploaded file is still sitting in the temporary\n    \t// directory. If it were saved, it would be moved into its \n    \t// permanent location. But since the user chose not to save it,\n    \t// we have to delete the temp file too. \n    \t// \n    \t// Eventually, we will likely add a dedicated mechanism\n    \t// for managing temp files, similar to (or part of) the storage \n    \t// access framework, that would allow us to handle specialized\n    \t// configurations - highly sensitive/private data, that \n    \t// has to be kept encrypted even in temp files, and such. \n    \t// But for now, we just delete the file directly on the \n    \t// local filesystem: \n\n    \ttry {\n    \t\tList<Path> generatedTempFiles = ingestService.listGeneratedTempFiles(\n    \t\t\t\tPaths.get(FileUtil.getFilesTempDirectory()), dataFile.getStorageIdentifier());\n    \t\tif (generatedTempFiles != null) {\n    \t\t\tfor (Path generated : generatedTempFiles) {\n    \t\t\t\tlogger.fine(\"(Deleting generated thumbnail file \" + generated.toString() + \")\");\n    \t\t\t\ttry {\n    \t\t\t\t\tFiles.delete(generated);\n    \t\t\t\t} catch (IOException ioex) {\n    \t\t\t\t\tlogger.warning(\"Failed to delete generated file \" + generated.toString());\n    \t\t\t\t}\n    \t\t\t}\n    \t\t}\n    \t\tString si = dataFile.getStorageIdentifier();\n    \t\tif (si.contains(\"://\")) {\n    \t\t\t//Direct upload files will already have a store id in their storageidentifier\n    \t\t\t//but they need to be associated with a dataset for the overall storagelocation to be calculated\n    \t\t\t//so we temporarily set the owner\n    \t\t\tif(dataFile.getOwner()!=null) {\n    \t\t\t\tlogger.warning(\"Datafile owner was not null as expected\");\n    \t\t\t}\n    \t\t\tdataFile.setOwner(dataset);\n    \t\t\t//Use one StorageIO to get the storageLocation and then create a direct storage storageIO class to perform the delete \n    \t\t\t// (since delete is forbidden except for direct storage)\n    \t\t\tString sl = DataAccess.getStorageIO(dataFile).getStorageLocation();\n    \t\t\tDataAccess.getDirectStorageIO(sl).delete();\n    \t\t\tdataFile.setOwner(null);\n    \t\t} else {\n    \t\t\t//Temp files sent to this method have no prefix, not even \"tmp://\"\n    \t\t\tFiles.delete(Paths.get(FileUtil.getFilesTempDirectory() + \"/\" + dataFile.getStorageIdentifier()));\n    \t\t}\n    \t} catch (IOException ioEx) {\n    \t\t// safe to ignore - it's just a temp file. \n    \t\tlogger.warning(ioEx.getMessage());\n    \t\tif(dataFile.getStorageIdentifier().contains(\"://\")) {\n    \t\t\tlogger.warning(\"Failed to delete temporary file \" + dataFile.getStorageIdentifier());\n    \t\t} else {\n    \t\t\tlogger.warning(\"Failed to delete temporary file \" + FileUtil.getFilesTempDirectory() + \"/\"\n    \t\t\t\t\t+ dataFile.getStorageIdentifier());\n    \t\t}\n    \t}\n    }\n\n    private void removeFileMetadataFromList(List<FileMetadata> fmds, FileMetadata fmToDelete) {\n        Iterator<FileMetadata> fmit = fmds.iterator();\n        while (fmit.hasNext()) {\n            FileMetadata fmd = fmit.next();\n            if (fmToDelete.getDataFile().getStorageIdentifier().equals(fmd.getDataFile().getStorageIdentifier())) {\n                fmit.remove();\n                break;\n                    }\n                }\n                }                \n                \n    private void removeDataFileFromList(List<DataFile> dfs, DataFile dfToDelete) {\n        Iterator<DataFile> dfit = dfs.iterator();\n        while (dfit.hasNext()) {\n            DataFile df = dfit.next();\n            if (dfToDelete.getStorageIdentifier().equals(df.getStorageIdentifier())) {\n                dfit.remove();\n                break;\n            }\n        }\n    }\n\n    public String saveWithTermsOfUse() {\n        logger.fine(\"saving terms of use, and the dataset version\");\n        return save();\n    }\n    \n    \n    /**\n     * Save for File Replace operations\n     * @return\n     * @throws FileReplaceException \n     */\n    public String saveReplacementFile() throws FileReplaceException{\n        \n        // Ahh, make sure it's a file replace operation\n        //\n        if (!isFileReplaceOperation()){\n            throw new FileReplaceException(\"Only use this for File Replace Operations\");\n        }\n\n        // Can we do a save?  \n        //  (redundant but ok, also called in main \"save\" event before forking here)      \n        //\n        if (!saveEnabled) {\n            return \"\";\n        }\n        // Sanity check 1\n        //\n        if (fileReplacePageHelper == null){\n            throw new NullPointerException(\"fileReplacePageHelper cannot be null\");\n        }\n        \n        // Make sure phase 1 ran -- button shouldn't be visible if it did not\n        //\n        if (!fileReplacePageHelper.wasPhase1Successful()){\n            throw new FileReplaceException(\"Save should only be called when a replacement file has been chosen.  (Phase 1 has to have completed)\");\n            \n        }\n\n        // Run save!!\n        //\n        if (fileReplacePageHelper.runSaveReplacementFile_Phase2()){\n            JsfHelper.addSuccessMessage(getBundleString(\"file.message.replaceSuccess\"));\n            // It worked!!!  Go to page of new file!!\n            return returnToFileLandingPageAfterReplace(fileReplacePageHelper.getFirstNewlyAddedFile());\n        }else{\n            // Uh oh.\n            String errMsg = fileReplacePageHelper.getErrorMessages();\n            \n            FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_ERROR, getBundleString(\"dataset.save.fail\"), errMsg));\n            logger.severe(\"Dataset save failed for replace operation: \" + errMsg);\n            return null;\n        }\n        \n    }    \n        \n    public String save() {\n        if (!saveEnabled) {\n            return \"\";\n        }\n        //Mirroring the checks for DcmUpload, make sure that the db version of the dataset is not locked. \n        //Also checking local version to save time - if data.isLocked() is true, the UpdateDatasetVersionCommand below would fail\n        if (dataset.getId() != null) {\n            Dataset lockTest = datasetService.find(dataset.getId());\n            if (dataset.isLockedFor(DatasetLock.Reason.EditInProgress) || lockTest.isLockedFor(DatasetLock.Reason.EditInProgress)) {\n                logger.log(Level.INFO, \"Couldn''t save dataset: {0}\", \"It is locked.\"\n                        + \"\");\n                String rootDataverseName = dataverseService.findRootDataverse().getName();\n                JH.addMessage(FacesMessage.SEVERITY_FATAL, getBundleString(\"dataset.locked.editInProgress.message\"),BundleUtil.getStringFromBundle(\"dataset.locked.editInProgress.message.details\", Arrays.asList(BrandingUtil.getSupportTeamName(null, rootDataverseName))));\n                return null;\n            }\n        }\n\n        if (isFileReplaceOperation()){\n            try {\n                return saveReplacementFile();\n            } catch (FileReplaceException ex) {\n                String errMsg = ex.getMessage();\n                FacesContext.getCurrentInstance().addMessage(null, new FacesMessage(FacesMessage.SEVERITY_ERROR, getBundleString(\"dataset.save.fail\"), errMsg));\n                logger.log(Level.SEVERE, \"Dataset save failed for replace operation: {0}\", errMsg);\n                return null;\n            }            \n        }\n\n                \n        int nOldFiles = workingVersion.getFileMetadatas().size();\n        int nNewFiles = newFiles.size();\n        int nExpectedFilesTotal = nOldFiles + nNewFiles; \n        \n        if (nNewFiles > 0) {\n            //SEK 10/15/2018 only apply the following tests if dataset has already been saved.\n            if (dataset.getId() != null) {\n                Dataset lockTest = datasetService.find(dataset.getId());\n                //SEK 09/19/18 Get Dataset again to test for lock just in case the user downloads the rsync script via the api while the \n                // edit files page is open and has already loaded a file in http upload for Dual Mode\n                if (dataset.isLockedFor(DatasetLock.Reason.DcmUpload) || lockTest.isLockedFor(DatasetLock.Reason.DcmUpload)) {\n                    logger.log(Level.INFO, \"Couldn''t save dataset: {0}\", \"DCM script has been downloaded for this dataset. Additonal files are not permitted.\"\n                            + \"\");\n                    populateDatasetUpdateFailureMessage();\n                    return null;\n                }\n                for (DatasetVersion dv : lockTest.getVersions()) {\n                    if (dv.isHasPackageFile()) {\n                        logger.log(Level.INFO, BundleUtil.getStringFromBundle(\"file.api.alreadyHasPackageFile\")\n                                + \"\");\n                        populateDatasetUpdateFailureMessage();\n                        return null;\n                    }\n                }\n            }\n                                \n            // Try to save the NEW files permanently: \n            List<DataFile> filesAdded = ingestService.saveAndAddFilesToDataset(workingVersion, newFiles);\n            \n            // reset the working list of fileMetadatas, as to only include the ones\n            // that have been added to the version successfully: \n            fileMetadatas.clear();\n            for (DataFile addedFile : filesAdded) {\n                fileMetadatas.add(addedFile.getFileMetadata());\n            }\n            filesAdded = null; \n        }\n        //boolean newDraftVersion = false;    \n\n        Boolean provJsonChanges = false;\n        \n        if(systemConfig.isProvCollectionEnabled()) {\n            Boolean provFreeChanges = provPopupFragmentBean.updatePageMetadatasWithProvFreeform(fileMetadatas);\n\n            try {\n                // Note that the user may have uploaded provenance metadata file(s)\n                // for some of the new files that have since failed to be permanently saved \n                // in storage (in the ingestService.saveAndAddFilesToDataset() step, above); \n                // these files have been dropped from the fileMetadatas list, and we \n                // are not adding them to the dataset; but the \n                // provenance update set still has entries for these failed files,\n                // so we are passing the fileMetadatas list to the saveStagedProvJson()\n                // method below - so that it doesn't attempt to save the entries \n                // that are no longer valid. \n                provJsonChanges = provPopupFragmentBean.saveStagedProvJson(false, fileMetadatas);\n            } catch (AbstractApiBean.WrappedResponse ex) {\n                JsfHelper.addErrorMessage(getBundleString(\"file.metadataTab.provenance.error\"));\n                Logger.getLogger(EditDatafilesPage.class.getName()).log(Level.SEVERE, null, ex);\n            }\n        }\n        logger.fine(\"issuing the dataset update command\");\n        // We are creating a new draft version or updating an existing draft;\n        // We'll use an Update command for this:\n        if (workingVersion.getId() != null) {\n            for (int i = 0; i < workingVersion.getFileMetadatas().size(); i++) {\n                for (FileMetadata fileMetadata : fileMetadatas) {\n                    if (fileMetadata.getDataFile().getStorageIdentifier() != null) {\n                        if (fileMetadata.getDataFile().getStorageIdentifier().equals(workingVersion.getFileMetadatas().get(i).getDataFile().getStorageIdentifier())) {\n                            workingVersion.getFileMetadatas().set(i, fileMetadata);\n                        }\n                    }\n                }\n            }\n        }\n        // Moves DataFile updates from popupFragment to page for saving\n        // This does not seem to collide with the tags updating below\n        if (systemConfig.isProvCollectionEnabled() && provJsonChanges) {\n            HashMap<String, ProvPopupFragmentBean.UpdatesEntry> provenanceUpdates = provPopupFragmentBean.getProvenanceUpdates();\n            for (int i = 0; i < dataset.getFiles().size(); i++) {\n                for (ProvPopupFragmentBean.UpdatesEntry ue : provenanceUpdates.values()) {\n                    if (ue.dataFile.getStorageIdentifier() != null) {\n                        if (ue.dataFile.getStorageIdentifier().equals(dataset.getFiles().get(i).getStorageIdentifier())) {\n                            dataset.getFiles().set(i, ue.dataFile);\n                        }\n                    }\n                }\n            }\n        }\n\n        // Tabular data tags are assigned to datafiles, not to\n        // version-specfic filemetadatas!\n        // So if tabular tags have been modified, we also need to\n        // refresh the list of datafiles, as found in dataset.getFiles(),\n        // similarly to what we've just done, above, for the filemetadatas.\n        // Otherwise, when we call UpdateDatasetCommand, it's not going\n        // to update the tags in the database (issue #2798).\n        // TODO: Is the above still true/is this still necessary?\n        // (and why?...)\n\n        if (tabularDataTagsUpdated) {\n            for (int i = 0; i < dataset.getFiles().size(); i++) {\n                for (FileMetadata fileMetadata : fileMetadatas) {\n                    if (fileMetadata.getDataFile().getStorageIdentifier() != null) {\n                        if (fileMetadata.getDataFile().getStorageIdentifier().equals(dataset.getFiles().get(i).getStorageIdentifier())) {\n                            dataset.getFiles().set(i, fileMetadata.getDataFile());\n                        }\n                    }\n                }\n            }\n            tabularDataTagsUpdated = false;\n        }\n\n        Map<Long, String> deleteStorageLocations = null;\n\n        if (!filesToBeDeleted.isEmpty()) {\n            deleteStorageLocations = datafileService.getPhysicalFilesToDelete(filesToBeDeleted);\n        }\n\n        Command<Dataset> cmd;\n        try {\n            cmd = new UpdateDatasetVersionCommand(dataset, dvRequestService.getDataverseRequest(), filesToBeDeleted, clone);\n            ((UpdateDatasetVersionCommand) cmd).setValidateLenient(true);\n            dataset = commandEngine.submit(cmd);\n\n        } catch (EJBException ex) {\n            StringBuilder error = new StringBuilder();\n            error.append(ex).append(\" \");\n            error.append(ex.getMessage()).append(\" \");\n            Throwable cause = ex;\n            while (cause.getCause() != null) {\n                cause = cause.getCause();\n                error.append(cause).append(\" \");\n                error.append(cause.getMessage()).append(\" \");\n            }\n            logger.log(Level.INFO, \"Couldn''t save dataset: {0}\", error.toString());\n            populateDatasetUpdateFailureMessage();\n            return null;\n        } catch (CommandException ex) {\n            // FacesContext.getCurrentInstance().addMessage(null, new\n            // FacesMessage(FacesMessage.SEVERITY_ERROR, \"Dataset Save Failed\", \" - \" +\n            // ex.toString()));\n            logger.log(Level.INFO, \"Couldn''t save dataset: {0}\", ex.getMessage());\n            populateDatasetUpdateFailureMessage();\n            return null;\n        }\n\n        // Have we just deleted some draft datafiles (successfully)?\n        // finalize the physical file deletes:\n        // (DataFileService will double-check that the datafiles no\n        // longer exist in the database, before attempting to delete\n        // the physical files)\n        if (deleteStorageLocations != null) {\n            datafileService.finalizeFileDeletes(deleteStorageLocations);\n        }\n        saveEnabled = false;\n\n        if (newFiles.size() > 0) {\n            logger.fine(\"clearing newfiles list.\");\n            newFiles.clear();\n            /*\n             - We decided not to bother obtaining persistent ids for new files \n             as they are uploaded and created. The identifiers will be assigned \n             later, when the version is published. \n             \n            logger.info(\"starting async job for obtaining persistent ids for files.\");\n            datasetService.obtainPersistentIdentifiersForDatafiles(dataset);\n            */\n        }\n                \n        workingVersion = dataset.getEditVersion();\n        logger.fine(\"working version id: \"+workingVersion.getId());\n       \n        if (mode == FileEditMode.SINGLE){\n            JsfHelper.addSuccessMessage(getBundleString(\"file.message.editSuccess\"));\n            \n        } else {\n            int nFilesTotal = workingVersion.getFileMetadatas().size();\n            if (nNewFiles == 0 || nFilesTotal == nExpectedFilesTotal) {\n                JsfHelper.addSuccessMessage(getBundleString(\"dataset.message.filesSuccess\"));\n            } else if (nFilesTotal == nOldFiles) {\n                JsfHelper.addErrorMessage(getBundleString(\"dataset.message.addFiles.Failure\"));\n            } else {\n                String warningMessage = getBundleString(\"dataset.message.addFiles.partialSuccess\");\n                warningMessage = warningMessage.replace(\"{0}\", \"\" + (nFilesTotal - nOldFiles));\n                warningMessage = warningMessage.replace(\"{1}\", \"\" + nNewFiles);\n                JsfHelper.addWarningMessage(warningMessage);\n            }\n        }\n\n        // Call Ingest Service one more time, to \n        // queue the data ingest jobs for asynchronous execution:\n        if (mode == FileEditMode.UPLOAD) {\n            ingestService.startIngestJobsForDataset(dataset, (AuthenticatedUser) session.getUser());\n        }\n\n        if (mode == FileEditMode.SINGLE && fileMetadatas.size() > 0) {\n            // If this was a \"single file edit\", i.e. an edit request sent from \n            // the individual File Landing page, we want to redirect back to \n            // the landing page. BUT ONLY if the file still exists - i.e., if \n            // the user hasn't just deleted it!\n            versionString = \"DRAFT\";\n            return returnToFileLandingPage();\n        }\n        \n        logger.fine(\"Redirecting to the dataset page, from the edit/upload page.\");\n        return returnToDraftVersion();\n    }\n    \n    private void populateDatasetUpdateFailureMessage(){\n            \n        JH.addMessage(FacesMessage.SEVERITY_FATAL, getBundleString(\"dataset.message.filesFailure\"));\n    }\n    \n    \n    \n    private String returnToDraftVersion(){      \n         return \"/dataset.xhtml?persistentId=\" + dataset.getGlobalId().asString() + \"&version=DRAFT&faces-redirect=true\";    \n    }\n    \n    private String returnToDatasetOnly(){\n         dataset = datasetService.find(dataset.getId());\n         return \"/dataset.xhtml?persistentId=\" + dataset.getGlobalId().asString()  +  \"&faces-redirect=true\";       \n    }\n    \n    private String returnToFileLandingPage() {\n        Long fileId = fileMetadatas.get(0).getDataFile().getId();   \n        if (versionString != null && versionString.equals(\"DRAFT\")){\n            return  \"/file.xhtml?fileId=\" + fileId  +  \"&version=DRAFT&faces-redirect=true\";\n        }\n        return  \"/file.xhtml?fileId=\" + fileId  +  \"&faces-redirect=true\";\n\n    }\n\n    private String returnToFileLandingPageAfterReplace(DataFile newFile) {\n        \n        if (newFile == null){\n            throw new NullPointerException(\"newFile cannot be null!\");\n        }\n        //Long datasetVersionId = newFile.getOwner().getLatestVersion().getId();\n        return \"/file.xhtml?fileId=\" + newFile.getId()  + \"&version=DRAFT&faces-redirect=true\";\n    }\n\n    \n    public String cancel() {\n        uploadInProgress = false;\n        if (mode == FileEditMode.SINGLE || mode == FileEditMode.SINGLE_REPLACE ) {\n            return returnToFileLandingPage();\n        }\n        //Files that have been finished and are now in the lower list on the page\n        for (DataFile newFile : newFiles) {\n            deleteTempFile(newFile);\n        }\n\n        //Files in the upload process but not yet finished\n        for (DataFile newFile : uploadedFiles) {\n            deleteTempFile(newFile);\n        }\n        if (workingVersion.getId() != null) {\n            return returnToDraftVersion();\n        }\n        return returnToDatasetOnly();\n    }\n\n    \n    /* deprecated; super inefficient, when called repeatedly on a long list \n       of files! \n       leaving the code here, commented out, for illustration purposes. -- 4.6\n    public boolean isDuplicate(FileMetadata fileMetadata) {\n\n        Map<String, Integer> MD5Map = new HashMap<String, Integer>();\n\n        // TODO: \n        // think of a way to do this that doesn't involve populating this \n        // map for every file on the page? \n        // may not be that much of a problem, if we paginate and never display \n        // more than a certain number of files... Still, needs to be revisited\n        // before the final 4.0. \n        // -- L.A. 4.0\n\n        // make a \"defensive copy\" to avoid java.util.ConcurrentModificationException from being thrown\n        // when uploading 100+ files\n        List<FileMetadata> wvCopy = new ArrayList<>(workingVersion.getFileMetadatas());\n        Iterator<FileMetadata> fmIt = wvCopy.iterator();\n\n        while (fmIt.hasNext()) {\n            FileMetadata fm = fmIt.next();\n            String md5 = fm.getDataFile().getChecksumValue();\n            if (md5 != null) {\n                if (MD5Map.get(md5) != null) {\n                    MD5Map.put(md5, MD5Map.get(md5).intValue() + 1);\n                } else {\n                    MD5Map.put(md5, 1);\n                }\n            }\n        }\n\n        return MD5Map.get(thisMd5) != null && MD5Map.get(thisMd5).intValue() > 1;\n    }*/\n   \n    private HttpClient getClient() {\n        // TODO: \n        // cache the http client? -- L.A. 4.0 alpha\n        return new HttpClient();\n    }\n\n    /**\n     * Is this page in File Replace mode\n     * \n     * @return \n     */\n    public boolean isFileReplaceOperation(){\n        return (mode == FileEditMode.SINGLE_REPLACE)&&(fileReplacePageHelper!= null);\n    }\n    \n    public boolean allowMultipleFileUpload(){\n        \n        return !isFileReplaceOperation();\n    }\n    \n    public boolean showFileUploadFragment(){\n        return mode == FileEditMode.UPLOAD || mode == FileEditMode.CREATE || mode == FileEditMode.SINGLE_REPLACE;\n    }\n    \n    \n    public boolean showFileUploadComponent(){\n        if (mode == FileEditMode.UPLOAD || mode == FileEditMode.CREATE) {\n           return true;\n        }\n        \n        if (isFileReplaceOperation()){\n            //msg(\"fileReplacePageHelper.showFileUploadComponent(): \"+ fileReplacePageHelper.showFileUploadComponent());\n            return fileReplacePageHelper.showFileUploadComponent();\n            }\n\n        return false;\n        //return false;\n    }\n    \n\n    /**\n     * Download a file from drop box\n     * \n     * @param fileLink\n     * @return \n     */\n    private InputStream getDropBoxInputStream(String fileLink, GetMethod dropBoxMethod){\n        \n        if (fileLink == null){\n            return null;\n        }\n        \n        // -----------------------------------------------------------\n        // Make http call, download the file: \n        // -----------------------------------------------------------\n        int status = 0;\n        //InputStream dropBoxStream = null;\n\n        try {\n            status = getClient().executeMethod(dropBoxMethod);\n            if (status == 200) {\n                return dropBoxMethod.getResponseBodyAsStream();\n            }\n        } catch (IOException ex) {\n            logger.log(Level.WARNING, \"Failed to access DropBox url: {0}!\", fileLink);\n            return null;\n        } \n\n        logger.log(Level.WARNING, \"Failed to get DropBox InputStream for file: {0}\", fileLink);\n        return null;\n    } // end: getDropBoxInputStream\n                  \n    \n    /**\n     * Using information from the DropBox choose, ingest the chosen files\n     *  https://www.dropbox.com/developers/dropins/chooser/js\n     * \n     * @param event\n     */\n    public void handleDropBoxUpload(ActionEvent event) {\n        if (!uploadInProgress) {\n            uploadInProgress = true;\n        }\n        logger.fine(\"handleDropBoxUpload\");\n        uploadComponentId = event.getComponent().getClientId();\n        \n        // -----------------------------------------------------------\n        // Read JSON object from the output of the DropBox Chooser: \n        // -----------------------------------------------------------\n        JsonReader dbJsonReader = Json.createReader(new StringReader(dropBoxSelection));\n        JsonArray dbArray = dbJsonReader.readArray();\n        dbJsonReader.close();\n\n        // -----------------------------------------------------------\n        // Iterate through the Dropbox file information (JSON)\n        // -----------------------------------------------------------\n        DataFile dFile = null;\n        GetMethod dropBoxMethod = null;\n        String localWarningMessage = null; \n        for (int i = 0; i < dbArray.size(); i++) {\n            JsonObject dbObject = dbArray.getJsonObject(i);\n\n            // -----------------------------------------------------------\n            // Parse information for a single file\n            // -----------------------------------------------------------\n            String fileLink = dbObject.getString(\"link\");\n            String fileName = dbObject.getString(\"name\");\n            int fileSize = dbObject.getInt(\"bytes\");\n\n            logger.fine(\"DropBox url: \" + fileLink + \", filename: \" + fileName + \", size: \" + fileSize);\n\n\n            /* ----------------------------\n                Check file size\n                - Max size NOT specified in db: default is unlimited\n                - Max size specified in db: check too make sure file is within limits\n            // ---------------------------- */\n            if ((!this.isUnlimitedUploadFileSize()) && (fileSize > this.getMaxFileUploadSizeInBytes())) {\n                String warningMessage = \"Dropbox file \\\"\" + fileName + \"\\\" exceeded the limit of \" + fileSize + \" bytes and was not uploaded.\";\n                //msg(warningMessage);\n                //FacesContext.getCurrentInstance().addMessage(event.getComponent().getClientId(), new FacesMessage(FacesMessage.SEVERITY_ERROR, \"upload failure\", warningMessage));\n                if (localWarningMessage == null) {\n                    localWarningMessage = warningMessage;\n                } else {\n                    localWarningMessage = localWarningMessage.concat(\"; \" + warningMessage);\n                }\n                continue; // skip to next file, and add error mesage\n            }\n\n            \n            dFile = null;\n            dropBoxMethod = new GetMethod(fileLink);\n\n            // -----------------------------------------------------------\n            // Download the file\n            // -----------------------------------------------------------\n            InputStream dropBoxStream = this.getDropBoxInputStream(fileLink, dropBoxMethod);\n            if (dropBoxStream==null){\n                logger.severe(\"Could not retrieve dropgox input stream for: \" + fileLink);\n                continue;  // Error skip this file\n            }\n            \n            // -----------------------------------------------------------\n            // Is this a FileReplaceOperation?  If so, then diverge!\n            // -----------------------------------------------------------\n            if (this.isFileReplaceOperation()){\n              this.handleReplaceFileUpload(event, dropBoxStream, fileName, FileUtil.MIME_TYPE_UNDETERMINED_DEFAULT, null, event);\n              this.setFileMetadataSelectedForTagsPopup(fileReplacePageHelper.getNewFileMetadatasBeforeSave().get(0));\n              return;\n             }\n            // -----------------------------------------------------------\n\n            \n            List<DataFile> datafiles = new ArrayList<>(); \n            \n            // -----------------------------------------------------------\n            // Send it through the ingest service\n            // -----------------------------------------------------------\n            try {\n\n                // Note: A single uploaded file may produce multiple datafiles - \n                // for example, multiple files can be extracted from an uncompressed\n                // zip file.\n                //datafiles = ingestService.createDataFiles(workingVersion, dropBoxStream, fileName, \"application/octet-stream\");\n                datafiles = FileUtil.createDataFiles(workingVersion, dropBoxStream, fileName, \"application/octet-stream\", null,null, systemConfig);\n                \n            } catch (IOException ex) {\n                this.logger.log(Level.SEVERE, \"Error during ingest of DropBox file {0} from link {1}\", new Object[]{fileName, fileLink});\n                continue;\n            }/*catch (FileExceedsMaxSizeException ex){\n                this.logger.log(Level.SEVERE, \"Error during ingest of DropBox file {0} from link {1}: {2}\", new Object[]{fileName, fileLink, ex.getMessage()});\n                continue;\n            }*/ finally {\n                // -----------------------------------------------------------\n                // release connection for dropBoxMethod\n                // -----------------------------------------------------------\n                \n                if (dropBoxMethod != null) {\n                    dropBoxMethod.releaseConnection();\n                }\n                \n                // -----------------------------------------------------------\n                // close the  dropBoxStream\n                // -----------------------------------------------------------\n                try {\n                    dropBoxStream.close();\n                } catch (IOException ex) {\n                    logger.log(Level.WARNING, \"Failed to close the dropBoxStream for file: {0}\", fileLink);\n                }\n            }\n            \n            if (datafiles == null){\n                this.logger.log(Level.SEVERE, \"Failed to create DataFile for DropBox file {0} from link {1}\", new Object[]{fileName, fileLink});\n                continue;\n            }else{    \n                // -----------------------------------------------------------\n                // Check if there are duplicate files or ingest warnings\n                // -----------------------------------------------------------\n                uploadWarningMessage = processUploadedFileList(datafiles);\n                logger.fine(\"Warning message during upload: \" + uploadWarningMessage);\n                /*if (warningMessage != null){\n                     logger.fine(\"trying to send faces message to \" + event.getComponent().getClientId());\n                     FacesContext.getCurrentInstance().addMessage(event.getComponent().getClientId(), new FacesMessage(FacesMessage.SEVERITY_ERROR, \"upload failure\", warningMessage));\n                     if (uploadWarningMessage == null) {\n                         uploadWarningMessage = warningMessage;\n                     } else {\n                         uploadWarningMessage = uploadWarningMessage.concat(\"; \"+warningMessage);\n                     }\n                }*/\n            }\n            if(!uploadInProgress) {\n                logger.warning(\"Upload in progress cancelled\");\n                for (DataFile newFile : datafiles) {\n                    deleteTempFile(newFile);\n                }\n            }\n        }\n        \n        if (localWarningMessage != null) {\n            if (uploadWarningMessage == null) {\n                uploadWarningMessage = localWarningMessage;\n            } else {\n                uploadWarningMessage = localWarningMessage.concat(\"; \" + uploadWarningMessage);\n            }\n        }\n    }\n    \n    public void uploadStarted() {\n        // uploadStarted() is triggered by PrimeFaces <p:upload onStart=... when an upload is \n        // started. It will be called *once*, even if it is a multiple file upload \n        // (either through drag-and-drop or select menu). \n        logger.fine(\"upload started\");\n        \n        uploadInProgress = true;        \n    }\n    \n    \n    private Boolean hasRsyncScript = false;\n    public Boolean isHasRsyncScript() {\n        return hasRsyncScript;\n    }\n\n    public void setHasRsyncScript(Boolean hasRsyncScript) {\n        this.hasRsyncScript = hasRsyncScript;\n    }\n\n    private  void setUpRsync() {\n        logger.fine(\"setUpRsync called...\");\n        if (DataCaptureModuleUtil.rsyncSupportEnabled(settingsWrapper.getValueForKey(SettingsServiceBean.Key.UploadMethods))\n                && dataset.getFiles().isEmpty()) { //only check for rsync if no files exist\n            try {\n                ScriptRequestResponse scriptRequestResponse = commandEngine.submit(new RequestRsyncScriptCommand(dvRequestService.getDataverseRequest(), dataset));\n                logger.fine(\"script: \" + scriptRequestResponse.getScript());\n                if (scriptRequestResponse.getScript() != null && !scriptRequestResponse.getScript().isEmpty()) {\n                    setRsyncScript(scriptRequestResponse.getScript());\n                    rsyncScriptFilename = DataCaptureModuleUtil.getScriptName(workingVersion);\n                    setHasRsyncScript(true);\n                } else {\n                    setHasRsyncScript(false);\n                }\n            } catch (EJBException ex) {\n                logger.warning(\"Problem getting rsync script (EJBException): \" + EjbUtil.ejbExceptionToString(ex));\n            } catch (RuntimeException ex) {\n                logger.warning(\"Problem getting rsync script (RuntimeException): \" + ex.getLocalizedMessage());\n            } catch (CommandException cex) {\n                logger.warning(\"Problem getting rsync script (Command Exception): \" + cex.getLocalizedMessage());\n            }\n        }\n    }\n    \n    public void downloadRsyncScript() {\n\n        FacesContext ctx = FacesContext.getCurrentInstance();\n        HttpServletResponse response = (HttpServletResponse) ctx.getExternalContext().getResponse();\n        response.setContentType(\"application/download\");\n\n        String contentDispositionString;\n\n        contentDispositionString = \"attachment;filename=\" + rsyncScriptFilename;\n        response.setHeader(\"Content-Disposition\", contentDispositionString);\n\n        try {\n            ServletOutputStream out = response.getOutputStream();\n            out.write(getRsyncScript().getBytes());\n            out.flush();\n            ctx.responseComplete();\n        } catch (IOException e) {\n            String error = \"Problem getting bytes from rsync script: \" + e;\n            logger.warning(error);\n            return; \n        }\n        \n        // If the script has been successfully downloaded, lock the dataset:\n        String lockInfoMessage = \"script downloaded\";\n        DatasetLock lock = datasetService.addDatasetLock(dataset.getId(), DatasetLock.Reason.DcmUpload, session.getUser() != null ? ((AuthenticatedUser)session.getUser()).getId() : null, lockInfoMessage);\n        if (lock != null) {\n            dataset.addLock(lock);\n        } else {\n            logger.log(Level.WARNING, \"Failed to lock the dataset (dataset id={0})\", dataset.getId());\n        }\n        \n    }\n    \n        /**\n     * The contents of the script.\n     */\n    private String rsyncScript = \"\";\n\n    public String getRsyncScript() {\n        return rsyncScript;\n    }\n\n    public void setRsyncScript(String rsyncScript) {\n        this.rsyncScript = rsyncScript;\n    }\n\n    private String rsyncScriptFilename;\n\n    public String getRsyncScriptFilename() {\n        return rsyncScriptFilename;\n    }\n\n    public void requestDirectUploadUrl() {\n        \n\n        \n        S3AccessIO<?> s3io = FileUtil.getS3AccessForDirectUpload(dataset);\n        if(s3io == null) {\n        \tFacesContext.getCurrentInstance().addMessage(uploadComponentId, new FacesMessage(FacesMessage.SEVERITY_ERROR, BundleUtil.getStringFromBundle(\"dataset.file.uploadWarning\"), \"Direct upload not supported for this dataset\"));\n        }\n        String url = null;\n        String storageIdentifier = null;\n        try {\n        \turl = s3io.generateTemporaryS3UploadUrl();\n        \tstorageIdentifier = FileUtil.getStorageIdentifierFromLocation(s3io.getStorageLocation());\n        } catch (IOException io) {\n        \tlogger.warning(io.getMessage());\n       \tFacesContext.getCurrentInstance().addMessage(uploadComponentId, new FacesMessage(FacesMessage.SEVERITY_ERROR, BundleUtil.getStringFromBundle(\"dataset.file.uploadWarning\"), \"Issue in connecting to S3 store for direct upload\"));\n       }\n\n    \tPrimeFaces.current().executeScript(\"uploadFileDirectly('\" + url + \"','\" + storageIdentifier + \"')\");\n    }\n    \n    public void uploadFinished() {\n        // This method is triggered from the page, by the <p:upload ... onComplete=...\n        // attribute. \n        // Note that its behavior is different from that of of <p:upload ... onStart=...\n        // that's triggered only once, even for a multiple file upload. In contrast, \n        // onComplete=... gets executed for each of the completed multiple upload events. \n        // So when you drag-and-drop a bunch of files, you CANNOT rely on onComplete=...\n        // to notify the page when the batch finishes uploading! There IS a way \n        // to detect ALL the current uploads completing: the p:upload widget has \n        // the property \"files\", that contains the list of all the files currently \n        // uploading; so checking on the size of the list tells you if any uploads\n        // are still in progress. Once it's zero, you know it's all done. \n        // This is super important - because if the user is uploading 1000 files \n        // via drag-and-drop, you don't want to re-render the entire page each \n        // time every single of the 1000 uploads finishes!\n        // (check editFilesFragment.xhtml for the exact code handling this; and \n        // http://stackoverflow.com/questions/20747201/when-multiple-upload-is-finished-in-pfileupload\n        // for more info). -- 4.6\n        logger.fine(\"upload finished\");\n\n        // Add the file(s) added during this last upload event, single or multiple, \n        // to the full list of new files, and the list of filemetadatas \n        // used to render the page:\n        \n        for (DataFile dataFile : uploadedFiles) {\n            fileMetadatas.add(dataFile.getFileMetadata());\n            newFiles.add(dataFile);\n        }\n        \n       \n        if(uploadInProgress) {\n            uploadedFiles = new ArrayList<>();\n            uploadInProgress = false;\n        }\n        // refresh the warning message below the upload component, if exists:\n        if (uploadComponentId != null) {\n            if (uploadWarningMessage != null) {\n                if (uploadWarningMessageIsNotAnError) {\n                    FacesContext.getCurrentInstance().addMessage(uploadComponentId, new FacesMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.file.uploadWarning\"), uploadWarningMessage));\n                } else {\n                    FacesContext.getCurrentInstance().addMessage(uploadComponentId, new FacesMessage(FacesMessage.SEVERITY_ERROR, BundleUtil.getStringFromBundle(\"dataset.file.uploadWarning\"), uploadWarningMessage));\n                }\n            } else if (uploadSuccessMessage != null) {\n                FacesContext.getCurrentInstance().addMessage(uploadComponentId, new FacesMessage(FacesMessage.SEVERITY_INFO, BundleUtil.getStringFromBundle(\"dataset.file.uploadWorked\"), uploadSuccessMessage));\n            }\n        }\n\n        if(isFileReplaceOperation() && fileReplacePageHelper.hasContentTypeWarning()){\n                    //RequestContext context = RequestContext.getCurrentInstance();\n                    //RequestContext.getCurrentInstance().update(\"datasetForm:fileTypeDifferentPopup\");\n                    PrimeFaces.current().ajax().update(\"datasetForm:fileTypeDifferentPopup\");\n                    //context.execute(\"PF('fileTypeDifferentPopup').show();\");\n                    PrimeFaces.current().executeScript(\"PF('fileTypeDifferentPopup').show();\");\n        }\n\n        // We clear the following duplicate warning labels, because we want to \n        // only inform the user of the duplicates dropped in the current upload \n        // attempt - for ex., one batch of drag-and-dropped files, or a single \n        // file uploaded through the file chooser. \n        dupeFileNamesExisting = null; \n        dupeFileNamesNew = null;\n        multipleDupesExisting = false;\n        multipleDupesNew = false; \n        uploadWarningMessage = null;\n        uploadSuccessMessage = null; \n    }\n    \n    private String warningMessageForPopUp;\n\n    public String getWarningMessageForPopUp() {\n        return warningMessageForPopUp;\n    }\n\n    public void setWarningMessageForPopUp(String warningMessageForPopUp) {\n        this.warningMessageForPopUp = warningMessageForPopUp;\n    }\n\n    private void handleReplaceFileUpload(FacesEvent event, InputStream inputStream, \n                        String fileName, \n                        String contentType,\n                        FileUploadEvent nativeUploadEvent,\n                        ActionEvent dropboxUploadEvent\n    ){\n\n        fileReplacePageHelper.resetReplaceFileHelper();\n\n        saveEnabled = false;\n        \n        uploadComponentId = event.getComponent().getClientId();\n        \n        if (fileReplacePageHelper.handleNativeFileUpload(inputStream,null,\n                                    fileName,\n                                    contentType, \n                                    null\n                                )){\n            saveEnabled = true;\n\n            /**\n             * If the file content type changed, let the user know\n             */\n            if (fileReplacePageHelper.hasContentTypeWarning()){\n                //Add warning to popup instead of page for Content Type Difference\n                setWarningMessageForPopUp(fileReplacePageHelper.getContentTypeWarning());\n                /* \n                    Note on the info messages - upload errors, warnings and success messages:\n                    Instead of trying to display the message here (commented out code below),\n                    we only save the message, as a string - and it will be displayed by \n                    the uploadFinished() method, triggered next, after the upload event\n                    is processed and, as the name suggests, finished. \n                    This is done in 2 stages like this so that when the upload component\n                    is called for large numbers of files, in multiple mode, the page could \n                    be updated and re-rendered just once, after all the uploads are finished - \n                    and not after each individual upload. Of course for the \"replace\" upload\n                    there is always only one... but we have to use this scheme for \n                    consistency. -- L.A. 4.6.1\n                   \n                */\n                //FacesContext.getCurrentInstance().addMessage(\n                //        uploadComponentId,                         \n                //        new FacesMessage(FacesMessage.SEVERITY_ERROR, \"upload warning\", uploadWarningMessage));\n            }\n            // See the comment above, on how upload messages are displayed.\n            \n            // Commented out the success message below - since we probably don't\n            // need it - the state of the page will indicate the success fairly \n            // unambiguously: the primefaces upload and the dropbox upload components\n            // will become disabled, and the uploaded file will appear on the page. \n            // But feel free to un-comment it, if you feel it could be useful. \n            // -- L.A. 4.6.1\n            //uploadSuccessMessage = \"Hey! It worked!\";\n                \n        } else {\n            // See the comment above, on how upload messages are displayed.\n            uploadWarningMessage = fileReplacePageHelper.getErrorMessages();\n//            uploadWarningMessage += \" ******* \";\n            \n            \n            if (nativeUploadEvent != null){\n//                uploadWarningMessage += \" nativeUploadEvent \";\n                \n            }\n            if (dropboxUploadEvent != null){\n//                uploadWarningMessage += \" dropboxUploadEvent \";\n            }\n            //FacesContext.getCurrentInstance().addMessage(\n            //    uploadComponentId,                         \n            //    new FacesMessage(FacesMessage.SEVERITY_ERROR, \"upload failure\", uploadWarningMessage));\n        }\n    }\n    \n    private void handleReplaceFileUpload(String fullStorageLocation, \n    \t\tString fileName, \n    \t\tString contentType, \n    \t\tString checkSum){\n\n    \tfileReplacePageHelper.resetReplaceFileHelper();\n    \tsaveEnabled = false;\n    \tString storageIdentifier = DataAccess.getStorarageIdFromLocation(fullStorageLocation);\n    \tif (fileReplacePageHelper.handleNativeFileUpload(null, storageIdentifier, fileName, contentType, checkSum)){\n    \t\tsaveEnabled = true;\n\n    \t\t/**\n    \t\t * If the file content type changed, let the user know\n    \t\t */\n    \t\tif (fileReplacePageHelper.hasContentTypeWarning()){\n    \t\t\t//Add warning to popup instead of page for Content Type Difference\n    \t\t\tsetWarningMessageForPopUp(fileReplacePageHelper.getContentTypeWarning());\n    \t\t}\n    \t} else {\n    \t\tuploadWarningMessage = fileReplacePageHelper.getErrorMessages();\n    \t}\n    }\n\n    private String uploadWarningMessage = null; \n    private String uploadSuccessMessage = null; \n    private String uploadComponentId = null; \n    \n    /**\n     * Handle native file replace\n     * @param event \n     * @throws java.io.IOException \n     */\n    public void handleFileUpload(FileUploadEvent event) throws IOException {\n        \n        if (!uploadInProgress) {\n            uploadInProgress = true;\n        }\n        \n        if (event == null){\n            throw new NullPointerException(\"event cannot be null\");\n        }\n        \n        UploadedFile uFile = event.getFile();\n        if (uFile == null){\n            throw new NullPointerException(\"uFile cannot be null\");\n        }\n\n\n        /**\n         * For File Replace, take a different code path\n         */\n        if (isFileReplaceOperation()){\n\n            handleReplaceFileUpload(event, uFile.getInputStream(),\n                                    uFile.getFileName(),\n                                    uFile.getContentType(),\n                                    event,\n                                    null);\n            if(fileReplacePageHelper.hasContentTypeWarning()){\n                    //RequestContext context = RequestContext.getCurrentInstance();\n                    //RequestContext.getCurrentInstance().update(\"datasetForm:fileTypeDifferentPopup\");\n                    //context.execute(\"PF('fileTypeDifferentPopup').show();\");\n                    PrimeFaces.current().ajax().update(\"datasetForm:fileTypeDifferentPopup\");\n                    PrimeFaces.current().executeScript(\"PF('fileTypeDifferentPopup').show();\");\n            }\n            return;\n               \n        }\n\n   \n        List<DataFile> dFileList = null;\n        \n        try {\n            // Note: A single uploaded file may produce multiple datafiles - \n            // for example, multiple files can be extracted from an uncompressed\n            // zip file. \n            dFileList = FileUtil.createDataFiles(workingVersion, uFile.getInputStream(), uFile.getFileName(), uFile.getContentType(), null, null, systemConfig);\n            \n        } catch (IOException ioex) {\n            logger.warning(\"Failed to process and/or save the file \" + uFile.getFileName() + \"; \" + ioex.getMessage());\n            return;\n        } /*catch (FileExceedsMaxSizeException ex) {\n            logger.warning(\"Failed to process and/or save the file \" + uFile.getFileName() + \"; \" + ex.getMessage());\n            return;\n        }*/\n\n        // -----------------------------------------------------------\n        // These raw datafiles are then post-processed, in order to drop any files \n        // already in the dataset/already uploaded, and to correct duplicate file names, etc. \n        // -----------------------------------------------------------\n        String warningMessage = processUploadedFileList(dFileList);\n        \n        if (warningMessage != null){\n            uploadWarningMessage = warningMessage;\n            FacesContext.getCurrentInstance().addMessage(event.getComponent().getClientId(), new FacesMessage(FacesMessage.SEVERITY_ERROR, BundleUtil.getStringFromBundle(\"dataset.file.uploadWarning\"), warningMessage));\n            // save the component id of the p:upload widget, so that we could \n            // send an info message there, from elsewhere in the code:\n            uploadComponentId = event.getComponent().getClientId();\n        }\n        \n        if(!uploadInProgress) {\n            logger.warning(\"Upload in progress cancelled\");\n            for (DataFile newFile : dFileList) {\n                deleteTempFile(newFile);\n            }\n        }\n    }\n\n    /**\n     * Using information from the DropBox choose, ingest the chosen files\n     *  https://www.dropbox.com/developers/dropins/chooser/js\n     * \n     * @param event\n     */\n    public void handleExternalUpload() {\n    \tMap<String,String> paramMap = FacesContext.getCurrentInstance().getExternalContext().getRequestParameterMap();\n    \t\n    \tthis.uploadComponentId = paramMap.get(\"uploadComponentId\");\n        String fullStorageIdentifier = paramMap.get(\"fullStorageIdentifier\");\n        String fileName = paramMap.get(\"fileName\");\n        String contentType = paramMap.get(\"contentType\");\n        String checksumType = paramMap.get(\"checksumType\");\n        String checksumValue = paramMap.get(\"checksumValue\");\n        \n        int lastColon = fullStorageIdentifier.lastIndexOf(':');\n        String storageLocation= fullStorageIdentifier.substring(0,lastColon) + \"/\" + dataset.getAuthorityForFileStorage() + \"/\" + dataset.getIdentifierForFileStorage() + \"/\" + fullStorageIdentifier.substring(lastColon+1);\n    \tif (!uploadInProgress) {\n    \t\tuploadInProgress = true;\n    \t}\n    \tlogger.fine(\"handleExternalUpload\");\n    \t\n    \tStorageIO<DvObject> sio;\n    \tString localWarningMessage = null;\n    \ttry {\n    \t\tsio = DataAccess.getDirectStorageIO(storageLocation);\n\n    \t\t//Populate metadata\n    \t\tsio.open(DataAccessOption.READ_ACCESS);\n    \t\t//get file size\n    \t\tlong fileSize = sio.getSize();\n\n\t\t\tif(StringUtils.isEmpty(contentType)) {\n\t\t\t\tcontentType = FileUtil.MIME_TYPE_UNDETERMINED_DEFAULT;\n\t\t\t}\n\t\t\t\n\t\t\tif(DataFile.ChecksumType.fromString(checksumType) != DataFile.ChecksumType.MD5 ) {\n\t\t\t\tString warningMessage = \"Non-MD5 checksums not yet supported in external uploads\";\n\t\t\t\tlocalWarningMessage = warningMessage;\n\t\t\t\t//ToDo - methods like handleReplaceFileUpload and classes like OptionalFileParams will need to track the algorithm in addition to the value to enable this\n\t\t\t}\n\t\t\t\n    \t\t/* ----------------------------\n                Check file size\n                - Max size NOT specified in db: default is unlimited\n                - Max size specified in db: check too make sure file is within limits\n            // ---------------------------- */\n    \t\tif ((!this.isUnlimitedUploadFileSize()) && (fileSize > this.getMaxFileUploadSizeInBytes())) {\n    \t\t\tString warningMessage = \"Uploaded file \\\"\" + fileName + \"\\\" exceeded the limit of \" + fileSize + \" bytes and was not uploaded.\";\n    \t\t\tsio.delete();\n    \t\t\tlocalWarningMessage = warningMessage;\n    \t\t} else {\n    \t\t\t// -----------------------------------------------------------\n    \t\t\t// Is this a FileReplaceOperation?  If so, then diverge!\n    \t\t\t// -----------------------------------------------------------\n    \t\t\tif (this.isFileReplaceOperation()){\n    \t\t\t\tthis.handleReplaceFileUpload(storageLocation, fileName, contentType, checksumValue);\n    \t\t\t\tthis.setFileMetadataSelectedForTagsPopup(fileReplacePageHelper.getNewFileMetadatasBeforeSave().get(0));\n    \t\t\t\treturn;\n    \t\t\t}\n    \t\t\t// -----------------------------------------------------------\n    \t\t\tList<DataFile> datafiles = new ArrayList<>(); \n\n    \t\t\t// -----------------------------------------------------------\n    \t\t\t// Send it through the ingest service\n    \t\t\t// -----------------------------------------------------------\n    \t\t\ttry {\n\n    \t\t\t\t// Note: A single uploaded file may produce multiple datafiles - \n    \t\t\t\t// for example, multiple files can be extracted from an uncompressed\n    \t\t\t\t// zip file.\n    \t\t\t\t//datafiles = ingestService.createDataFiles(workingVersion, dropBoxStream, fileName, \"application/octet-stream\");\n\n\n    \t\t\t\tdatafiles = FileUtil.createDataFiles(workingVersion, null, fileName, contentType, fullStorageIdentifier, checksumValue, systemConfig);\n    \t\t\t} catch (IOException ex) {\n    \t\t\t\tlogger.log(Level.SEVERE, \"Error during ingest of file {0}\", new Object[]{fileName});\n    \t\t\t}\n\n    \t\t\tif (datafiles == null){\n    \t\t\t\tlogger.log(Level.SEVERE, \"Failed to create DataFile for file {0}\", new Object[]{fileName});\n    \t\t\t}else{    \n    \t\t\t\t// -----------------------------------------------------------\n    \t\t\t\t// Check if there are duplicate files or ingest warnings\n    \t\t\t\t// -----------------------------------------------------------\n    \t\t\t\tuploadWarningMessage = processUploadedFileList(datafiles);\n    \t\t\t}\n    \t\t\tif(!uploadInProgress) {\n    \t\t\t\tlogger.warning(\"Upload in progress cancelled\");\n    \t\t\t\tfor (DataFile newFile : datafiles) {\n    \t\t\t\t\tdeleteTempFile(newFile);\n    \t\t\t\t}\n    \t\t\t}\n    \t\t}\n    \t} catch (IOException e) {\n    \t\tlogger.log(Level.SEVERE, \"Failed to create DataFile for file {0}: {1}\", new Object[]{fileName, e.getMessage()});\n    \t}\n    \tif (localWarningMessage != null) {\n    \t\tif (uploadWarningMessage == null) {\n    \t\t\tuploadWarningMessage = localWarningMessage;\n    \t\t} else {\n    \t\t\tuploadWarningMessage = localWarningMessage.concat(\"; \" + uploadWarningMessage);\n    \t\t}\n    \t}\n    }\n    \n    /**\n     *  After uploading via the site or Dropbox, \n     *  check the list of DataFile objects\n     * @param dFileList \n     */\n    \n    private String dupeFileNamesExisting = null; \n    private String dupeFileNamesNew = null;\n    private boolean multipleDupesExisting = false;\n    private boolean multipleDupesNew = false;\n    private boolean uploadInProgress = false;\n    \n    private String processUploadedFileList(List<DataFile> dFileList) {\n        if (dFileList == null) {\n            return null;\n        }\n\n        DataFile dataFile;\n        String warningMessage = null;\n\n        // NOTE: for native file uploads, the dFileList will only \n        // contain 1 file--method is called for every file even if the UI shows \"simultaneous uploads\"\n        \n        // -----------------------------------------------------------\n        // Iterate through list of DataFile objects\n        // -----------------------------------------------------------\n        for (DataFile dFileList1 : dFileList) {\n            dataFile = dFileList1;\n            // -----------------------------------------------------------\n            // Check for ingest warnings\n            // -----------------------------------------------------------\n            if (dataFile.isIngestProblem()) {\n                if (dataFile.getIngestReportMessage() != null) {\n                    if (warningMessage == null) {\n                        warningMessage = dataFile.getIngestReportMessage();\n                    } else {\n                        warningMessage = warningMessage.concat(\"; \" + dataFile.getIngestReportMessage());\n                    }\n                }\n                dataFile.setIngestDone();\n            }\n\n            // -----------------------------------------------------------\n            // Check for duplicates -- e.g. file is already in the dataset, \n            // or if another file with the same checksum has already been \n            // uploaded.\n            // -----------------------------------------------------------\n            if (isFileAlreadyInDataset(dataFile)) {\n                if (dupeFileNamesExisting == null) {\n                    dupeFileNamesExisting = dataFile.getFileMetadata().getLabel();\n                } else {\n                    dupeFileNamesExisting = dupeFileNamesExisting.concat(\", \" + dataFile.getFileMetadata().getLabel());\n                    multipleDupesExisting = true;\n                }\n                // remove temp file\n                deleteTempFile(dataFile);\n            } else if (isFileAlreadyUploaded(dataFile)) {\n                if (dupeFileNamesNew == null) {\n                    dupeFileNamesNew = dataFile.getFileMetadata().getLabel();\n                } else {\n                    dupeFileNamesNew = dupeFileNamesNew.concat(\", \" + dataFile.getFileMetadata().getLabel());\n                    multipleDupesNew = true;\n                }\n                // remove temp file\n                deleteTempFile(dataFile);\n            } else {\n                // OK, this one is not a duplicate, we want it. \n                // But let's check if its filename is a duplicate of another \n                // file already uploaded, or already in the dataset:\n                dataFile.getFileMetadata().setLabel(duplicateFilenameCheck(dataFile.getFileMetadata()));\n                if (isTemporaryPreviewAvailable(dataFile.getStorageIdentifier(), dataFile.getContentType())) {\n                    dataFile.setPreviewImageAvailable(true);\n                }\n                uploadedFiles.add(dataFile);\n                // We are NOT adding the fileMetadata to the list that is being used\n                // to render the page; we'll do that once we know that all the individual uploads\n                // in this batch (as in, a bunch of drag-and-dropped files) have finished. \n                //fileMetadatas.add(dataFile.getFileMetadata());\n            }\n\n            /*\n             preserved old, pre 4.6 code - mainly as an illustration of how we used to do this. \n            \n            if (!isDuplicate(dataFile.getFileMetadata())) {\n                newFiles.add(dataFile);        // looks good\n                fileMetadatas.add(dataFile.getFileMetadata());\n            } else {\n                if (duplicateFileNames == null) {\n                    duplicateFileNames = dataFile.getFileMetadata().getLabel();\n                } else {\n                    duplicateFileNames = duplicateFileNames.concat(\", \" + dataFile.getFileMetadata().getLabel());\n                    multipleDupes = true;\n                }\n\n                // remove the file from the dataset (since createDataFiles has already linked\n                // it to the dataset!\n                // first, through the filemetadata list, then through tht datafiles list:\n                Iterator<FileMetadata> fmIt = dataset.getEditVersion().getFileMetadatas().iterator();\n                while (fmIt.hasNext()) {\n                    FileMetadata fm = fmIt.next();\n                    if (fm.getId() == null && dataFile.getStorageIdentifier().equals(fm.getDataFile().getStorageIdentifier())) {\n                        fmIt.remove();\n                        break;\n                    }\n                }\n\n                Iterator<DataFile> dfIt = dataset.getFiles().iterator();\n                while (dfIt.hasNext()) {\n                    DataFile dfn = dfIt.next();\n                    if (dfn.getId() == null && dataFile.getStorageIdentifier().equals(dfn.getStorageIdentifier())) {\n                        dfIt.remove();\n                        break;\n                    }\n                }\n            } */\n        }\n        \n        // -----------------------------------------------------------\n        // Format error message for duplicate files\n        // (note the separate messages for the files already in the dataset, \n        // and the newly uploaded ones)\n        // -----------------------------------------------------------\n        if (dupeFileNamesExisting != null) {\n            String duplicateFilesErrorMessage = null;\n            if (multipleDupesExisting) {\n                duplicateFilesErrorMessage =  getBundleString(\"dataset.files.exist\") + dupeFileNamesExisting + getBundleString(\"dataset.file.skip\");\n            } else {\n            \tduplicateFilesErrorMessage = getBundleString(\"dataset.file.exist\") + dupeFileNamesExisting;\n            }\n            if (warningMessage == null) {\n                warningMessage = duplicateFilesErrorMessage;\n            } else {\n                warningMessage = warningMessage.concat(\"; \" + duplicateFilesErrorMessage);\n            }\n        }\n\n        if (dupeFileNamesNew != null) {\n            String duplicateFilesErrorMessage = null;\n            if (multipleDupesNew) {\n            \tduplicateFilesErrorMessage = getBundleString(\"dataset.files.duplicate\") + dupeFileNamesNew + getBundleString(\"dataset.file.skip\");\n            } else {\n            \tduplicateFilesErrorMessage = getBundleString(\"dataset.file.duplicate\") + dupeFileNamesNew + getBundleString(\"dataset.file.skip\");\n            }\n\n            if (warningMessage == null) {\n                warningMessage = duplicateFilesErrorMessage;\n            } else {\n                warningMessage = warningMessage.concat(\"; \" + duplicateFilesErrorMessage);\n            }\n        }\n\n        if (warningMessage != null) {\n            logger.severe(warningMessage);\n            return warningMessage;\n        }\n\n        return null;\n    }\n    \n    private Map<String, String> temporaryThumbnailsMap = new HashMap<>();\n    \n    public boolean isTemporaryPreviewAvailable(String fileSystemId, String mimeType) {\n        if (temporaryThumbnailsMap.get(fileSystemId) != null && !temporaryThumbnailsMap.get(fileSystemId).isEmpty()) {\n            return true;\n        }\n        \n        if (\"\".equals(temporaryThumbnailsMap.get(fileSystemId))) {\n            // we've already looked once - and there's no thumbnail.\n            return false;\n        }\n        \n        String filesRootDirectory = System.getProperty(\"dataverse.files.directory\");\n        if (filesRootDirectory == null || filesRootDirectory.isEmpty()) {\n            filesRootDirectory = \"/tmp/files\";\n        }\n\n        String fileSystemName = filesRootDirectory + \"/temp/\" + fileSystemId;\n        \n        String imageThumbFileName = null;\n        \n        // ATTENTION! TODO: the current version of the method below may not be checking if files are already cached!\n        if (\"application/pdf\".equals(mimeType)) {\n            imageThumbFileName = ImageThumbConverter.generatePDFThumbnailFromFile(fileSystemName, ImageThumbConverter.DEFAULT_THUMBNAIL_SIZE);\n        } else if (mimeType != null && mimeType.startsWith(\"image/\")) {\n            imageThumbFileName = ImageThumbConverter.generateImageThumbnailFromFile(fileSystemName, ImageThumbConverter.DEFAULT_THUMBNAIL_SIZE);\n        }\n        \n        if (imageThumbFileName != null) {\n            File imageThumbFile = new File(imageThumbFileName);\n            if (imageThumbFile.exists()) {\n                String previewAsBase64 = ImageThumbConverter.getImageAsBase64FromFile(imageThumbFile); \n                if (previewAsBase64 != null) {\n                    temporaryThumbnailsMap.put(fileSystemId, previewAsBase64);\n                    return true;\n                } else {\n                    temporaryThumbnailsMap.put(fileSystemId, \"\");\n                }\n            }\n        }\n            \n        return false;\n    }\n    \n    public String getTemporaryPreviewAsBase64(String fileSystemId) {\n        return temporaryThumbnailsMap.get(fileSystemId);\n    }\n\n    private Set<String> fileLabelsExisting = null; \n    \n    private String duplicateFilenameCheck(FileMetadata fileMetadata) {\n        if (fileLabelsExisting == null) {\n            fileLabelsExisting = IngestUtil.existingPathNamesAsSet(workingVersion);\n        }\n        \n        return IngestUtil.duplicateFilenameCheck(fileMetadata, fileLabelsExisting);\n    }\n\n    private  Map<String, Integer> checksumMapOld = null; // checksums of the files already in the dataset\n    private  Map<String, Integer> checksumMapNew = null; // checksums of the new files already uploaded\n    \n    private void initChecksumMap() {\n        checksumMapOld = new HashMap<>();\n\n        Iterator<FileMetadata> fmIt = workingVersion.getFileMetadatas().iterator();\n\n        while (fmIt.hasNext()) {\n            FileMetadata fm = fmIt.next();\n            if (fm.getDataFile() != null && fm.getDataFile().getId() != null) {\n                String chksum = fm.getDataFile().getChecksumValue();\n                if (chksum != null) {\n                    checksumMapOld.put(chksum, 1);\n\n                }\n            }\n        }\n\n    }\n    \n    private boolean isFileAlreadyInDataset(DataFile dataFile) {\n        if (checksumMapOld == null) {\n            initChecksumMap();\n        }\n        \n        String chksum = dataFile.getChecksumValue();\n        \n        return chksum == null ? false : checksumMapOld.get(chksum) != null;\n    }\n    \n    private boolean isFileAlreadyUploaded(DataFile dataFile) {\n        if (checksumMapNew == null) {\n            checksumMapNew = new HashMap<>();\n        }\n        \n        String chksum = dataFile.getChecksumValue();\n        \n        if (chksum == null) {\n            return false;\n        }\n        \n        if (checksumMapNew.get(chksum) != null) {\n            return true;\n        }\n        \n        checksumMapNew.put(chksum, 1);\n        return false;\n    }\n    \n \n    public boolean isLocked() {\n        if (dataset != null) {\n            logger.log(Level.FINE, \"checking lock status of dataset {0}\", dataset.getId());\n            if (dataset.isLocked()) {\n                // refresh the dataset and version, if the current working\n                // version of the dataset is locked:\n            }\n            Dataset lookedupDataset = datasetService.find(dataset.getId());\n            \n            if ( (lookedupDataset!=null) && lookedupDataset.isLocked() ) {\n                logger.fine(\"locked!\");\n                return true;\n            }\n        }\n        return false;\n    }\n    \n    public boolean isThumbnailAvailable(FileMetadata fileMetadata) {\n        // new and optimized logic: \n        // - check download permission here (should be cached - so it's free!)\n        // - only then ask the file service if the thumbnail is available/exists.\n        // the service itself no longer checks download permissions.  \n        if (!fileDownloadHelper.canDownloadFile(fileMetadata)) {\n            return false;\n        }\n\n        return datafileService.isThumbnailAvailable(fileMetadata.getDataFile());\n    }\n    \n\n    \n    private Boolean lockedFromEditsVar;\n    \n    public boolean isLockedFromEdits() {\n        if(null == lockedFromEditsVar ) {\n            try {\n                permissionService.checkEditDatasetLock(dataset, dvRequestService.getDataverseRequest(), new UpdateDatasetVersionCommand(dataset, dvRequestService.getDataverseRequest()));\n                lockedFromEditsVar = false;\n            } catch (IllegalCommandException ex) {\n                lockedFromEditsVar = true;\n            }\n        }\n        return lockedFromEditsVar;\n    }\n    \n    // Methods for edit functions that are performed on one file at a time, \n    // in popups that block the rest of the page:\n    \n    private FileMetadata fileMetadataSelected = null;\n\n    public void  setFileMetadataSelected(FileMetadata fm){\n       setFileMetadataSelected(fm, null); \n    }\n    \n    public void setFileMetadataSelected(FileMetadata fm, String guestbook) {\n\n        fileMetadataSelected = fm;\n        logger.log(Level.FINE, \"set the file for the advanced options popup ({0})\", fileMetadataSelected.getLabel());\n    }\n\n    public FileMetadata getFileMetadataSelected() {\n        if (fileMetadataSelected != null) {\n            logger.log(Level.FINE, \"returning file metadata for the advanced options popup ({0})\", fileMetadataSelected.getLabel());\n        } else {\n            logger.fine(\"file metadata for the advanced options popup is null.\");\n        }\n        return fileMetadataSelected;\n    }\n\n    public void clearFileMetadataSelected() {\n        fileMetadataSelected = null;\n    }\n    \n    public boolean isDesignatedDatasetThumbnail (FileMetadata fileMetadata) {\n        if (fileMetadata != null) {\n            if (fileMetadata.getDataFile() != null) {\n                if (fileMetadata.getDataFile().getId() != null) {\n                    //if (fileMetadata.getDataFile().getOwner() != null) {\n                        if (fileMetadata.getDataFile().equals(dataset.getThumbnailFile())) {\n                            return true;\n                        }\n                    //}\n                }\n            }\n        }\n        return false;\n    }\n    \n    /* \n     * Items for the \"Designated this image as the Dataset thumbnail: \n     */\n    \n    private FileMetadata fileMetadataSelectedForThumbnailPopup = null; \n\n    /**\n     * @param fm\n     * @todo For consistency, we should disallow users from setting the\n     * thumbnail to a restricted file. We enforce this rule in the newer\n     * workflow in dataset-widgets.xhtml. The logic to show the \"Set Thumbnail\"\n     * button is in editFilesFragment.xhtml and it would be nice to move it to\n     * Java since it's getting long and a bit complicated.\n     */\n    public void  setFileMetadataSelectedForThumbnailPopup(FileMetadata fm){\n       fileMetadataSelectedForThumbnailPopup = fm; \n       alreadyDesignatedAsDatasetThumbnail = getUseAsDatasetThumbnail();\n\n    }\n    \n    public FileMetadata getFileMetadataSelectedForThumbnailPopup() {\n        return fileMetadataSelectedForThumbnailPopup;\n    }\n    \n    public void clearFileMetadataSelectedForThumbnailPopup() {\n        fileMetadataSelectedForThumbnailPopup = null;\n    }\n    \n    private boolean alreadyDesignatedAsDatasetThumbnail = false; \n    \n    public boolean getUseAsDatasetThumbnail() {\n\n        return isDesignatedDatasetThumbnail(fileMetadataSelectedForThumbnailPopup);\n    }\n\n    public void setUseAsDatasetThumbnail(boolean useAsThumbnail) {\n        if (fileMetadataSelectedForThumbnailPopup != null) {\n            if (fileMetadataSelectedForThumbnailPopup.getDataFile() != null) {\n                if (useAsThumbnail) {\n                    dataset.setThumbnailFile(fileMetadataSelectedForThumbnailPopup.getDataFile());\n                } else if (getUseAsDatasetThumbnail()) {\n                    dataset.setThumbnailFile(null);\n                }\n            }\n        }\n    }\n\n    public void saveAsDesignatedThumbnail() {\n        logger.fine(\"saving as the designated thumbnail\");\n        // We don't need to do anything specific to save this setting, because\n        // the setUseAsDatasetThumbnail() method, above, has already updated the\n        // file object appropriately. \n        // However, once the \"save\" button is pressed, we want to show a success message, if this is \n        // a new image has been designated as such:\n        if (getUseAsDatasetThumbnail() && !alreadyDesignatedAsDatasetThumbnail) {\n            String successMessage = getBundleString(\"file.assignedDataverseImage.success\");\n            logger.fine(successMessage);\n            successMessage = successMessage.replace(\"{0}\", fileMetadataSelectedForThumbnailPopup.getLabel());\n            JsfHelper.addFlashMessage(successMessage);\n        }\n\n        // And reset the selected fileMetadata:\n        fileMetadataSelectedForThumbnailPopup = null;\n    }\n\n    public void deleteDatasetLogoAndUseThisDataFileAsThumbnailInstead() {\n        logger.log(Level.FINE, \"For dataset id {0} the current thumbnail is from a dataset logo rather than a dataset file, blowing away the logo and using this FileMetadata id instead: {1}\", new Object[]{dataset.getId(), fileMetadataSelectedForThumbnailPopup});\n        /**\n         * @todo Rather than deleting and merging right away, try to respect how\n         * this page seems to stage actions and giving the user a chance to\n         * review before clicking \"Save Changes\".\n         */\n        try {\n            DatasetThumbnail datasetThumbnail = commandEngine.submit(new UpdateDatasetThumbnailCommand(dvRequestService.getDataverseRequest(), dataset, UpdateDatasetThumbnailCommand.UserIntent.setDatasetFileAsThumbnail, fileMetadataSelectedForThumbnailPopup.getDataFile().getId(), null));\n            // look up the dataset again because the UpdateDatasetThumbnailCommand mutates (merges) the dataset\n            dataset = datasetService.find(dataset.getId());\n        } catch (CommandException ex) {\n            String error = \"Problem setting thumbnail for dataset id \" + dataset.getId() + \".: \" + ex;\n            // show this error to the user?\n            logger.info(error);\n        }\n    }\n\n    public boolean isThumbnailIsFromDatasetLogoRatherThanDatafile() {\n        DatasetThumbnail datasetThumbnail = dataset.getDatasetThumbnail();\n        return datasetThumbnail != null && !datasetThumbnail.isFromDataFile();\n    }\n\n    /* \n     * Items for the \"Tags (Categories)\" popup.\n     *\n     */\n    private FileMetadata fileMetadataSelectedForTagsPopup = null; \n\n    public void  setFileMetadataSelectedForTagsPopup(FileMetadata fm){\n        fileMetadataSelectedForTagsPopup = fm;\n    }\n    \n    public FileMetadata getFileMetadataSelectedForTagsPopup() {\n        return fileMetadataSelectedForTagsPopup;\n    }\n    \n    public void clearFileMetadataSelectedForTagsPopup() {\n        fileMetadataSelectedForTagsPopup = null;\n    }\n    \n    /*\n     * 1. Tabular File Tags: \n     */\n    \n    private List<String> tabFileTags = null;\n\n    public List<String> getTabFileTags() {\n        if (tabFileTags == null) {\n            tabFileTags = DataFileTag.listTags();\n        }\n        return tabFileTags;\n    }\n\n    public void setTabFileTags(List<String> tabFileTags) {\n        this.tabFileTags = tabFileTags;\n    }\n    \n    private String[] selectedTabFileTags = {};\n\n    public String[] getSelectedTabFileTags() {\n        return selectedTabFileTags;\n    }\n\n    public void setSelectedTabFileTags(String[] selectedTabFileTags) {\n        this.selectedTabFileTags = selectedTabFileTags;\n    }\n\n    private String[] selectedTags = {};\n    \n    public void refreshTagsPopUp(FileMetadata fm){\n        setFileMetadataSelectedForTagsPopup(fm);\n        refreshCategoriesByName();\n        refreshTabFileTagsByName();\n    }\n    \n    private List<String> tabFileTagsByName;\n\n    public List<String> getTabFileTagsByName() {\n        return tabFileTagsByName;\n    }\n\n    public void setTabFileTagsByName(List<String> tabFileTagsByName) {\n        this.tabFileTagsByName = tabFileTagsByName;\n    }\n    \n    private void refreshTabFileTagsByName() {\n        tabFileTagsByName = new ArrayList<>();\n        if (fileMetadataSelectedForTagsPopup.getDataFile().getTags() != null) {\n            for (int i = 0; i < fileMetadataSelectedForTagsPopup.getDataFile().getTags().size(); i++) {\n                tabFileTagsByName.add(fileMetadataSelectedForTagsPopup.getDataFile().getTags().get(i).getTypeLabel());\n            }\n        }\n        refreshSelectedTabFileTags();\n    }\n\n    private void refreshSelectedTabFileTags() {\n        selectedTabFileTags = null;\n        selectedTabFileTags = new String[0];\n        if (tabFileTagsByName.size() > 0) {\n            selectedTabFileTags = new String[tabFileTagsByName.size()];\n            for (int i = 0; i < tabFileTagsByName.size(); i++) {\n                selectedTabFileTags[i] = tabFileTagsByName.get(i);\n            }\n        }\n        Arrays.sort(selectedTabFileTags);\n    }\n    \n    private void refreshCategoriesByName(){\n        categoriesByName= new ArrayList<>();\n        for (String category: dataset.getCategoriesByName() ){\n            categoriesByName.add(category);\n        }\n        refreshSelectedTags();\n    }\n    \n    \n    private List<String> categoriesByName;\n\n    public List<String> getCategoriesByName() {\n        return categoriesByName;\n    }\n\n    public void setCategoriesByName(List<String> categoriesByName) {\n        this.categoriesByName = categoriesByName;\n    }\n    \n    private void refreshSelectedTags() {\n        selectedTags = null;\n        selectedTags = new String[0];\n        List<String> selectedCategoriesByName = new ArrayList<>();\n\n        if (fileMetadataSelectedForTagsPopup.getCategories() != null) {\n            for (int i = 0; i < fileMetadataSelectedForTagsPopup.getCategories().size(); i++) {\n                if (!selectedCategoriesByName.contains(fileMetadataSelectedForTagsPopup.getCategories().get(i).getName())) {\n                    selectedCategoriesByName.add(fileMetadataSelectedForTagsPopup.getCategories().get(i).getName());\n                }\n            }\n        }\n\n        if (selectedCategoriesByName.size() > 0) {\n            selectedTags = new String[selectedCategoriesByName.size()];\n            for (int i = 0; i < selectedCategoriesByName.size(); i++) {\n                selectedTags[i] = selectedCategoriesByName.get(i);\n            }\n        }\n        Arrays.sort(selectedTags);\n    }\n\n    public String[] getSelectedTags() {\n        return selectedTags;\n    }\n\n    public void setSelectedTags(String[] selectedTags) {\n        this.selectedTags = selectedTags;\n    }\n    \n\n\n    /*\n     * \"File Tags\" (aka \"File Categories\"): \n    */\n    \n    private String newCategoryName = null;\n\n    public String getNewCategoryName() {\n        return newCategoryName;\n    }\n\n    public void setNewCategoryName(String newCategoryName) {\n        this.newCategoryName = newCategoryName;\n    }\n       \n    public String saveNewCategory() {\n       \n        if (newCategoryName != null && !newCategoryName.isEmpty()) {\n            categoriesByName.add(newCategoryName);\n        }\n        //Now increase size of selectedTags and add new category\n        String[] temp = new String[selectedTags.length + 1];\n        System.arraycopy(selectedTags, 0, temp, 0, selectedTags.length);\n        selectedTags = temp;\n        selectedTags[selectedTags.length - 1] = newCategoryName;\n        //Blank out added category\n        newCategoryName = \"\";\n        return \"\";\n    }\n\n    /* This method handles saving both \"tabular file tags\" and \n     * \"file categories\" (which are also considered \"tags\" in 4.0)\n    */\n    public void saveFileTagsAndCategories() {\n        if (fileMetadataSelectedForTagsPopup == null) {\n            logger.fine(\"No FileMetadata selected for the categories popup\");\n            return; \n        }\n        // 1. File categories:\n        /*\n        In order to get the cancel button to work we had to separate the selected tags \n        from the file metadata and re-add them on save\n        \n        */\n        \n        fileMetadataSelectedForTagsPopup.setCategories(new ArrayList<>());\n        \n        // New, custom file category (if specified):\n        if (newCategoryName != null) {\n            logger.fine(\"Adding new category, \" + newCategoryName + \" for file \" + fileMetadataSelectedForTagsPopup.getLabel());\n            fileMetadataSelectedForTagsPopup.addCategoryByName(newCategoryName);\n        } else {\n            logger.fine(\"no category specified\");\n        }\n        newCategoryName = null;\n        \n        // File Categories selected from the list of existing categories: \n        if (selectedTags != null) {\n            for (String selectedTag : selectedTags) {\n                \n                fileMetadataSelectedForTagsPopup.addCategoryByName(selectedTag);\n            }\n        }        \n                \n        // 2. Tabular DataFile Tags: \n        \n        if (fileMetadataSelectedForTagsPopup.getDataFile() != null && tabularDataTagsUpdated && selectedTabFileTags != null) {\n            fileMetadataSelectedForTagsPopup.getDataFile().setTags(null);\n            for (String selectedTabFileTag : selectedTabFileTags) {\n                DataFileTag tag = new DataFileTag();\n                try {\n                    tag.setTypeByLabel(selectedTabFileTag);\n                    tag.setDataFile(fileMetadataSelectedForTagsPopup.getDataFile());\n                    fileMetadataSelectedForTagsPopup.getDataFile().addTag(tag);\n\n                } catch (IllegalArgumentException iax) {\n                    // ignore \n                }\n            }\n        }\n        \n        fileMetadataSelectedForTagsPopup = null;\n\n    }\n    \n    public void handleFileCategoriesSelection(final AjaxBehaviorEvent event) {\n        if (selectedTags != null) {\n            selectedTags = selectedTags.clone();\n        }\n    }\n    \n    public void handleTabularTagsSelection(final AjaxBehaviorEvent event) {\n        tabularDataTagsUpdated = true;\n    }\n        \n    /* \n     * Items for the \"Advanced (Ingest) Options\" popup. \n     * \n     */\n    private FileMetadata fileMetadataSelectedForIngestOptionsPopup = null; \n\n    public void  setFileMetadataSelectedForIngestOptionsPopup(FileMetadata fm){\n       fileMetadataSelectedForIngestOptionsPopup = fm; \n    }\n    \n    public FileMetadata getFileMetadataSelectedForIngestOptionsPopup() {\n        return fileMetadataSelectedForIngestOptionsPopup;\n    }\n    \n    public void clearFileMetadataSelectedForIngestOptionsPopup() {\n        fileMetadataSelectedForIngestOptionsPopup = null;\n    }\n    \n    private String ingestLanguageEncoding = null;\n\n    public String getIngestLanguageEncoding() {\n        if (ingestLanguageEncoding == null) {\n            return BundleUtil.getStringFromBundle(\"editdatafilepage.defaultLanguageEncoding\");\n        }\n        return ingestLanguageEncoding;\n    }\n\n    public void setIngestLanguageEncoding(String ingestLanguageEncoding) {\n        this.ingestLanguageEncoding = ingestLanguageEncoding;\n    }\n\n    public void setIngestEncoding(String ingestEncoding) {\n        ingestLanguageEncoding = ingestEncoding;\n    }\n\n    private String savedLabelsTempFile = null;\n\n    public void handleLabelsFileUpload(FileUploadEvent event) {\n        logger.fine(\"entering handleUpload method.\");\n        UploadedFile file = event.getFile();\n\n        if (file != null) {\n\n            InputStream uploadStream = null;\n            try {\n                uploadStream = file.getInputStream();\n            } catch (IOException ioex) {\n                logger.info(\"the file \" + file.getFileName() + \" failed to upload!\");\n                List<String> args = Arrays.asList(file.getFileName());\n                String msg = BundleUtil.getStringFromBundle(\"dataset.file.uploadFailure.detailmsg\", args);\n                FacesMessage message = new FacesMessage(FacesMessage.SEVERITY_WARN, BundleUtil.getStringFromBundle(\"dataset.file.uploadFailure\"), msg);\n                FacesContext.getCurrentInstance().addMessage(null, message);\n                return;\n            }\n\n            savedLabelsTempFile = saveTempFile(uploadStream);\n\n            logger.fine(file.getFileName() + \" is successfully uploaded.\");\n            List<String> args = Arrays.asList(file.getFileName());\n            FacesMessage message = new FacesMessage(BundleUtil.getStringFromBundle(\"dataset.file.upload\", args));\n            FacesContext.getCurrentInstance().addMessage(null, message);\n        }\n\n        // process file (i.e., just save it in a temp location; for now):\n    }\n\n    private String saveTempFile(InputStream input) {\n        if (input == null) {\n            return null;\n        }\n        byte[] buffer = new byte[8192];\n        int bytesRead = 0;\n        File labelsFile = null;\n        FileOutputStream output = null;\n        try {\n            labelsFile = File.createTempFile(\"tempIngestLabels.\", \".txt\");\n            output = new FileOutputStream(labelsFile);\n            while ((bytesRead = input.read(buffer)) > -1) {\n                output.write(buffer, 0, bytesRead);\n            }\n        } catch (IOException ioex) {\n            return null;\n        } finally {\n            IOUtils.closeQuietly(input);\n            IOUtils.closeQuietly(output);\n        }\n        if (labelsFile != null) {\n            return labelsFile.getAbsolutePath();\n        }\n        return null;\n    }\n\n    public void saveAdvancedOptions() {\n\n        // Language encoding for SPSS SAV (and, possibly, other tabular ingests:) \n        if (ingestLanguageEncoding != null) {\n            if (fileMetadataSelectedForIngestOptionsPopup != null && fileMetadataSelectedForIngestOptionsPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest() == null) {\n                    IngestRequest ingestRequest = new IngestRequest();\n                    ingestRequest.setDataFile(fileMetadataSelectedForIngestOptionsPopup.getDataFile());\n                    fileMetadataSelectedForIngestOptionsPopup.getDataFile().setIngestRequest(ingestRequest);\n\n                }\n                fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest().setTextEncoding(ingestLanguageEncoding);\n            }\n        }\n        ingestLanguageEncoding = null;\n\n        // Extra labels for SPSS POR (and, possibly, other tabular ingests:)\n        // (we are adding this parameter to the IngestRequest now, instead of back\n        // when it was uploaded. This is because we want the user to be able to \n        // hit cancel and bail out, until they actually click 'save' in the \n        // \"advanced options\" popup) -- L.A. 4.0 beta 11\n        if (savedLabelsTempFile != null) {\n            if (fileMetadataSelectedForIngestOptionsPopup != null && fileMetadataSelectedForIngestOptionsPopup.getDataFile() != null) {\n                if (fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest() == null) {\n                    IngestRequest ingestRequest = new IngestRequest();\n                    ingestRequest.setDataFile(fileMetadataSelectedForIngestOptionsPopup.getDataFile());\n                    fileMetadataSelectedForIngestOptionsPopup.getDataFile().setIngestRequest(ingestRequest);\n                }\n                fileMetadataSelectedForIngestOptionsPopup.getDataFile().getIngestRequest().setLabelsFile(savedLabelsTempFile);\n            }\n        }\n        savedLabelsTempFile = null;\n\n        fileMetadataSelectedForIngestOptionsPopup = null;\n    }\n\n    public boolean rsyncUploadSupported() {\n    \t// ToDo - rsync was written before multiple store support and currently is hardcoded to use the \"s3\" store. \n    \t// When those restrictions are lifted/rsync can be configured per store, this test should check that setting\n    \t// instead of testing for the 's3\" store.\n    \treturn settingsWrapper.isRsyncUpload() && dataset.getDataverseContext().getEffectiveStorageDriverId().equals(\"s3\");\n    }\n    \n    \n    private void populateFileMetadatas() {\n        fileMetadatas = new ArrayList<>();\n        if (selectedFileIdsList == null || selectedFileIdsList.isEmpty()) {\n            return;\n        }\n\n        for (FileMetadata fmd : workingVersion.getFileMetadatas()) {\n            for (Long id : selectedFileIdsList) {\n                if (id.intValue() == fmd.getDataFile().getId().intValue()) {\n                    fileMetadatas.add(fmd);\n                }\n            }\n        }       \n    }    \n}\n", "idx": 28, "id": 41998, "msg": "", "proj": "IQSS-dataverse", "lang": "java"}
{"patch": "@@ -0,0 +1,39 @@\n+require 'spec_helper'\n+\n+describe Travis::Build do\n+  describe 'by_lang' do\n+    it 'maps anything java-ish to PureJava' do\n+      %w(java-woot java7 javaZOMBIES).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::PureJava)\n+      end\n+    end\n+\n+    it 'maps c++, cpp, and cplusplus to Cpp' do\n+      %w(c++ cpp cplusplus).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Cpp)\n+      end\n+    end\n+\n+    it 'maps objective-c to ObjectiveC' do\n+      expect(subject.by_lang('objective-c')).to eq(Travis::Build::Script::ObjectiveC)\n+    end\n+\n+    it 'maps bash, sh, and shell to Generic' do\n+      %w(bash sh shell).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Generic)\n+      end\n+    end\n+\n+    it 'maps known languages to their implementations' do\n+      %w(android c cpp clojure erlang go groovy haskell node_js perl php python rust scala).each do |lang|\n+        expect(subject.by_lang(lang)).to_not eq(Travis::Build::Script::Ruby)\n+      end\n+    end\n+\n+    it 'maps unknown languages to Ruby' do\n+      %w(brainfudge objective-d rubby).each do |lang|\n+        expect(subject.by_lang(lang)).to eq(Travis::Build::Script::Ruby)\n+      end\n+    end\n+  end\n+end", "y": 1, "oldf": "", "idx": 1, "id": 12061, "msg": "IMHO positive tests will be better. Also `cpp` is tested before.", "proj": "travis-ci-travis-build", "lang": "rb"}
{"patch": "@@ -775,7 +775,7 @@ TEST (rpc, wallet_representative_set_force)\n \t}\n \tASSERT_EQ (200, response.status);\n \t{\n-\t\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n+\t\tauto transaction (system.nodes[0]->wallets.tx_begin_read ());\n \t\tASSERT_EQ (key.pub, system.nodes[0]->wallets.items.begin ()->second->store.representative (transaction));\n \t}\n \tnano::account representative (0);", "y": 0, "oldf": "#include <algorithm>\n#include <boost/beast.hpp>\n#include <gtest/gtest.h>\n#include <nano/core_test/testutil.hpp>\n#include <nano/lib/ipc.hpp>\n#include <nano/lib/rpcconfig.hpp>\n#include <nano/lib/timer.hpp>\n#include <nano/node/ipc.hpp>\n#include <nano/node/json_handler.hpp>\n#include <nano/node/node_rpc_config.hpp>\n#include <nano/node/testing.hpp>\n#include <nano/rpc/rpc.hpp>\n#include <nano/rpc/rpc_request_processor.hpp>\n\nusing namespace std::chrono_literals;\n\nnamespace\n{\nclass test_response\n{\npublic:\n\ttest_response (boost::property_tree::ptree const & request_a, boost::asio::io_context & io_ctx) :\n\trequest (request_a),\n\tsock (io_ctx)\n\t{\n\t}\n\n\ttest_response (boost::property_tree::ptree const & request_a, uint16_t port, boost::asio::io_context & io_ctx) :\n\trequest (request_a),\n\tsock (io_ctx)\n\t{\n\t\trun (port);\n\t}\n\n\tvoid run (uint16_t port)\n\t{\n\t\tsock.async_connect (nano::tcp_endpoint (boost::asio::ip::address_v6::loopback (), port), [this](boost::system::error_code const & ec) {\n\t\t\tif (!ec)\n\t\t\t{\n\t\t\t\tstd::stringstream ostream;\n\t\t\t\tboost::property_tree::write_json (ostream, request);\n\t\t\t\treq.method (boost::beast::http::verb::post);\n\t\t\t\treq.target (\"/\");\n\t\t\t\treq.version (11);\n\t\t\t\tostream.flush ();\n\t\t\t\treq.body () = ostream.str ();\n\t\t\t\treq.prepare_payload ();\n\t\t\t\tboost::beast::http::async_write (sock, req, [this](boost::system::error_code const & ec, size_t bytes_transferred) {\n\t\t\t\t\tif (!ec)\n\t\t\t\t\t{\n\t\t\t\t\t\tboost::beast::http::async_read (sock, sb, resp, [this](boost::system::error_code const & ec, size_t bytes_transferred) {\n\t\t\t\t\t\t\tif (!ec)\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tstd::stringstream body (resp.body ());\n\t\t\t\t\t\t\t\ttry\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tboost::property_tree::read_json (body, json);\n\t\t\t\t\t\t\t\t\tstatus = 200;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t\tcatch (std::exception &)\n\t\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\t\tstatus = 500;\n\t\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\t}\n\t\t\t\t\t\t\telse\n\t\t\t\t\t\t\t{\n\t\t\t\t\t\t\t\tstatus = 400;\n\t\t\t\t\t\t\t};\n\t\t\t\t\t\t});\n\t\t\t\t\t}\n\t\t\t\t\telse\n\t\t\t\t\t{\n\t\t\t\t\t\tstatus = 600;\n\t\t\t\t\t}\n\t\t\t\t});\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t\tstatus = 400;\n\t\t\t}\n\t\t});\n\t}\n\tboost::property_tree::ptree const & request;\n\tboost::asio::ip::tcp::socket sock;\n\tboost::property_tree::ptree json;\n\tboost::beast::flat_buffer sb;\n\tboost::beast::http::request<boost::beast::http::string_body> req;\n\tboost::beast::http::response<boost::beast::http::string_body> resp;\n\tstd::atomic<int> status{ 0 };\n};\n\nvoid enable_ipc_transport_tcp (nano::ipc::ipc_config_tcp_socket & transport_tcp, uint16_t ipc_port)\n{\n\ttransport_tcp.enabled = true;\n\ttransport_tcp.port = ipc_port;\n}\n\nvoid enable_ipc_transport_tcp (nano::ipc::ipc_config_tcp_socket & transport_tcp)\n{\n\tstatic nano::network_constants network_constants;\n\tenable_ipc_transport_tcp (transport_tcp, network_constants.default_ipc_port);\n}\n\nvoid reset_confirmation_height (nano::block_store & store, nano::account const & account)\n{\n\tauto transaction = store.tx_begin_write ();\n\tnano::account_info account_info;\n\tstore.account_get (transaction, account, account_info);\n\taccount_info.confirmation_height = 0;\n\tstore.account_put (transaction, account, account_info);\n}\n\nvoid check_block_response_count (nano::system & system, nano::rpc & rpc, boost::property_tree::ptree & request, uint64_t size_count)\n{\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (size_count, response.json.get_child (\"blocks\").front ().second.size ());\n}\n}\n\nTEST (rpc, account_balance)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_balance\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string balance_text (response.json.get<std::string> (\"balance\"));\n\tASSERT_EQ (\"340282366920938463463374607431768211455\", balance_text);\n\tstd::string pending_text (response.json.get<std::string> (\"pending\"));\n\tASSERT_EQ (\"0\", pending_text);\n}\n\nTEST (rpc, account_block_count)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_block_count\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string block_count_text (response.json.get<std::string> (\"block_count\"));\n\tASSERT_EQ (\"1\", block_count_text);\n}\n\nTEST (rpc, account_create)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_create\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response0 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response0.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response0.status);\n\tauto account_text0 (response0.json.get<std::string> (\"account\"));\n\tnano::uint256_union account0;\n\tASSERT_FALSE (account0.decode_account (account_text0));\n\tASSERT_TRUE (system.wallet (0)->exists (account0));\n\tuint64_t max_index (std::numeric_limits<uint32_t>::max ());\n\trequest.put (\"index\", max_index);\n\ttest_response response1 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto account_text1 (response1.json.get<std::string> (\"account\"));\n\tnano::uint256_union account1;\n\tASSERT_FALSE (account1.decode_account (account_text1));\n\tASSERT_TRUE (system.wallet (0)->exists (account1));\n\trequest.put (\"index\", max_index + 1);\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (response2.json.get<std::string> (\"error\"), \"Invalid index\");\n}\n\nTEST (rpc, account_weight)\n{\n\tnano::keypair key;\n\tnano::system system (24000, 1);\n\tnano::block_hash latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::change_block block (latest, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tASSERT_EQ (nano::process_result::progress, node1.process (block).code);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_weight\");\n\trequest.put (\"account\", key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string balance_text (response.json.get<std::string> (\"weight\"));\n\tASSERT_EQ (\"340282366920938463463374607431768211455\", balance_text);\n}\n\nTEST (rpc, wallet_contains)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tnode->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_contains\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string exists_text (response.json.get<std::string> (\"exists\"));\n\tASSERT_EQ (\"1\", exists_text);\n}\n\nTEST (rpc, wallet_doesnt_contain)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tnode->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_contains\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string exists_text (response.json.get<std::string> (\"exists\"));\n\tASSERT_EQ (\"0\", exists_text);\n}\n\nTEST (rpc, validate_account_number)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"validate_account_number\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tstd::string exists_text (response.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (\"1\", exists_text);\n}\n\nTEST (rpc, validate_account_invalid)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tstd::string account;\n\tnano::test_genesis_key.pub.encode_account (account);\n\taccount[0] ^= 0x1;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"validate_account_number\");\n\trequest.put (\"account\", account);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string exists_text (response.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (\"0\", exists_text);\n}\n\nTEST (rpc, send)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"send\");\n\trequest.put (\"source\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"destination\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"amount\", \"100\");\n\tsystem.deadline_set (10s);\n\tboost::thread thread2 ([&system]() {\n\t\twhile (system.nodes[0]->balance (nano::test_genesis_key.pub) == nano::genesis_amount)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t});\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string block_text (response.json.get<std::string> (\"block\"));\n\tnano::block_hash block;\n\tASSERT_FALSE (block.decode_hex (block_text));\n\tASSERT_TRUE (node->ledger.block_exists (block));\n\tASSERT_EQ (node->latest (nano::test_genesis_key.pub), block);\n\tthread2.join ();\n}\n\nTEST (rpc, send_fail)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tnode->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"send\");\n\trequest.put (\"source\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"destination\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"amount\", \"100\");\n\tstd::atomic<bool> done (false);\n\tsystem.deadline_set (10s);\n\tboost::thread thread2 ([&system, &done]() {\n\t\twhile (!done)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t});\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tdone = true;\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), \"Account not found in wallet\");\n\tthread2.join ();\n}\n\nTEST (rpc, send_work)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"send\");\n\trequest.put (\"source\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"destination\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"amount\", \"100\");\n\trequest.put (\"work\", \"1\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (10s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), \"Invalid work\");\n\trequest.erase (\"work\");\n\trequest.put (\"work\", nano::to_string_hex (system.nodes[0]->work_generate_blocking (system.nodes[0]->latest (nano::test_genesis_key.pub))));\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (10s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tstd::string block_text (response2.json.get<std::string> (\"block\"));\n\tnano::block_hash block;\n\tASSERT_FALSE (block.decode_hex (block_text));\n\tASSERT_TRUE (system.nodes[0]->ledger.block_exists (block));\n\tASSERT_EQ (system.nodes[0]->latest (nano::test_genesis_key.pub), block);\n}\n\nTEST (rpc, send_idempotent)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"send\");\n\trequest.put (\"source\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"destination\", nano::account (0).to_account ());\n\trequest.put (\"amount\", (nano::genesis_amount - (nano::genesis_amount / 4)).convert_to<std::string> ());\n\trequest.put (\"id\", \"123abc\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string block_text (response.json.get<std::string> (\"block\"));\n\tnano::block_hash block;\n\tASSERT_FALSE (block.decode_hex (block_text));\n\tASSERT_TRUE (system.nodes[0]->ledger.block_exists (block));\n\tASSERT_EQ (system.nodes[0]->balance (nano::test_genesis_key.pub), nano::genesis_amount / 4);\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (\"\", response2.json.get<std::string> (\"error\", \"\"));\n\tASSERT_EQ (block_text, response2.json.get<std::string> (\"block\"));\n\tASSERT_EQ (system.nodes[0]->balance (nano::test_genesis_key.pub), nano::genesis_amount / 4);\n\trequest.erase (\"id\");\n\trequest.put (\"id\", \"456def\");\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (response3.json.get<std::string> (\"error\"), \"Insufficient balance\");\n}\n\nTEST (rpc, stop)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"stop\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t};\n}\n\nTEST (rpc, wallet_add)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::keypair key1;\n\tstd::string key_text;\n\tkey1.prv.data.encode_hex (key_text);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_add\");\n\trequest.put (\"key\", key_text);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"account\"));\n\tASSERT_EQ (account_text1, key1.pub.to_account ());\n\tASSERT_TRUE (system.wallet (0)->exists (key1.pub));\n}\n\nTEST (rpc, wallet_password_valid)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"password_valid\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (account_text1, \"1\");\n}\n\nTEST (rpc, wallet_password_change)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"password_change\");\n\trequest.put (\"password\", \"test\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"changed\"));\n\tASSERT_EQ (account_text1, \"1\");\n\tauto transaction (system.wallet (0)->wallets.tx_begin (true));\n\tASSERT_TRUE (system.wallet (0)->store.valid_password (transaction));\n\tASSERT_TRUE (system.wallet (0)->enter_password (transaction, \"\"));\n\tASSERT_FALSE (system.wallet (0)->store.valid_password (transaction));\n\tASSERT_FALSE (system.wallet (0)->enter_password (transaction, \"test\"));\n\tASSERT_TRUE (system.wallet (0)->store.valid_password (transaction));\n}\n\nTEST (rpc, wallet_password_enter)\n{\n\tnano::system system (24000, 1);\n\tnano::raw_key password_l;\n\tpassword_l.data.clear ();\n\tsystem.deadline_set (10s);\n\twhile (password_l.data == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t\tsystem.wallet (0)->store.password.value (password_l);\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"password_enter\");\n\trequest.put (\"password\", \"\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (account_text1, \"1\");\n}\n\nTEST (rpc, wallet_representative)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_representative\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"representative\"));\n\tASSERT_EQ (account_text1, nano::genesis_account.to_account ());\n}\n\nTEST (rpc, wallet_representative_set)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\tnano::keypair key;\n\trequest.put (\"action\", \"wallet_representative_set\");\n\trequest.put (\"representative\", key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\tASSERT_EQ (key.pub, system.nodes[0]->wallets.items.begin ()->second->store.representative (transaction));\n}\n\nTEST (rpc, wallet_representative_set_force)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\tnano::keypair key;\n\trequest.put (\"action\", \"wallet_representative_set\");\n\trequest.put (\"representative\", key.pub.to_account ());\n\trequest.put (\"update_existing_accounts\", true);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\t{\n\t\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\t\tASSERT_EQ (key.pub, system.nodes[0]->wallets.items.begin ()->second->store.representative (transaction));\n\t}\n\tnano::account representative (0);\n\twhile (representative != key.pub)\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin_read ());\n\t\tnano::account_info info;\n\t\tif (!system.nodes[0]->store.account_get (transaction, nano::test_genesis_key.pub, info))\n\t\t{\n\t\t\tauto block (system.nodes[0]->store.block_get (transaction, info.rep_block));\n\t\t\tassert (block != nullptr);\n\t\t\trepresentative = block->representative ();\n\t\t}\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, account_list)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::keypair key2;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key2.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"account_list\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & accounts_node (response.json.get_child (\"accounts\"));\n\tstd::vector<nano::uint256_union> accounts;\n\tfor (auto i (accounts_node.begin ()), j (accounts_node.end ()); i != j; ++i)\n\t{\n\t\tauto account (i->second.get<std::string> (\"\"));\n\t\tnano::uint256_union number;\n\t\tASSERT_FALSE (number.decode_account (account));\n\t\taccounts.push_back (number);\n\t}\n\tASSERT_EQ (2, accounts.size ());\n\tfor (auto i (accounts.begin ()), j (accounts.end ()); i != j; ++i)\n\t{\n\t\tASSERT_TRUE (system.wallet (0)->exists (*i));\n\t}\n}\n\nTEST (rpc, wallet_key_valid)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_key_valid\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string exists_text (response.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (\"1\", exists_text);\n}\n\nTEST (rpc, wallet_create)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_create\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string wallet_text (response.json.get<std::string> (\"wallet\"));\n\tnano::uint256_union wallet_id;\n\tASSERT_FALSE (wallet_id.decode_hex (wallet_text));\n\tASSERT_NE (system.nodes[0]->wallets.items.end (), system.nodes[0]->wallets.items.find (wallet_id));\n}\n\nTEST (rpc, wallet_create_seed)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair seed;\n\tnano::raw_key prv;\n\tnano::deterministic_key (seed.pub, 0, prv.data);\n\tauto pub (nano::pub_key (prv.data));\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_create\");\n\trequest.put (\"seed\", seed.pub.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string wallet_text (response.json.get<std::string> (\"wallet\"));\n\tnano::uint256_union wallet_id;\n\tASSERT_FALSE (wallet_id.decode_hex (wallet_text));\n\tauto existing (system.nodes[0]->wallets.items.find (wallet_id));\n\tASSERT_NE (system.nodes[0]->wallets.items.end (), existing);\n\t{\n\t\tauto transaction (system.nodes[0]->wallets.tx_begin_read ());\n\t\tnano::raw_key seed0;\n\t\texisting->second->store.seed (seed0, transaction);\n\t\tASSERT_EQ (seed.pub, seed0.data);\n\t}\n\tauto account_text (response.json.get<std::string> (\"last_restored_account\"));\n\tnano::uint256_union account;\n\tASSERT_FALSE (account.decode_account (account_text));\n\tASSERT_TRUE (existing->second->exists (account));\n\tASSERT_EQ (pub, account);\n\tASSERT_EQ (\"1\", response.json.get<std::string> (\"restored_count\"));\n}\n\nTEST (rpc, wallet_export)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_export\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string wallet_json (response.json.get<std::string> (\"json\"));\n\tbool error (false);\n\tauto transaction (system.nodes[0]->wallets.tx_begin (true));\n\tnano::kdf kdf;\n\tnano::wallet_store store (error, kdf, transaction, nano::genesis_account, 1, \"0\", wallet_json);\n\tASSERT_FALSE (error);\n\tASSERT_TRUE (store.exists (transaction, nano::test_genesis_key.pub));\n}\n\nTEST (rpc, wallet_destroy)\n{\n\tnano::system system (24000, 1);\n\tauto wallet_id (system.nodes[0]->wallets.items.begin ()->first);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_destroy\");\n\trequest.put (\"wallet\", wallet_id.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (system.nodes[0]->wallets.items.end (), system.nodes[0]->wallets.items.find (wallet_id));\n}\n\nTEST (rpc, account_move)\n{\n\tnano::system system (24000, 1);\n\tauto wallet_id (system.nodes[0]->wallets.items.begin ()->first);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tauto destination (system.wallet (0));\n\tnano::keypair key;\n\tdestination->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair source_id;\n\tauto source (system.nodes[0]->wallets.create (source_id.pub));\n\tsource->insert_adhoc (key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_move\");\n\trequest.put (\"wallet\", wallet_id.to_string ());\n\trequest.put (\"source\", source_id.pub.to_string ());\n\tboost::property_tree::ptree keys;\n\tboost::property_tree::ptree entry;\n\tentry.put (\"\", key.pub.to_account ());\n\tkeys.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"accounts\", keys);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"1\", response.json.get<std::string> (\"moved\"));\n\tASSERT_TRUE (destination->exists (key.pub));\n\tASSERT_TRUE (destination->exists (nano::test_genesis_key.pub));\n\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\tASSERT_EQ (source->store.end (), source->store.begin (transaction));\n}\n\nTEST (rpc, block)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block\");\n\trequest.put (\"hash\", system.nodes[0]->latest (nano::genesis_account).to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto contents (response.json.get<std::string> (\"contents\"));\n\tASSERT_FALSE (contents.empty ());\n\tASSERT_TRUE (response.json.get<bool> (\"confirmed\")); // Genesis block is confirmed by default\n}\n\nTEST (rpc, block_account)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::genesis genesis;\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_account\");\n\trequest.put (\"hash\", genesis.hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text (response.json.get<std::string> (\"account\"));\n\tnano::account account;\n\tASSERT_FALSE (account.decode_account (account_text));\n}\n\nTEST (rpc, chain)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\tASSERT_NE (nullptr, block);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"chain\");\n\trequest.put (\"block\", block->hash ().to_string ());\n\trequest.put (\"count\", std::to_string (std::numeric_limits<uint64_t>::max ()));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (2, blocks.size ());\n\tASSERT_EQ (block->hash (), blocks[0]);\n\tASSERT_EQ (genesis, blocks[1]);\n}\n\nTEST (rpc, chain_limit)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\tASSERT_NE (nullptr, block);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"chain\");\n\trequest.put (\"block\", block->hash ().to_string ());\n\trequest.put (\"count\", 1);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (1, blocks.size ());\n\tASSERT_EQ (block->hash (), blocks[0]);\n}\n\nTEST (rpc, chain_offset)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\tASSERT_NE (nullptr, block);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"chain\");\n\trequest.put (\"block\", block->hash ().to_string ());\n\trequest.put (\"count\", std::to_string (std::numeric_limits<uint64_t>::max ()));\n\trequest.put (\"offset\", 1);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (1, blocks.size ());\n\tASSERT_EQ (genesis, blocks[0]);\n}\n\nTEST (rpc, frontier)\n{\n\tnano::system system (24000, 1);\n\tstd::unordered_map<nano::account, nano::block_hash> source;\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tfor (auto i (0); i < 1000; ++i)\n\t\t{\n\t\t\tnano::keypair key;\n\t\t\tsource[key.pub] = key.prv.data;\n\t\t\tsystem.nodes[0]->store.account_put (transaction, key.pub, nano::account_info (key.prv.data, 0, 0, 0, 0, 0, 0, nano::epoch::epoch_0));\n\t\t}\n\t}\n\tnano::keypair key;\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"frontiers\");\n\trequest.put (\"account\", nano::account (0).to_account ());\n\trequest.put (\"count\", std::to_string (std::numeric_limits<uint64_t>::max ()));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & frontiers_node (response.json.get_child (\"frontiers\"));\n\tstd::unordered_map<nano::account, nano::block_hash> frontiers;\n\tfor (auto i (frontiers_node.begin ()), j (frontiers_node.end ()); i != j; ++i)\n\t{\n\t\tnano::account account;\n\t\taccount.decode_account (i->first);\n\t\tnano::block_hash frontier;\n\t\tfrontier.decode_hex (i->second.get<std::string> (\"\"));\n\t\tfrontiers[account] = frontier;\n\t}\n\tASSERT_EQ (1, frontiers.erase (nano::test_genesis_key.pub));\n\tASSERT_EQ (source, frontiers);\n}\n\nTEST (rpc, frontier_limited)\n{\n\tnano::system system (24000, 1);\n\tstd::unordered_map<nano::account, nano::block_hash> source;\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tfor (auto i (0); i < 1000; ++i)\n\t\t{\n\t\t\tnano::keypair key;\n\t\t\tsource[key.pub] = key.prv.data;\n\t\t\tsystem.nodes[0]->store.account_put (transaction, key.pub, nano::account_info (key.prv.data, 0, 0, 0, 0, 0, 0, nano::epoch::epoch_0));\n\t\t}\n\t}\n\tnano::keypair key;\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"frontiers\");\n\trequest.put (\"account\", nano::account (0).to_account ());\n\trequest.put (\"count\", std::to_string (100));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & frontiers_node (response.json.get_child (\"frontiers\"));\n\tASSERT_EQ (100, frontiers_node.size ());\n}\n\nTEST (rpc, frontier_startpoint)\n{\n\tnano::system system (24000, 1);\n\tstd::unordered_map<nano::account, nano::block_hash> source;\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tfor (auto i (0); i < 1000; ++i)\n\t\t{\n\t\t\tnano::keypair key;\n\t\t\tsource[key.pub] = key.prv.data;\n\t\t\tsystem.nodes[0]->store.account_put (transaction, key.pub, nano::account_info (key.prv.data, 0, 0, 0, 0, 0, 0, nano::epoch::epoch_0));\n\t\t}\n\t}\n\tnano::keypair key;\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"frontiers\");\n\trequest.put (\"account\", source.begin ()->first.to_account ());\n\trequest.put (\"count\", std::to_string (1));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & frontiers_node (response.json.get_child (\"frontiers\"));\n\tASSERT_EQ (1, frontiers_node.size ());\n\tASSERT_EQ (source.begin ()->first.to_account (), frontiers_node.begin ()->first);\n}\n\nTEST (rpc, history)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto change (system.wallet (0)->change_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub));\n\tASSERT_NE (nullptr, change);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tauto receive (system.wallet (0)->receive_action (*send, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tauto node0 (system.nodes[0]);\n\tnano::genesis genesis;\n\tnano::state_block usend (nano::genesis_account, node0->latest (nano::genesis_account), nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, nano::genesis_account, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tnano::state_block ureceive (nano::genesis_account, usend.hash (), nano::genesis_account, nano::genesis_amount, usend.hash (), nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tnano::state_block uchange (nano::genesis_account, ureceive.hash (), nano::keypair ().pub, nano::genesis_amount, 0, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\t{\n\t\tauto transaction (node0->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, usend).code);\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, ureceive).code);\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, uchange).code);\n\t}\n\tenable_ipc_transport_tcp (node0->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node0, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"history\");\n\trequest.put (\"hash\", uchange.hash ().to_string ());\n\trequest.put (\"count\", 100);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::vector<std::tuple<std::string, std::string, std::string, std::string>> history_l;\n\tauto & history_node (response.json.get_child (\"history\"));\n\tfor (auto i (history_node.begin ()), n (history_node.end ()); i != n; ++i)\n\t{\n\t\thistory_l.push_back (std::make_tuple (i->second.get<std::string> (\"type\"), i->second.get<std::string> (\"account\"), i->second.get<std::string> (\"amount\"), i->second.get<std::string> (\"hash\")));\n\t}\n\tASSERT_EQ (5, history_l.size ());\n\tASSERT_EQ (\"receive\", std::get<0> (history_l[0]));\n\tASSERT_EQ (ureceive.hash ().to_string (), std::get<3> (history_l[0]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[0]));\n\tASSERT_EQ (nano::Gxrb_ratio.convert_to<std::string> (), std::get<2> (history_l[0]));\n\tASSERT_EQ (5, history_l.size ());\n\tASSERT_EQ (\"send\", std::get<0> (history_l[1]));\n\tASSERT_EQ (usend.hash ().to_string (), std::get<3> (history_l[1]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[1]));\n\tASSERT_EQ (nano::Gxrb_ratio.convert_to<std::string> (), std::get<2> (history_l[1]));\n\tASSERT_EQ (\"receive\", std::get<0> (history_l[2]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[2]));\n\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), std::get<2> (history_l[2]));\n\tASSERT_EQ (receive->hash ().to_string (), std::get<3> (history_l[2]));\n\tASSERT_EQ (\"send\", std::get<0> (history_l[3]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[3]));\n\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), std::get<2> (history_l[3]));\n\tASSERT_EQ (send->hash ().to_string (), std::get<3> (history_l[3]));\n\tASSERT_EQ (\"receive\", std::get<0> (history_l[4]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[4]));\n\tASSERT_EQ (nano::genesis_amount.convert_to<std::string> (), std::get<2> (history_l[4]));\n\tASSERT_EQ (genesis.hash ().to_string (), std::get<3> (history_l[4]));\n}\n\nTEST (rpc, account_history)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto change (system.wallet (0)->change_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub));\n\tASSERT_NE (nullptr, change);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tauto receive (system.wallet (0)->receive_action (*send, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tauto node0 (system.nodes[0]);\n\tnano::genesis genesis;\n\tnano::state_block usend (nano::genesis_account, node0->latest (nano::genesis_account), nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, nano::genesis_account, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tnano::state_block ureceive (nano::genesis_account, usend.hash (), nano::genesis_account, nano::genesis_amount, usend.hash (), nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tnano::state_block uchange (nano::genesis_account, ureceive.hash (), nano::keypair ().pub, nano::genesis_amount, 0, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\t{\n\t\tauto transaction (node0->store.tx_begin_write ());\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, usend).code);\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, ureceive).code);\n\t\tASSERT_EQ (nano::process_result::progress, node0->ledger.process (transaction, uchange).code);\n\t}\n\tenable_ipc_transport_tcp (node0->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node0, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\t{\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"account_history\");\n\t\trequest.put (\"account\", nano::genesis_account.to_account ());\n\t\trequest.put (\"count\", 100);\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tstd::vector<std::tuple<std::string, std::string, std::string, std::string, std::string>> history_l;\n\t\tauto & history_node (response.json.get_child (\"history\"));\n\t\tfor (auto i (history_node.begin ()), n (history_node.end ()); i != n; ++i)\n\t\t{\n\t\t\thistory_l.push_back (std::make_tuple (i->second.get<std::string> (\"type\"), i->second.get<std::string> (\"account\"), i->second.get<std::string> (\"amount\"), i->second.get<std::string> (\"hash\"), i->second.get<std::string> (\"height\")));\n\t\t}\n\n\t\tASSERT_EQ (5, history_l.size ());\n\t\tASSERT_EQ (\"receive\", std::get<0> (history_l[0]));\n\t\tASSERT_EQ (ureceive.hash ().to_string (), std::get<3> (history_l[0]));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[0]));\n\t\tASSERT_EQ (nano::Gxrb_ratio.convert_to<std::string> (), std::get<2> (history_l[0]));\n\t\tASSERT_EQ (\"6\", std::get<4> (history_l[0])); // change block (height 7) is skipped by account_history since \"raw\" is not set\n\t\tASSERT_EQ (\"send\", std::get<0> (history_l[1]));\n\t\tASSERT_EQ (usend.hash ().to_string (), std::get<3> (history_l[1]));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[1]));\n\t\tASSERT_EQ (nano::Gxrb_ratio.convert_to<std::string> (), std::get<2> (history_l[1]));\n\t\tASSERT_EQ (\"5\", std::get<4> (history_l[1]));\n\t\tASSERT_EQ (\"receive\", std::get<0> (history_l[2]));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[2]));\n\t\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), std::get<2> (history_l[2]));\n\t\tASSERT_EQ (receive->hash ().to_string (), std::get<3> (history_l[2]));\n\t\tASSERT_EQ (\"4\", std::get<4> (history_l[2]));\n\t\tASSERT_EQ (\"send\", std::get<0> (history_l[3]));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[3]));\n\t\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), std::get<2> (history_l[3]));\n\t\tASSERT_EQ (send->hash ().to_string (), std::get<3> (history_l[3]));\n\t\tASSERT_EQ (\"3\", std::get<4> (history_l[3]));\n\t\tASSERT_EQ (\"receive\", std::get<0> (history_l[4]));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[4]));\n\t\tASSERT_EQ (nano::genesis_amount.convert_to<std::string> (), std::get<2> (history_l[4]));\n\t\tASSERT_EQ (genesis.hash ().to_string (), std::get<3> (history_l[4]));\n\t\tASSERT_EQ (\"1\", std::get<4> (history_l[4])); // change block (height 2) is skipped\n\t}\n\t// Test count and reverse\n\t{\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"account_history\");\n\t\trequest.put (\"account\", nano::genesis_account.to_account ());\n\t\trequest.put (\"reverse\", true);\n\t\trequest.put (\"count\", 1);\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & history_node (response.json.get_child (\"history\"));\n\t\tASSERT_EQ (1, history_node.size ());\n\t\tASSERT_EQ (\"1\", history_node.begin ()->second.get<std::string> (\"height\"));\n\t\tASSERT_EQ (change->hash ().to_string (), response.json.get<std::string> (\"next\"));\n\t}\n\n\t// Test filtering\n\tauto account2 (system.wallet (0)->deterministic_insert ());\n\tauto send2 (system.wallet (0)->send_action (nano::test_genesis_key.pub, account2, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send2);\n\tauto receive2 (system.wallet (0)->receive_action (*send2, account2, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive2);\n\t{\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"account_history\");\n\t\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\t\tboost::property_tree::ptree other_account;\n\t\tother_account.put (\"\", account2.to_account ());\n\t\tboost::property_tree::ptree filtered_accounts;\n\t\tfiltered_accounts.push_back (std::make_pair (\"\", other_account));\n\t\trequest.add_child (\"account_filter\", filtered_accounts);\n\t\trequest.put (\"count\", 100);\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tauto history_node (response.json.get_child (\"history\"));\n\t\tASSERT_EQ (history_node.size (), 1);\n\t}\n}\n\nTEST (rpc, history_count)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto change (system.wallet (0)->change_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub));\n\tASSERT_NE (nullptr, change);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tauto receive (system.wallet (0)->receive_action (*send, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"history\");\n\trequest.put (\"hash\", receive->hash ().to_string ());\n\trequest.put (\"count\", 1);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & history_node (response.json.get_child (\"history\"));\n\tASSERT_EQ (1, history_node.size ());\n}\n\nTEST (rpc, process_block)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->latest (nano::test_genesis_key.pub) != send.hash ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tstd::string send_hash (response.json.get<std::string> (\"hash\"));\n\tASSERT_EQ (send.hash ().to_string (), send_hash);\n}\n\nTEST (rpc, process_block_no_work)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsend.block_work_set (0);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_FALSE (response.json.get<std::string> (\"error\", \"\").empty ());\n}\n\nTEST (rpc, process_republish)\n{\n\tnano::system system (24000, 2);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->latest (nano::test_genesis_key.pub) != send.hash ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, process_subtype_send)\n{\n\tnano::system system (24000, 2);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, latest, nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\trequest.put (\"subtype\", \"receive\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::error_code ec (nano::error_rpc::invalid_subtype_balance);\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"change\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (response2.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"send\");\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (send.hash ().to_string (), response3.json.get<std::string> (\"hash\"));\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->latest (nano::test_genesis_key.pub) != send.hash ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, process_subtype_open)\n{\n\tnano::system system (24000, 2);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, latest, nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\t{\n\t\tauto transaction (node1.store.tx_begin_write ());\n\t\tASSERT_EQ (nano::process_result::progress, node1.ledger.process (transaction, send).code);\n\t}\n\tnode1.active.start (std::make_shared<nano::state_block> (send));\n\tnano::state_block open (key.pub, 0, key.pub, nano::Gxrb_ratio, send.hash (), key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\topen.serialize_json (json);\n\trequest.put (\"block\", json);\n\trequest.put (\"subtype\", \"send\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::error_code ec (nano::error_rpc::invalid_subtype_balance);\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"epoch\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (response2.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"open\");\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (open.hash ().to_string (), response3.json.get<std::string> (\"hash\"));\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->latest (key.pub) != open.hash ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, process_subtype_receive)\n{\n\tnano::system system (24000, 2);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, latest, nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, nano::test_genesis_key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\t{\n\t\tauto transaction (node1.store.tx_begin_write ());\n\t\tASSERT_EQ (nano::process_result::progress, node1.ledger.process (transaction, send).code);\n\t}\n\tnode1.active.start (std::make_shared<nano::state_block> (send));\n\tnano::state_block receive (nano::test_genesis_key.pub, send.hash (), nano::test_genesis_key.pub, nano::genesis_amount, send.hash (), nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (send.hash ()));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"process\");\n\tstd::string json;\n\treceive.serialize_json (json);\n\trequest.put (\"block\", json);\n\trequest.put (\"subtype\", \"send\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::error_code ec (nano::error_rpc::invalid_subtype_balance);\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"open\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tec = nano::error_rpc::invalid_subtype_previous;\n\tASSERT_EQ (response2.json.get<std::string> (\"error\"), ec.message ());\n\trequest.put (\"subtype\", \"receive\");\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (receive.hash ().to_string (), response3.json.get<std::string> (\"hash\"));\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->latest (nano::test_genesis_key.pub) != receive.hash ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, keepalive)\n{\n\tnano::system system (24000, 1);\n\tnano::node_init init1;\n\tauto node1 (std::make_shared<nano::node> (init1, system.io_ctx, 24001, nano::unique_path (), system.alarm, system.logging, system.work));\n\tnode1->start ();\n\tsystem.nodes.push_back (node1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"keepalive\");\n\tauto address (boost::str (boost::format (\"%1%\") % node1->network.endpoint ().address ()));\n\tauto port (boost::str (boost::format (\"%1%\") % node1->network.endpoint ().port ()));\n\trequest.put (\"address\", address);\n\trequest.put (\"port\", port);\n\tASSERT_EQ (nullptr, system.nodes[0]->network.udp_channels.channel (node1->network.endpoint ()));\n\tASSERT_EQ (0, system.nodes[0]->network.size ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->network.udp_channels.channel (node1->network.endpoint ()) == nullptr)\n\t{\n\t\tASSERT_EQ (0, system.nodes[0]->network.size ());\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tnode1->stop ();\n}\n\nTEST (rpc, payment_init)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair wallet_id;\n\tauto wallet (node1->wallets.create (wallet_id.pub));\n\tASSERT_TRUE (node1->wallets.items.find (wallet_id.pub) != node1->wallets.items.end ());\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"payment_init\");\n\trequest.put (\"wallet\", wallet_id.pub.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"Ready\", response.json.get<std::string> (\"status\"));\n}\n\nTEST (rpc, payment_begin_end)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair wallet_id;\n\tauto wallet (node1->wallets.create (wallet_id.pub));\n\tASSERT_TRUE (node1->wallets.items.find (wallet_id.pub) != node1->wallets.items.end ());\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_begin\");\n\trequest1.put (\"wallet\", wallet_id.pub.to_string ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto account_text (response1.json.get<std::string> (\"account\"));\n\tnano::uint256_union account;\n\tASSERT_FALSE (account.decode_account (account_text));\n\tASSERT_TRUE (wallet->exists (account));\n\tnano::block_hash root1;\n\t{\n\t\tauto transaction (node1->store.tx_begin ());\n\t\troot1 = node1->ledger.latest_root (transaction, account);\n\t}\n\tuint64_t work (0);\n\twhile (!nano::work_validate (root1, work))\n\t{\n\t\t++work;\n\t\tASSERT_LT (work, 50);\n\t}\n\tsystem.deadline_set (10s);\n\twhile (nano::work_validate (root1, work))\n\t{\n\t\tauto ec = system.poll ();\n\t\tauto transaction (wallet->wallets.tx_begin ());\n\t\tASSERT_FALSE (wallet->store.work_get (transaction, account, work));\n\t\tASSERT_NO_ERROR (ec);\n\t}\n\tASSERT_EQ (wallet->free_accounts.end (), wallet->free_accounts.find (account));\n\tboost::property_tree::ptree request2;\n\trequest2.put (\"action\", \"payment_end\");\n\trequest2.put (\"wallet\", wallet_id.pub.to_string ());\n\trequest2.put (\"account\", account.to_account ());\n\ttest_response response2 (request2, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_TRUE (wallet->exists (account));\n\tASSERT_NE (wallet->free_accounts.end (), wallet->free_accounts.find (account));\n\trpc.stop ();\n\tsystem.stop ();\n}\n\nTEST (rpc, payment_end_nonempty)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto transaction (node1->wallets.tx_begin ());\n\tsystem.wallet (0)->init_free_accounts (transaction);\n\tauto wallet_id (node1->wallets.items.begin ()->first);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_end\");\n\trequest1.put (\"wallet\", wallet_id.to_string ());\n\trequest1.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_FALSE (response1.json.get<std::string> (\"error\", \"\").empty ());\n}\n\nTEST (rpc, payment_zero_balance)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto transaction (node1->wallets.tx_begin ());\n\tsystem.wallet (0)->init_free_accounts (transaction);\n\tauto wallet_id (node1->wallets.items.begin ()->first);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_begin\");\n\trequest1.put (\"wallet\", wallet_id.to_string ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto account_text (response1.json.get<std::string> (\"account\"));\n\tnano::uint256_union account;\n\tASSERT_FALSE (account.decode_account (account_text));\n\tASSERT_NE (nano::test_genesis_key.pub, account);\n}\n\nTEST (rpc, payment_begin_reuse)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair wallet_id;\n\tauto wallet (node1->wallets.create (wallet_id.pub));\n\tASSERT_TRUE (node1->wallets.items.find (wallet_id.pub) != node1->wallets.items.end ());\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_begin\");\n\trequest1.put (\"wallet\", wallet_id.pub.to_string ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto account_text (response1.json.get<std::string> (\"account\"));\n\tnano::uint256_union account;\n\tASSERT_FALSE (account.decode_account (account_text));\n\tASSERT_TRUE (wallet->exists (account));\n\tASSERT_EQ (wallet->free_accounts.end (), wallet->free_accounts.find (account));\n\tboost::property_tree::ptree request2;\n\trequest2.put (\"action\", \"payment_end\");\n\trequest2.put (\"wallet\", wallet_id.pub.to_string ());\n\trequest2.put (\"account\", account.to_account ());\n\ttest_response response2 (request2, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_TRUE (wallet->exists (account));\n\tASSERT_NE (wallet->free_accounts.end (), wallet->free_accounts.find (account));\n\ttest_response response3 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tauto account2_text (response1.json.get<std::string> (\"account\"));\n\tnano::uint256_union account2;\n\tASSERT_FALSE (account2.decode_account (account2_text));\n\tASSERT_EQ (account, account2);\n}\n\nTEST (rpc, payment_begin_locked)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair wallet_id;\n\tauto wallet (node1->wallets.create (wallet_id.pub));\n\t{\n\t\tauto transaction (wallet->wallets.tx_begin (true));\n\t\twallet->store.rekey (transaction, \"1\");\n\t\tASSERT_TRUE (wallet->store.attempt_password (transaction, \"\"));\n\t}\n\tASSERT_TRUE (node1->wallets.items.find (wallet_id.pub) != node1->wallets.items.end ());\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_begin\");\n\trequest1.put (\"wallet\", wallet_id.pub.to_string ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_FALSE (response1.json.get<std::string> (\"error\", \"\").empty ());\n}\n\nTEST (rpc, payment_wait)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"payment_wait\");\n\trequest1.put (\"account\", key.pub.to_account ());\n\trequest1.put (\"amount\", nano::amount (nano::Mxrb_ratio).to_string_dec ());\n\trequest1.put (\"timeout\", \"100\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"nothing\", response1.json.get<std::string> (\"status\"));\n\trequest1.put (\"timeout\", \"100000\");\n\tsystem.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Mxrb_ratio);\n\tsystem.alarm.add (std::chrono::steady_clock::now () + std::chrono::milliseconds (500), [&]() {\n\t\tsystem.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Mxrb_ratio);\n\t});\n\ttest_response response2 (request1, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (\"success\", response2.json.get<std::string> (\"status\"));\n\trequest1.put (\"amount\", nano::amount (nano::Mxrb_ratio * 2).to_string_dec ());\n\ttest_response response3 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (\"success\", response2.json.get<std::string> (\"status\"));\n}\n\nTEST (rpc, peers)\n{\n\tnano::system system (24000, 2);\n\tnano::endpoint endpoint (boost::asio::ip::address_v6::from_string (\"fc00::1\"), 4000);\n\tauto node = system.nodes.front ();\n\tnode->network.udp_channels.insert (endpoint, nano::protocol_version);\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"peers\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & peers_node (response.json.get_child (\"peers\"));\n\tASSERT_EQ (2, peers_node.size ());\n\tASSERT_EQ (std::to_string (nano::protocol_version), peers_node.get<std::string> (\"UDP: [::1]:24001\"));\n\t// Previously \"[::ffff:80.80.80.80]:4000\", but IPv4 address cause \"No such node thrown in the test body\" issue with peers_node.get\n\tstd::stringstream endpoint_text;\n\tendpoint_text << endpoint;\n\tASSERT_EQ (std::to_string (nano::protocol_version), peers_node.get<std::string> (\"UDP: \" + endpoint_text.str ()));\n}\n\nTEST (rpc, peers_node_id)\n{\n\tnano::system system (24000, 2);\n\tnano::endpoint endpoint (boost::asio::ip::address_v6::from_string (\"fc00::1\"), 4000);\n\tauto node = system.nodes.front ();\n\tnode->network.udp_channels.insert (endpoint, nano::protocol_version);\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"peers\");\n\trequest.put (\"peer_details\", true);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & peers_node (response.json.get_child (\"peers\"));\n\tASSERT_EQ (2, peers_node.size ());\n\tauto tree1 (peers_node.get_child (\"UDP: [::1]:24001\"));\n\tASSERT_EQ (std::to_string (nano::protocol_version), tree1.get<std::string> (\"protocol_version\"));\n\tASSERT_EQ (system.nodes[1]->node_id.pub.to_account (), tree1.get<std::string> (\"node_id\"));\n\tstd::stringstream endpoint_text;\n\tendpoint_text << endpoint;\n\tauto tree2 (peers_node.get_child (\"UDP: \" + endpoint_text.str ()));\n\tASSERT_EQ (std::to_string (nano::protocol_version), tree2.get<std::string> (\"protocol_version\"));\n\tASSERT_EQ (\"\", tree2.get<std::string> (\"node_id\"));\n}\n\nTEST (rpc, pending)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key1;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto block1 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key1.pub, 100));\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[0]->active.active (*block1))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"pending\");\n\trequest.put (\"account\", key1.pub.to_account ());\n\trequest.put (\"count\", \"100\");\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\t\tASSERT_EQ (1, blocks_node.size ());\n\t\tnano::block_hash hash (blocks_node.begin ()->second.get<std::string> (\"\"));\n\t\tASSERT_EQ (block1->hash (), hash);\n\t}\n\trequest.put (\"sorting\", \"true\"); // Sorting test\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\t\tASSERT_EQ (1, blocks_node.size ());\n\t\tnano::block_hash hash (blocks_node.begin ()->first);\n\t\tASSERT_EQ (block1->hash (), hash);\n\t\tstd::string amount (blocks_node.begin ()->second.get<std::string> (\"\"));\n\t\tASSERT_EQ (\"100\", amount);\n\t}\n\trequest.put (\"threshold\", \"100\"); // Threshold test\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\t\tASSERT_EQ (1, blocks_node.size ());\n\t\tstd::unordered_map<nano::block_hash, nano::uint128_union> blocks;\n\t\tfor (auto i (blocks_node.begin ()), j (blocks_node.end ()); i != j; ++i)\n\t\t{\n\t\t\tnano::block_hash hash;\n\t\t\thash.decode_hex (i->first);\n\t\t\tnano::uint128_union amount;\n\t\t\tamount.decode_dec (i->second.get<std::string> (\"\"));\n\t\t\tblocks[hash] = amount;\n\t\t\tboost::optional<std::string> source (i->second.get_optional<std::string> (\"source\"));\n\t\t\tASSERT_FALSE (source.is_initialized ());\n\t\t\tboost::optional<uint8_t> min_version (i->second.get_optional<uint8_t> (\"min_version\"));\n\t\t\tASSERT_FALSE (min_version.is_initialized ());\n\t\t}\n\t\tASSERT_EQ (blocks[block1->hash ()], 100);\n\t}\n\trequest.put (\"threshold\", \"101\");\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\t\tASSERT_EQ (0, blocks_node.size ());\n\t}\n\trequest.put (\"threshold\", \"0\");\n\trequest.put (\"source\", \"true\");\n\trequest.put (\"min_version\", \"true\");\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\t\tASSERT_EQ (1, blocks_node.size ());\n\t\tstd::unordered_map<nano::block_hash, nano::uint128_union> amounts;\n\t\tstd::unordered_map<nano::block_hash, nano::account> sources;\n\t\tfor (auto i (blocks_node.begin ()), j (blocks_node.end ()); i != j; ++i)\n\t\t{\n\t\t\tnano::block_hash hash;\n\t\t\thash.decode_hex (i->first);\n\t\t\tamounts[hash].decode_dec (i->second.get<std::string> (\"amount\"));\n\t\t\tsources[hash].decode_account (i->second.get<std::string> (\"source\"));\n\t\t\tASSERT_EQ (i->second.get<uint8_t> (\"min_version\"), 0);\n\t\t}\n\t\tASSERT_EQ (amounts[block1->hash ()], 100);\n\t\tASSERT_EQ (sources[block1->hash ()], nano::test_genesis_key.pub);\n\t}\n\n\trequest.put (\"account\", key1.pub.to_account ());\n\trequest.put (\"source\", \"false\");\n\trequest.put (\"min_version\", \"false\");\n\n\tauto check_block_response_count = [&system, &request, &rpc](size_t size) {\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\n\t\tASSERT_EQ (200, response.status);\n\t\tASSERT_EQ (size, response.json.get_child (\"blocks\").size ());\n\t};\n\n\trequest.put (\"include_only_confirmed\", \"true\");\n\tcheck_block_response_count (1);\n\treset_confirmation_height (system.nodes.front ()->store, block1->account ());\n\tcheck_block_response_count (0);\n}\n\nTEST (rpc, search_pending)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto wallet (system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block block (latest, nano::test_genesis_key.pub, nano::genesis_amount - system.nodes[0]->config.receive_minimum.number (), nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.nodes[0]->work_generate_blocking (latest));\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->ledger.process (transaction, block).code);\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"search_pending\");\n\trequest.put (\"wallet\", wallet);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->balance (nano::test_genesis_key.pub) != nano::genesis_amount)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, version)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"version\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"rpc_version\"));\n\tASSERT_EQ (200, response1.status);\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin ());\n\t\tASSERT_EQ (std::to_string (node1->store.version_get (transaction)), response1.json.get<std::string> (\"store_version\"));\n\t}\n\tASSERT_EQ (std::to_string (nano::protocol_version), response1.json.get<std::string> (\"protocol_version\"));\n\tif (NANO_VERSION_PATCH == 0)\n\t{\n\t\tASSERT_EQ (boost::str (boost::format (\"Nano %1%\") % NANO_MAJOR_MINOR_VERSION), response1.json.get<std::string> (\"node_vendor\"));\n\t}\n\telse\n\t{\n\t\tASSERT_EQ (boost::str (boost::format (\"Nano %1%\") % NANO_MAJOR_MINOR_RC_VERSION), response1.json.get<std::string> (\"node_vendor\"));\n\t}\n\tauto headers (response1.resp.base ());\n\tauto allow (headers.at (\"Allow\"));\n\tauto content_type (headers.at (\"Content-Type\"));\n\tauto access_control_allow_origin (headers.at (\"Access-Control-Allow-Origin\"));\n\tauto access_control_allow_methods (headers.at (\"Access-Control-Allow-Methods\"));\n\tauto access_control_allow_headers (headers.at (\"Access-Control-Allow-Headers\"));\n\tauto connection (headers.at (\"Connection\"));\n\tASSERT_EQ (\"POST, OPTIONS\", allow);\n\tASSERT_EQ (\"application/json\", content_type);\n\tASSERT_EQ (\"*\", access_control_allow_origin);\n\tASSERT_EQ (allow, access_control_allow_methods);\n\tASSERT_EQ (\"Accept, Accept-Language, Content-Language, Content-Type\", access_control_allow_headers);\n\tASSERT_EQ (\"close\", connection);\n}\n\nTEST (rpc, work_generate)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::block_hash hash1 (1);\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"work_generate\");\n\trequest1.put (\"hash\", hash1.to_string ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto work1 (response1.json.get<std::string> (\"work\"));\n\tuint64_t work2;\n\tASSERT_FALSE (nano::from_string_hex (work1, work2));\n\tASSERT_FALSE (nano::work_validate (hash1, work2));\n}\n\nTEST (rpc, work_generate_difficulty)\n{\n\tnano::system system (24000, 1);\n\tauto node1 (system.nodes[0]);\n\tenable_ipc_transport_tcp (node1->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::block_hash hash1 (1);\n\tuint64_t difficulty1 (0xfff0000000000000);\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"work_generate\");\n\trequest1.put (\"hash\", hash1.to_string ());\n\trequest1.put (\"difficulty\", nano::to_string_hex (difficulty1));\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (10s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto work_text1 (response1.json.get<std::string> (\"work\"));\n\tuint64_t work1;\n\tASSERT_FALSE (nano::from_string_hex (work_text1, work1));\n\tuint64_t result_difficulty1;\n\tASSERT_FALSE (nano::work_validate (hash1, work1, &result_difficulty1));\n\tASSERT_GE (result_difficulty1, difficulty1);\n\tuint64_t difficulty2 (0xffff000000000000);\n\trequest1.put (\"difficulty\", nano::to_string_hex (difficulty2));\n\ttest_response response2 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (20s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tauto work_text2 (response2.json.get<std::string> (\"work\"));\n\tuint64_t work2;\n\tASSERT_FALSE (nano::from_string_hex (work_text2, work2));\n\tuint64_t result_difficulty2;\n\tASSERT_FALSE (nano::work_validate (hash1, work2, &result_difficulty2));\n\tASSERT_GE (result_difficulty2, difficulty2);\n\tuint64_t difficulty3 (node_rpc_config.max_work_generate_difficulty + 1);\n\trequest1.put (\"difficulty\", nano::to_string_hex (difficulty3));\n\ttest_response response3 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tstd::error_code ec (nano::error_rpc::difficulty_limit);\n\tASSERT_EQ (response3.json.get<std::string> (\"error\"), ec.message ());\n}\n\nTEST (rpc, work_cancel)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::block_hash hash1 (1);\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"work_cancel\");\n\trequest1.put (\"hash\", hash1.to_string ());\n\tstd::atomic<bool> done (false);\n\tsystem.deadline_set (10s);\n\twhile (!done)\n\t{\n\t\tsystem.work.generate (hash1, [&done](boost::optional<uint64_t> work_a) {\n\t\t\tdone = !work_a;\n\t\t});\n\t\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\t\tstd::error_code ec;\n\t\twhile (response1.status == 0)\n\t\t{\n\t\t\tec = system.poll ();\n\t\t}\n\t\tASSERT_EQ (200, response1.status);\n\t\tASSERT_NO_ERROR (ec);\n\t}\n}\n\nTEST (rpc, work_peer_bad)\n{\n\tnano::system system (24000, 2);\n\tauto & node1 (*system.nodes[0]);\n\tauto & node2 (*system.nodes[1]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnode2.config.work_peers.push_back (std::make_pair (boost::asio::ip::address_v6::any ().to_string (), 0));\n\tnano::block_hash hash1 (1);\n\tstd::atomic<uint64_t> work (0);\n\tnode2.work_generate (hash1, [&work](uint64_t work_a) {\n\t\twork = work_a;\n\t});\n\tsystem.deadline_set (5s);\n\twhile (nano::work_validate (hash1, work))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, work_peer_one)\n{\n\tnano::system system (24000, 2);\n\tauto & node1 (*system.nodes[0]);\n\tauto & node2 (*system.nodes[1]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnode2.config.work_peers.push_back (std::make_pair (node1.network.endpoint ().address ().to_string (), rpc.config.port));\n\tnano::keypair key1;\n\tuint64_t work (0);\n\tnode2.work_generate (key1.pub, [&work](uint64_t work_a) {\n\t\twork = work_a;\n\t});\n\tsystem.deadline_set (5s);\n\twhile (nano::work_validate (key1.pub, work))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, work_peer_many)\n{\n\tnano::system system1 (24000, 1);\n\tnano::system system2 (24001, 1);\n\tnano::system system3 (24002, 1);\n\tnano::system system4 (24003, 1);\n\tauto & node1 (*system1.nodes[0]);\n\tauto & node2 (*system2.nodes[0]);\n\tauto & node3 (*system3.nodes[0]);\n\tauto & node4 (*system4.nodes[0]);\n\tnano::keypair key;\n\tnano::rpc_config config2 (true);\n\tconfig2.port += 0;\n\tenable_ipc_transport_tcp (node2.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server2 (node2, node_rpc_config);\n\tnano::ipc_rpc_processor ipc_rpc_processor2 (system2.io_ctx, config2);\n\tnano::rpc rpc2 (system2.io_ctx, config2, ipc_rpc_processor2);\n\trpc2.start ();\n\tnano::rpc_config config3 (true);\n\tconfig3.port += 1;\n\tenable_ipc_transport_tcp (node3.config.ipc_config.transport_tcp, node3.network_params.network.default_ipc_port + 1);\n\tnano::ipc::ipc_server ipc_server3 (node3, node_rpc_config);\n\tnano::ipc_rpc_processor ipc_rpc_processor3 (system3.io_ctx, config3);\n\tnano::rpc rpc3 (system3.io_ctx, config3, ipc_rpc_processor3);\n\trpc3.start ();\n\tnano::rpc_config config4 (true);\n\tconfig4.port += 2;\n\tenable_ipc_transport_tcp (node4.config.ipc_config.transport_tcp, node4.network_params.network.default_ipc_port + 2);\n\tnano::ipc::ipc_server ipc_server4 (node4, node_rpc_config);\n\tnano::ipc_rpc_processor ipc_rpc_processor4 (system4.io_ctx, config4);\n\tnano::rpc rpc4 (system2.io_ctx, config4, ipc_rpc_processor4);\n\trpc4.start ();\n\tnode1.config.work_peers.push_back (std::make_pair (node2.network.endpoint ().address ().to_string (), rpc2.config.port));\n\tnode1.config.work_peers.push_back (std::make_pair (node3.network.endpoint ().address ().to_string (), rpc3.config.port));\n\tnode1.config.work_peers.push_back (std::make_pair (node4.network.endpoint ().address ().to_string (), rpc4.config.port));\n\n\tfor (auto i (0); i < 10; ++i)\n\t{\n\t\tnano::keypair key1;\n\t\tuint64_t work (0);\n\t\tnode1.work_generate (key1.pub, [&work](uint64_t work_a) {\n\t\t\twork = work_a;\n\t\t});\n\t\twhile (nano::work_validate (key1.pub, work))\n\t\t{\n\t\t\tsystem1.poll ();\n\t\t\tsystem2.poll ();\n\t\t\tsystem3.poll ();\n\t\t\tsystem4.poll ();\n\t\t}\n\t}\n}\n\nTEST (rpc, block_count)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"block_count\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"count\"));\n\tASSERT_EQ (\"0\", response1.json.get<std::string> (\"unchecked\"));\n}\n\nTEST (rpc, frontier_count)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"frontier_count\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"count\"));\n}\n\nTEST (rpc, account_count)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"account_count\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"count\"));\n}\n\nTEST (rpc, available_supply)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"available_supply\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"0\", response1.json.get<std::string> (\"available\"));\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\ttest_response response2 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (\"1\", response2.json.get<std::string> (\"available\"));\n\tauto block2 (system.wallet (0)->send_action (nano::test_genesis_key.pub, 0, 100)); // Sending to burning 0 account\n\ttest_response response3 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tASSERT_EQ (\"1\", response3.json.get<std::string> (\"available\"));\n}\n\nTEST (rpc, mrai_to_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"mrai_to_raw\");\n\trequest1.put (\"amount\", \"1\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (nano::Mxrb_ratio.convert_to<std::string> (), response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, mrai_from_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"mrai_from_raw\");\n\trequest1.put (\"amount\", nano::Mxrb_ratio.convert_to<std::string> ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, krai_to_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"krai_to_raw\");\n\trequest1.put (\"amount\", \"1\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (nano::kxrb_ratio.convert_to<std::string> (), response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, krai_from_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"krai_from_raw\");\n\trequest1.put (\"amount\", nano::kxrb_ratio.convert_to<std::string> ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, nano_to_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"nano_to_raw\");\n\trequest1.put (\"amount\", \"1\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (nano::xrb_ratio.convert_to<std::string> (), response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, nano_from_raw)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"nano_from_raw\");\n\trequest1.put (\"amount\", nano::xrb_ratio.convert_to<std::string> ());\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"1\", response1.json.get<std::string> (\"amount\"));\n}\n\nTEST (rpc, account_representative)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\trequest.put (\"account\", nano::genesis_account.to_account ());\n\trequest.put (\"action\", \"account_representative\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"representative\"));\n\tASSERT_EQ (account_text1, nano::genesis_account.to_account ());\n}\n\nTEST (rpc, account_representative_set)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tnano::keypair rep;\n\trequest.put (\"account\", nano::genesis_account.to_account ());\n\trequest.put (\"representative\", rep.pub.to_account ());\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"action\", \"account_representative_set\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string block_text1 (response.json.get<std::string> (\"block\"));\n\tnano::block_hash hash;\n\tASSERT_FALSE (hash.decode_hex (block_text1));\n\tASSERT_FALSE (hash.is_zero ());\n\tauto transaction (system.nodes[0]->store.tx_begin ());\n\tASSERT_TRUE (system.nodes[0]->store.block_exists (transaction, hash));\n\tASSERT_EQ (rep.pub, system.nodes[0]->store.block_get (transaction, hash)->representative ());\n}\n\nTEST (rpc, bootstrap)\n{\n\tnano::system system0 (24000, 1);\n\tnano::system system1 (24001, 1);\n\tauto latest (system1.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, nano::genesis_account, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system1.nodes[0]->work_generate_blocking (latest));\n\t{\n\t\tauto transaction (system1.nodes[0]->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, system1.nodes[0]->ledger.process (transaction, send).code);\n\t}\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"bootstrap\");\n\trequest.put (\"address\", \"::ffff:127.0.0.1\");\n\trequest.put (\"port\", system1.nodes[0]->network.endpoint ().port ());\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tsystem1.deadline_set (10s);\n\twhile (system0.nodes[0]->latest (nano::genesis_account) != system1.nodes[0]->latest (nano::genesis_account))\n\t{\n\t\tASSERT_NO_ERROR (system0.poll ());\n\t\tASSERT_NO_ERROR (system1.poll ());\n\t}\n}\n\nTEST (rpc, account_remove)\n{\n\tnano::system system0 (24000, 1);\n\tauto key1 (system0.wallet (0)->deterministic_insert ());\n\tASSERT_TRUE (system0.wallet (0)->exists (key1));\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_remove\");\n\trequest.put (\"wallet\", system0.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"account\", key1.to_account ());\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_FALSE (system0.wallet (0)->exists (key1));\n}\n\nTEST (rpc, representatives)\n{\n\tnano::system system0 (24000, 1);\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"representatives\");\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & representatives_node (response.json.get_child (\"representatives\"));\n\tstd::vector<nano::account> representatives;\n\tfor (auto i (representatives_node.begin ()), n (representatives_node.end ()); i != n; ++i)\n\t{\n\t\tnano::account account;\n\t\tASSERT_FALSE (account.decode_account (i->first));\n\t\trepresentatives.push_back (account);\n\t}\n\tASSERT_EQ (1, representatives.size ());\n\tASSERT_EQ (nano::genesis_account, representatives[0]);\n}\n\n// wallet_seed is only available over IPC's unsafe encoding, and when running on test network\nTEST (rpc, wallet_seed)\n{\n\tnano::system system (24000, 1);\n\tnano::raw_key seed;\n\t{\n\t\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\t\tsystem.wallet (0)->store.seed (seed, transaction);\n\t}\n\tauto & node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_seed\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc_config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\t{\n\t\tstd::string seed_text (response.json.get<std::string> (\"seed\"));\n\t\tASSERT_EQ (seed.data.to_string (), seed_text);\n\t}\n}\n\nTEST (rpc, wallet_change_seed)\n{\n\tnano::system system0 (24000, 1);\n\tnano::keypair seed;\n\t{\n\t\tauto transaction (system0.nodes[0]->wallets.tx_begin ());\n\t\tnano::raw_key seed0;\n\t\tsystem0.wallet (0)->store.seed (seed0, transaction);\n\t\tASSERT_NE (seed.pub, seed0.data);\n\t}\n\tnano::raw_key prv;\n\tnano::deterministic_key (seed.pub, 0, prv.data);\n\tauto pub (nano::pub_key (prv.data));\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_change_seed\");\n\trequest.put (\"wallet\", system0.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"seed\", seed.pub.to_string ());\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\tsystem0.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system0.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\t{\n\t\tauto transaction (system0.nodes[0]->wallets.tx_begin ());\n\t\tnano::raw_key seed0;\n\t\tsystem0.wallet (0)->store.seed (seed0, transaction);\n\t\tASSERT_EQ (seed.pub, seed0.data);\n\t}\n\tauto account_text (response.json.get<std::string> (\"last_restored_account\"));\n\tnano::uint256_union account;\n\tASSERT_FALSE (account.decode_account (account_text));\n\tASSERT_TRUE (system0.wallet (0)->exists (account));\n\tASSERT_EQ (pub, account);\n\tASSERT_EQ (\"1\", response.json.get<std::string> (\"restored_count\"));\n}\n\nTEST (rpc, wallet_frontiers)\n{\n\tnano::system system0 (24000, 1);\n\tsystem0.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_frontiers\");\n\trequest.put (\"wallet\", system0.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & frontiers_node (response.json.get_child (\"frontiers\"));\n\tstd::vector<nano::account> frontiers;\n\tfor (auto i (frontiers_node.begin ()), n (frontiers_node.end ()); i != n; ++i)\n\t{\n\t\tfrontiers.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (1, frontiers.size ());\n\tASSERT_EQ (system0.nodes[0]->latest (nano::genesis_account), frontiers[0]);\n}\n\nTEST (rpc, work_validate)\n{\n\tnano::network_params params;\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tnano::block_hash hash (1);\n\tuint64_t work1 (node1.work_generate_blocking (hash));\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"work_validate\");\n\trequest.put (\"hash\", hash.to_string ());\n\trequest.put (\"work\", nano::to_string_hex (work1));\n\ttest_response response1 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tstd::string validate_text1 (response1.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (\"1\", validate_text1);\n\tuint64_t work2 (0);\n\trequest.put (\"work\", nano::to_string_hex (work2));\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tstd::string validate_text2 (response2.json.get<std::string> (\"valid\"));\n\tASSERT_EQ (\"0\", validate_text2);\n\tuint64_t result_difficulty;\n\tASSERT_FALSE (nano::work_validate (hash, work1, &result_difficulty));\n\tASSERT_GE (result_difficulty, params.network.publish_threshold);\n\trequest.put (\"work\", nano::to_string_hex (work1));\n\trequest.put (\"difficulty\", nano::to_string_hex (result_difficulty));\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tbool validate3 (response3.json.get<bool> (\"valid\"));\n\tASSERT_TRUE (validate3);\n\tuint64_t difficulty4 (0xfff0000000000000);\n\trequest.put (\"work\", nano::to_string_hex (work1));\n\trequest.put (\"difficulty\", nano::to_string_hex (difficulty4));\n\ttest_response response4 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response4.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response4.status);\n\tbool validate4 (response4.json.get<bool> (\"valid\"));\n\tASSERT_EQ (result_difficulty >= difficulty4, validate4);\n\tuint64_t work3 (node1.work_generate_blocking (hash, difficulty4));\n\trequest.put (\"work\", nano::to_string_hex (work3));\n\ttest_response response5 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response5.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response5.status);\n\tbool validate5 (response5.json.get<bool> (\"valid\"));\n\tASSERT_TRUE (validate5);\n}\n\nTEST (rpc, successors)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\tASSERT_NE (nullptr, block);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"successors\");\n\trequest.put (\"block\", genesis.to_string ());\n\trequest.put (\"count\", std::to_string (std::numeric_limits<uint64_t>::max ()));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (2, blocks.size ());\n\tASSERT_EQ (genesis, blocks[0]);\n\tASSERT_EQ (block->hash (), blocks[1]);\n\t// RPC chain \"reverse\" option\n\trequest.put (\"action\", \"chain\");\n\trequest.put (\"reverse\", \"true\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tASSERT_EQ (response.json, response2.json);\n}\n\nTEST (rpc, bootstrap_any)\n{\n\tnano::system system0 (24000, 1);\n\tnano::system system1 (24001, 1);\n\tauto latest (system1.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, nano::genesis_account, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system1.nodes[0]->work_generate_blocking (latest));\n\t{\n\t\tauto transaction (system1.nodes[0]->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, system1.nodes[0]->ledger.process (transaction, send).code);\n\t}\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"bootstrap_any\");\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tstd::string success (response.json.get<std::string> (\"success\"));\n\tASSERT_TRUE (success.empty ());\n}\n\nTEST (rpc, republish)\n{\n\tnano::system system (24000, 2);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (node1.latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tnode1.process (send);\n\tnano::open_block open (send.hash (), key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, node1.process (open).code);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"republish\");\n\trequest.put (\"hash\", send.hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->balance (nano::test_genesis_key.pub) == nano::genesis_amount)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (1, blocks.size ());\n\tASSERT_EQ (send.hash (), blocks[0]);\n\n\trequest.put (\"hash\", genesis.hash ().to_string ());\n\trequest.put (\"count\", 1);\n\ttest_response response1 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tblocks_node = response1.json.get_child (\"blocks\");\n\tblocks.clear ();\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (1, blocks.size ());\n\tASSERT_EQ (genesis.hash (), blocks[0]);\n\n\trequest.put (\"hash\", open.hash ().to_string ());\n\trequest.put (\"sources\", 2);\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tblocks_node = response2.json.get_child (\"blocks\");\n\tblocks.clear ();\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (3, blocks.size ());\n\tASSERT_EQ (genesis.hash (), blocks[0]);\n\tASSERT_EQ (send.hash (), blocks[1]);\n\tASSERT_EQ (open.hash (), blocks[2]);\n}\n\nTEST (rpc, deterministic_key)\n{\n\tnano::system system0 (24000, 1);\n\tnano::raw_key seed;\n\t{\n\t\tauto transaction (system0.nodes[0]->wallets.tx_begin ());\n\t\tsystem0.wallet (0)->store.seed (seed, transaction);\n\t}\n\tnano::account account0 (system0.wallet (0)->deterministic_insert ());\n\tnano::account account1 (system0.wallet (0)->deterministic_insert ());\n\tnano::account account2 (system0.wallet (0)->deterministic_insert ());\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"deterministic_key\");\n\trequest.put (\"seed\", seed.data.to_string ());\n\trequest.put (\"index\", \"0\");\n\ttest_response response0 (request, rpc.config.port, system0.io_ctx);\n\twhile (response0.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response0.status);\n\tstd::string validate_text (response0.json.get<std::string> (\"account\"));\n\tASSERT_EQ (account0.to_account (), validate_text);\n\trequest.put (\"index\", \"2\");\n\ttest_response response1 (request, rpc.config.port, system0.io_ctx);\n\twhile (response1.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response1.status);\n\tvalidate_text = response1.json.get<std::string> (\"account\");\n\tASSERT_NE (account1.to_account (), validate_text);\n\tASSERT_EQ (account2.to_account (), validate_text);\n}\n\nTEST (rpc, accounts_balances)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"accounts_balances\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", nano::test_genesis_key.pub.to_account ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"accounts\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tfor (auto & balances : response.json.get_child (\"balances\"))\n\t{\n\t\tstd::string account_text (balances.first);\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string balance_text (balances.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (\"340282366920938463463374607431768211455\", balance_text);\n\t\tstd::string pending_text (balances.second.get<std::string> (\"pending\"));\n\t\tASSERT_EQ (\"0\", pending_text);\n\t}\n}\n\nTEST (rpc, accounts_frontiers)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"accounts_frontiers\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", nano::test_genesis_key.pub.to_account ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"accounts\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tfor (auto & frontiers : response.json.get_child (\"frontiers\"))\n\t{\n\t\tstd::string account_text (frontiers.first);\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string frontier_text (frontiers.second.get<std::string> (\"\"));\n\t\tASSERT_EQ (system.nodes[0]->latest (nano::genesis_account), frontier_text);\n\t}\n}\n\nTEST (rpc, accounts_pending)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key1;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto block1 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key1.pub, 100));\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[0]->active.active (*block1))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"accounts_pending\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", key1.pub.to_account ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"accounts\", peers_l);\n\trequest.put (\"count\", \"100\");\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tfor (auto & blocks : response.json.get_child (\"blocks\"))\n\t\t{\n\t\t\tstd::string account_text (blocks.first);\n\t\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\t\tnano::block_hash hash1 (blocks.second.begin ()->second.get<std::string> (\"\"));\n\t\t\tASSERT_EQ (block1->hash (), hash1);\n\t\t}\n\t}\n\trequest.put (\"sorting\", \"true\"); // Sorting test\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tfor (auto & blocks : response.json.get_child (\"blocks\"))\n\t\t{\n\t\t\tstd::string account_text (blocks.first);\n\t\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\t\tnano::block_hash hash1 (blocks.second.begin ()->first);\n\t\t\tASSERT_EQ (block1->hash (), hash1);\n\t\t\tstd::string amount (blocks.second.begin ()->second.get<std::string> (\"\"));\n\t\t\tASSERT_EQ (\"100\", amount);\n\t\t}\n\t}\n\trequest.put (\"threshold\", \"100\"); // Threshold test\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tstd::unordered_map<nano::block_hash, nano::uint128_union> blocks;\n\t\tfor (auto & pending : response.json.get_child (\"blocks\"))\n\t\t{\n\t\t\tstd::string account_text (pending.first);\n\t\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\t\tfor (auto i (pending.second.begin ()), j (pending.second.end ()); i != j; ++i)\n\t\t\t{\n\t\t\t\tnano::block_hash hash;\n\t\t\t\thash.decode_hex (i->first);\n\t\t\t\tnano::uint128_union amount;\n\t\t\t\tamount.decode_dec (i->second.get<std::string> (\"\"));\n\t\t\t\tblocks[hash] = amount;\n\t\t\t\tboost::optional<std::string> source (i->second.get_optional<std::string> (\"source\"));\n\t\t\t\tASSERT_FALSE (source.is_initialized ());\n\t\t\t}\n\t\t}\n\t\tASSERT_EQ (blocks[block1->hash ()], 100);\n\t}\n\trequest.put (\"source\", \"true\");\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tstd::unordered_map<nano::block_hash, nano::uint128_union> amounts;\n\t\tstd::unordered_map<nano::block_hash, nano::account> sources;\n\t\tfor (auto & pending : response.json.get_child (\"blocks\"))\n\t\t{\n\t\t\tstd::string account_text (pending.first);\n\t\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\t\tfor (auto i (pending.second.begin ()), j (pending.second.end ()); i != j; ++i)\n\t\t\t{\n\t\t\t\tnano::block_hash hash;\n\t\t\t\thash.decode_hex (i->first);\n\t\t\t\tamounts[hash].decode_dec (i->second.get<std::string> (\"amount\"));\n\t\t\t\tsources[hash].decode_account (i->second.get<std::string> (\"source\"));\n\t\t\t}\n\t\t}\n\t\tASSERT_EQ (amounts[block1->hash ()], 100);\n\t\tASSERT_EQ (sources[block1->hash ()], nano::test_genesis_key.pub);\n\t}\n\n\trequest.put (\"include_only_confirmed\", \"true\");\n\tcheck_block_response_count (system, rpc, request, 1);\n\treset_confirmation_height (system.nodes.front ()->store, block1->account ());\n\tcheck_block_response_count (system, rpc, request, 0);\n}\n\nTEST (rpc, blocks)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"blocks\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", system.nodes[0]->latest (nano::genesis_account).to_string ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"hashes\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tfor (auto & blocks : response.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string hash_text (blocks.first);\n\t\tASSERT_EQ (system.nodes[0]->latest (nano::genesis_account).to_string (), hash_text);\n\t\tstd::string blocks_text (blocks.second.get<std::string> (\"\"));\n\t\tASSERT_FALSE (blocks_text.empty ());\n\t}\n}\n\nTEST (rpc, wallet_info)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\tnano::account account (system.wallet (0)->deterministic_insert ());\n\t{\n\t\tauto transaction (system.nodes[0]->wallets.tx_begin (true));\n\t\tsystem.wallet (0)->store.erase (transaction, account);\n\t}\n\taccount = system.wallet (0)->deterministic_insert ();\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_info\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string balance_text (response.json.get<std::string> (\"balance\"));\n\tASSERT_EQ (\"340282366920938463463374607431768211454\", balance_text);\n\tstd::string pending_text (response.json.get<std::string> (\"pending\"));\n\tASSERT_EQ (\"1\", pending_text);\n\tstd::string count_text (response.json.get<std::string> (\"accounts_count\"));\n\tASSERT_EQ (\"3\", count_text);\n\tstd::string adhoc_count (response.json.get<std::string> (\"adhoc_count\"));\n\tASSERT_EQ (\"2\", adhoc_count);\n\tstd::string deterministic_count (response.json.get<std::string> (\"deterministic_count\"));\n\tASSERT_EQ (\"1\", deterministic_count);\n\tstd::string index_text (response.json.get<std::string> (\"deterministic_index\"));\n\tASSERT_EQ (\"2\", index_text);\n}\n\nTEST (rpc, wallet_balances)\n{\n\tnano::system system0 (24000, 1);\n\tsystem0.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_balances\");\n\trequest.put (\"wallet\", system0.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tfor (auto & balances : response.json.get_child (\"balances\"))\n\t{\n\t\tstd::string account_text (balances.first);\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string balance_text (balances.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (\"340282366920938463463374607431768211455\", balance_text);\n\t\tstd::string pending_text (balances.second.get<std::string> (\"pending\"));\n\t\tASSERT_EQ (\"0\", pending_text);\n\t}\n\tnano::keypair key;\n\tsystem0.wallet (0)->insert_adhoc (key.prv);\n\tauto send (system0.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, 1));\n\trequest.put (\"threshold\", \"2\");\n\ttest_response response1 (request, rpc.config.port, system0.io_ctx);\n\twhile (response1.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response1.status);\n\tfor (auto & balances : response1.json.get_child (\"balances\"))\n\t{\n\t\tstd::string account_text (balances.first);\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string balance_text (balances.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (\"340282366920938463463374607431768211454\", balance_text);\n\t\tstd::string pending_text (balances.second.get<std::string> (\"pending\"));\n\t\tASSERT_EQ (\"0\", pending_text);\n\t}\n}\n\nTEST (rpc, pending_exists)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key1;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto hash0 (system.nodes[0]->latest (nano::genesis_account));\n\tauto block1 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key1.pub, 100));\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[0]->active.active (*block1))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\n\tauto pending_exists = [&system, &request, &rpc](const char * exists_a) {\n\t\ttest_response response0 (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response0.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response0.status);\n\t\tstd::string exists_text (response0.json.get<std::string> (\"exists\"));\n\t\tASSERT_EQ (exists_a, exists_text);\n\t};\n\n\trequest.put (\"action\", \"pending_exists\");\n\trequest.put (\"hash\", hash0.to_string ());\n\tpending_exists (\"0\");\n\n\trequest.put (\"hash\", block1->hash ().to_string ());\n\tpending_exists (\"1\");\n\n\trequest.put (\"include_only_confirmed\", \"true\");\n\tpending_exists (\"1\");\n\treset_confirmation_height (system.nodes.front ()->store, block1->account ());\n\tpending_exists (\"0\");\n}\n\nTEST (rpc, wallet_pending)\n{\n\tnano::system system0 (24000, 1);\n\tnano::keypair key1;\n\tsystem0.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem0.wallet (0)->insert_adhoc (key1.prv);\n\tauto block1 (system0.wallet (0)->send_action (nano::test_genesis_key.pub, key1.pub, 100));\n\tauto iterations (0);\n\twhile (system0.nodes[0]->active.active (*block1))\n\t{\n\t\tsystem0.poll ();\n\t\t++iterations;\n\t\tASSERT_LT (iterations, 200);\n\t}\n\tauto & node = system0.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system0.io_ctx, rpc_config);\n\tnano::rpc rpc (system0.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_pending\");\n\trequest.put (\"wallet\", system0.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"count\", \"100\");\n\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (1, response.json.get_child (\"blocks\").size ());\n\tfor (auto & pending : response.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string account_text (pending.first);\n\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\tnano::block_hash hash1 (pending.second.begin ()->second.get<std::string> (\"\"));\n\t\tASSERT_EQ (block1->hash (), hash1);\n\t}\n\trequest.put (\"threshold\", \"100\"); // Threshold test\n\ttest_response response0 (request, rpc.config.port, system0.io_ctx);\n\twhile (response0.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response0.status);\n\tstd::unordered_map<nano::block_hash, nano::uint128_union> blocks;\n\tASSERT_EQ (1, response0.json.get_child (\"blocks\").size ());\n\tfor (auto & pending : response0.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string account_text (pending.first);\n\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\tfor (auto i (pending.second.begin ()), j (pending.second.end ()); i != j; ++i)\n\t\t{\n\t\t\tnano::block_hash hash;\n\t\t\thash.decode_hex (i->first);\n\t\t\tnano::uint128_union amount;\n\t\t\tamount.decode_dec (i->second.get<std::string> (\"\"));\n\t\t\tblocks[hash] = amount;\n\t\t\tboost::optional<std::string> source (i->second.get_optional<std::string> (\"source\"));\n\t\t\tASSERT_FALSE (source.is_initialized ());\n\t\t\tboost::optional<uint8_t> min_version (i->second.get_optional<uint8_t> (\"min_version\"));\n\t\t\tASSERT_FALSE (min_version.is_initialized ());\n\t\t}\n\t}\n\tASSERT_EQ (blocks[block1->hash ()], 100);\n\trequest.put (\"threshold\", \"101\");\n\ttest_response response1 (request, rpc.config.port, system0.io_ctx);\n\twhile (response1.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto & pending1 (response1.json.get_child (\"blocks\"));\n\tASSERT_EQ (0, pending1.size ());\n\trequest.put (\"threshold\", \"0\");\n\trequest.put (\"source\", \"true\");\n\trequest.put (\"min_version\", \"true\");\n\ttest_response response2 (request, rpc.config.port, system0.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tsystem0.poll ();\n\t}\n\tASSERT_EQ (200, response2.status);\n\tstd::unordered_map<nano::block_hash, nano::uint128_union> amounts;\n\tstd::unordered_map<nano::block_hash, nano::account> sources;\n\tASSERT_EQ (1, response0.json.get_child (\"blocks\").size ());\n\tfor (auto & pending : response2.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string account_text (pending.first);\n\t\tASSERT_EQ (key1.pub.to_account (), account_text);\n\t\tfor (auto i (pending.second.begin ()), j (pending.second.end ()); i != j; ++i)\n\t\t{\n\t\t\tnano::block_hash hash;\n\t\t\thash.decode_hex (i->first);\n\t\t\tamounts[hash].decode_dec (i->second.get<std::string> (\"amount\"));\n\t\t\tsources[hash].decode_account (i->second.get<std::string> (\"source\"));\n\t\t\tASSERT_EQ (i->second.get<uint8_t> (\"min_version\"), 0);\n\t\t}\n\t}\n\tASSERT_EQ (amounts[block1->hash ()], 100);\n\tASSERT_EQ (sources[block1->hash ()], nano::test_genesis_key.pub);\n\n\trequest.put (\"include_only_confirmed\", \"true\");\n\tcheck_block_response_count (system0, rpc, request, 1);\n\treset_confirmation_height (system0.nodes.front ()->store, block1->account ());\n\n\t{\n\t\ttest_response response (request, rpc.config.port, system0.io_ctx);\n\t\tsystem0.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system0.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tASSERT_EQ (0, response.json.get_child (\"blocks\").size ());\n\t}\n}\n\nTEST (rpc, receive_minimum)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"receive_minimum\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string amount (response.json.get<std::string> (\"amount\"));\n\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), amount);\n}\n\nTEST (rpc, receive_minimum_set)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"receive_minimum_set\");\n\trequest.put (\"amount\", \"100\");\n\tASSERT_NE (system.nodes[0]->config.receive_minimum.to_string_dec (), \"100\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string success (response.json.get<std::string> (\"success\"));\n\tASSERT_TRUE (success.empty ());\n\tASSERT_EQ (system.nodes[0]->config.receive_minimum.to_string_dec (), \"100\");\n}\n\nTEST (rpc, work_get)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->work_cache_blocking (nano::test_genesis_key.pub, system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"work_get\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string work_text (response.json.get<std::string> (\"work\"));\n\tuint64_t work (1);\n\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\tsystem.nodes[0]->wallets.items.begin ()->second->store.work_get (transaction, nano::genesis_account, work);\n\tASSERT_EQ (nano::to_string_hex (work), work_text);\n}\n\nTEST (rpc, wallet_work_get)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->work_cache_blocking (nano::test_genesis_key.pub, system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_work_get\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\tfor (auto & works : response.json.get_child (\"works\"))\n\t{\n\t\tstd::string account_text (works.first);\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string work_text (works.second.get<std::string> (\"\"));\n\t\tuint64_t work (1);\n\t\tsystem.nodes[0]->wallets.items.begin ()->second->store.work_get (transaction, nano::genesis_account, work);\n\t\tASSERT_EQ (nano::to_string_hex (work), work_text);\n\t}\n}\n\nTEST (rpc, work_set)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tuint64_t work0 (100);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"work_set\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"work\", nano::to_string_hex (work0));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string success (response.json.get<std::string> (\"success\"));\n\tASSERT_TRUE (success.empty ());\n\tuint64_t work1 (1);\n\tauto transaction (system.nodes[0]->wallets.tx_begin ());\n\tsystem.nodes[0]->wallets.items.begin ()->second->store.work_get (transaction, nano::genesis_account, work1);\n\tASSERT_EQ (work1, work0);\n}\n\nTEST (rpc, search_pending_all)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block block (latest, nano::test_genesis_key.pub, nano::genesis_amount - system.nodes[0]->config.receive_minimum.number (), nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.nodes[0]->work_generate_blocking (latest));\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->ledger.process (transaction, block).code);\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"search_pending_all\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->balance (nano::test_genesis_key.pub) != nano::genesis_amount)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n}\n\nTEST (rpc, wallet_republish)\n{\n\tnano::system system (24000, 1);\n\tnano::genesis genesis;\n\tnano::keypair key;\n\twhile (key.pub < nano::test_genesis_key.pub)\n\t{\n\t\tnano::keypair key1;\n\t\tkey.pub = key1.pub;\n\t\tkey.prv.data = key1.prv.data;\n\t}\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsystem.nodes[0]->process (send);\n\tnano::open_block open (send.hash (), key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->process (open).code);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_republish\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"count\", 1);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & blocks_node (response.json.get_child (\"blocks\"));\n\tstd::vector<nano::block_hash> blocks;\n\tfor (auto i (blocks_node.begin ()), n (blocks_node.end ()); i != n; ++i)\n\t{\n\t\tblocks.push_back (nano::block_hash (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (2, blocks.size ());\n\tASSERT_EQ (send.hash (), blocks[0]);\n\tASSERT_EQ (open.hash (), blocks[1]);\n}\n\nTEST (rpc, delegators)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsystem.nodes[0]->process (send);\n\tnano::open_block open (send.hash (), nano::test_genesis_key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->process (open).code);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"delegators\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & delegators_node (response.json.get_child (\"delegators\"));\n\tboost::property_tree::ptree delegators;\n\tfor (auto i (delegators_node.begin ()), n (delegators_node.end ()); i != n; ++i)\n\t{\n\t\tdelegators.put ((i->first), (i->second.get<std::string> (\"\")));\n\t}\n\tASSERT_EQ (2, delegators.size ());\n\tASSERT_EQ (\"100\", delegators.get<std::string> (nano::test_genesis_key.pub.to_account ()));\n\tASSERT_EQ (\"340282366920938463463374607431768211355\", delegators.get<std::string> (key.pub.to_account ()));\n}\n\nTEST (rpc, delegators_count)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (node1.latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tnode1.process (send);\n\tnano::open_block open (send.hash (), nano::test_genesis_key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->process (open).code);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"delegators_count\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string count (response.json.get<std::string> (\"count\"));\n\tASSERT_EQ (\"2\", count);\n}\n\nTEST (rpc, account_info)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsystem.nodes[0]->process (send);\n\tauto time (nano::seconds_since_epoch ());\n\n\t{\n\t\tauto transaction = system.nodes[0]->store.tx_begin_write ();\n\t\tnano::account_info account_info;\n\t\tASSERT_FALSE (node1.store.account_get (transaction, nano::test_genesis_key.pub, account_info));\n\t\taccount_info.confirmation_height = 1;\n\t\tnode1.store.account_put (transaction, nano::test_genesis_key.pub, account_info);\n\t}\n\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_info\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string frontier (response.json.get<std::string> (\"frontier\"));\n\tASSERT_EQ (send.hash ().to_string (), frontier);\n\tstd::string open_block (response.json.get<std::string> (\"open_block\"));\n\tASSERT_EQ (genesis.hash ().to_string (), open_block);\n\tstd::string representative_block (response.json.get<std::string> (\"representative_block\"));\n\tASSERT_EQ (genesis.hash ().to_string (), representative_block);\n\tstd::string balance (response.json.get<std::string> (\"balance\"));\n\tASSERT_EQ (\"100\", balance);\n\tstd::string modified_timestamp (response.json.get<std::string> (\"modified_timestamp\"));\n\tASSERT_LT (std::abs ((long)time - stol (modified_timestamp)), 5);\n\tstd::string block_count (response.json.get<std::string> (\"block_count\"));\n\tASSERT_EQ (\"2\", block_count);\n\tstd::string confirmation_height (response.json.get<std::string> (\"confirmation_height\"));\n\tASSERT_EQ (\"1\", confirmation_height);\n\tASSERT_EQ (0, response.json.get<uint8_t> (\"account_version\"));\n\tboost::optional<std::string> weight (response.json.get_optional<std::string> (\"weight\"));\n\tASSERT_FALSE (weight.is_initialized ());\n\tboost::optional<std::string> pending (response.json.get_optional<std::string> (\"pending\"));\n\tASSERT_FALSE (pending.is_initialized ());\n\tboost::optional<std::string> representative (response.json.get_optional<std::string> (\"representative\"));\n\tASSERT_FALSE (representative.is_initialized ());\n\t// Test for optional values\n\trequest.put (\"weight\", \"true\");\n\trequest.put (\"pending\", \"1\");\n\trequest.put (\"representative\", \"1\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tstd::string weight2 (response2.json.get<std::string> (\"weight\"));\n\tASSERT_EQ (\"100\", weight2);\n\tstd::string pending2 (response2.json.get<std::string> (\"pending\"));\n\tASSERT_EQ (\"0\", pending2);\n\tstd::string representative2 (response2.json.get<std::string> (\"representative\"));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), representative2);\n}\n\n/** Make sure we can use json block literals instead of string as input */\nTEST (rpc, json_block_input)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, node1.latest (nano::test_genesis_key.pub), nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"sign\");\n\trequest.put (\"json_block\", \"true\");\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"account\", key.pub.to_account ());\n\tboost::property_tree::ptree json;\n\tsend.serialize_json (json);\n\trequest.add_child (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\n\tbool json_error{ false };\n\tnano::state_block block (json_error, response.json.get_child (\"block\"));\n\tASSERT_FALSE (json_error);\n\n\tASSERT_FALSE (nano::validate_message (key.pub, send.hash (), block.block_signature ()));\n\tASSERT_NE (block.block_signature (), send.block_signature ());\n\tASSERT_EQ (block.hash (), send.hash ());\n}\n\n/** Make sure we can receive json block literals instead of string as output */\nTEST (rpc, json_block_output)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsystem.nodes[0]->process (send);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_info\");\n\trequest.put (\"json_block\", \"true\");\n\trequest.put (\"hash\", send.hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\n\t// Make sure contents contains a valid JSON subtree instread of stringified json\n\tbool json_error{ false };\n\tnano::send_block send_from_json (json_error, response.json.get_child (\"contents\"));\n\tASSERT_FALSE (json_error);\n}\n\nTEST (rpc, blocks_info)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"blocks_info\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", system.nodes[0]->latest (nano::genesis_account).to_string ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"hashes\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tfor (auto & blocks : response.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string hash_text (blocks.first);\n\t\tASSERT_EQ (system.nodes[0]->latest (nano::genesis_account).to_string (), hash_text);\n\t\tstd::string account_text (blocks.second.get<std::string> (\"block_account\"));\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), account_text);\n\t\tstd::string amount_text (blocks.second.get<std::string> (\"amount\"));\n\t\tASSERT_EQ (nano::genesis_amount.convert_to<std::string> (), amount_text);\n\t\tstd::string blocks_text (blocks.second.get<std::string> (\"contents\"));\n\t\tASSERT_FALSE (blocks_text.empty ());\n\t\tboost::optional<std::string> pending (blocks.second.get_optional<std::string> (\"pending\"));\n\t\tASSERT_FALSE (pending.is_initialized ());\n\t\tboost::optional<std::string> source (blocks.second.get_optional<std::string> (\"source_account\"));\n\t\tASSERT_FALSE (source.is_initialized ());\n\t\tstd::string balance_text (blocks.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (nano::genesis_amount.convert_to<std::string> (), balance_text);\n\t\tASSERT_TRUE (blocks.second.get<bool> (\"confirmed\")); // Genesis block is confirmed by default\n\t}\n\t// Test for optional values\n\trequest.put (\"source\", \"true\");\n\trequest.put (\"pending\", \"1\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tfor (auto & blocks : response2.json.get_child (\"blocks\"))\n\t{\n\t\tstd::string source (blocks.second.get<std::string> (\"source_account\"));\n\t\tASSERT_EQ (\"0\", source);\n\t\tstd::string pending (blocks.second.get<std::string> (\"pending\"));\n\t\tASSERT_EQ (\"0\", pending);\n\t}\n}\n\nTEST (rpc, blocks_info_subtype)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, nano::Gxrb_ratio));\n\tASSERT_NE (nullptr, send);\n\tauto receive (system.wallet (0)->receive_action (*send, key.pub, nano::Gxrb_ratio));\n\tASSERT_NE (nullptr, receive);\n\tauto change (system.wallet (0)->change_action (nano::test_genesis_key.pub, key.pub));\n\tASSERT_NE (nullptr, change);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"blocks_info\");\n\tboost::property_tree::ptree peers_l;\n\tboost::property_tree::ptree entry;\n\tentry.put (\"\", send->hash ().to_string ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\tentry.put (\"\", receive->hash ().to_string ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\tentry.put (\"\", change->hash ().to_string ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"hashes\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto & blocks (response.json.get_child (\"blocks\"));\n\tASSERT_EQ (3, blocks.size ());\n\tauto send_subtype (blocks.get_child (send->hash ().to_string ()).get<std::string> (\"subtype\"));\n\tASSERT_EQ (send_subtype, \"send\");\n\tauto receive_subtype (blocks.get_child (receive->hash ().to_string ()).get<std::string> (\"subtype\"));\n\tASSERT_EQ (receive_subtype, \"receive\");\n\tauto change_subtype (blocks.get_child (change->hash ().to_string ()).get<std::string> (\"subtype\"));\n\tASSERT_EQ (change_subtype, \"change\");\n}\n\nTEST (rpc, work_peers_all)\n{\n\tnano::system system (24000, 1);\n\tauto & node1 (*system.nodes[0]);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"work_peer_add\");\n\trequest.put (\"address\", \"::1\");\n\trequest.put (\"port\", \"0\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string success (response.json.get<std::string> (\"success\", \"\"));\n\tASSERT_TRUE (success.empty ());\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"work_peers\");\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tauto & peers_node (response1.json.get_child (\"work_peers\"));\n\tstd::vector<std::string> peers;\n\tfor (auto i (peers_node.begin ()), n (peers_node.end ()); i != n; ++i)\n\t{\n\t\tpeers.push_back (i->second.get<std::string> (\"\"));\n\t}\n\tASSERT_EQ (1, peers.size ());\n\tASSERT_EQ (\"::1:0\", peers[0]);\n\tboost::property_tree::ptree request2;\n\trequest2.put (\"action\", \"work_peers_clear\");\n\ttest_response response2 (request2, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tsuccess = response2.json.get<std::string> (\"success\", \"\");\n\tASSERT_TRUE (success.empty ());\n\ttest_response response3 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response3.status);\n\tpeers_node = response3.json.get_child (\"work_peers\");\n\tASSERT_EQ (0, peers_node.size ());\n}\n\nTEST (rpc, block_count_type)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tauto receive (system.wallet (0)->receive_action (*send, nano::test_genesis_key.pub, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_count_type\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string send_count (response.json.get<std::string> (\"send\"));\n\tASSERT_EQ (\"0\", send_count);\n\tstd::string receive_count (response.json.get<std::string> (\"receive\"));\n\tASSERT_EQ (\"0\", receive_count);\n\tstd::string open_count (response.json.get<std::string> (\"open\"));\n\tASSERT_EQ (\"1\", open_count);\n\tstd::string change_count (response.json.get<std::string> (\"change\"));\n\tASSERT_EQ (\"0\", change_count);\n\tstd::string state_count (response.json.get<std::string> (\"state\"));\n\tASSERT_EQ (\"2\", state_count);\n}\n\nTEST (rpc, ledger)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (node1.latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tnode1.process (send);\n\tnano::open_block open (send.hash (), nano::test_genesis_key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, node1.process (open).code);\n\tauto time (nano::seconds_since_epoch ());\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"ledger\");\n\trequest.put (\"sorting\", \"1\");\n\trequest.put (\"count\", \"1\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tfor (auto & accounts : response.json.get_child (\"accounts\"))\n\t{\n\t\tstd::string account_text (accounts.first);\n\t\tASSERT_EQ (key.pub.to_account (), account_text);\n\t\tstd::string frontier (accounts.second.get<std::string> (\"frontier\"));\n\t\tASSERT_EQ (open.hash ().to_string (), frontier);\n\t\tstd::string open_block (accounts.second.get<std::string> (\"open_block\"));\n\t\tASSERT_EQ (open.hash ().to_string (), open_block);\n\t\tstd::string representative_block (accounts.second.get<std::string> (\"representative_block\"));\n\t\tASSERT_EQ (open.hash ().to_string (), representative_block);\n\t\tstd::string balance_text (accounts.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (\"340282366920938463463374607431768211355\", balance_text);\n\t\tstd::string modified_timestamp (accounts.second.get<std::string> (\"modified_timestamp\"));\n\t\tASSERT_LT (std::abs ((long)time - stol (modified_timestamp)), 5);\n\t\tstd::string block_count (accounts.second.get<std::string> (\"block_count\"));\n\t\tASSERT_EQ (\"1\", block_count);\n\t\tboost::optional<std::string> weight (accounts.second.get_optional<std::string> (\"weight\"));\n\t\tASSERT_FALSE (weight.is_initialized ());\n\t\tboost::optional<std::string> pending (accounts.second.get_optional<std::string> (\"pending\"));\n\t\tASSERT_FALSE (pending.is_initialized ());\n\t\tboost::optional<std::string> representative (accounts.second.get_optional<std::string> (\"representative\"));\n\t\tASSERT_FALSE (representative.is_initialized ());\n\t}\n\t// Test for optional values\n\trequest.put (\"weight\", \"1\");\n\trequest.put (\"pending\", \"1\");\n\trequest.put (\"representative\", \"true\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tfor (auto & accounts : response2.json.get_child (\"accounts\"))\n\t{\n\t\tboost::optional<std::string> weight (accounts.second.get_optional<std::string> (\"weight\"));\n\t\tASSERT_TRUE (weight.is_initialized ());\n\t\tASSERT_EQ (\"0\", weight.get ());\n\t\tboost::optional<std::string> pending (accounts.second.get_optional<std::string> (\"pending\"));\n\t\tASSERT_TRUE (pending.is_initialized ());\n\t\tASSERT_EQ (\"0\", pending.get ());\n\t\tboost::optional<std::string> representative (accounts.second.get_optional<std::string> (\"representative\"));\n\t\tASSERT_TRUE (representative.is_initialized ());\n\t\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), representative.get ());\n\t}\n}\n\nTEST (rpc, accounts_create)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"accounts_create\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"count\", \"8\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & accounts (response.json.get_child (\"accounts\"));\n\tfor (auto i (accounts.begin ()), n (accounts.end ()); i != n; ++i)\n\t{\n\t\tstd::string account_text (i->second.get<std::string> (\"\"));\n\t\tnano::uint256_union account;\n\t\tASSERT_FALSE (account.decode_account (account_text));\n\t\tASSERT_TRUE (system.wallet (0)->exists (account));\n\t}\n\tASSERT_EQ (8, accounts.size ());\n}\n\nTEST (rpc, block_create)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (node1.latest (nano::test_genesis_key.pub));\n\tauto send_work = node1.work_generate_blocking (latest);\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, send_work);\n\tauto open_work = node1.work_generate_blocking (key.pub);\n\tnano::open_block open (send.hash (), nano::test_genesis_key.pub, key.pub, key.prv, key.pub, open_work);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_create\");\n\trequest.put (\"type\", \"send\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"previous\", latest.to_string ());\n\trequest.put (\"amount\", \"340282366920938463463374607431768211355\");\n\trequest.put (\"destination\", key.pub.to_account ());\n\trequest.put (\"work\", nano::to_string_hex (send_work));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string send_hash (response.json.get<std::string> (\"hash\"));\n\tASSERT_EQ (send.hash ().to_string (), send_hash);\n\tauto send_text (response.json.get<std::string> (\"block\"));\n\tboost::property_tree::ptree block_l;\n\tstd::stringstream block_stream (send_text);\n\tboost::property_tree::read_json (block_stream, block_l);\n\tauto send_block (nano::deserialize_block_json (block_l));\n\tASSERT_EQ (send.hash (), send_block->hash ());\n\tsystem.nodes[0]->process (send);\n\tboost::property_tree::ptree request1;\n\trequest1.put (\"action\", \"block_create\");\n\trequest1.put (\"type\", \"open\");\n\tstd::string key_text;\n\tkey.prv.data.encode_hex (key_text);\n\trequest1.put (\"key\", key_text);\n\trequest1.put (\"representative\", nano::test_genesis_key.pub.to_account ());\n\trequest1.put (\"source\", send.hash ().to_string ());\n\trequest1.put (\"work\", nano::to_string_hex (open_work));\n\ttest_response response1 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response1.status);\n\tstd::string open_hash (response1.json.get<std::string> (\"hash\"));\n\tASSERT_EQ (open.hash ().to_string (), open_hash);\n\tauto open_text (response1.json.get<std::string> (\"block\"));\n\tstd::stringstream block_stream1 (open_text);\n\tboost::property_tree::read_json (block_stream1, block_l);\n\tauto open_block (nano::deserialize_block_json (block_l));\n\tASSERT_EQ (open.hash (), open_block->hash ());\n\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->process (open).code);\n\trequest1.put (\"representative\", key.pub.to_account ());\n\ttest_response response2 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response2.status);\n\tstd::string open2_hash (response2.json.get<std::string> (\"hash\"));\n\tASSERT_NE (open.hash ().to_string (), open2_hash); // different blocks with wrong representative\n\tauto change_work = node1.work_generate_blocking (open.hash ());\n\tnano::change_block change (open.hash (), key.pub, key.prv, key.pub, change_work);\n\trequest1.put (\"type\", \"change\");\n\trequest1.put (\"work\", nano::to_string_hex (change_work));\n\ttest_response response4 (request1, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response4.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response4.status);\n\tstd::string change_hash (response4.json.get<std::string> (\"hash\"));\n\tASSERT_EQ (change.hash ().to_string (), change_hash);\n\tauto change_text (response4.json.get<std::string> (\"block\"));\n\tstd::stringstream block_stream4 (change_text);\n\tboost::property_tree::read_json (block_stream4, block_l);\n\tauto change_block (nano::deserialize_block_json (block_l));\n\tASSERT_EQ (change.hash (), change_block->hash ());\n\tASSERT_EQ (nano::process_result::progress, node1.process (change).code);\n\tnano::send_block send2 (send.hash (), key.pub, 0, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (send.hash ()));\n\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->process (send2).code);\n\tboost::property_tree::ptree request2;\n\trequest2.put (\"action\", \"block_create\");\n\trequest2.put (\"type\", \"receive\");\n\trequest2.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest2.put (\"account\", key.pub.to_account ());\n\trequest2.put (\"source\", send2.hash ().to_string ());\n\trequest2.put (\"previous\", change.hash ().to_string ());\n\trequest2.put (\"work\", nano::to_string_hex (node1.work_generate_blocking (change.hash ())));\n\ttest_response response5 (request2, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response5.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response5.status);\n\tstd::string receive_hash (response4.json.get<std::string> (\"hash\"));\n\tauto receive_text (response5.json.get<std::string> (\"block\"));\n\tstd::stringstream block_stream5 (change_text);\n\tboost::property_tree::read_json (block_stream5, block_l);\n\tauto receive_block (nano::deserialize_block_json (block_l));\n\tASSERT_EQ (receive_hash, receive_block->hash ().to_string ());\n\tsystem.nodes[0]->process_active (std::move (receive_block));\n\tlatest = system.nodes[0]->latest (key.pub);\n\tASSERT_EQ (receive_hash, latest.to_string ());\n}\n\nTEST (rpc, block_create_state)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_create\");\n\trequest.put (\"type\", \"state\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"previous\", genesis.hash ().to_string ());\n\trequest.put (\"representative\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"balance\", (nano::genesis_amount - nano::Gxrb_ratio).convert_to<std::string> ());\n\trequest.put (\"link\", key.pub.to_account ());\n\trequest.put (\"work\", nano::to_string_hex (system.nodes[0]->work_generate_blocking (genesis.hash ())));\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string state_hash (response.json.get<std::string> (\"hash\"));\n\tauto state_text (response.json.get<std::string> (\"block\"));\n\tstd::stringstream block_stream (state_text);\n\tboost::property_tree::ptree block_l;\n\tboost::property_tree::read_json (block_stream, block_l);\n\tauto state_block (nano::deserialize_block_json (block_l));\n\tASSERT_NE (nullptr, state_block);\n\tASSERT_EQ (nano::block_type::state, state_block->type ());\n\tASSERT_EQ (state_hash, state_block->hash ().to_string ());\n\tauto process_result (system.nodes[0]->process (*state_block));\n\tASSERT_EQ (nano::process_result::progress, process_result.code);\n}\n\nTEST (rpc, block_create_state_open)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto send_block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tASSERT_NE (nullptr, send_block);\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_create\");\n\trequest.put (\"type\", \"state\");\n\trequest.put (\"key\", key.prv.data.to_string ());\n\trequest.put (\"account\", key.pub.to_account ());\n\trequest.put (\"previous\", 0);\n\trequest.put (\"representative\", nano::test_genesis_key.pub.to_account ());\n\trequest.put (\"balance\", nano::Gxrb_ratio.convert_to<std::string> ());\n\trequest.put (\"link\", send_block->hash ().to_string ());\n\trequest.put (\"work\", nano::to_string_hex (system.nodes[0]->work_generate_blocking (send_block->hash ())));\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string state_hash (response.json.get<std::string> (\"hash\"));\n\tauto state_text (response.json.get<std::string> (\"block\"));\n\tstd::stringstream block_stream (state_text);\n\tboost::property_tree::ptree block_l;\n\tboost::property_tree::read_json (block_stream, block_l);\n\tauto state_block (nano::deserialize_block_json (block_l));\n\tASSERT_NE (nullptr, state_block);\n\tASSERT_EQ (nano::block_type::state, state_block->type ());\n\tASSERT_EQ (state_hash, state_block->hash ().to_string ());\n\tASSERT_TRUE (system.nodes[0]->latest (key.pub).is_zero ());\n\tauto process_result (system.nodes[0]->process (*state_block));\n\tASSERT_EQ (nano::process_result::progress, process_result.code);\n\tASSERT_FALSE (system.nodes[0]->latest (key.pub).is_zero ());\n}\n\n// Missing \"work\" parameter should cause work to be generated for us.\nTEST (rpc, block_create_state_request_work)\n{\n\tnano::genesis genesis;\n\n\t// Test work generation for state blocks both with and without previous (in the latter\n\t// case, the account will be used for work generation)\n\tstd::vector<std::string> previous_test_input{ genesis.hash ().to_string (), std::string (\"0\") };\n\tfor (auto previous : previous_test_input)\n\t{\n\t\tnano::system system (24000, 1);\n\t\tnano::keypair key;\n\t\tnano::genesis genesis;\n\t\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"block_create\");\n\t\trequest.put (\"type\", \"state\");\n\t\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\t\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\t\trequest.put (\"representative\", nano::test_genesis_key.pub.to_account ());\n\t\trequest.put (\"balance\", (nano::genesis_amount - nano::Gxrb_ratio).convert_to<std::string> ());\n\t\trequest.put (\"link\", key.pub.to_account ());\n\t\trequest.put (\"previous\", previous);\n\t\tauto node = system.nodes.front ();\n\t\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\t\tnano::node_rpc_config node_rpc_config;\n\t\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\t\tnano::rpc_config rpc_config (true);\n\t\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\t\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\t\trpc.start ();\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tboost::property_tree::ptree block_l;\n\t\tstd::stringstream block_stream (response.json.get<std::string> (\"block\"));\n\t\tboost::property_tree::read_json (block_stream, block_l);\n\t\tauto block (nano::deserialize_block_json (block_l));\n\t\tASSERT_NE (nullptr, block);\n\t\tASSERT_FALSE (nano::work_validate (*block));\n\t}\n}\n\nTEST (rpc, block_hash)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tauto & node1 (*system.nodes[0]);\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_hash\");\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string send_hash (response.json.get<std::string> (\"hash\"));\n\tASSERT_EQ (send.hash ().to_string (), send_hash);\n}\n\nTEST (rpc, wallet_lock)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\t{\n\t\tauto transaction (system.wallet (0)->wallets.tx_begin ());\n\t\tASSERT_TRUE (system.wallet (0)->store.valid_password (transaction));\n\t}\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_lock\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"locked\"));\n\tASSERT_EQ (account_text1, \"1\");\n\tauto transaction (system.wallet (0)->wallets.tx_begin ());\n\tASSERT_FALSE (system.wallet (0)->store.valid_password (transaction));\n}\n\nTEST (rpc, wallet_locked)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_locked\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string account_text1 (response.json.get<std::string> (\"locked\"));\n\tASSERT_EQ (account_text1, \"0\");\n}\n\nTEST (rpc, wallet_create_fail)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\t// lmdb_max_dbs should be removed once the wallet store is refactored to support more wallets.\n\tfor (int i = 0; i < 127; i++)\n\t{\n\t\tnano::keypair key;\n\t\tnode->wallets.create (key.pub);\n\t}\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_create\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (\"Failed to create wallet. Increase lmdb_max_dbs in node config\", response.json.get<std::string> (\"error\"));\n}\n\nTEST (rpc, wallet_ledger)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tauto & node1 (*system.nodes[0]);\n\tauto latest (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tnano::send_block send (latest, key.pub, 100, nano::test_genesis_key.prv, nano::test_genesis_key.pub, node1.work_generate_blocking (latest));\n\tsystem.nodes[0]->process (send);\n\tnano::open_block open (send.hash (), nano::test_genesis_key.pub, key.pub, key.prv, key.pub, node1.work_generate_blocking (key.pub));\n\tASSERT_EQ (nano::process_result::progress, node1.process (open).code);\n\tauto time (nano::seconds_since_epoch ());\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_ledger\");\n\trequest.put (\"wallet\", system.nodes[0]->wallets.items.begin ()->first.to_string ());\n\trequest.put (\"sorting\", \"1\");\n\trequest.put (\"count\", \"1\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tfor (auto & accounts : response.json.get_child (\"accounts\"))\n\t{\n\t\tstd::string account_text (accounts.first);\n\t\tASSERT_EQ (key.pub.to_account (), account_text);\n\t\tstd::string frontier (accounts.second.get<std::string> (\"frontier\"));\n\t\tASSERT_EQ (open.hash ().to_string (), frontier);\n\t\tstd::string open_block (accounts.second.get<std::string> (\"open_block\"));\n\t\tASSERT_EQ (open.hash ().to_string (), open_block);\n\t\tstd::string representative_block (accounts.second.get<std::string> (\"representative_block\"));\n\t\tASSERT_EQ (open.hash ().to_string (), representative_block);\n\t\tstd::string balance_text (accounts.second.get<std::string> (\"balance\"));\n\t\tASSERT_EQ (\"340282366920938463463374607431768211355\", balance_text);\n\t\tstd::string modified_timestamp (accounts.second.get<std::string> (\"modified_timestamp\"));\n\t\tASSERT_LT (std::abs ((long)time - stol (modified_timestamp)), 5);\n\t\tstd::string block_count (accounts.second.get<std::string> (\"block_count\"));\n\t\tASSERT_EQ (\"1\", block_count);\n\t\tboost::optional<std::string> weight (accounts.second.get_optional<std::string> (\"weight\"));\n\t\tASSERT_FALSE (weight.is_initialized ());\n\t\tboost::optional<std::string> pending (accounts.second.get_optional<std::string> (\"pending\"));\n\t\tASSERT_FALSE (pending.is_initialized ());\n\t\tboost::optional<std::string> representative (accounts.second.get_optional<std::string> (\"representative\"));\n\t\tASSERT_FALSE (representative.is_initialized ());\n\t}\n\t// Test for optional values\n\trequest.put (\"weight\", \"true\");\n\trequest.put (\"pending\", \"1\");\n\trequest.put (\"representative\", \"false\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tfor (auto & accounts : response2.json.get_child (\"accounts\"))\n\t{\n\t\tboost::optional<std::string> weight (accounts.second.get_optional<std::string> (\"weight\"));\n\t\tASSERT_TRUE (weight.is_initialized ());\n\t\tASSERT_EQ (\"0\", weight.get ());\n\t\tboost::optional<std::string> pending (accounts.second.get_optional<std::string> (\"pending\"));\n\t\tASSERT_TRUE (pending.is_initialized ());\n\t\tASSERT_EQ (\"0\", pending.get ());\n\t\tboost::optional<std::string> representative (accounts.second.get_optional<std::string> (\"representative\"));\n\t\tASSERT_FALSE (representative.is_initialized ());\n\t}\n}\n\nTEST (rpc, wallet_add_watch)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"action\", \"wallet_add_watch\");\n\tboost::property_tree::ptree entry;\n\tboost::property_tree::ptree peers_l;\n\tentry.put (\"\", nano::test_genesis_key.pub.to_account ());\n\tpeers_l.push_back (std::make_pair (\"\", entry));\n\trequest.add_child (\"accounts\", peers_l);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string success (response.json.get<std::string> (\"success\"));\n\tASSERT_TRUE (success.empty ());\n\tASSERT_TRUE (system.wallet (0)->exists (nano::test_genesis_key.pub));\n}\n\nTEST (rpc, online_reps)\n{\n\tnano::system system (24000, 2);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tASSERT_TRUE (system.nodes[1]->online_reps.online_stake () == system.nodes[1]->config.online_weight_minimum.number ());\n\tauto send_block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tASSERT_NE (nullptr, send_block);\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[1]->online_reps.list ().empty ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tenable_ipc_transport_tcp (system.nodes[1]->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*system.nodes[1], node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"representatives_online\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto representatives (response.json.get_child (\"representatives\"));\n\tauto item (representatives.begin ());\n\tASSERT_NE (representatives.end (), item);\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), item->second.get<std::string> (\"\"));\n\tboost::optional<std::string> weight (item->second.get_optional<std::string> (\"weight\"));\n\tASSERT_FALSE (weight.is_initialized ());\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[1]->block (send_block->hash ()) == nullptr)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\t//Test weight option\n\trequest.put (\"weight\", \"true\");\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto representatives2 (response2.json.get_child (\"representatives\"));\n\tauto item2 (representatives2.begin ());\n\tASSERT_NE (representatives2.end (), item2);\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), item2->first);\n\tauto weight2 (item2->second.get<std::string> (\"weight\"));\n\tASSERT_EQ (system.nodes[1]->weight (nano::test_genesis_key.pub).convert_to<std::string> (), weight2);\n\t//Test accounts filter\n\tauto new_rep (system.wallet (1)->deterministic_insert ());\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, new_rep, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[1]->block (send->hash ()) == nullptr)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto receive (system.wallet (1)->receive_action (*send, new_rep, system.nodes[0]->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[1]->block (receive->hash ()) == nullptr)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto change (system.wallet (0)->change_action (nano::test_genesis_key.pub, new_rep));\n\tASSERT_NE (nullptr, change);\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[1]->block (change->hash ()) == nullptr)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tsystem.deadline_set (5s);\n\twhile (system.nodes[1]->online_reps.list ().size () != 2)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tboost::property_tree::ptree child_rep;\n\tchild_rep.put (\"\", new_rep.to_account ());\n\tboost::property_tree::ptree filtered_accounts;\n\tfiltered_accounts.push_back (std::make_pair (\"\", child_rep));\n\trequest.add_child (\"accounts\", filtered_accounts);\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto representatives3 (response3.json.get_child (\"representatives\"));\n\tauto item3 (representatives3.begin ());\n\tASSERT_NE (representatives3.end (), item3);\n\tASSERT_EQ (new_rep.to_account (), item3->first);\n\tASSERT_EQ (representatives3.size (), 1);\n\tsystem.nodes[1]->stop ();\n}\n\n// If this test fails, try increasing the num_blocks size.\nTEST (rpc, confirmation_height_currently_processing)\n{\n\t// The chains should be longer than the\tbatch_write_size to test the amount of blocks confirmed is correct.\n\tbool delay_frontier_confirmation_height_updating = true;\n\tnano::system system;\n\tauto node = system.add_node (nano::node_config (24000, system.logging), delay_frontier_confirmation_height_updating);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\n\t// Do enough blocks to reliably call RPC before the confirmation height has finished\n\tconstexpr auto num_blocks = 500;\n\tauto previous_genesis_chain_hash = node->latest (nano::test_genesis_key.pub);\n\t{\n\t\tauto transaction = node->store.tx_begin_write ();\n\t\tfor (auto i = num_blocks; i > 0; --i)\n\t\t{\n\t\t\tnano::send_block send (previous_genesis_chain_hash, nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio + i + 1, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.work.generate (previous_genesis_chain_hash));\n\t\t\tASSERT_EQ (nano::process_result::progress, node->ledger.process (transaction, send).code);\n\t\t\tprevious_genesis_chain_hash = send.hash ();\n\t\t}\n\n\t\tnano::keypair key1;\n\t\tnano::send_block send (previous_genesis_chain_hash, key1.pub, nano::genesis_amount - nano::Gxrb_ratio - 1, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.work.generate (previous_genesis_chain_hash));\n\t\tASSERT_EQ (nano::process_result::progress, node->ledger.process (transaction, send).code);\n\t\tprevious_genesis_chain_hash = send.hash ();\n\t}\n\n\tstd::shared_ptr<nano::block> frontier;\n\t{\n\t\tauto transaction = node->store.tx_begin_read ();\n\t\tfrontier = node->store.block_get (transaction, previous_genesis_chain_hash);\n\t}\n\n\t// Begin process for confirming the block (and setting confirmation height)\n\tnode->block_confirm (frontier);\n\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"confirmation_height_currently_processing\");\n\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\n\tsystem.deadline_set (10s);\n\twhile (!node->pending_confirmation_height.is_processing_block (previous_genesis_chain_hash))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\t// Make the request\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (10s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto hash (response.json.get<std::string> (\"hash\"));\n\t\tASSERT_EQ (frontier->hash ().to_string (), hash);\n\t}\n\n\t// Wait until confirmation has been set\n\tsystem.deadline_set (10s);\n\twhile (true)\n\t{\n\t\tauto transaction = node->store.tx_begin_read ();\n\t\tif (node->ledger.block_confirmed (transaction, frontier->hash ()))\n\t\t{\n\t\t\tbreak;\n\t\t}\n\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\t// Make the same request, it should now return an error\n\t{\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (10s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tstd::error_code ec (nano::error_rpc::confirmation_height_not_processing);\n\t\tASSERT_EQ (response.json.get<std::string> (\"error\"), ec.message ());\n\t}\n}\n\nTEST (rpc, confirmation_history)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tASSERT_TRUE (system.nodes[0]->active.list_confirmed ().empty ());\n\tauto block (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->active.list_confirmed ().empty ())\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"confirmation_history\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto representatives (response.json.get_child (\"confirmations\"));\n\tauto item (representatives.begin ());\n\tASSERT_NE (representatives.end (), item);\n\tauto hash (item->second.get<std::string> (\"hash\"));\n\tauto tally (item->second.get<std::string> (\"tally\"));\n\tASSERT_FALSE (item->second.get<std::string> (\"duration\", \"\").empty ());\n\tASSERT_FALSE (item->second.get<std::string> (\"time\", \"\").empty ());\n\tASSERT_EQ (block->hash ().to_string (), hash);\n\tnano::amount tally_num;\n\ttally_num.decode_dec (tally);\n\tassert (tally_num == nano::genesis_amount || tally_num == (nano::genesis_amount - nano::Gxrb_ratio));\n\tsystem.stop ();\n}\n\nTEST (rpc, confirmation_history_hash)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tASSERT_TRUE (system.nodes[0]->active.list_confirmed ().empty ());\n\tauto send1 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tauto send2 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tauto send3 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, nano::Gxrb_ratio));\n\tsystem.deadline_set (10s);\n\twhile (system.nodes[0]->active.list_confirmed ().size () != 3)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"confirmation_history\");\n\trequest.put (\"hash\", send2->hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto representatives (response.json.get_child (\"confirmations\"));\n\tASSERT_EQ (representatives.size (), 1);\n\tauto item (representatives.begin ());\n\tASSERT_NE (representatives.end (), item);\n\tauto hash (item->second.get<std::string> (\"hash\"));\n\tauto tally (item->second.get<std::string> (\"tally\"));\n\tASSERT_FALSE (item->second.get<std::string> (\"duration\", \"\").empty ());\n\tASSERT_FALSE (item->second.get<std::string> (\"time\", \"\").empty ());\n\tASSERT_EQ (send2->hash ().to_string (), hash);\n\tnano::amount tally_num;\n\ttally_num.decode_dec (tally);\n\tassert (tally_num == nano::genesis_amount || tally_num == (nano::genesis_amount - nano::Gxrb_ratio) || tally_num == (nano::genesis_amount - 2 * nano::Gxrb_ratio) || tally_num == (nano::genesis_amount - 3 * nano::Gxrb_ratio));\n\tsystem.stop ();\n}\n\nTEST (rpc, block_confirm)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::genesis genesis;\n\tauto send1 (std::make_shared<nano::state_block> (nano::test_genesis_key.pub, genesis.hash (), nano::test_genesis_key.pub, nano::genesis_amount - nano::Gxrb_ratio, nano::test_genesis_key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.nodes[0]->work_generate_blocking (genesis.hash ())));\n\t{\n\t\tauto transaction (system.nodes[0]->store.tx_begin (true));\n\t\tASSERT_EQ (nano::process_result::progress, system.nodes[0]->ledger.process (transaction, *send1).code);\n\t}\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_confirm\");\n\trequest.put (\"hash\", send1->hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"1\", response.json.get<std::string> (\"started\"));\n}\n\nTEST (rpc, block_confirm_absent)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_confirm\");\n\trequest.put (\"hash\", \"0\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"Block not found\", response.json.get<std::string> (\"error\"));\n}\n\nTEST (rpc, block_confirm_confirmed)\n{\n\tnano::system system (24000, 1);\n\tnano::node_init init;\n\tauto path (nano::unique_path ());\n\tnano::node_config config;\n\tconfig.peering_port = 24001;\n\tconfig.callback_address = \"localhost\";\n\tconfig.callback_port = 24002;\n\tconfig.callback_target = \"/\";\n\tconfig.logging.init (path);\n\tauto node (std::make_shared<nano::node> (init, system.io_ctx, path, system.alarm, config, system.work));\n\tnode->start ();\n\tsystem.nodes.push_back (node);\n\tnano::genesis genesis;\n\t{\n\t\tauto transaction (node->store.tx_begin_read ());\n\t\tASSERT_TRUE (node->ledger.block_confirmed (transaction, genesis.hash ()));\n\t}\n\tASSERT_EQ (0, node->stats.count (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out));\n\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_confirm\");\n\trequest.put (\"hash\", genesis.hash ().to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"1\", response.json.get<std::string> (\"started\"));\n\t// Check confirmation history\n\tauto confirmed (node->active.list_confirmed ());\n\tASSERT_EQ (1, confirmed.size ());\n\tASSERT_EQ (genesis.hash (), confirmed.begin ()->winner->hash ());\n\t// Check callback\n\tsystem.deadline_set (5s);\n\twhile (node->stats.count (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out) == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\t// Callback result is error because callback target port isn't listening\n\tASSERT_EQ (1, node->stats.count (nano::stat::type::error, nano::stat::detail::http_callback, nano::stat::dir::out));\n\tnode->stop ();\n}\n\nTEST (rpc, node_id)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"node_id\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (system.nodes[0]->node_id.prv.data.to_string (), response.json.get<std::string> (\"private\"));\n\tASSERT_EQ (system.nodes[0]->node_id.pub.to_account (), response.json.get<std::string> (\"as_account\"));\n}\n\nTEST (rpc, stats_clear)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tsystem.nodes[0]->stats.inc (nano::stat::type::ledger, nano::stat::dir::in);\n\tASSERT_EQ (1, system.nodes[0]->stats.count (nano::stat::type::ledger, nano::stat::dir::in));\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"stats_clear\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tstd::string success (response.json.get<std::string> (\"success\"));\n\tASSERT_TRUE (success.empty ());\n\tASSERT_EQ (0, system.nodes[0]->stats.count (nano::stat::type::ledger, nano::stat::dir::in));\n\tASSERT_LE (system.nodes[0]->stats.last_reset ().count (), 5);\n}\n\nTEST (rpc, unopened)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::account account1 (1), account2 (account1.number () + 1);\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, account1, 1));\n\tASSERT_NE (nullptr, send);\n\tauto send2 (system.wallet (0)->send_action (nano::test_genesis_key.pub, account2, 2));\n\tASSERT_NE (nullptr, send2);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\t{\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"unopened\");\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & accounts (response.json.get_child (\"accounts\"));\n\t\tASSERT_EQ (2, accounts.size ());\n\t\tASSERT_EQ (\"1\", accounts.get<std::string> (account1.to_account ()));\n\t\tASSERT_EQ (\"2\", accounts.get<std::string> (account2.to_account ()));\n\t}\n\t{\n\t\t// starting at second account should get a single result\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"unopened\");\n\t\trequest.put (\"account\", account2.to_account ());\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & accounts (response.json.get_child (\"accounts\"));\n\t\tASSERT_EQ (1, accounts.size ());\n\t\tASSERT_EQ (\"2\", accounts.get<std::string> (account2.to_account ()));\n\t}\n\t{\n\t\t// starting at third account should get no results\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"unopened\");\n\t\trequest.put (\"account\", nano::account (account2.number () + 1).to_account ());\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & accounts (response.json.get_child (\"accounts\"));\n\t\tASSERT_EQ (0, accounts.size ());\n\t}\n\t{\n\t\t// using count=1 should get a single result\n\t\tboost::property_tree::ptree request;\n\t\trequest.put (\"action\", \"unopened\");\n\t\trequest.put (\"count\", \"1\");\n\t\ttest_response response (request, rpc.config.port, system.io_ctx);\n\t\tsystem.deadline_set (5s);\n\t\twhile (response.status == 0)\n\t\t{\n\t\t\tASSERT_NO_ERROR (system.poll ());\n\t\t}\n\t\tASSERT_EQ (200, response.status);\n\t\tauto & accounts (response.json.get_child (\"accounts\"));\n\t\tASSERT_EQ (1, accounts.size ());\n\t\tASSERT_EQ (\"1\", accounts.get<std::string> (account1.to_account ()));\n\t}\n}\n\nTEST (rpc, unopened_burn)\n{\n\tnano::system system (24000, 1);\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto genesis (system.nodes[0]->latest (nano::test_genesis_key.pub));\n\tASSERT_FALSE (genesis.is_zero ());\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::burn_account, 1));\n\tASSERT_NE (nullptr, send);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"unopened\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & accounts (response.json.get_child (\"accounts\"));\n\tASSERT_EQ (0, accounts.size ());\n}\n\nTEST (rpc, unopened_no_accounts)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"unopened\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto & accounts (response.json.get_child (\"accounts\"));\n\tASSERT_EQ (0, accounts.size ());\n}\n\nTEST (rpc, uptime)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"uptime\");\n\tstd::this_thread::sleep_for (std::chrono::seconds (1));\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_LE (1, response.json.get<int> (\"seconds\"));\n}\n\nTEST (rpc, wallet_history)\n{\n\tnano::system system (24000, 1);\n\tauto node0 (system.nodes[0]);\n\tnano::genesis genesis;\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tauto timestamp1 (nano::seconds_since_epoch ());\n\tauto send (system.wallet (0)->send_action (nano::test_genesis_key.pub, nano::test_genesis_key.pub, node0->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send);\n\tstd::this_thread::sleep_for (std::chrono::milliseconds (1000));\n\tauto timestamp2 (nano::seconds_since_epoch ());\n\tauto receive (system.wallet (0)->receive_action (*send, nano::test_genesis_key.pub, node0->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, receive);\n\tnano::keypair key;\n\tstd::this_thread::sleep_for (std::chrono::milliseconds (1000));\n\tauto timestamp3 (nano::seconds_since_epoch ());\n\tauto send2 (system.wallet (0)->send_action (nano::test_genesis_key.pub, key.pub, node0->config.receive_minimum.number ()));\n\tASSERT_NE (nullptr, send2);\n\tsystem.deadline_set (10s);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"wallet_history\");\n\trequest.put (\"wallet\", node0->wallets.items.begin ()->first.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::vector<std::tuple<std::string, std::string, std::string, std::string, std::string, std::string>> history_l;\n\tauto & history_node (response.json.get_child (\"history\"));\n\tfor (auto i (history_node.begin ()), n (history_node.end ()); i != n; ++i)\n\t{\n\t\thistory_l.push_back (std::make_tuple (i->second.get<std::string> (\"type\"), i->second.get<std::string> (\"account\"), i->second.get<std::string> (\"amount\"), i->second.get<std::string> (\"hash\"), i->second.get<std::string> (\"block_account\"), i->second.get<std::string> (\"local_timestamp\")));\n\t}\n\tASSERT_EQ (4, history_l.size ());\n\tASSERT_EQ (\"send\", std::get<0> (history_l[0]));\n\tASSERT_EQ (key.pub.to_account (), std::get<1> (history_l[0]));\n\tASSERT_EQ (node0->config.receive_minimum.to_string_dec (), std::get<2> (history_l[0]));\n\tASSERT_EQ (send2->hash ().to_string (), std::get<3> (history_l[0]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<4> (history_l[0]));\n\tASSERT_EQ (std::to_string (timestamp3), std::get<5> (history_l[0]));\n\tASSERT_EQ (\"receive\", std::get<0> (history_l[1]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[1]));\n\tASSERT_EQ (node0->config.receive_minimum.to_string_dec (), std::get<2> (history_l[1]));\n\tASSERT_EQ (receive->hash ().to_string (), std::get<3> (history_l[1]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<4> (history_l[1]));\n\tASSERT_EQ (std::to_string (timestamp2), std::get<5> (history_l[1]));\n\tASSERT_EQ (\"send\", std::get<0> (history_l[2]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[2]));\n\tASSERT_EQ (node0->config.receive_minimum.to_string_dec (), std::get<2> (history_l[2]));\n\tASSERT_EQ (send->hash ().to_string (), std::get<3> (history_l[2]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<4> (history_l[2]));\n\tASSERT_EQ (std::to_string (timestamp1), std::get<5> (history_l[2]));\n\t// Genesis block\n\tASSERT_EQ (\"receive\", std::get<0> (history_l[3]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<1> (history_l[3]));\n\tASSERT_EQ (nano::genesis_amount.convert_to<std::string> (), std::get<2> (history_l[3]));\n\tASSERT_EQ (genesis.hash ().to_string (), std::get<3> (history_l[3]));\n\tASSERT_EQ (nano::test_genesis_key.pub.to_account (), std::get<4> (history_l[3]));\n}\n\nTEST (rpc, sign_hash)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, node1.latest (nano::test_genesis_key.pub), nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"sign\");\n\trequest.put (\"hash\", send.hash ().to_string ());\n\trequest.put (\"key\", key.prv.data.to_string ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::error_code ec (nano::error_rpc::sign_hash_disabled);\n\tASSERT_EQ (response.json.get<std::string> (\"error\"), ec.message ());\n\tnode_rpc_config.enable_sign_hash = true;\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\twhile (response2.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response2.status);\n\tnano::signature signature;\n\tstd::string signature_text (response2.json.get<std::string> (\"signature\"));\n\tASSERT_FALSE (signature.decode_hex (signature_text));\n\tASSERT_FALSE (nano::validate_message (key.pub, send.hash (), signature));\n}\n\nTEST (rpc, sign_block)\n{\n\tnano::system system (24000, 1);\n\tnano::keypair key;\n\tauto & node1 (*system.nodes[0]);\n\tnano::state_block send (nano::genesis_account, node1.latest (nano::test_genesis_key.pub), nano::genesis_account, nano::genesis_amount - nano::Gxrb_ratio, key.pub, nano::test_genesis_key.prv, nano::test_genesis_key.pub, 0);\n\tenable_ipc_transport_tcp (node1.config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (node1, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"sign\");\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\tstd::string wallet;\n\tsystem.nodes[0]->wallets.items.begin ()->first.encode_hex (wallet);\n\trequest.put (\"wallet\", wallet);\n\trequest.put (\"account\", key.pub.to_account ());\n\tstd::string json;\n\tsend.serialize_json (json);\n\trequest.put (\"block\", json);\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\twhile (response.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response.status);\n\tauto contents (response.json.get<std::string> (\"block\"));\n\tboost::property_tree::ptree block_l;\n\tstd::stringstream block_stream (contents);\n\tboost::property_tree::read_json (block_stream, block_l);\n\tauto block (nano::deserialize_block_json (block_l));\n\tASSERT_FALSE (nano::validate_message (key.pub, send.hash (), block->block_signature ()));\n\tASSERT_NE (block->block_signature (), send.block_signature ());\n\tASSERT_EQ (block->hash (), send.hash ());\n}\n\nTEST (rpc, memory_stats)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\n\t// Preliminary test adding to the vote uniquer and checking json output is correct\n\tnano::keypair key;\n\tauto block (std::make_shared<nano::state_block> (0, 0, 0, 0, 0, key.prv, key.pub, 0));\n\tstd::vector<nano::block_hash> hashes;\n\thashes.push_back (block->hash ());\n\tauto vote (std::make_shared<nano::vote> (key.pub, key.prv, 0, hashes));\n\tnode->vote_uniquer.unique (vote);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"stats\");\n\trequest.put (\"type\", \"objects\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\n\tASSERT_EQ (response.json.get_child (\"node\").get_child (\"vote_uniquer\").get_child (\"votes\").get<std::string> (\"count\"), \"1\");\n}\n\nTEST (rpc, block_confirmed)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"block_info\");\n\trequest.put (\"hash\", \"bad_hash1337\");\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tASSERT_EQ (\"Invalid block hash\", response.json.get<std::string> (\"error\"));\n\n\trequest.put (\"hash\", \"0\");\n\ttest_response response1 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response1.status == 0)\n\t{\n\t\tsystem.poll ();\n\t}\n\tASSERT_EQ (200, response1.status);\n\tASSERT_EQ (\"Block not found\", response1.json.get<std::string> (\"error\"));\n\n\tsystem.wallet (0)->insert_adhoc (nano::test_genesis_key.prv);\n\tnano::keypair key;\n\tsystem.wallet (0)->insert_adhoc (key.prv);\n\n\t// Open an account directly in the ledger\n\t{\n\t\tauto transaction = node->store.tx_begin_write ();\n\t\tnano::block_hash latest (node->latest (nano::test_genesis_key.pub));\n\t\tnano::send_block send1 (latest, key.pub, 300, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.work.generate (latest));\n\t\tASSERT_EQ (nano::process_result::progress, node->ledger.process (transaction, send1).code);\n\n\t\tnano::open_block open1 (send1.hash (), nano::genesis_account, key.pub, key.prv, key.pub, system.work.generate (key.pub));\n\t\tASSERT_EQ (nano::process_result::progress, node->ledger.process (transaction, open1).code);\n\t}\n\n\t// This should not be confirmed\n\tnano::block_hash latest (node->latest (nano::test_genesis_key.pub));\n\trequest.put (\"hash\", latest.to_string ());\n\ttest_response response2 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response2.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\tASSERT_EQ (200, response2.status);\n\tASSERT_FALSE (response2.json.get<bool> (\"confirmed\"));\n\n\t// Create and process a new send block\n\tauto send = std::make_shared<nano::send_block> (latest, key.pub, 10, nano::test_genesis_key.prv, nano::test_genesis_key.pub, system.work.generate (latest));\n\tnode->process_active (send);\n\tnode->block_processor.flush ();\n\tsystem.deadline_set (10s);\n\twhile (!node->confirmation_height_processor.is_processing_block (send->hash ()))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\t// Wait until the confirmation height has been set\n\tsystem.deadline_set (10s);\n\twhile (true)\n\t{\n\t\tauto transaction = node->store.tx_begin_read ();\n\t\tif (node->ledger.block_confirmed (transaction, send->hash ()))\n\t\t{\n\t\t\tbreak;\n\t\t}\n\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\t// Should no longer be processing the block after confirmation is set\n\tASSERT_FALSE (node->confirmation_height_processor.is_processing_block (send->hash ()));\n\n\t// Requesting confirmation for this should now succeed\n\trequest.put (\"hash\", send->hash ().to_string ());\n\ttest_response response3 (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response3.status == 0)\n\t{\n\t\tASSERT_FALSE (system.poll ());\n\t}\n\n\tASSERT_EQ (200, response3.status);\n\tASSERT_TRUE (response3.json.get<bool> (\"confirmed\"));\n}\n\n// This is mainly to check for threading issues with TSAN\nTEST (rpc, simultaneous_calls)\n{\n\t// This tests simulatenous calls to the same node in different threads\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tnano::thread_runner runner (system.io_ctx, node->config.io_threads);\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::ipc::ipc_server ipc_server (*node, node_rpc_config);\n\tnano::rpc_config rpc_config (true);\n\trpc_config.num_ipc_connections = 8;\n\tnano::ipc_rpc_processor ipc_rpc_processor (system.io_ctx, rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, ipc_rpc_processor);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_block_count\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\n\tconstexpr auto num = 100;\n\tstd::array<std::unique_ptr<test_response>, num> test_responses;\n\tfor (int i = 0; i < num; ++i)\n\t{\n\t\ttest_responses[i] = std::make_unique<test_response> (request, system.io_ctx);\n\t}\n\n\tstd::promise<void> promise;\n\tstd::atomic<int> count{ num };\n\tfor (int i = 0; i < num; ++i)\n\t{\n\t\t// clang-format off\n\t\tstd::thread ([&test_responses, &promise, &count, i, port = rpc.config.port ]() {\n\t\t\ttest_responses[i]->run (port);\n\t\t\tif (--count == 0)\n\t\t\t{\n\t\t\t\tpromise.set_value ();\n\t\t\t}\n\t\t})\n\t\t.detach ();\n\t\t// clang-format on\n\t}\n\n\tpromise.get_future ().wait ();\n\n\tsystem.deadline_set (10s);\n\twhile (std::any_of (test_responses.begin (), test_responses.end (), [](const auto & test_response) { return test_response->status == 0; }))\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\n\tfor (int i = 0; i < num; ++i)\n\t{\n\t\tASSERT_EQ (200, test_responses[i]->status);\n\t\tstd::string block_count_text (test_responses[i]->json.get<std::string> (\"block_count\"));\n\t\tASSERT_EQ (\"1\", block_count_text);\n\t}\n\trpc.stop ();\n\tsystem.stop ();\n\tipc_server.stop ();\n\tsystem.io_ctx.stop ();\n\trunner.join ();\n}\n\n// This tests that the inprocess RPC (i.e without using IPC) works correctly\nTEST (rpc, in_process)\n{\n\tnano::system system (24000, 1);\n\tauto node = system.nodes.front ();\n\tenable_ipc_transport_tcp (node->config.ipc_config.transport_tcp);\n\tnano::rpc_config rpc_config (true);\n\tnano::node_rpc_config node_rpc_config;\n\tnano::inprocess_rpc_handler inprocess_rpc_handler (*node, node_rpc_config);\n\tnano::rpc rpc (system.io_ctx, rpc_config, inprocess_rpc_handler);\n\trpc.start ();\n\tboost::property_tree::ptree request;\n\trequest.put (\"action\", \"account_balance\");\n\trequest.put (\"account\", nano::test_genesis_key.pub.to_account ());\n\ttest_response response (request, rpc.config.port, system.io_ctx);\n\tsystem.deadline_set (5s);\n\twhile (response.status == 0)\n\t{\n\t\tASSERT_NO_ERROR (system.poll ());\n\t}\n\tASSERT_EQ (200, response.status);\n\tstd::string balance_text (response.json.get<std::string> (\"balance\"));\n\tASSERT_EQ (\"340282366920938463463374607431768211455\", balance_text);\n\tstd::string pending_text (response.json.get<std::string> (\"pending\"));\n\tASSERT_EQ (\"0\", pending_text);\n}\n\nTEST (rpc_config, serialization)\n{\n\tnano::rpc_config config1;\n\tconfig1.address = boost::asio::ip::address_v6::any ();\n\tconfig1.port = 10;\n\tconfig1.enable_control = true;\n\tnano::jsonconfig tree;\n\tconfig1.serialize_json (tree);\n\tnano::rpc_config config2;\n\tASSERT_NE (config2.address, config1.address);\n\tASSERT_NE (config2.port, config1.port);\n\tASSERT_NE (config2.enable_control, config1.enable_control);\n\tbool upgraded{ false };\n\tconfig2.deserialize_json (upgraded, tree);\n\tASSERT_EQ (config2.address, config1.address);\n\tASSERT_EQ (config2.port, config1.port);\n\tASSERT_EQ (config2.enable_control, config1.enable_control);\n}\n\nTEST (rpc_config, migrate)\n{\n\tnano::jsonconfig rpc;\n\trpc.put (\"address\", \"::1\");\n\trpc.put (\"port\", 11111);\n\n\tbool updated = false;\n\tauto data_path = nano::unique_path ();\n\tboost::filesystem::create_directory (data_path);\n\tnano::node_rpc_config nano_rpc_config;\n\tnano_rpc_config.deserialize_json (updated, rpc, data_path);\n\tASSERT_TRUE (updated);\n\n\t// Check that the rpc config file is created\n\tauto rpc_path = nano::get_rpc_config_path (data_path);\n\tnano::rpc_config rpc_config;\n\tnano::jsonconfig json;\n\tupdated = false;\n\tASSERT_FALSE (json.read_and_update (rpc_config, rpc_path));\n\tASSERT_FALSE (updated);\n\n\tASSERT_EQ (rpc_config.port, 11111);\n}\n", "idx": 3, "id": 15495, "msg": "", "proj": "nanocurrency-nano-node", "lang": "cpp"}
{"patch": "@@ -255,7 +255,7 @@ func TestTransportSpec(t *testing.T) {\n \t\t\t},\n \t\t\twantErrors: []string{\n \t\t\t\t\"cannot configure peer chooser for HTTP outbound\",\n-\t\t\t\t\"no recognized peer list in config: got least-pending\",\n+\t\t\t\t`failed to read attribute \"least-pending\"`,\n \t\t\t},\n \t\t},\n \t}", "y": 1, "oldf": "// Copyright (c) 2017 Uber Technologies, Inc.\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\npackage http\n\nimport (\n\t\"fmt\"\n\t\"net/http\"\n\t\"testing\"\n\t\"time\"\n\n\t\"go.uber.org/yarpc/x/config\"\n\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestTransportSpec(t *testing.T) {\n\t// This test is a cross-product of the transport, inbound and outbound\n\t// test assertions.\n\t//\n\t// Configuration, environment variables, and TransportSpec options are all\n\t// combined. If any entry had a non-empty wantErrors, any test case with\n\t// that entry is expected to fail.\n\t//\n\t// If the inbound and outbound tests state that they are both empty, the\n\t// test case will be skipped because we don't build a transport if there\n\t// is no inbound or outbound.\n\n\ttype attrs map[string]interface{}\n\n\ttype transportTest struct {\n\t\tdesc string            // description\n\t\tcfg  attrs             // transport.http section of the config\n\t\tenv  map[string]string // environment variables\n\t\topts []Option          // transport spec options\n\n\t\twantErrors []string\n\t\twantClient *wantHTTPClient\n\t}\n\n\ttype wantInbound struct {\n\t\tAddress    string\n\t\tMux        *http.ServeMux\n\t\tMuxPattern string\n\t}\n\n\ttype inboundTest struct {\n\t\tdesc string            // description\n\t\tcfg  attrs             // inbounds.http section of the config\n\t\tenv  map[string]string // environment variables\n\t\topts []Option          // transport spec options\n\n\t\tempty bool // whether this test case is empty\n\n\t\twantErrors  []string\n\t\twantInbound *wantInbound\n\t}\n\n\ttype wantOutbound struct {\n\t\tURLTemplate string\n\t\tHeaders     http.Header\n\t}\n\n\ttype outboundTest struct {\n\t\tdesc string            // description\n\t\tcfg  attrs             // outbounds section of the config\n\t\tenv  map[string]string // environment variables\n\t\topts []Option          // transport spec options\n\n\t\tempty bool // whether this test case is empty\n\n\t\twantErrors    []string\n\t\twantOutbounds map[string]wantOutbound\n\t}\n\n\ttransportTests := []transportTest{\n\t\t{\n\t\t\tdesc: \"no transport config\",\n\t\t\twantClient: &wantHTTPClient{\n\t\t\t\tKeepAlive:           30 * time.Second,\n\t\t\t\tMaxIdleConnsPerHost: 2,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"transport options\",\n\t\t\topts: []Option{\n\t\t\t\tKeepAlive(5 * time.Second),\n\t\t\t\tMaxIdleConnsPerHost(42),\n\t\t\t},\n\t\t\twantClient: &wantHTTPClient{\n\t\t\t\tKeepAlive:           5 * time.Second,\n\t\t\t\tMaxIdleConnsPerHost: 42,\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"explicit transport config\",\n\t\t\tcfg:  attrs{\"keepAlive\": \"5s\"},\n\t\t\twantClient: &wantHTTPClient{\n\t\t\t\tKeepAlive:           5 * time.Second,\n\t\t\t\tMaxIdleConnsPerHost: 2,\n\t\t\t},\n\t\t},\n\t}\n\n\tserveMux := http.NewServeMux()\n\n\tinboundTests := []inboundTest{\n\t\t{desc: \"no inbound\", empty: true},\n\t\t{\n\t\t\tdesc:       \"inbound without address\",\n\t\t\tcfg:        attrs{},\n\t\t\twantErrors: []string{\"inbound address is required\"},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"simple inbound\",\n\t\t\tcfg:         attrs{\"address\": \":8080\"},\n\t\t\twantInbound: &wantInbound{Address: \":8080\"},\n\t\t},\n\t\t{\n\t\t\tdesc:        \"inbound interpolation\",\n\t\t\tcfg:         attrs{\"address\": \"${HOST:}:${PORT}\"},\n\t\t\tenv:         map[string]string{\"HOST\": \"127.0.0.1\", \"PORT\": \"80\"},\n\t\t\twantInbound: &wantInbound{Address: \"127.0.0.1:80\"},\n\t\t},\n\t\t{\n\t\t\tdesc: \"serve mux\",\n\t\t\tcfg:  attrs{\"address\": \":8080\"},\n\t\t\topts: []Option{\n\t\t\t\tMux(\"/yarpc\", serveMux),\n\t\t\t},\n\t\t\twantInbound: &wantInbound{\n\t\t\t\tAddress:    \":8080\",\n\t\t\t\tMux:        serveMux,\n\t\t\t\tMuxPattern: \"/yarpc\",\n\t\t\t},\n\t\t},\n\t}\n\n\toutboundTests := []outboundTest{\n\t\t{desc: \"no outbound\", empty: true},\n\t\t{\n\t\t\tdesc: \"simple outbound\",\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\"url\": \"http://localhost:4040/yarpc\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantOutbounds: map[string]wantOutbound{\n\t\t\t\t\"myservice\": {\n\t\t\t\t\tURLTemplate: \"http://localhost:4040/yarpc\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"outbound interpolation\",\n\t\t\tenv:  map[string]string{\"ADDR\": \"127.0.0.1:80\"},\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\"url\": \"http://${ADDR}/yarpc\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantOutbounds: map[string]wantOutbound{\n\t\t\t\t\"myservice\": {\n\t\t\t\t\tURLTemplate: \"http://127.0.0.1:80/yarpc\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"outbound url template option\",\n\t\t\topts: []Option{\n\t\t\t\tURLTemplate(\"http://127.0.0.1:8080/yarpc\"),\n\t\t\t},\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\"peer\": \"127.0.0.1:8888\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantOutbounds: map[string]wantOutbound{\n\t\t\t\t\"myservice\": {\n\t\t\t\t\tURLTemplate: \"http://127.0.0.1:8080/yarpc\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"outbound url template option override\",\n\t\t\topts: []Option{\n\t\t\t\tURLTemplate(\"http://127.0.0.1:8080/yarpc\"),\n\t\t\t},\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\n\t\t\t\t\t\t\"url\":  \"http://host/yarpc/v1\",\n\t\t\t\t\t\t\"peer\": \"127.0.0.1:8888\",\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantOutbounds: map[string]wantOutbound{\n\t\t\t\t\"myservice\": {\n\t\t\t\t\tURLTemplate: \"http://host/yarpc/v1\",\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"outbound header options\",\n\t\t\topts: []Option{\n\t\t\t\tAddHeader(\"X-Token\", \"token-1\"),\n\t\t\t\tAddHeader(\"X-Token-2\", \"token-2\"),\n\t\t\t\tAddHeader(\"X-Token\", \"token-3\"),\n\t\t\t},\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\"url\": \"http://localhost/\"},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantOutbounds: map[string]wantOutbound{\n\t\t\t\t\"myservice\": {\n\t\t\t\t\tURLTemplate: \"http://localhost/\",\n\t\t\t\t\tHeaders: http.Header{\n\t\t\t\t\t\t\"X-Token\":   {\"token-1\", \"token-3\"},\n\t\t\t\t\t\t\"X-Token-2\": {\"token-2\"},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\tdesc: \"outbound peer build error\",\n\t\t\tcfg: attrs{\n\t\t\t\t\"myservice\": attrs{\n\t\t\t\t\t\"http\": attrs{\n\t\t\t\t\t\t\"least-pending\": []string{\n\t\t\t\t\t\t\t\"127.0.0.1:8080\",\n\t\t\t\t\t\t\t\"127.0.0.1:8081\",\n\t\t\t\t\t\t\t\"127.0.0.1:8082\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\twantErrors: []string{\n\t\t\t\t\"cannot configure peer chooser for HTTP outbound\",\n\t\t\t\t\"no recognized peer list in config: got least-pending\",\n\t\t\t},\n\t\t},\n\t}\n\n\trunTest := func(t *testing.T, trans transportTest, inbound inboundTest, outbound outboundTest) {\n\t\tenv := make(map[string]string)\n\t\tfor k, v := range trans.env {\n\t\t\tenv[k] = v\n\t\t}\n\t\tfor k, v := range inbound.env {\n\t\t\tenv[k] = v\n\t\t}\n\t\tfor k, v := range outbound.env {\n\t\t\tenv[k] = v\n\t\t}\n\t\tconfigurator := config.New(config.InterpolationResolver(mapResolver(env)))\n\n\t\topts := append(append(trans.opts, inbound.opts...), outbound.opts...)\n\t\tif trans.wantClient != nil {\n\t\t\topts = append(opts, useFakeBuildClient(t, trans.wantClient))\n\t\t}\n\t\terr := configurator.RegisterTransport(TransportSpec(opts...))\n\t\trequire.NoError(t, err, \"failed to register transport spec\")\n\n\t\tcfgData := make(attrs)\n\t\tif trans.cfg != nil {\n\t\t\tcfgData[\"transports\"] = attrs{\"http\": trans.cfg}\n\t\t}\n\t\tif inbound.cfg != nil {\n\t\t\tcfgData[\"inbounds\"] = attrs{\"http\": inbound.cfg}\n\t\t}\n\t\tif outbound.cfg != nil {\n\t\t\tcfgData[\"outbounds\"] = outbound.cfg\n\t\t}\n\t\tcfg, err := configurator.LoadConfig(\"foo\", cfgData)\n\n\t\twantErrors := append(append(trans.wantErrors, inbound.wantErrors...), outbound.wantErrors...)\n\t\tif len(wantErrors) > 0 {\n\t\t\trequire.Error(t, err, \"expected failure\")\n\t\t\tfor _, msg := range wantErrors {\n\t\t\t\tassert.Contains(t, err.Error(), msg)\n\t\t\t}\n\t\t\treturn\n\t\t}\n\n\t\trequire.NoError(t, err, \"expected success\")\n\n\t\tif want := inbound.wantInbound; want != nil {\n\t\t\tib, ok := cfg.Inbounds[0].(*Inbound)\n\t\t\tif assert.True(t, ok, \"expected *Inbound, got %T\", cfg.Inbounds[0]) {\n\t\t\t\tassert.Equal(t, want.Address, ib.addr, \"inbound address should match\")\n\t\t\t\tassert.Equal(t, want.MuxPattern, ib.muxPattern,\n\t\t\t\t\t\"inbound mux pattern should match\")\n\t\t\t\tassert.True(t, want.Mux == ib.mux, \"inbound mux should match\")\n\t\t\t\t// == because we want it to be the same object\n\t\t\t}\n\t\t}\n\n\t\tfor svc, want := range outbound.wantOutbounds {\n\t\t\tob, ok := cfg.Outbounds[svc].Unary.(*Outbound)\n\t\t\tif assert.True(t, ok, \"expected *Outbound for %q, got %T\", cfg.Outbounds[svc].Unary) {\n\t\t\t\t// Verify that we install a oneway too\n\t\t\t\t_, ok := cfg.Outbounds[svc].Oneway.(*Outbound)\n\t\t\t\tassert.True(t, ok, \"expected *Outbound for %q oneway, got %T\", cfg.Outbounds[svc].Oneway)\n\n\t\t\t\tassert.Equal(t, want.URLTemplate, ob.urlTemplate.String(), \"outbound URLTemplate should match\")\n\t\t\t\tassert.Equal(t, want.Headers, ob.headers, \"outbound headers should match\")\n\t\t\t}\n\n\t\t}\n\t}\n\n\tfor _, transTT := range transportTests {\n\t\tfor _, inboundTT := range inboundTests {\n\t\t\tfor _, outboundTT := range outboundTests {\n\t\t\t\t// Special case: No inbounds and outbounds so we have nothing\n\t\t\t\t// to test.\n\t\t\t\tif inboundTT.empty && outboundTT.empty {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\n\t\t\t\tdesc := fmt.Sprintf(\"%v/%v/%v\", transTT.desc, inboundTT.desc, outboundTT.desc)\n\t\t\t\tt.Run(desc, func(t *testing.T) {\n\t\t\t\t\trunTest(t, transTT, inboundTT, outboundTT)\n\t\t\t\t})\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc mapResolver(m map[string]string) func(string) (string, bool) {\n\treturn func(k string) (v string, ok bool) {\n\t\tif m != nil {\n\t\t\tv, ok = m[k]\n\t\t}\n\t\treturn\n\t}\n}\n\ntype wantHTTPClient struct {\n\tKeepAlive           time.Duration\n\tMaxIdleConnsPerHost int\n}\n\n// useFakeBuildClient verifies the configuration we use to build an HTTP\n// client.\nfunc useFakeBuildClient(t *testing.T, want *wantHTTPClient) TransportOption {\n\treturn buildClient(func(cfg *transportConfig) *http.Client {\n\t\tassert.Equal(t, want.KeepAlive, cfg.keepAlive, \"http.Client: KeepAlive should match\")\n\t\tassert.Equal(t, want.MaxIdleConnsPerHost, cfg.maxIdleConnsPerHost,\n\t\t\t\"http.Client: MaxIdleConnsPerHost should match\")\n\t\treturn buildHTTPClient(cfg)\n\t})\n}\n", "idx": 1, "id": 13501, "msg": "should this be something like > \"least-pending\" is not a known peer list", "proj": "yarpc-yarpc-go", "lang": "go"}
{"patch": "@@ -296,7 +296,10 @@ namespace pwiz.Skyline.Util\n             int i = RemoveExisting(item);\n             if (i != -1 && i < index)\n                 index--;\n-            _dict.Add(item.GetKey(), item);\n+            if (item == null)\n+                Assume.Fail(@\"unexpected null item\");\n+            else\n+                _dict.Add(item.GetKey(), item);\n             base.InsertItem(index, item);\n         }\n ", "y": 0, "oldf": "\ufeff/*\r\n * Original author: Brendan MacLean <brendanx .at. u.washington.edu>,\r\n *                  MacCoss Lab, Department of Genome Sciences, UW\r\n *\r\n * Copyright 2009 University of Washington - Seattle, WA\r\n * \r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n *     http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n */\r\nusing System;\r\nusing System.Collections;\r\nusing System.Collections.Generic;\r\nusing System.Diagnostics;\r\nusing System.Globalization;\r\nusing System.IO;\r\nusing System.Linq;\r\nusing System.Text;\r\nusing System.Collections.ObjectModel;\r\nusing System.Net;\r\nusing System.Reflection;\r\nusing System.Text.RegularExpressions;\r\nusing System.Threading;\r\nusing System.Threading.Tasks;\r\nusing System.Windows.Forms;\r\nusing pwiz.Common.Collections;\r\nusing pwiz.Common.SystemUtil;\r\nusing pwiz.Skyline.FileUI;\r\nusing pwiz.Skyline.Properties;\r\nusing pwiz.Skyline.SettingsUI;\r\nusing pwiz.Skyline.Util.Extensions;\r\n\r\nnamespace pwiz.Skyline.Util\r\n{\r\n    /// <summary>\r\n    /// Implement on an element for use with <see cref=\"MappedList{TKey,TValue}\"/>.\r\n    /// </summary>\r\n    /// <typeparam name=\"TKey\">Key type in the map</typeparam>\r\n    public interface IKeyContainer<out TKey>\r\n    {\r\n        TKey GetKey();\r\n    }\r\n\r\n    public interface IEquivalenceTestable<in T>\r\n    {\r\n        bool IsEquivalent(T other);\r\n    }\r\n\r\n    /// <summary>\r\n    /// Base class for use with elements to be stored in\r\n    /// <see cref=\"MappedList{TKey,TValue}\"/>.\r\n    /// </summary>\r\n    public abstract class NamedElement : IKeyContainer<string>\r\n    {\r\n        protected NamedElement(string name)\r\n        {\r\n            Name = name;\r\n        }\r\n\r\n        public string Name { get; private set; }\r\n\r\n        public virtual string GetKey()\r\n        {\r\n            return Name;\r\n        }\r\n\r\n        #region object overrides\r\n\r\n        public bool Equals(NamedElement obj)\r\n        {\r\n            if (ReferenceEquals(null, obj)) return false;\r\n            if (ReferenceEquals(this, obj)) return true;\r\n            return Equals(obj.Name, Name);\r\n        }\r\n\r\n        public override bool Equals(object obj)\r\n        {\r\n            if (ReferenceEquals(null, obj)) return false;\r\n            if (ReferenceEquals(this, obj)) return true;\r\n            if (obj.GetType() != typeof(NamedElement)) return false;\r\n            return Equals((NamedElement)obj);\r\n        }\r\n\r\n        public override int GetHashCode()\r\n        {\r\n            return Name.GetHashCode();\r\n        }\r\n\r\n        #endregion\r\n    }\r\n\r\n    /// <summary>\r\n    /// Allows access to a default state for a collection that allows\r\n    /// editing.\r\n    /// </summary>\r\n    /// <typeparam name=\"TItem\">The type of the items in the collection</typeparam>\r\n    public interface IListDefaults<TItem>\r\n    {\r\n        /// <summary>\r\n        /// Gets the current revision index for this list\r\n        /// </summary>\r\n        int RevisionIndexCurrent { get; }\r\n\r\n        /// <summary>\r\n        /// Gets the default collection as an enumerable list.\r\n        /// </summary>\r\n        /// <returns>The default collection</returns>\r\n        IEnumerable<TItem> GetDefaults(int revisionIndex);\r\n\r\n        /// <summary>\r\n        /// Gets the localized display name for an item in this list\r\n        /// usually replacing names for the default items with localized text.\r\n        /// </summary>\r\n        /// <param name=\"item\">The item for which to get the display text</param>\r\n        /// <returns>Localized display text for default items or user supplied text for other items</returns>\r\n        string GetDisplayName(TItem item);\r\n    }\r\n\r\n    /// <summary>\r\n    /// Exposes properties necessary for using <see cref=\"EditListDlg{T,TItem}\"/>\r\n    /// to edit a list.\r\n    /// </summary>\r\n    public interface IListEditorSupport\r\n    {\r\n        /// <summary>\r\n        /// The title to display on <see cref=\"EditListDlg{T,TItem}\"/>\r\n        /// </summary>\r\n        string Title { get; }\r\n\r\n        /// <summary>\r\n        /// Label string for the listbox that shows this list being\r\n        /// edited.\r\n        /// </summary>\r\n        string Label { get; }\r\n\r\n        /// <summary>\r\n        /// True if the list can be reset to its default contents.\r\n        /// </summary>\r\n        bool AllowReset { get; }\r\n\r\n        /// <summary>\r\n        /// The number of default items that should be exclude when editing\r\n        /// the list.  Useful when some default items cannot be edited\r\n        /// or removed from the list.\r\n        /// </summary>\r\n        int ExcludeDefaults { get; }\r\n    }\r\n\r\n    /// <summary>\r\n    /// Implement this interfact to support the <see cref=\"EditListDlg{T,TItem}\"/>.\r\n    /// </summary>\r\n    /// <typeparam name=\"TItem\">Type of items in the list to be edited</typeparam>\r\n    public interface IListEditor<TItem>\r\n    {\r\n        /// <summary>\r\n        /// Exposes ability to edit a list of items.\r\n        /// </summary>\r\n        /// <returns>The new list after editing, or null if the user cancelled</returns>\r\n        IEnumerable<TItem> EditList(Control owner, object tag);\r\n\r\n        /// <summary>\r\n        /// Returns true, if a new list is accepted to replace the current list\r\n        /// </summary>\r\n        bool AcceptList(Control owner, IList<TItem> listNew);\r\n    }\r\n\r\n    /// <summary>\r\n    /// Implement this interfact to support the <see cref=\"ShareListDlg{T,TItem}\"/>.\r\n    /// </summary>\r\n    /// <typeparam name=\"TItem\">Type of items in the list to be edited</typeparam>\r\n    public interface IListSerializer<TItem>\r\n    {\r\n        Type SerialType { get; }\r\n\r\n        Type DeserialType { get; }\r\n\r\n        ICollection<TItem> CreateEmptyList();\r\n\r\n        bool ContainsKey(string key);\r\n    }\r\n\r\n    /// <summary>\r\n    /// Implement this interface to support the \"Add\" and \"Edit\"\r\n    /// buttons in the <see cref=\"EditListDlg{T,TItem}\"/>.\r\n    /// </summary>\r\n    /// <typeparam name=\"TItem\">Type of items in the list to be edited</typeparam>\r\n    public interface IItemEditor<TItem>\r\n    {\r\n        /// <summary>\r\n        /// Exposes the ability to create a new item for this list.\r\n        /// </summary>\r\n        /// <param name=\"owner\">Window requesting the edit</param>\r\n        /// <param name=\"existing\">A list of existing items of this type</param>\r\n        /// <param name=\"tag\">Object passed to the list editor for use in item editors</param>\r\n        /// <returns>The new item, or null if the user cancelled</returns>\r\n        TItem NewItem(Control owner, IEnumerable<TItem> existing, object tag);\r\n\r\n        /// <summary>\r\n        /// Exposes the ability to edit an individual item, return\r\n        /// a new modified item.  Items are considered immutable,\r\n        /// so successful return value will always be a new item.\r\n        /// </summary>\r\n        /// <param name=\"owner\">Window requesting the edit</param>\r\n        /// <param name=\"item\">The item to edit</param>\r\n        /// <param name=\"existing\">A list of existing items of this type</param>\r\n        /// <param name=\"tag\">Object passed to the list editor for use in item editors</param>\r\n        /// <returns>The new item, or null if the user cancelled</returns>\r\n        TItem EditItem(Control owner, TItem item, IEnumerable<TItem> existing, object tag);\r\n\r\n        /// <summary>\r\n        /// Copies an item for this list, with the copied item's name reset\r\n        /// to the empty string.\r\n        /// </summary>\r\n        /// <param name=\"item\">The item to copy</param>\r\n        /// <returns>The copied item with empty name</returns>\r\n        TItem CopyItem(TItem item);\r\n    }\r\n\r\n    /// <summary>\r\n    /// A generic ordered list based on Collection&lt;TValue>, with\r\n    /// elements also stored in a private dictionary for fast lookup.\r\n    /// Sort of a substitute for LinkedHashMap in Java.\r\n    /// </summary>\r\n    /// <typeparam name=\"TKey\">Type of the key used in the map</typeparam>\r\n    /// <typeparam name=\"TValue\">Type stored in the collection</typeparam>\r\n    public class MappedList<TKey, TValue>\r\n        : Collection<TValue>\r\n        where TValue : IKeyContainer<TKey>\r\n    {\r\n        private readonly Dictionary<TKey, TValue> _dict = new Dictionary<TKey, TValue>();\r\n\r\n        public TValue this[TKey name]\r\n        {\r\n            get\r\n            {\r\n                return _dict[name];\r\n            }\r\n        }\r\n\r\n        public IEnumerable<TKey> Keys\r\n        {\r\n            get\r\n            {\r\n                foreach (TValue value in this)\r\n                    yield return value.GetKey();\r\n            }\r\n        }\r\n\r\n        public bool ContainsKey(TKey key)\r\n        {\r\n            return _dict.ContainsKey(key);\r\n        }\r\n\r\n        public bool TryGetValue(TKey key, out TValue value)\r\n        {\r\n            return _dict.TryGetValue(key, out value);\r\n        }\r\n\r\n        public void SetValue(TValue value)\r\n        {\r\n            TValue valueCurrent;\r\n            if (TryGetValue(value.GetKey(), out valueCurrent))\r\n            {\r\n                SetItem(IndexOf(valueCurrent), value);\r\n            }\r\n            else\r\n            {\r\n                Add(value);\r\n            }\r\n        }\r\n\r\n        public void AddRange(IEnumerable<TValue> collection)\r\n        {\r\n            foreach (TValue item in collection)\r\n                Add(item);\r\n        }\r\n\r\n        #region Collection<TValue> Overrides\r\n\r\n        protected override void ClearItems()\r\n        {\r\n            _dict.Clear();\r\n            base.ClearItems();\r\n        }\r\n\r\n        protected override void InsertItem(int index, TValue item)\r\n        {\r\n            int i = RemoveExisting(item);\r\n            if (i != -1 && i < index)\r\n                index--;\r\n            _dict.Add(item.GetKey(), item);\r\n            base.InsertItem(index, item);\r\n        }\r\n\r\n        protected override void RemoveItem(int index)\r\n        {\r\n            _dict.Remove(this[index].GetKey());\r\n            base.RemoveItem(index);\r\n        }\r\n\r\n        protected override void SetItem(int index, TValue item)\r\n        {\r\n            TKey key = this[index].GetKey();\r\n\r\n            // If setting to a list item that has a different key\r\n            // from what is at this location currently, then any\r\n            // existing value with the same key must be removed\r\n            // from its current location.\r\n            if (!Equals(key, item.GetKey()))\r\n            {\r\n                int i = RemoveExisting(item);\r\n                if (i != -1 && i < index)\r\n                    index--;\r\n\r\n                // If the index pointed at an item with a different\r\n                // key, then removing some other item cannot leave\r\n                // the index out of range.\r\n                Debug.Assert(index < Items.Count);\r\n            }\r\n            _dict.Remove(key);\r\n            _dict.Add(item.GetKey(), item);\r\n            base.SetItem(index, item);                \r\n        }\r\n\r\n        /// <summary>\r\n        /// Used to help ensure that only one copy of the keyed elements\r\n        /// can exist in the list at any time.\r\n        /// </summary>\r\n        /// <param name=\"item\">An item to remove</param>\r\n        /// <returns>The index from which it was removed, or -1 if not found</returns>\r\n        private int RemoveExisting(TValue item)\r\n        {\r\n            TKey key = item.GetKey();\r\n            if (_dict.ContainsKey(key))\r\n            {\r\n                _dict.Remove(key);\r\n                for (int i = 0; i < Items.Count; i++)\r\n                {\r\n                    if (Equals(Items[i].GetKey(), item.GetKey()))\r\n                    {\r\n                        RemoveAt(i);\r\n                        return i;\r\n                    }\r\n                }\r\n            }\r\n            return -1;\r\n        }\r\n\r\n        #endregion // Collection<TValue> Overrides\r\n    }\r\n\r\n    public class MultiMap<TKey, TValue>\r\n    {\r\n        readonly Dictionary<TKey, List<TValue>> _dict;\r\n\r\n        public MultiMap()            \r\n        {\r\n            _dict = new Dictionary<TKey, List<TValue>>();\r\n        }\r\n\r\n        public MultiMap(int capacity)\r\n        {\r\n            _dict = new Dictionary<TKey, List<TValue>>(capacity);\r\n        }\r\n\r\n        public void Add(TKey key, TValue value)\r\n        {\r\n            List<TValue> values;\r\n            if (_dict.TryGetValue(key, out values))\r\n                values.Add(value);\r\n            else\r\n                _dict[key] = new List<TValue> { value };\r\n        }\r\n\r\n        public IEnumerable<TKey> Keys { get { return _dict.Keys; } }\r\n\r\n        public IList<TValue> this[TKey key] { get { return _dict[key]; } }\r\n\r\n        public bool TryGetValue(TKey key, out IList<TValue> values)\r\n        {\r\n            List<TValue> listValues;\r\n            if (_dict.TryGetValue(key, out listValues))\r\n            {\r\n                values = listValues;\r\n                return true;\r\n            }\r\n            values = null;\r\n            return false;\r\n        }\r\n    }\r\n\r\n    public static class MapUtil\r\n    {\r\n        public static MultiMap<TKey, TValue> ToMultiMap<TKey, TValue>(this IEnumerable<TValue> values, Func<TValue, TKey> keySelector)\r\n        {\r\n            MultiMap<TKey, TValue> map = new MultiMap<TKey, TValue>();\r\n            foreach (TValue value in values)\r\n                map.Add(keySelector(value), value);\r\n            return map;\r\n        }\r\n    }\r\n\r\n    /// <summary>\r\n    /// A read-only list class for the case when a list most commonly contains a\r\n    /// single entry, but must also support multiple entries.  This list may not\r\n    /// be empty, thought it may contain a single null element.\r\n    /// </summary>\r\n    /// <typeparam name=\"TItem\">Type of the elements in the list</typeparam>\r\n    public class OneOrManyList<TItem> : AbstractReadOnlyList<TItem>\r\n    {\r\n        private ImmutableList<TItem> _list;\r\n\r\n        public OneOrManyList(params TItem[] elements)\r\n        {\r\n            _list = ImmutableList.ValueOf(elements);\r\n        }\r\n\r\n        public OneOrManyList(IList<TItem> elements)\r\n        {\r\n            _list = ImmutableList.ValueOf(elements);\r\n        }\r\n\r\n        public override int Count\r\n        {\r\n            get { return _list.Count; }\r\n        }\r\n\r\n        public override TItem this[int index]\r\n        {\r\n            get\r\n            {\r\n                return _list[index];\r\n            }\r\n        }\r\n\r\n        public OneOrManyList<TItem> ChangeAt(int index, TItem item)\r\n        {\r\n            return new OneOrManyList<TItem>(_list.ReplaceAt(index, item));\r\n        }\r\n\r\n        #region object overrides\r\n\r\n        public bool Equals(OneOrManyList<TItem> obj)\r\n        {\r\n            if (ReferenceEquals(null, obj)) return false;\r\n            if (ReferenceEquals(this, obj)) return true;\r\n            return _list.Equals(obj._list);\r\n        }\r\n\r\n        public override bool Equals(object obj)\r\n        {\r\n            if (ReferenceEquals(null, obj)) return false;\r\n            if (ReferenceEquals(this, obj)) return true;\r\n            if (obj.GetType() != GetType()) return false;\r\n            return Equals((OneOrManyList<TItem>) obj);\r\n        }\r\n\r\n        public override int GetHashCode()\r\n        {\r\n            return _list.GetHashCode();\r\n        }\r\n\r\n        #endregion\r\n    }\r\n\r\n    /// <summary>\r\n    /// A singleton list that allows its one value to be changed\r\n    /// </summary>\r\n    public class SingletonList<T> : IList<T>\r\n    {\r\n        private T _item;\r\n\r\n        public SingletonList(T item)\r\n        {\r\n            _item = item;\r\n        }\r\n\r\n        public IEnumerator<T> GetEnumerator()\r\n        {\r\n            yield return _item;\r\n        }\r\n\r\n        IEnumerator IEnumerable.GetEnumerator()\r\n        {\r\n            return GetEnumerator();\r\n        }\r\n\r\n        public void Add(T item)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public void Clear()\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public bool Contains(T item)\r\n        {\r\n            if (item == null)\r\n                return _item == null;\r\n\r\n            return item.Equals(_item);\r\n        }\r\n\r\n        public void CopyTo(T[] array, int arrayIndex)\r\n        {\r\n            if (array == null)\r\n                throw new ArgumentNullException(nameof(array));\r\n\r\n            array[arrayIndex] = _item;\r\n        }\r\n\r\n        public bool Remove(T item)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public int Count\r\n        {\r\n            get { return 1; }\r\n        }\r\n\r\n        public bool IsReadOnly\r\n        {\r\n            get { return false; }\r\n        }\r\n\r\n        public int IndexOf(T item)\r\n        {\r\n            return Contains(item) ? 0 : -1;\r\n        }\r\n\r\n        public void Insert(int index, T item)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public void RemoveAt(int index)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public T this[int index]\r\n        {\r\n            get\r\n            {\r\n                if (index != 0)\r\n                    throw new IndexOutOfRangeException();\r\n                return _item;\r\n            }\r\n            set\r\n            {\r\n                if (index != 0)\r\n                    throw new IndexOutOfRangeException();\r\n                _item = value;\r\n            }\r\n        }\r\n    }\r\n\r\n\r\n    /// <summary>\r\n    /// Exposes a set of generic Array extension utility functions.\r\n    /// </summary>\r\n    public static class ArrayUtil\r\n    {\r\n        /// <summary>\r\n        /// Returns the length of an array or zero if it is null.\r\n        /// </summary>\r\n        public static int SafeLength<TItem>(this IList<TItem> values)\r\n        {\r\n            return values != null ? values.Count : 0;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Parses an array of items from a string, which are separated by\r\n        /// a specific character (e.g. \"1, 2, 3\" or \"3.5; 4.5; 5.5\").  Whitespace\r\n        /// is trimmed.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of items in the array returned</typeparam>\r\n        /// <param name=\"values\">The string to parse</param>\r\n        /// <param name=\"conv\">An instance of a string to T converter</param>\r\n        /// <param name=\"separatorChar\">The separator character</param>\r\n        /// <param name=\"defaults\">A default array to return, if the string is null or empty</param>\r\n        /// <returns></returns>\r\n        public static TItem[] Parse<TItem>(string values, Converter<string, TItem> conv,\r\n            char separatorChar, params TItem[] defaults)\r\n        {\r\n            if (!string.IsNullOrEmpty(values))\r\n            {\r\n                try\r\n                {\r\n                    List<TItem> list = new List<TItem>();\r\n                    string[] parts = values.Split(separatorChar);\r\n                    foreach (string part in parts)\r\n                        list.Add(conv(part.Trim()));\r\n                    return list.ToArray();\r\n                }\r\n// ReSharper disable EmptyGeneralCatchClause\r\n                catch (Exception)\r\n// ReSharper restore EmptyGeneralCatchClause\r\n                {\r\n                }\r\n            }\r\n            return defaults;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Joins the ToString() value for an array of objects, with a specified\r\n        /// separator character between each item.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">The type of the items in the array</typeparam>\r\n        /// <param name=\"values\">The array of items to join</param>\r\n        /// <param name=\"separator\">The separator character to place between strings</param>\r\n        /// <returns>A joined string of items with intervening separators</returns>\r\n        public static string ToString<TItem>(this IList<TItem> values, string separator)\r\n        {\r\n            StringBuilder sb = new StringBuilder();\r\n            foreach (TItem value in values)\r\n            {\r\n                if (sb.Length > 0)\r\n                    sb.Append(separator);\r\n                sb.Append(value);\r\n            }\r\n            return sb.ToString();\r\n        }\r\n\r\n        public static TItem[] ToArrayStd<TItem>(this IList<TItem> list)\r\n        {\r\n            var a = list as TItem[];\r\n            if (a == null)\r\n            {\r\n                a = new TItem[list.Count];\r\n                for (int i = 0; i < a.Length; i++)\r\n                    a[i] = list[i];\r\n            }\r\n            return a;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Gets a <see cref=\"IEnumerable{T}\"/> for enumerating over an Array.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of items in the array</typeparam>\r\n        /// <param name=\"values\">Array instance</param>\r\n        /// <param name=\"forward\">True if the enumerator should be forward, False if reversed</param>\r\n        /// <returns>The enumeration of the Array</returns>\r\n        public static IEnumerable<TItem> GetEnumerator<TItem>(this IList<TItem> values, bool forward)\r\n        {\r\n            if (forward)\r\n            {\r\n                foreach (TItem value in values)\r\n                    yield return value;\r\n            }\r\n            else\r\n            {\r\n                for (int i = values.Count - 1; i >= 0; i--)\r\n                    yield return values[i];\r\n            }\r\n        }\r\n\r\n        public const int RANDOM_SEED = 7 * 7 * 7 * 7 * 7; // 7^5 recommended by Brian S.\r\n\r\n        /// <summary>\r\n        /// Creates a random order of indexes into an array for a random linear walk\r\n        /// through an array.\r\n        /// </summary>\r\n        public static IEnumerable<TItem> RandomOrder<TItem>(this IList<TItem> list, int? seed = null)\r\n        {\r\n            int count = list.Count;\r\n            var indexOrder = new int[count];\r\n            for (int i = 0; i < count; i++)\r\n                indexOrder[i] = i;\r\n            Random r = seed.HasValue ? new Random(seed.Value) : new Random();\r\n            for (int i = 0; i < count; i++)\r\n                Helpers.Swap(ref indexOrder[0], ref indexOrder[r.Next(count)]);\r\n            foreach (int i in indexOrder)\r\n            {\r\n                yield return list[i];\r\n            }\r\n        }\r\n\r\n        /// <summary>\r\n        /// Searches an Array for an item that is reference equal with\r\n        /// a specified item to find.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of item in the array</typeparam>\r\n        /// <param name=\"values\">The Array to search</param>\r\n        /// <param name=\"find\">The item to find</param>\r\n        /// <returns>The index in the Array of the specified reference, or -1 if not found</returns>\r\n        public static int IndexOfReference<TItem>(this IList<TItem> values, TItem find)\r\n        {\r\n            return values.IndexOf(value => ReferenceEquals(value, find));\r\n        }\r\n\r\n        /// <summary>\r\n        /// Searches an Array for an item that matches criteria specified\r\n        /// through a delegate function.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of item in the array</typeparam>\r\n        /// <param name=\"values\">The Array to search</param>\r\n        /// <param name=\"found\">Delegate accepting an item, and returning true if it matches</param>\r\n        /// <returns>The index in the Array of the match, or -1 if not found</returns>\r\n        public static int IndexOf<TItem>(this IList<TItem> values, Predicate<TItem> found)\r\n        {\r\n            return IndexOf(values, found, 0);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Searches an Array for an item that matches criteria specified\r\n        /// through a delegate function. Search starts at the given index.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of item in the array</typeparam>\r\n        /// <param name=\"values\">The Array to search</param>\r\n        /// <param name=\"found\">Delegate accepting an item, and returning true if it matches</param>\r\n        /// <param name=\"startIndex\">Starting index of the search.</param>\r\n        /// <returns>The index in the Array of the match, or -1 if not found</returns>\r\n        public static int IndexOf<TItem>(this IList<TItem> values, Predicate<TItem> found, int startIndex)\r\n        {\r\n            for (int i = startIndex; i < values.Count; i++)\r\n            {\r\n                if (found(values[i]))\r\n                    return i;\r\n            }\r\n            return -1;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Searches backward in an Array for an item that matches criteria specified\r\n        /// through a delegate function.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of item in the array</typeparam>\r\n        /// <param name=\"values\">The Array to search</param>\r\n        /// <param name=\"found\">Delegate accepting an item, and returning true if it matches</param>\r\n        /// <returns>The index in the Array of the last match, or -1 if not found</returns>\r\n        public static int LastIndexOf<TItem>(this IList<TItem> values, Predicate<TItem> found)\r\n        {\r\n            for (int i = values.Count - 1; i >= 0; i--)\r\n            {\r\n                if (found(values[i]))\r\n                    return i;\r\n            }\r\n            return -1;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Searches an Array for an item that matches criteria specified\r\n        /// through a delegate function.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of item in the array</typeparam>\r\n        /// <param name=\"values\">The Array to search</param>\r\n        /// <param name=\"found\">Delegate accepting an item, and returning true if it matches</param>\r\n        /// <returns>True if the accepting function returns true for an element</returns>\r\n        public static bool Contains<TItem>(this IEnumerable<TItem> values, Predicate<TItem> found)\r\n        {\r\n            foreach (TItem value in values)\r\n            {\r\n                if (found(value))\r\n                    return true;\r\n            }\r\n            return false;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Checks for equality of all items in an IEnumerable without regard for order.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of items in the IEnumerable</typeparam>\r\n        /// <param name=\"values1\">First IEnumerable in the comparison</param>\r\n        /// <param name=\"values2\">Second IEnumerable in the comparison</param>\r\n        /// <returns>True if all items in one IEnumerable are found in the other, and IEnumerables are same length</returns>\r\n        public static bool ContainsAll<TItem>(this IEnumerable<TItem> values1, IEnumerable<TItem> values2)\r\n        {\r\n            var set1 = values1.ToHashSet();\r\n            var set2 = values2.ToHashSet();\r\n            return set1.Count == set2.Count && set1.IsSubsetOf(set2);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Checks for deep equality, or equality of all items in an Array.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of items in the array</typeparam>\r\n        /// <param name=\"values1\">First array in the comparison</param>\r\n        /// <param name=\"values2\">Second array in the comparison</param>\r\n        /// <returns>True if all items in both arrays in identical positions are Equal</returns>\r\n        public static bool EqualsDeep<TItem>(IList<TItem> values1, IList<TItem> values2)\r\n        {\r\n            if (values1 == null && values2 == null)\r\n                return true;\r\n            if (values1 == null || values2 == null)\r\n                return false;\r\n            if (values1.Count != values2.Count)\r\n                return false;\r\n            for (int i = 0; i < values1.Count; i++)\r\n            {\r\n                if (!Equals(values1[i], values2[i]))\r\n                    return false;\r\n            }\r\n            return true;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Checks for deep equality, or equality of all items in a Dictionary.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItemKey\">Type of items in the dictionary keys</typeparam>\r\n        /// <typeparam name=\"TItemValue\">Type of items in the dictionary values</typeparam>\r\n        /// <param name=\"values1\">First array in the comparison</param>\r\n        /// <param name=\"values2\">Second array in the comparison</param>\r\n        /// <returns>True if all items in both arrays in identical positions are Equal</returns>\r\n        public static bool EqualsDeep<TItemKey, TItemValue>(IDictionary<TItemKey, TItemValue> values1,\r\n            IDictionary<TItemKey, TItemValue> values2)\r\n        {\r\n            if (values1 == null && values2 == null)\r\n                return true;\r\n            if (values1 == null || values2 == null)\r\n                return false;\r\n            if (values1.Count != values2.Count)\r\n                return false;\r\n            foreach (var keyValuePair1 in values1)\r\n            {\r\n                TItemValue value2;\r\n                if (!values2.TryGetValue(keyValuePair1.Key, out value2))\r\n                    return false;\r\n                if (!Equals(keyValuePair1.Value, value2))\r\n                    return false;\r\n            }\r\n            return true;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Constructs a hash-code for an Array from all of the items in\r\n        /// the array.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of the items in the array</typeparam>\r\n        /// <param name=\"values\">The Array instance</param>\r\n        /// <returns>A hash-code value constructed from all items in the array</returns>\r\n        public static int GetHashCodeDeep<TItem>(this IList<TItem> values)\r\n        {\r\n            return values.GetHashCodeDeep(v => v.GetHashCode());\r\n        }\r\n\r\n        public static int GetHashCodeDeep<TItem>(this IList<TItem> values, Func<TItem, int> getHashCode)\r\n        {\r\n            unchecked\r\n            {\r\n                int result = 0;\r\n                foreach (TItem value in values)\r\n                    result = (result * 397) ^ (!Equals(value, default(TItem)) ? getHashCode(value) : 0);\r\n                return result;\r\n            }\r\n        }\r\n\r\n        /// <summary>\r\n        /// Checks if all elements in one list are <see cref=\"object.ReferenceEquals\"/>\r\n        /// with the elements in another list.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of the list elements</typeparam>\r\n        /// <param name=\"values1\">The first list in the comparison</param>\r\n        /// <param name=\"values2\">The second list in the comparison</param>\r\n        /// <returns>True if all references in the lists are equal to each other</returns>\r\n        public static bool ReferencesEqual<TItem>(IList<TItem> values1, IList<TItem> values2)\r\n        {\r\n            if (values1 == null && values2 == null)\r\n                return true;\r\n            if (values1 == null || values2 == null)\r\n                return false;\r\n            if (values1.Count != values2.Count)\r\n                return false;\r\n            for (int i = 0; i < values1.Count; i++)\r\n            {\r\n                if (!ReferenceEquals(values1[i], values2[i]))\r\n                    return false;\r\n            }\r\n            return true;\r\n        }\r\n\r\n        public static bool InnerReferencesEqual<TItem, TItemList>(IList<TItemList> values1, IList<TItemList> values2)\r\n            where TItemList : IList<TItem>\r\n        {\r\n            if (values1 == null && values2 == null)\r\n                return true;\r\n            if (values1 == null || values2 == null)\r\n                return false;\r\n            if (values1.Count != values2.Count)\r\n                return false;\r\n            for (int i = 0; i < values1.Count; i++)\r\n            {\r\n                if (!ReferencesEqual(values1[i], values2[i]))\r\n                    return false;\r\n            }\r\n            return true;\r\n            \r\n        }\r\n\r\n        /// <summary>\r\n        /// Enumerates two lists assigning references from the second list to\r\n        /// entries in the first list, where they are equal.  Useful for maintaining\r\n        /// reference equality when recalculating values. Similar to <see cref=\"Helpers.AssignIfEquals{T}\"/>.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of the list elements</typeparam>\r\n        /// <param name=\"values1\">The first list in the comparison</param>\r\n        /// <param name=\"values2\">The second list in the comparison</param>\r\n        public static void AssignIfEqualsDeep<TItem>(IList<TItem> values1, IList<TItem> values2)\r\n        {\r\n            if (values1 == null || values2 == null)\r\n                return;\r\n            for (int i = 0, len = Math.Min(values1.Count, values2.Count); i < len; i++)\r\n            {\r\n                if (Equals(values1[i], values2[i]))\r\n                    values1[i] = values2[i];\r\n            }\r\n        }\r\n\r\n        /// <summary>\r\n        /// Sort an array and produce an output array that shows how the indexes of the\r\n        /// elements have been reordered.  The indexing array can then be applied to a\r\n        /// different array to follow the ordering of the initial array.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of array elements</typeparam>\r\n        /// <param name=\"array\">Array to sort</param>\r\n        /// <param name=\"sortIndexes\">Records how indexes were changed as a result of sorting</param>\r\n        public static void Sort<TItem>(TItem[] array, out int[] sortIndexes)\r\n        {\r\n            sortIndexes = new int[array.Length];\r\n            for (int i = 0; i < array.Length; i++)\r\n                sortIndexes[i] = i;\r\n            Array.Sort(array, sortIndexes);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Use when you have more than just one other array to sort. Otherwise, consider using Linq\r\n        /// </summary>\r\n        public static void Sort<TItem>(TItem[] array, params TItem[][] secondaryArrays)\r\n        {\r\n            int[] sortIndexes;\r\n            Sort(array, out sortIndexes);\r\n            int len = array.Length;\r\n            TItem[] buffer = new TItem[len];\r\n            foreach (var secondaryArray in secondaryArrays.Where(a => a != null))\r\n                ApplyOrder(sortIndexes, secondaryArray, buffer);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Apply the ordering gotten from the sorting of an array (see Sort method above)\r\n        /// to a new array.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of array elements</typeparam>\r\n        /// <param name=\"sortIndexes\">Array of indexes that recorded sort operations</param>\r\n        /// <param name=\"array\">Array to be reordered using the index array</param>\r\n        /// <param name=\"buffer\">An optional buffer to use to avoid allocating a new array and force in-place sorting</param>\r\n        /// <returns>A sorted version of the original array</returns>\r\n        public static TItem[] ApplyOrder<TItem>(int[] sortIndexes, TItem[] array, TItem[] buffer = null)\r\n        {\r\n            TItem[] ordered;\r\n            int len = array.Length;\r\n            if (buffer == null)\r\n                ordered = new TItem[len];\r\n            else\r\n            {\r\n                Array.Copy(array, buffer, len);\r\n                ordered = array;\r\n                array = buffer;\r\n            }\r\n            for (int i = 0; i < array.Length; i++)\r\n                ordered[i] = array[sortIndexes[i]];\r\n            return ordered;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Returns true if the given array is not in sort order.\r\n        /// </summary>\r\n        /// <param name=\"array\"></param>\r\n        /// <returns>True if array needs to be sorted</returns>\r\n        public static bool NeedsSort(float[] array)\r\n        {\r\n            for (int i = 0; i < array.Length - 1; i++)\r\n                if (array[i] > array[i + 1])\r\n                    return true;\r\n            return false;\r\n        }\r\n    }\r\n\r\n    /// <summary>\r\n    /// Read a potentially large array into a list of arrays in order to avoid very large memory allocations.\r\n    /// We are trying to avoid not only memory fragmentation issues, but also the size limit of 2 gigabytes.\r\n    /// </summary>\r\n    public class BlockedArray<TItem> : IReadOnlyList<TItem>\r\n    {\r\n        private readonly List<TItem[]> _blocks;\r\n        private readonly int _itemCount;\r\n\r\n        /// <summary>\r\n        /// Empty array.\r\n        /// </summary>\r\n        public BlockedArray()\r\n        {\r\n        }\r\n\r\n        /// <summary>\r\n        /// Read an array into blocks.\r\n        /// </summary>\r\n        /// <param name=\"readItems\">Function to read a number of items and return them in an array.</param>\r\n        /// <param name=\"itemCount\">Total number of items to read.</param>\r\n        /// <param name=\"itemSize\">Size of each item in bytes.</param>\r\n        /// <param name=\"bytesPerBlock\">Maximum size of a block in bytes.</param>\r\n        /// <param name=\"progressMonitor\">Optional progress monitor for reporting progress over long periods</param>\r\n        /// <param name=\"status\">Optional progress status object for reporting progress</param>\r\n        public BlockedArray(Func<int, TItem[]> readItems, int itemCount, int itemSize, int bytesPerBlock,\r\n            IProgressMonitor progressMonitor = null, IProgressStatus status = null)\r\n        {\r\n            Assume.IsTrue(itemSize < bytesPerBlock);    // Make sure these values aren't flipped\r\n\r\n            _itemCount = itemCount;\r\n            _blocks = new List<TItem[]>();\r\n\r\n            var itemsPerBlock = bytesPerBlock/itemSize;\r\n            int startPercent = status != null ? status.PercentComplete : 0;\r\n            while (itemCount > 0)\r\n            {\r\n                _blocks.Add(readItems(Math.Min(itemCount, itemsPerBlock)));\r\n                itemCount -= itemsPerBlock;\r\n\r\n                if (progressMonitor != null && status != null)\r\n                {\r\n                    int currentPercent = (int)(100 - ((100.0 - startPercent) * itemCount) / _itemCount);\r\n                    if (currentPercent != status.PercentComplete)\r\n                        progressMonitor.UpdateProgress(status = status.ChangePercentComplete(currentPercent));\r\n                }\r\n            }\r\n        }\r\n\r\n        /// <summary>\r\n        /// Copy a list into blocks.\r\n        /// </summary>\r\n        /// <param name=\"items\">Items to copy.</param>\r\n        /// <param name=\"itemSize\">Size of each item in bytes.</param>\r\n        /// <param name=\"bytesPerBlock\">Maximum size of a block in bytes.</param>\r\n        public BlockedArray(IList<TItem> items, int itemSize, int bytesPerBlock)\r\n        {\r\n            _itemCount = items.Count;\r\n            _blocks = new List<TItem[]>();\r\n\r\n            var itemsPerBlock = bytesPerBlock/itemSize;\r\n            TItem[] block = null;\r\n            for (int index = 0; index < _itemCount; index++)\r\n            {\r\n                var inBlockIndex = index%itemsPerBlock;\r\n                if (inBlockIndex == 0)\r\n                {\r\n                    block = new TItem[Math.Min(_itemCount - index, itemsPerBlock)];\r\n                    _blocks.Add(block);\r\n                }\r\n// ReSharper disable PossibleNullReferenceException\r\n                block[inBlockIndex] = items[index];\r\n// ReSharper restore PossibleNullReferenceException\r\n            }\r\n        }\r\n\r\n        public BlockedArray(BlockedArrayList<TItem> items)\r\n        {\r\n            _itemCount = items.Count;\r\n            _blocks = items.GetBlocks().ToList();\r\n        }\r\n\r\n        public static BlockedArray<TItem> Convert<TItemSrc>(BlockedArrayList<TItemSrc> blockedArrayList,\r\n            Func<TItemSrc, TItem> converter)\r\n        {\r\n            return new BlockedArray<TItem>(blockedArrayList.GetBlocks()\r\n                .Select(block => block.Select(converter).ToArray())\r\n                .ToList(),\r\n                blockedArrayList.Count);\r\n        }\r\n\r\n        private BlockedArray(List<TItem[]> blocks, int itemCount)\r\n        {\r\n            _itemCount = itemCount;\r\n            _blocks = blocks;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Number of items in this array.\r\n        /// </summary>\r\n        public int Length { get { return _itemCount; } }\r\n\r\n        public int Count { get { return Length; } }\r\n\r\n        public IEnumerable<TItem[]> Blocks { get { return _blocks; } }\r\n\r\n        /// <summary>\r\n        /// Return the item corresponding to the given index.\r\n        /// </summary>\r\n        /// <param name=\"index\">Array index.</param>\r\n        public TItem this[int index]\r\n        {\r\n            get\r\n            {\r\n                if (index >= _itemCount)\r\n                    throw new IndexOutOfRangeException();\r\n                var blockLength = _blocks[0].Length;\r\n                var blockIndex = index/blockLength;\r\n                var itemIndex = index%blockLength;\r\n                return _blocks[blockIndex][itemIndex];\r\n            }\r\n        }\r\n\r\n        IEnumerator IEnumerable.GetEnumerator()\r\n        {\r\n            return GetEnumerator();\r\n        }\r\n\r\n        public IEnumerator<TItem> GetEnumerator()\r\n        {\r\n            return _blocks.SelectMany(block => block).Take(_itemCount).GetEnumerator();\r\n        }\r\n\r\n        /// <summary>\r\n        /// Write the array.\r\n        /// </summary>\r\n        /// <param name=\"writeAction\">Action to write one array block.</param>\r\n        public void WriteArray(Action<TItem[]> writeAction)\r\n        {\r\n            if (_blocks != null)\r\n            {\r\n                foreach (var block in _blocks)\r\n                    writeAction(block);\r\n            }\r\n        }\r\n\r\n        /// <summary>\r\n        /// Write the array.\r\n        /// </summary>\r\n        /// <param name=\"writeAction\">Action to write one array block.</param>\r\n        /// <param name=\"startIndex\">First index to write.</param>\r\n        /// <param name=\"count\">How many items to write.</param>\r\n        public void WriteArray(Action<TItem[], int, int> writeAction, int startIndex, int count)\r\n        {\r\n            if (startIndex + count > _itemCount)\r\n                throw new IndexOutOfRangeException();\r\n            var blockLength = _blocks[0].Length;\r\n            while (count > 0)\r\n            {\r\n                var blockIndex = startIndex/blockLength;\r\n                var itemIndex = startIndex%blockLength;\r\n                var writeCount = Math.Min(count, blockLength - itemIndex);\r\n                writeAction(_blocks[blockIndex], itemIndex, writeCount);\r\n                startIndex += writeCount;\r\n                count -= writeCount;\r\n            }\r\n        }\r\n\r\n        public BlockedArray<TItem> ChangeAll(Func<TItem, TItem> changeElement)\r\n        {\r\n            var newBlocks = new List<TItem[]>();\r\n            foreach (var block in _blocks)\r\n            {\r\n                var newBlock = new TItem[block.Length];\r\n                newBlocks.Add(newBlock);\r\n\r\n                for (int i = 0; i < block.Length; i++)\r\n                    newBlock[i] = changeElement(block[i]);\r\n            }\r\n            return new BlockedArray<TItem>(newBlocks, _itemCount);\r\n        }\r\n    }\r\n\r\n    public class BlockedArrayList<TItem> : IList<TItem>\r\n    {\r\n        private List<List<TItem>> _blocks = new List<List<TItem>>{new List<TItem>()};\r\n        private int _itemCount;\r\n        private readonly int _itemsPerBlock;\r\n\r\n        public BlockedArrayList(int itemSize, int bytesPerBlock)\r\n        {\r\n            _itemsPerBlock = bytesPerBlock/itemSize;\r\n        }\r\n\r\n        public IEnumerator<TItem> GetEnumerator()\r\n        {\r\n            return _blocks.SelectMany(block => block).GetEnumerator();\r\n        }\r\n\r\n        IEnumerator IEnumerable.GetEnumerator()\r\n        {\r\n            return GetEnumerator();\r\n        }\r\n\r\n        public void Add(TItem item)\r\n        {\r\n            var block = _blocks.Last();\r\n            if (block.Count >= _itemsPerBlock)\r\n            {\r\n                block = new List<TItem>();\r\n                _blocks.Add(block);\r\n            }\r\n            block.Add(item);\r\n            _itemCount++;\r\n        }\r\n\r\n        public void AddRange(IList<TItem> chromTransitions)\r\n        {\r\n            // CONSIDER: Make this faster than adding one at a time?\r\n            foreach (var t in chromTransitions)\r\n            {\r\n                Add(t);\r\n            }\r\n        }\r\n\r\n        public void AddRange(BlockedArray<TItem> chromTransitions)\r\n        {\r\n            int transferCount = chromTransitions.Count;\r\n            int blockIndex = 0;\r\n            int itemIndex = 0;\r\n            var chromTransitionBlocks = chromTransitions.Blocks.ToArray();\r\n\r\n            while (transferCount > 0)\r\n            {\r\n                var blockSrc = chromTransitionBlocks[blockIndex];\r\n                int copyCount = blockSrc.Length - itemIndex;\r\n\r\n                var blockDest = _blocks.Last();\r\n                if (blockDest.Count >= _itemsPerBlock)\r\n                {\r\n                    // Pre-allocate a new list to the smaller of the number of items\r\n                    // to copy or the total items per block\r\n                    blockDest = new List<TItem>(Math.Min(copyCount, _itemsPerBlock));\r\n                    _blocks.Add(blockDest);\r\n                }\r\n                // Copy everything remaining in current source block or the maximum left in the destination block\r\n                int remainder = _itemsPerBlock - blockDest.Count;\r\n                if (copyCount <= remainder)\r\n                {\r\n                    blockDest.AddRange(blockSrc.Skip(itemIndex).Take(copyCount));\r\n                    blockIndex++;\r\n                    itemIndex = 0;\r\n                }\r\n                else\r\n                {\r\n                    copyCount = remainder;\r\n                    blockDest.AddRange(blockSrc.Skip(itemIndex).Take(copyCount));\r\n                    itemIndex += copyCount;\r\n                }\r\n                transferCount -= copyCount;\r\n                _itemCount += copyCount;\r\n            }\r\n        }\r\n\r\n        public void Clear()\r\n        {\r\n            _blocks = new List<List<TItem>> { new List<TItem>() };\r\n            _itemCount = 0;\r\n        }\r\n\r\n        public bool Contains(TItem item)\r\n        {\r\n            return _blocks.Contains(l => l.Contains(item));\r\n        }\r\n\r\n        public void CopyTo(TItem[] array, int arrayIndex)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public bool Remove(TItem item)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public int Count { get { return _itemCount; } }\r\n        public bool IsReadOnly { get { return false; } }\r\n\r\n        public IEnumerable<TItem[]> GetBlocks()\r\n        {\r\n            return _blocks.Select(b => b.ToArray());\r\n        }\r\n\r\n        public int IndexOf(TItem item)\r\n        {\r\n            int index = 0;\r\n            foreach (var block in _blocks)\r\n            {\r\n                foreach (var itemTest in block)\r\n                {\r\n                    if (Equals(item, itemTest))\r\n                        return index;\r\n                    index++;\r\n                }\r\n            }\r\n            return -1;\r\n        }\r\n\r\n        public TItem this[int index]\r\n        {\r\n            get\r\n            {\r\n                if (index >= _itemCount)\r\n                    throw new IndexOutOfRangeException();\r\n                var blockIndex = index / _itemsPerBlock;\r\n                var itemIndex = index % _itemsPerBlock;\r\n                return _blocks[blockIndex][itemIndex];\r\n            }\r\n            set\r\n            {\r\n                throw new NotSupportedException();\r\n            }\r\n        }\r\n\r\n        public void Insert(int index, TItem item)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public void RemoveAt(int index)\r\n        {\r\n            throw new NotSupportedException();\r\n        }\r\n\r\n        public BlockedArray<TItem> ToBlockedArray()\r\n        {\r\n            return new BlockedArray<TItem>(this);\r\n        }\r\n\r\n        public void Reorder(IEnumerable<int> newOrder)\r\n        {\r\n            var blockNext = new List<TItem>(_blocks[0].Count);\r\n            var blocksNew = new List<List<TItem>>(_blocks.Count) { blockNext };\r\n            foreach (var i in newOrder)\r\n            {\r\n                if (blockNext.Count == _itemsPerBlock)\r\n                {\r\n                    blockNext = new List<TItem>(_blocks[blocksNew.Count].Count);\r\n                    blocksNew.Add(blockNext);\r\n                }\r\n                blockNext.Add(this[i]);\r\n            }\r\n            _blocks = blocksNew;\r\n        }\r\n\r\n        public void Sort()\r\n        {\r\n            Sort(Comparer<TItem>.Default.Compare);\r\n        }\r\n\r\n        public void Sort(Comparison<TItem> compare)\r\n        {\r\n            foreach (var block in _blocks)\r\n                block.Sort(compare);\r\n            if (_blocks.Count < 2)\r\n                return;\r\n\r\n            try\r\n            {\r\n                // Merge sort the blocks into new list\r\n                var nextIndexes = new int[_blocks.Count];\r\n                var blockNext = new List<TItem>(_blocks[0].Count);\r\n                var blocksNew = new List<List<TItem>>(_blocks.Count) { blockNext };\r\n                for (int i = 0; i < _itemCount; i++)\r\n                {\r\n                    if (blockNext.Count == _itemsPerBlock)\r\n                    {\r\n                        blockNext = new List<TItem>(_blocks[blocksNew.Count].Count);\r\n                        blocksNew.Add(blockNext);\r\n                    }\r\n                    int iBlockMin = 0;\r\n                    for (int iBlock = 1; iBlock < _blocks.Count; iBlock++)\r\n                    {\r\n                        int iNext = nextIndexes[iBlock];\r\n                        int iMin = nextIndexes[iBlockMin];\r\n                        if (iNext >= _blocks[iBlock].Count)\r\n                            continue;\r\n                        if (iMin >= _blocks[iBlockMin].Count || compare(_blocks[iBlock][iNext], _blocks[iBlockMin][iMin]) < 1)\r\n                            iBlockMin = iBlock;\r\n                    }\r\n                    blockNext.Add(_blocks[iBlockMin][nextIndexes[iBlockMin]]);\r\n                    nextIndexes[iBlockMin]++;\r\n                }\r\n                _blocks = blocksNew;\r\n            }\r\n            catch (Exception e)\r\n            {\r\n                Trace.WriteLine(e);\r\n                throw;\r\n            }\r\n        }\r\n    }\r\n\r\n    /// <summary>\r\n    /// A set of generic, static helper functions.\r\n    /// </summary>\r\n    public static partial class Helpers\r\n    {\r\n        /// <summary>\r\n        /// Swaps two reference values in memory, making each contain\r\n        /// the reference the other started with.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">Type of the two values</typeparam>\r\n        /// <param name=\"val1\">Left value</param>\r\n        /// <param name=\"val2\">Right value</param>\r\n        public static void Swap<TItem>(ref TItem val1, ref TItem val2)\r\n        {\r\n            TItem tmp = val1;\r\n            val1 = val2;\r\n            val2 = tmp;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Assigns the a source reference to the intended destination,\r\n        /// only if they are <see cref=\"object.Equals(object,object)\"/>.\r\n        /// \r\n        /// This can be useful in combination with immutable objects,\r\n        /// allowing the caller choose an existing object already referenced\r\n        /// in a data structure over a newly created instance, if the two\r\n        /// are identical in value.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\"></typeparam>\r\n        /// <param name=\"dest\"></param>\r\n        /// <param name=\"src\"></param>\r\n        public static void AssignIfEquals<TItem>(ref TItem dest, TItem src)\r\n        {\r\n            if (Equals(dest, src))\r\n                dest = src;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Compare two IEnumerable instances for equality.\r\n        /// </summary>\r\n        /// <typeparam name=\"TItem\">The type of element being enumerated</typeparam>\r\n        /// <param name=\"e1\">The first IEnumerable</param>\r\n        /// <param name=\"e2\">The second IEnumberable</param>\r\n        /// <returns>True if the two IEnumerables enumerate over equal objects</returns>\r\n        public static bool Equals<TItem>(IEnumerable<TItem> e1, IEnumerable<TItem> e2)\r\n        {\r\n            IEnumerator<TItem> enum1 = e1.GetEnumerator();\r\n            IEnumerator<TItem> enum2 = e2.GetEnumerator();\r\n            bool b1, b2;\r\n            while (MoveNext(enum1, out b1, enum2, out b2))\r\n            {\r\n                if (!Equals(enum1.Current, enum2.Current))\r\n                    break;\r\n            }\r\n\r\n            // If both enums have advanced to completion without finding\r\n            // a difference, then they are equal.\r\n            return (!b1 && !b2);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Call MoveNext on two IEnumerator instances in one operation,\r\n        /// but avoid short-circuiting of (e1.MoveNext() && e2.MoveNext),\r\n        /// and pass the return values of both as out parameters.\r\n        /// </summary>\r\n        /// <param name=\"e1\">First Enumerator to advance</param>\r\n        /// <param name=\"b1\">Return value of e1.MoveNext()</param>\r\n        /// <param name=\"e2\">Second Enumerator to advance</param>\r\n        /// <param name=\"b2\">Return value of e2.MoveNext()</param>\r\n        /// <returns>True if both calls to MoveNext() succeed</returns>\r\n        private static bool MoveNext(IEnumerator e1, out bool b1,\r\n            IEnumerator e2, out bool b2)\r\n        {\r\n            b1 = e1.MoveNext();\r\n            b2 = e2.MoveNext();\r\n            return b1 && b2;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Parses an enum value from a string, returning a default value,\r\n        /// if the string fails to parse.\r\n        /// </summary>\r\n        /// <typeparam name=\"TEnum\">The enum type</typeparam>\r\n        /// <param name=\"value\">The string to parse</param>\r\n        /// <param name=\"defaultValue\">The value to return, if parsing fails</param>\r\n        /// <returns>An enum value of type <see cref=\"TEnum\"/></returns>\r\n        public static TEnum ParseEnum<TEnum>(string value, TEnum defaultValue)\r\n        {\r\n            try\r\n            {\r\n                return (TEnum)Enum.Parse(typeof(TEnum), value, true);\r\n            }\r\n            catch (Exception)\r\n            {\r\n                return defaultValue;\r\n            }                            \r\n        }\r\n\r\n        /// <summary>\r\n        /// Given a localized string and an array of localized strings with the\r\n        /// index of each localized string matching the desired enum value for\r\n        /// an enum type, returns the enum value corresponding to the localized string.\r\n        /// </summary>\r\n        /// <typeparam name=\"TEnum\">The enum type</typeparam>\r\n        /// <param name=\"value\">The localized string for which the enum value is desired</param>\r\n        /// <param name=\"localizedStrings\">Array of all localized strings</param>\r\n        /// <returns>An enum value of type <see cref=\"TEnum\"/></returns>\r\n        public static TEnum EnumFromLocalizedString<TEnum>(string value, string[] localizedStrings)\r\n        {\r\n            int i = localizedStrings.IndexOf(v => Equals(v, value));\r\n            if (i == -1)\r\n                throw new ArgumentException(string.Format(@\"The string '{0}' does not match an enum value ({1})\", value, string.Join(@\", \", localizedStrings)));\r\n            return (TEnum) (object) i;            \r\n        }\r\n\r\n        public static TEnum EnumFromLocalizedString<TEnum>(string value, string[] localizedStrings, TEnum defaultValue)\r\n        {\r\n            int i = localizedStrings.IndexOf(v => Equals(v, value));\r\n            return (i == -1 ? defaultValue : (TEnum) (object) i);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Enumerate all possible values of the given enum type.\r\n        /// </summary>\r\n        public static IEnumerable<TEnum> GetEnumValues<TEnum>()\r\n        {\r\n            return Enum.GetValues(typeof (TEnum)).Cast<TEnum>();\r\n        }\r\n\r\n        public static int CountEnumValues<TEnum>()\r\n        {\r\n            return Enum.GetValues(typeof (TEnum)).Length;\r\n        }\r\n\r\n        public static string MakeId(IEnumerable<char> name)\r\n        {\r\n            return MakeId(name, false);\r\n        }\r\n\r\n        public static string MakeId(IEnumerable<char> name, bool capitalize)\r\n        {\r\n            StringBuilder sb = new StringBuilder();\r\n            char lastC = '\\0'; \r\n            foreach (var c in name)\r\n            {\r\n                if (char.IsLetterOrDigit(c))\r\n                {\r\n                    if (lastC == ' ')\r\n                        sb.Append('_');\r\n                    lastC = c;\r\n                    if (capitalize && sb.Length == 0)\r\n                        sb.Append(c.ToString(CultureInfo.InvariantCulture).ToUpperInvariant());\r\n                    else\r\n                        sb.Append(c);\r\n                }\r\n                // Must start with a letter or digit\r\n                else if (lastC != '\\0')\r\n                {\r\n                    // After the start _ okay (dashes turned out to be problematic)\r\n                    if (c == '_' /* || c == '-'*/)\r\n                        sb.Append(lastC = c);\r\n                    // All other characters are replaced with _, but once the next\r\n                    // letter or number is seen.\r\n                    else if (char.IsLetterOrDigit(lastC))\r\n                        lastC = ' ';\r\n                }\r\n            }\r\n            return sb.ToString();\r\n        }\r\n\r\n        // ReSharper disable LocalizableElement\r\n        private static readonly Regex REGEX_XML_ID = new Regex(\"/^[:_A-Za-z][-.:_A-Za-z0-9]*$/\");\r\n        private const string XML_ID_FIRST_CHARS = \":_ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\";\r\n        private const string XML_ID_FOLLOW_CHARS = \"-.:_ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\";\r\n        private const string XML_NON_ID_SEPARATOR_CHARS = \";[]{}()!|\\\\/\\\"'<>\";\r\n        private const string XML_NON_ID_PUNCTUATION_CHARS = \",?\";\r\n        // ReSharper restore LocalizableElement\r\n\r\n        public static string MakeXmlId(string name)\r\n        {\r\n            if (string.IsNullOrEmpty(name))\r\n                throw new InvalidOperationException(\r\n                    Resources.Helpers_MakeXmlId_Failure_creating_XML_ID_Input_string_may_not_be_empty);\r\n            if (REGEX_XML_ID.IsMatch(name))\r\n                return name;\r\n\r\n            var sb = new StringBuilder();\r\n            int i = 0;\r\n            if (XML_ID_FIRST_CHARS.Contains(name[i]))\r\n                sb.Append(name[i++]);\r\n            else\r\n            {\r\n                sb.Append('_');\r\n                // If the first character is not allowable, advance past it.\r\n                // Otherwise, keep it in the ID.\r\n                if (!XML_ID_FOLLOW_CHARS.Contains(name[i]))\r\n                    i++;\r\n            }\r\n            for (; i < name.Length; i++)\r\n            {\r\n                char c = name[i];\r\n                if (XML_ID_FOLLOW_CHARS.Contains(c))\r\n                    sb.Append(c);\r\n                else if (char.IsWhiteSpace(c))\r\n                    sb.Append('_');\r\n                else if (XML_NON_ID_SEPARATOR_CHARS.Contains(c))\r\n                    sb.Append(':');\r\n                else if (XML_NON_ID_PUNCTUATION_CHARS.Contains(c))\r\n                    sb.Append('.');\r\n                else\r\n                    sb.Append('-');\r\n            }\r\n            return sb.ToString();\r\n        }\r\n\r\n        /// <summary>\r\n        /// Given a proposed name and a set of existing names, returns a unique name by adding\r\n        /// or incrementing an integer suffix.\r\n        /// </summary>\r\n        /// <param name=\"name\">A proposed name to add</param>\r\n        /// <param name=\"set\">A set of existing names</param>\r\n        /// <returns>A new unique name that can be safely added to the existing set without name conflict</returns>\r\n        public static string GetUniqueName(string name, ICollection<string> set)\r\n        {\r\n            return GetUniqueName(name, s => !set.Contains(s));\r\n        }\r\n\r\n        public static string GetUniqueName(string name, Func<string, bool> isUnique)\r\n        {\r\n            if (isUnique(name))\r\n                return name;\r\n\r\n            int num = 1;\r\n            // If the name has an integer suffix, start searching with the base name\r\n            // and the integer suffix incremented by 1.\r\n            int i = GetIntSuffixStart(name);\r\n            if (i < name.Length)\r\n            {\r\n                num = int.Parse(name.Substring(i)) + 1;\r\n                name = name.Substring(0, i);\r\n            }\r\n            // Loop until a unique base name and integer suffix combination is found.\r\n            while (!isUnique(name + num))\r\n                num++;\r\n            return name + num;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Given a name returns the start index of an integer suffix, if the name has one,\r\n        /// or the length of the string, if no integer suffix is present.\r\n        /// </summary>\r\n        /// <param name=\"name\">A name to analyze</param>\r\n        /// <returns>The starting position of an integer suffix or the length of the string, if the name does not have one</returns>\r\n        private static int GetIntSuffixStart(string name)\r\n        {\r\n            for (int i = name.Length; i > 0; i--)\r\n            {\r\n                int num;\r\n                if (!int.TryParse(name.Substring(i - 1), out num))\r\n                    return i;\r\n            }\r\n            return 0;\r\n        }\r\n\r\n        public static List<string> EnsureUniqueNames(List<string> names, HashSet<string> reservedNames = null)\r\n        {\r\n            var setUsedNames = reservedNames ?? new HashSet<string>();\r\n            var result = new List<string>();\r\n            for (int i = 0; i < names.Count; i++)\r\n            {\r\n                string baseName = names[i];\r\n                // Make sure the next name added is unique\r\n                string name = (baseName.Length != 0 ? baseName : @\"1\");\r\n                for (int suffix = 2; setUsedNames.Contains(name); suffix++)\r\n                    name = baseName + suffix;\r\n                result.Add(name);\r\n                // Add this name to the used set\r\n                setUsedNames.Add(name);\r\n            }\r\n            return result;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Count the number of lines in the file specified.\r\n        /// </summary>\r\n        /// <param name=\"f\">The filename to count lines in.</param>\r\n        /// <returns>The number of lines in the file.</returns>\r\n        public static long CountLinesInFile(string f)\r\n        {\r\n            long count = 0;\r\n            using (StreamReader r = new StreamReader(f))\r\n            {\r\n                while (r.ReadLine() != null)\r\n                    count++;\r\n            }\r\n            return count;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Count the number of lines in the string specified.\r\n        /// </summary>\r\n        /// <param name=\"s\">The string to count lines in.</param>\r\n        /// <returns>The number of lines in the string.</returns>\r\n        public static long CountLinesInString(string s)\r\n        {\r\n            long count = 1;\r\n            int start = 0;\r\n            while ((start = s.IndexOf('\\n', start)) != -1)\r\n            {\r\n                count++;\r\n                start++;\r\n            }\r\n            return count;\r\n        }\r\n\r\n        private const char LABEL_SEP_CHAR = '_';\r\n        private const string ELIPSIS = \"...\";\r\n        private static readonly char[] SPACE_CHARS = { '_', '-', ' ', '.', ',' };\r\n\r\n        /// <summary>\r\n        /// Finds repetitive text in labels and removes the text to save space.\r\n        /// </summary>\r\n        /// <param name=\"labels\">The labels we are removing redundant text from.</param>\r\n        /// <param name=\"startLabelIndex\">Index we want to start looking at, in case the Expected/Library\r\n        /// label is showing.</param>\r\n        /// <returns>Return </returns>\r\n        public static bool RemoveRepeatedLabelText(string[] labels, int startLabelIndex)\r\n        {\r\n            // Check to see if there are any labels. \r\n            if (labels.Length == startLabelIndex)\r\n                return false;\r\n\r\n            // Creat a normalized set of labels to test for repeated text\r\n            string[] labelsRemove = new string[labels.Length];\r\n\r\n            Array.Copy(labels, labelsRemove, labels.Length);\r\n\r\n            if (startLabelIndex != 0)\r\n            {\r\n                labelsRemove = new string[labelsRemove.Length - startLabelIndex];\r\n                Array.Copy(labels, startLabelIndex, labelsRemove, 0, labelsRemove.Length);\r\n            }\r\n\r\n            for (int i = 0; i < labelsRemove.Length; i++)\r\n                labelsRemove[i] = NormalizeSeparators(labelsRemove[i]);\r\n\r\n            var labelParts = labelsRemove[0].Split(LABEL_SEP_CHAR);\r\n\r\n            // If all labels start with the first part\r\n            string replaceString = labelParts[0];\r\n            string partFirst = replaceString + LABEL_SEP_CHAR;\r\n            if (!labelsRemove.Contains(label => !label.StartsWith(partFirst)))\r\n            {\r\n                RemoveString(labels, startLabelIndex, replaceString, ReplaceLocation.start);\r\n                return true;\r\n            }\r\n\r\n            // If all labels end with the last part\r\n            replaceString = labelParts[labelParts.Length - 1];\r\n            string partLast = LABEL_SEP_CHAR + replaceString;\r\n            if (!labelsRemove.Contains(label => !label.EndsWith(partLast)))\r\n            {\r\n                RemoveString(labels, startLabelIndex, replaceString, ReplaceLocation.end);\r\n                return true;\r\n            }\r\n\r\n            for (int i = 1 ; i < labelParts.Length - 1; i++)\r\n            {\r\n                replaceString = labelParts[i];\r\n                if (string.IsNullOrEmpty(replaceString))\r\n                    continue;\r\n                string partMiddle = LABEL_SEP_CHAR + replaceString + LABEL_SEP_CHAR;\r\n                // If all labels contain the middle part\r\n                if (!labelsRemove.Contains(label => !label.Contains(partMiddle)))\r\n                {\r\n                    RemoveString(labels, startLabelIndex, replaceString, ReplaceLocation.middle);\r\n                    return true;\r\n                }\r\n            }\r\n\r\n            return false;\r\n        }\r\n\r\n        private static bool IsSpaceChar(char c)\r\n        {\r\n            return SPACE_CHARS.Contains(c);\r\n        }\r\n\r\n        private static string NormalizeSeparators(string startLabelText)\r\n        {\r\n            startLabelText = startLabelText.Replace(ELIPSIS, LABEL_SEP_CHAR.ToString(CultureInfo.InvariantCulture));\r\n            foreach (var spaceChar in SPACE_CHARS)\r\n            {\r\n                startLabelText = startLabelText.Replace(spaceChar, LABEL_SEP_CHAR);\r\n            }\r\n\r\n            return startLabelText;\r\n        }\r\n\r\n        /// <summary>\r\n        /// Truncates labels.\r\n        /// </summary>\r\n        /// <param name=\"labels\">Labels text will be removed from.</param>\r\n        /// <param name=\"startLabelIndex\">Index we want to start looking at, in case the Expected/Library\r\n        /// label is showing.</param>\r\n        /// <param name=\"replaceString\">Text being removed from labels.</param>\r\n        /// <param name=\"location\">Expected location of the replacement text</param>\r\n        public static void RemoveString(string[] labels, int startLabelIndex, string replaceString, ReplaceLocation location)\r\n        {\r\n            for (int i = startLabelIndex; i < labels.Length; i++)\r\n                labels[i] = RemoveString(labels[i], replaceString, location);\r\n        }\r\n\r\n        public enum ReplaceLocation {start, middle, end}\r\n\r\n        private static string RemoveString(string label, string replaceString, ReplaceLocation location)\r\n        {\r\n            int startIndex = -1;\r\n            while ((startIndex = label.IndexOf(replaceString, startIndex + 1, StringComparison.Ordinal)) != -1)\r\n            {\r\n                int endIndex = startIndex + replaceString.Length;\r\n                // Not start string and does not end with space\r\n                if ((startIndex != 0 && !IsSpaceChar(label[startIndex - 1])) || \r\n                    (startIndex == 0 && location != ReplaceLocation.start))\r\n                    continue;\r\n                \r\n                // Not end string and does not start with space\r\n                if ((endIndex != label.Length && !IsSpaceChar(label[endIndex])) ||\r\n                    (endIndex == label.Length && location != ReplaceLocation.end))\r\n                    continue;\r\n                \r\n                bool elipsisSeen = false;\r\n                bool middle = true;\r\n                // Check left of the string for the start of the label or a space char\r\n                if (startIndex == 0)\r\n                    middle = false;\r\n                else if (startIndex >= ELIPSIS.Length && label.LastIndexOf(ELIPSIS, startIndex, StringComparison.Ordinal) == startIndex - ELIPSIS.Length)\r\n                    elipsisSeen = true;\r\n                else\r\n                    startIndex--;\r\n                \r\n                // Check right of the string for the end of the label or a space char\r\n                if (endIndex == label.Length)\r\n                    middle = false;\r\n                else if (label.IndexOf(ELIPSIS, endIndex, StringComparison.Ordinal) == endIndex)\r\n                    elipsisSeen = true;\r\n                else\r\n                    endIndex++;\r\n                label = label.Remove(startIndex, endIndex - startIndex);\r\n                // Insert an elipsis, if this is in the middle and no elipsis has been seen\r\n                if (middle && !elipsisSeen && location == ReplaceLocation.middle)\r\n                    label = label.Insert(startIndex, ELIPSIS);\r\n                return label;\r\n            }\r\n            return label;\r\n        }\r\n\r\n        public static string TruncateString(string s, int length)\r\n        {\r\n            return s.Length <= length ? s : s.Substring(0, length - ELIPSIS.Length) + ELIPSIS;\r\n        }\r\n\r\n\r\n        /// <summary>\r\n        /// Try an action that might throw an exception commonly related to a file move or delete.\r\n        /// If it fails, sleep for the indicated period and try again.\r\n        /// \r\n        /// N.B. \"TryTwice\" is a historical misnomer since it actually defaults to trying four times,\r\n        /// but the intent is clear: try more than once. Further historical note: formerly this only\r\n        /// handled IOException, but in looping tests we also see UnauthorizedAccessException as a result\r\n        /// of file locks that haven't been released yet.\r\n        /// </summary>\r\n        /// <param name=\"action\">action to try</param>\r\n        /// <param name=\"loopCount\">how many loops to try before failing</param>\r\n        /// <param name=\"milliseconds\">how long (in milliseconds) to wait before the action is retried</param>\r\n        public static void TryTwice(Action action, int loopCount = 4, int milliseconds = 500)\r\n        {\r\n            for (int i = 1; i<loopCount; i++)\r\n            {\r\n                try\r\n                {\r\n                    action();\r\n                    return;\r\n                }\r\n                catch (IOException exIO)\r\n                {\r\n                    ReportExceptionForRetry(milliseconds, exIO, i, loopCount);\r\n                }\r\n                catch (UnauthorizedAccessException exUA)\r\n                {\r\n                    ReportExceptionForRetry(milliseconds, exUA, i, loopCount);\r\n                }\r\n            }\r\n\r\n            // Try the last time, and let the exception go.\r\n            action();\r\n        }\r\n\r\n        private static void ReportExceptionForRetry(int milliseconds, Exception x, int loopCount, int maxLoopCount)\r\n        {\r\n            Trace.WriteLine(string.Format(@\"Encountered the following exception (attempt {0} of {1}):\", loopCount, maxLoopCount));\r\n            Trace.WriteLine(x.Message);\r\n            Thread.Sleep(milliseconds);\r\n        }\r\n\r\n        /// <summary>\r\n        /// Try an action that might throw an exception.  If it does, sleep for a little while and\r\n        /// try the action one more time.  This oddity is necessary because certain file system\r\n        /// operations (like moving a directory) can fail due to temporary file locks held by\r\n        /// anti-virus software.\r\n        /// </summary>\r\n        /// <typeparam name=\"TEx\">type of exception to catch</typeparam>\r\n        /// <param name=\"action\">action to try</param>\r\n        /// <param name=\"loopCount\">how many loops to try before failing</param>\r\n        /// <param name=\"milliseconds\">how long (in milliseconds) to wait before the action is retried</param>\r\n        public static void Try<TEx>(Action action, int loopCount = 4, int milliseconds = 500) where TEx : Exception\r\n        {\r\n            for (int i = 1; i < loopCount; i++)\r\n            {\r\n                try\r\n                {\r\n                    action();\r\n                    return;\r\n                }\r\n                catch (TEx x)\r\n                {\r\n                    ReportExceptionForRetry(milliseconds, x, i, loopCount);\r\n                }\r\n            }\r\n\r\n            // Try the last time, and let the exception go.\r\n            action();\r\n        }\r\n\r\n        public static void WrapAndThrowException(Exception x)\r\n        {\r\n            // The thrown exception needs to be preserved to preserve\r\n            // the original stack trace from which it was thrown.  In some cases,\r\n            // its type must also be preserved, because existing code handles certain\r\n            // exception types.  If this case threw only TargetInvocationException,\r\n            // then more frequently the code would just have to have a blanket catch\r\n            // of the base exception type, which could hide coding errors.\r\n            if (x is InvalidDataException)\r\n                throw new InvalidDataException(x.Message, x);\r\n            if (x is IOException)\r\n                throw new IOException(x.Message, x);\r\n            if (x is OperationCanceledException)\r\n                throw new OperationCanceledException(x.Message, x);\r\n            throw new TargetInvocationException(x.Message, x);            \r\n        }\r\n\r\n        public static double? ParseNullableDouble(string s)\r\n        {\r\n            double d;\r\n            return double.TryParse(s, out d) ? d : (double?)null;\r\n        }\r\n\r\n        public static string NullableDoubleToString(double? d)\r\n        {\r\n            return d.HasValue ? d.Value.ToString(LocalizationHelper.CurrentCulture) : String.Empty;\r\n        }\r\n    }\r\n\r\n    /// <summary>\r\n    /// This is a replacement for Debug.Assert, having the advantage that it is not omitted in a retail build.\r\n    /// </summary>\r\n    public static class Assume\r\n    {\r\n        public static void IsTrue(bool condition, string error = \"\")\r\n        {\r\n            if (!condition)\r\n                Fail(error);\r\n        }\r\n\r\n        public static void IsFalse(bool condition, string error = \"\")\r\n        {\r\n            if (condition)\r\n                Fail(error);\r\n        }\r\n\r\n        public static void IsNotNull(object o, string parameterName = \"\")\r\n        {\r\n            if (o == null)\r\n                Fail(string.IsNullOrEmpty(parameterName) ? @\"null object\" : parameterName + @\" is null\");\r\n        }\r\n\r\n        public static void IsNull(object o, string parameterName = \"\")\r\n        {\r\n            if (o != null)\r\n                Fail(string.IsNullOrEmpty(parameterName) ? @\"non-null object\" : parameterName + @\" is not null\");\r\n        }\r\n\r\n        public static void AreEqual(object left, object right, string error = \"\")\r\n        {\r\n            if (!Equals(left, right))\r\n                Fail(error);\r\n        }\r\n\r\n        public static void AreNotEqual(object left, object right, string error = \"\")\r\n        {\r\n            if (Equals(left, right))\r\n                Fail(error);\r\n        }\r\n\r\n        public static void AreEqual(double expected, double actual, double delta, string error = \"\")\r\n        {\r\n            if (Math.Abs(expected-actual) > delta)\r\n                Fail(error);\r\n        }\r\n\r\n        public static void Fail(string error = \"\")\r\n        {\r\n            throw new AssumptionException(error);\r\n        }\r\n\r\n        /// <summary>\r\n        /// This function does two things: it returns the value of a nullable that we assume has a value (this\r\n        /// avoids Resharper warnings), and it throws an exception if the nullable unexpectedly has no value.\r\n        /// </summary>\r\n        /// <param name=\"value\">a nullable int that is expected to have a value</param>\r\n        /// <returns>the value of the nullable int</returns>\r\n        public static T Value<T>(T? value) where T : struct\r\n        {\r\n            if (!value.HasValue)\r\n                Fail(@\"Nullable_was_expected_to_have_a_value\"); \r\n            return value.Value;\r\n        }\r\n    }\r\n\r\n    public class AssumptionException : Exception\r\n    {\r\n        public AssumptionException(string message)\r\n            : base(message)\r\n        {\r\n        }\r\n    }\r\n\r\n    public static class MathEx\r\n    {\r\n        public static double RoundAboveZero(float value, int startDigits, int mostDigits)\r\n        {\r\n            for (int i = startDigits; i <= mostDigits; i++)\r\n            {\r\n                double rounded = Math.Round(value, i);\r\n                if (rounded > 0)\r\n                    return rounded;\r\n            }\r\n            return 0;\r\n        }\r\n    }\r\n\r\n    public static class ExceptionUtil\r\n    {\r\n        public static string GetMessage(Exception ex)\r\n        {\r\n            // Drill down to see if the innermost exception was an out-of-memory exception.\r\n            var innerException = ex;\r\n            while (innerException.InnerException != null)\r\n                innerException = innerException.InnerException;\r\n            if (innerException is OutOfMemoryException)\r\n            {\r\n                string memoryMessage = String.Format(Resources.SkylineWindow_CompleteProgressUI_Ran_Out_Of_Memory, Program.Name);\r\n                if (!Install.Is64Bit && Environment.Is64BitOperatingSystem)\r\n                {\r\n                    memoryMessage += String.Format(Resources.SkylineWindow_CompleteProgressUI_version_issue, Program.Name);\r\n                }\r\n                return TextUtil.LineSeparate(ex.Message, memoryMessage);\r\n            }\r\n            return ex.Message;\r\n        }\r\n\r\n        public static string GetStackTraceText(Exception exception, StackTrace stackTraceExceptionCaughtAt = null, bool showMessage = true)\r\n        {\r\n            StringBuilder stackTrace = new StringBuilder();\r\n            if (showMessage)\r\n                stackTrace.AppendLine(@\"Stack trace:\").AppendLine();\r\n\r\n            stackTrace.AppendLine(exception.StackTrace).AppendLine();\r\n\r\n            for (var x = exception.InnerException; x != null; x = x.InnerException)\r\n            {\r\n                if (ReferenceEquals(x, exception.InnerException))\r\n                    stackTrace.AppendLine(@\"Inner exceptions:\");\r\n                else\r\n                    stackTrace.AppendLine(@\"---------------------------------------------------------------\");\r\n                stackTrace.Append(@\"Exception type: \").Append(x.GetType().FullName).AppendLine();\r\n                stackTrace.Append(@\"Error message: \").AppendLine(x.Message);\r\n                stackTrace.AppendLine(x.Message).AppendLine(x.StackTrace);\r\n            }\r\n            if (null != stackTraceExceptionCaughtAt)\r\n            {\r\n                stackTrace.AppendLine(@\"Exception caught at: \");\r\n                stackTrace.AppendLine(stackTraceExceptionCaughtAt.ToString());\r\n            }\r\n            return stackTrace.ToString();\r\n        }\r\n    }\r\n\r\n    public static class ParallelEx\r\n    {\r\n        // This can be set to true to make debugging easier.\r\n        public static readonly bool SINGLE_THREADED = false;\r\n\r\n        private static readonly ParallelOptions PARALLEL_OPTIONS = new ParallelOptions\r\n        {\r\n            MaxDegreeOfParallelism = SINGLE_THREADED ? 1 : -1\r\n        };\r\n\r\n        private class IntHolder\r\n        {\r\n            public IntHolder(int theInt)\r\n            {\r\n                TheInt = theInt;\r\n            }\r\n\r\n            public int TheInt { get; private set; }\r\n        }\r\n\r\n        public static int GetThreadCount(int? maxThreads = null)\r\n        {\r\n            if (SINGLE_THREADED)\r\n                return 1;\r\n            int threadCount = Environment.ProcessorCount;\r\n            int maxThreadCount = maxThreads ?? 8; // Trial with maximum of 8\r\n            if (threadCount > maxThreadCount)\r\n                threadCount = maxThreadCount;\r\n            return threadCount;\r\n        }\r\n\r\n        public static void For(int fromInclusive, int toExclusive, Action<int> body, Action<AggregateException> catchClause = null, int? maxThreads = null)\r\n        {\r\n            Action<int> localBody = i =>\r\n            {\r\n                LocalizationHelper.InitThread(); // Ensure appropriate culture\r\n                body(i);\r\n            };\r\n            LoopWithExceptionHandling(() =>\r\n            {\r\n                using (var worker = new QueueWorker<IntHolder>(null, (h, i) => localBody(h.TheInt)))\r\n                {\r\n                    worker.RunAsync(GetThreadCount(maxThreads), typeof(ParallelEx).Name);\r\n                    for (int i = fromInclusive; i < toExclusive; i++)\r\n                    {\r\n                        if (worker.Exception != null)\r\n                            break;\r\n                        worker.Add(new IntHolder(i));\r\n                    }\r\n                    worker.DoneAdding(true);\r\n                    if (worker.Exception != null)\r\n                        throw new AggregateException(@\"Exception in Parallel.For\", worker.Exception);   \r\n                }\r\n            }, catchClause);\r\n//            LoopWithExceptionHandling(() => Parallel.For(fromInclusive, toExclusive, PARALLEL_OPTIONS, localBody), catchClause);\r\n        }\r\n\r\n        public static void ForEach<TSource>(IEnumerable<TSource> source, Action<TSource> body, Action<AggregateException> catchClause = null, int? maxThreads = null) where TSource : class\r\n        {\r\n            Action<TSource> localBody = o =>\r\n            {\r\n                LocalizationHelper.InitThread(); // Ensure appropriate culture\r\n                body(o);\r\n            };\r\n            LoopWithExceptionHandling(() =>\r\n            {\r\n                using (var worker = new QueueWorker<TSource>(null, (s, i) => localBody(s)))\r\n                {\r\n                    worker.RunAsync(GetThreadCount(maxThreads), typeof(ParallelEx).Name);\r\n                    foreach (TSource s in source)\r\n                    {\r\n                        if (worker.Exception != null)\r\n                            break;\r\n                        worker.Add(s);\r\n                    }\r\n                    worker.DoneAdding(true);\r\n                    if (worker.Exception != null)\r\n                        throw new AggregateException(@\"Exception in Parallel.ForEx\", worker.Exception); \r\n                }\r\n            }, catchClause);\r\n//            LoopWithExceptionHandling(() => Parallel.ForEach(source, PARALLEL_OPTIONS, localBody), catchClause);\r\n        }\r\n\r\n        private static void LoopWithExceptionHandling(Action loop, Action<AggregateException> catchClause)\r\n        {\r\n            try\r\n            {\r\n                loop();\r\n            }\r\n            catch (AggregateException x)\r\n            {\r\n                Exception ex = null;\r\n                x.Handle(inner =>\r\n                {\r\n                    if (inner is OperationCanceledException)\r\n                    {\r\n                        if (!(ex is OperationCanceledException))\r\n                            ex = inner;\r\n                        return true;\r\n                    }\r\n                    if (catchClause == null)\r\n                    {\r\n                        if (ex == null)\r\n                            ex = inner;\r\n                        return true;\r\n                    }\r\n                    return false;\r\n                });\r\n\r\n                if (ex != null)\r\n                    Helpers.WrapAndThrowException(ex);\r\n                if (catchClause != null)\r\n                    catchClause(x);\r\n            }\r\n        }\r\n    }\r\n\r\n    public class Alarms\r\n    {\r\n        private readonly Dictionary<object, AlarmInfo> _timers =\r\n            new Dictionary<object, AlarmInfo>();\r\n\r\n        private class AlarmInfo\r\n        {\r\n            public System.Windows.Forms.Timer Timer;\r\n            public long Ticks;\r\n        }\r\n\r\n        public void Run(Control control, int milliseconds, object id, Action action)\r\n        {\r\n            try\r\n            {\r\n                control.Invoke(new Action(() =>\r\n                {\r\n                    lock (_timers)\r\n                    {\r\n                        var alarmTicks = DateTime.Now.Ticks + milliseconds*TimeSpan.TicksPerMillisecond;\r\n                        if (_timers.ContainsKey(id))\r\n                        {\r\n                            if (_timers[id].Ticks <= alarmTicks)\r\n                                return;\r\n                            _timers[id].Timer.Dispose();\r\n                        }\r\n                        var timer = new System.Windows.Forms.Timer {Interval = milliseconds};\r\n                        timer.Tick += (sender, args) => TimerTick(id, action);\r\n                        _timers[id] = new AlarmInfo {Timer = timer, Ticks = alarmTicks};\r\n                        timer.Start();\r\n                    }\r\n                }));\r\n            }\r\n            catch (InvalidOperationException)\r\n            {\r\n            }\r\n        }\r\n\r\n        private void TimerTick(object id, Action action)\r\n        {\r\n            lock (_timers)\r\n            {\r\n                var alarmInfo = _timers[id];\r\n                _timers.Remove(id);\r\n                alarmInfo.Timer.Dispose();\r\n            }\r\n            action();\r\n        }\r\n    }\r\n\r\n    public static class SecurityProtocolInitializer\r\n    {\r\n        // Make sure we can negotiate with HTTPS servers that demand TLS 1.2 (default in dotNet 4.6, but has to be turned on in 4.5)\r\n        public static void Initialize()\r\n        {\r\n            ServicePointManager.SecurityProtocol |= (SecurityProtocolType.Tls | SecurityProtocolType.Tls11 | SecurityProtocolType.Tls12);\r\n        }\r\n    }\r\n\r\n}\r\n", "idx": 1, "id": 13087, "msg": "", "proj": "ProteoWizard-pwiz", "lang": ".cs"}
{"patch": "@@ -11,14 +11,17 @@ import shellapi\n import winUser\n import os\n \n-\n def openUserConfigurationDirectory():\n \t\"\"\"Opens directory containing config files for the current user\"\"\"\n-\timport config\n-\tpath = config.getUserDefaultConfigPath()\n-\tif not path:\n-\t\traise ValueError(\"no user default config path\")\n-\tconfig.initConfigPath(path)\n+\timport globalVars\n+\ttry:\n+\t\tpath = globalVars.appArgs.configPath\n+\texcept AttributeError:\n+\t\timport config\n+\t\tpath = config.getUserDefaultConfigPath()\n+\t\tif not path:\n+\t\t\traise ValueError(\"no user default config path\")\n+\t\tconfig.initConfigPath(path)\n \tshellapi.ShellExecute(0, None, path, None, None, winUser.SW_SHOWNORMAL)\n \n ", "y": 1, "oldf": "# -*- coding: UTF-8 -*-\n# A part of NonVisual Desktop Access (NVDA)\n# Copyright (C) 2020 NV Access Limited, \u0141ukasz Golonka\n# This file may be used under the terms of the GNU General Public License, version 2 or later.\n# For more details see: https://www.gnu.org/licenses/gpl-2.0.html\n\n\"\"\" System related functions.\"\"\"\nimport ctypes\nimport winKernel\nimport shellapi\nimport winUser\nimport os\n\n\ndef openUserConfigurationDirectory():\n\t\"\"\"Opens directory containing config files for the current user\"\"\"\n\timport config\n\tpath = config.getUserDefaultConfigPath()\n\tif not path:\n\t\traise ValueError(\"no user default config path\")\n\tconfig.initConfigPath(path)\n\tshellapi.ShellExecute(0, None, path, None, None, winUser.SW_SHOWNORMAL)\n\n\nTokenUIAccess = 26\n\n\ndef hasUiAccess():\n\ttoken = ctypes.wintypes.HANDLE()\n\tctypes.windll.advapi32.OpenProcessToken(\n\t\tctypes.windll.kernel32.GetCurrentProcess(),\n\t\twinKernel.MAXIMUM_ALLOWED,\n\t\tctypes.byref(token)\n\t)\n\ttry:\n\t\tval = ctypes.wintypes.DWORD()\n\t\tctypes.windll.advapi32.GetTokenInformation(\n\t\t\ttoken,\n\t\t\tTokenUIAccess,\n\t\t\tctypes.byref(val),\n\t\t\tctypes.sizeof(ctypes.wintypes.DWORD),\n\t\t\tctypes.byref(ctypes.wintypes.DWORD())\n\t\t)\n\t\treturn bool(val.value)\n\tfinally:\n\t\tctypes.windll.kernel32.CloseHandle(token)\n\n\ndef execElevated(path, params=None, wait=False, handleAlreadyElevated=False):\n\timport subprocess\n\tif params is not None:\n\t\tparams = subprocess.list2cmdline(params)\n\tsei = shellapi.SHELLEXECUTEINFO(lpFile=os.path.abspath(path), lpParameters=params, nShow=winUser.SW_HIDE)\n\t# IsUserAnAdmin is apparently deprecated so may not work above Windows 8\n\tif not handleAlreadyElevated or not ctypes.windll.shell32.IsUserAnAdmin():\n\t\tsei.lpVerb = \"runas\"\n\tif wait:\n\t\tsei.fMask = shellapi.SEE_MASK_NOCLOSEPROCESS\n\tshellapi.ShellExecuteEx(sei)\n\tif wait:\n\t\ttry:\n\t\t\th = ctypes.wintypes.HANDLE(sei.hProcess)\n\t\t\tmsg = ctypes.wintypes.MSG()\n\t\t\twhile ctypes.windll.user32.MsgWaitForMultipleObjects(1, ctypes.byref(h), False, -1, 255) == 1:\n\t\t\t\twhile ctypes.windll.user32.PeekMessageW(ctypes.byref(msg), None, 0, 0, 1):\n\t\t\t\t\tctypes.windll.user32.TranslateMessage(ctypes.byref(msg))\n\t\t\t\t\tctypes.windll.user32.DispatchMessageW(ctypes.byref(msg))\n\t\t\treturn winKernel.GetExitCodeProcess(sei.hProcess)\n\t\tfinally:\n\t\t\twinKernel.closeHandle(sei.hProcess)\n", "idx": 1, "id": 28845, "msg": "When do you expect to get an `AttributeError`? Looking at `source/core.py` line 222, I think you can rely on `appArgs.configPath` being set correctly.", "proj": "nvaccess-nvda", "lang": "py"}
{"patch": "@@ -1036,7 +1036,7 @@ class AnnotationTest extends TestCase\n                     }'\n             ],\n             'falsableFunctionAllowedWhenBooleanExpected' => [\n-                '<?php\n+                'code' => '<?php\n \n                     /** @psalm-return bool */\n                     function alwaysFalse1()", "y": 0, "oldf": "<?php\n\nnamespace Psalm\\Tests;\n\nuse Psalm\\Config;\nuse Psalm\\Context;\nuse Psalm\\Exception\\CodeException;\nuse Psalm\\Tests\\Traits\\InvalidCodeAnalysisTestTrait;\nuse Psalm\\Tests\\Traits\\ValidCodeAnalysisTestTrait;\n\nuse const DIRECTORY_SEPARATOR;\n\nclass AnnotationTest extends TestCase\n{\n    use InvalidCodeAnalysisTestTrait;\n    use ValidCodeAnalysisTestTrait;\n\n    public function setUp(): void\n    {\n        parent::setUp();\n        $codebase = $this->project_analyzer->getCodebase();\n        $codebase->reportUnusedVariables();\n    }\n\n    public function testLessSpecificImplementedReturnTypeWithDocblockOnMultipleLines(): void\n    {\n        $this->expectException(CodeException::class);\n        $this->expectExceptionMessage('LessSpecificImplementedReturnType - somefile.php:5:');\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n\n                /**\n                 * @method int test()\n                 * @method \\DateTime modify($modify)\n                 */\n                class WTF extends \\DateTime { }'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    public function testLessSpecificImplementedReturnTypeWithDocblockOnMultipleLinesWithMultipleClasses(): void\n    {\n        $this->expectException(CodeException::class);\n        $this->expectExceptionMessage('LessSpecificImplementedReturnType - somefile.php:15:');\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n\n            class ParentClass\n            {\n                /**\n                 * @return $this\n                 */\n                public function execute()\n                {\n                    return $this;\n                }\n            }\n\n            /**\n             * @method self execute()\n             */\n            class BreakingThings extends ParentClass\n            {\n            }'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    public function testLessSpecificImplementedReturnTypeWithDescription(): void\n    {\n        $this->expectException(CodeException::class);\n        $this->expectExceptionMessage('LessSpecificImplementedReturnType - somefile.php:7:');\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n                /**\n                 * test test test\n                 * test rambling text\n                 * test test text\n                 *\n                 * @method \\DateTime modify($modify)\n                 */\n                class WTF extends \\DateTime { }'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    public function testInvalidParamDefault(): void\n    {\n        $this->expectException(CodeException::class);\n        $this->expectExceptionMessage('InvalidParamDefault');\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n                /**\n                 * @param array $arr\n                 * @return void\n                 */\n                function foo($arr = false) {}'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    public function testInvalidParamDefaultButAllowedInConfig(): void\n    {\n        Config::getInstance()->add_param_default_to_docblock_type = true;\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n                /**\n                 * @param array $_arr\n                 * @return void\n                 */\n                function foo($_arr = false) {}\n                foo(false);\n                foo([\"hello\"]);'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    public function testInvalidTypehintParamDefaultButAllowedInConfig(): void\n    {\n        $this->expectException(CodeException::class);\n        $this->expectExceptionMessage('InvalidParamDefault');\n\n        Config::getInstance()->add_param_default_to_docblock_type = true;\n\n        $this->addFile(\n            'somefile.php',\n            '<?php\n                function foo(array $arr = false) : void {}'\n        );\n\n        $this->analyzeFile('somefile.php', new Context());\n    }\n\n    /**\n     * @return iterable<string,array{string,assertions?:array<string,string>,error_levels?:string[]}>\n     */\n    public function providerValidCodeParse(): iterable\n    {\n        return [\n            'nopType' => [\n                '<?php\n                    $_a = \"hello\";\n\n                    /** @var int $_a */',\n                'assertions' => [\n                    '$_a' => 'int',\n                ],\n            ],\n            'validDocblockReturn' => [\n                '<?php\n                    /**\n                     * @return string\n                     */\n                    function fooFoo(): string {\n                        return \"boop\";\n                    }\n\n                    /**\n                     * @return array<int, string>\n                     */\n                    function foo2(): array {\n                        return [\"hello\"];\n                    }\n\n                    /**\n                     * @return array<int, string>\n                     */\n                    function foo3(): array {\n                        return [\"hello\"];\n                    }',\n            ],\n            'reassertWithIs' => [\n                '<?php\n                    /** @param array $a */\n                    function foo($a): void {\n                        if (is_array($a)) {\n                            // do something\n                        }\n                    }',\n                'assertions' => [],\n                'error_level' => ['RedundantConditionGivenDocblockType'],\n            ],\n            'checkArrayWithIs' => [\n                '<?php\n                    /** @param mixed $b */\n                    function foo($b): void {\n                        /**\n                         * @psalm-suppress UnnecessaryVarAnnotation\n                         * @var array\n                         */\n                        $a = (array)$b;\n                        if (is_array($a)) {\n                            // do something\n                        }\n                    }',\n                'assertions' => [],\n                'error_level' => ['RedundantConditionGivenDocblockType'],\n            ],\n            'goodDocblock' => [\n                '<?php\n                    class A {\n                        /**\n                         * @param A $a\n                         * @param bool $b\n                         */\n                        public function g(A $a, $b): void {\n                        }\n                    }',\n            ],\n            'goodDocblockInNamespace' => [\n                '<?php\n                    namespace Foo;\n\n                    class A {\n                        /**\n                         * @param \\Foo\\A $a\n                         * @param bool $b\n                         */\n                        public function g(A $a, $b): void {\n                        }\n                    }',\n            ],\n\n            'ignoreNullableReturn' => [\n                '<?php\n                    class A {\n                        /** @var int */\n                        public $bar = 5;\n                        public function foo(): void {}\n                    }\n\n                    /**\n                     * @return ?A\n                     * @psalm-ignore-nullable-return\n                     */\n                    function makeA() {\n                        return rand(0, 1) ? new A() : null;\n                    }\n\n                    function takeA(A $_a): void { }\n\n                    $a = makeA();\n                    $a->foo();\n                    $a->bar = 7;\n                    takeA($a);',\n            ],\n            'invalidDocblockParamSuppress' => [\n                '<?php\n                    /**\n                     * @param int $_bar\n                     * @psalm-suppress MismatchingDocblockParamType\n                     */\n                    function fooFoo(array $_bar): void {\n                    }',\n            ],\n            'differentDocblockParamClassSuppress' => [\n                '<?php\n                    class A {}\n                    class B {}\n\n                    /**\n                     * @param B $_bar\n                     * @psalm-suppress MismatchingDocblockParamType\n                     */\n                    function fooFoo(A $_bar): void {\n                    }',\n            ],\n            'varDocblock' => [\n                '<?php\n                    /** @var array<Exception> */\n                    $a = [];\n\n                    echo $a[0]->getMessage();',\n            ],\n            'ignoreVarDocblock' => [\n                '<?php\n                    /**\n                     * @var array<Exception>\n                     * @ignore-var\n                     */\n                    $a = [];\n\n                    $a[0]->getMessage();',\n                'assertions' => [],\n                'error_level' => ['EmptyArrayAccess', 'MixedMethodCall'],\n            ],\n            'psalmIgnoreVarDocblock' => [\n                '<?php\n                    /**\n                     * @var array<Exception>\n                     * @psalm-ignore-var\n                     */\n                    $a = [];\n\n                    $a[0]->getMessage();',\n                'assertions' => [],\n                'error_level' => ['EmptyArrayAccess', 'MixedMethodCall'],\n            ],\n            'mixedDocblockParamTypeDefinedInParent' => [\n                '<?php\n                    class A {\n                        /** @param mixed $a */\n                        public function foo($a): void {}\n                    }\n\n                    class B extends A {\n                        public function foo($a): void {}\n                    }',\n            ],\n            'intDocblockParamTypeDefinedInParent' => [\n                '<?php\n                    class A {\n                        /** @param int $a */\n                        public function foo($a): void {}\n                    }\n\n                    class B extends A {\n                        public function foo($a): void {}\n                    }',\n            ],\n            'varSelf' => [\n                '<?php\n                    class A\n                    {\n                        public function foo(): void {}\n\n                        public function getMeAgain(): void {\n                            /** @var self */\n                            $me = $this;\n                            $me->foo();\n                        }\n                    }',\n            ],\n            'psalmVar' => [\n                '<?php\n                    class A\n                    {\n                        /** @psalm-var array<int, string> */\n                        public $foo = [];\n\n                        public function updateFoo(): void {\n                            $this->foo[5] = \"hello\";\n                        }\n                    }',\n            ],\n            'psalmParam' => [\n                '<?php\n                    function takesInt(int $_a): void {}\n\n                    /**\n                     * @psalm-param  array<int, string> $a\n                     * @param string[] $a\n                     */\n                    function foo(array $a): void {\n                        foreach ($a as $key => $_value) {\n                            takesInt($key);\n                        }\n                    }',\n            ],\n            'returnDocblock' => [\n                '<?php\n                    function foo(int $i): int {\n                        /** @var int */\n                        return $i;\n                    }',\n            ],\n            'doubleVar' => [\n                '<?php\n                    function foo() : array {\n                        return [\"hello\" => new stdClass, \"goodbye\" => new stdClass];\n                    }\n\n                    $_a = null;\n                    $_b = null;\n\n                    /**\n                     * @var string $_key\n                     * @var stdClass $_value\n                     */\n                    foreach (foo() as $_key => $_value) {\n                        $_a = $_key;\n                        $_b = $_value;\n                    }',\n                'assertions' => [\n                    '$_a' => 'null|string',\n                    '$_b' => 'null|stdClass',\n                ],\n            ],\n            'allowOptionalParamsToBeEmptyArray' => [\n                '<?php\n                    /** @param array{b?: int, c?: string} $_a */\n                    function foo(array $_a = []) : void {}',\n            ],\n            'allowEmptyVarAnnotation' => [\n                '<?php\n                    /**\n                     * @param $_x\n                     */\n                    function example(array $_x) : void {}',\n            ],\n            'allowCapitalisedNamespacedString' => [\n                '<?php\n                    namespace Foo;\n\n                    /**\n                     * @param String $_x\n                     */\n                    function example(string $_x) : void {}',\n            ],\n            'megaClosureAnnotationWithoutSpacing' => [\n                '<?php\n                    /** @var array{a:Closure():(array<mixed, mixed>|null), b?:Closure():array<mixed, mixed>, c?:Closure():array<mixed, mixed>, d?:Closure():array<mixed, mixed>, e?:Closure():(array{f:null|string, g:null|string, h:null|string, i:string, j:mixed, k:mixed, l:mixed, m:mixed, n:bool, o?:array{0:string}}|null), p?:Closure():(array{f:null|string, g:null|string, h:null|string, q:string, i:string, j:mixed, k:mixed, l:mixed, m:mixed, n:bool, o?:array{0:string}}|null), r?:Closure():(array<mixed, mixed>|null), s:array<mixed, mixed>} */\n                    $arr = [];\n\n                    $arr[\"a\"]();',\n            ],\n            'megaClosureAnnotationWithSpacing' => [\n                '<?php\n                    /**\n                     * @var array{\n                     * a: Closure() : (array<mixed, mixed>|null),\n                     * b?: Closure() : array<mixed, mixed>,\n                     * c?: Closure() : array<mixed, mixed>,\n                     * d?: Closure() : array<mixed, mixed>,\n                     * e?: Closure() : (array{\n                     *   f: null|string,\n                     *   g: null|string,\n                     *   h: null|string,\n                     *   i: string,\n                     *   j: mixed,\n                     *   k: mixed,\n                     *   l: mixed,\n                     *   m: mixed,\n                     *   n: bool,\n                     *   o?: array{0:string}\n                     * }|null),\n                     * p?: Closure() : (array{\n                     *   f: null|string,\n                     *   g: null|string,\n                     *   h: null|string,\n                     *   q: string,\n                     *   i: string,\n                     *   j: mixed,\n                     *   k: mixed,\n                     *   l: mixed,\n                     *   m: mixed,\n                     *   n: bool,\n                     *   o?: array{0:string}\n                     * }|null),\n                     * r?: Closure() : (array<mixed, mixed>|null),\n                     * s: array<mixed, mixed>\n                     * }\n                     *\n                     * Some text\n                     */\n                    $arr = [];\n\n                    $arr[\"a\"]();',\n            ],\n            'multipeLineGenericArray' => [\n                '<?php\n                    /**\n                     * @psalm-type MiddlewareArray = array<\n                     *     class-string<\\Exception>,\n                     *     array<int, string>\n                     * >\n                     *\n                     * @psalm-type RuleArray = array{\n                     *     rule: string,\n                     *     controller?: class-string<\\Exception>,\n                     *     redirect?: string,\n                     *     code?: int,\n                     *     type?: string,\n                     *     middleware?: MiddlewareArray\n                     * }\n                     *\n                     * Foo Bar\n                     */\n                    class A {}',\n            ],\n            'builtInClassInAShape' => [\n                '<?php\n                    /**\n                     * @return array{d:Exception}\n                     * @psalm-suppress InvalidReturnType\n                     */\n                    function f() {}'\n            ],\n            'slashAfter?' => [\n                '<?php\n                    namespace ns;\n\n                    /** @param ?\\stdClass $_s */\n                    function foo($_s) : void {\n                    }\n\n                    foo(null);\n                    foo(new \\stdClass);',\n            ],\n            'returnTypeShouldBeNullable' => [\n                '<?php\n                    /**\n                     * @return stdClass\n                     */\n                    function foo() : ?stdClass {\n                        return rand(0, 1) ? new stdClass : null;\n                    }\n\n                    $f = foo();\n                    if ($f) {}',\n            ],\n            'spreadOperatorAnnotation' => [\n                '<?php\n                    /** @param string[] $_s */\n                    function foo(string ...$_s) : void {}\n                    /** @param string ...$_s */\n                    function bar(string ...$_s) : void {}\n                    foo(\"hello\", \"goodbye\");\n                    bar(\"hello\", \"goodbye\");\n                    foo(...[\"hello\", \"goodbye\"]);\n                    bar(...[\"hello\", \"goodbye\"]);',\n            ],\n            'spreadOperatorByRefAnnotation' => [\n                '<?php\n                    /** @param string &...$s */\n                    function foo(&...$s) : void {}\n                    /** @param string ...&$s */\n                    function bar(&...$s) : void {}\n                    /** @param string[] &$s */\n                    function bat(&...$s) : void {}\n\n                    $a = \"hello\";\n                    $b = \"goodbye\";\n                    $c = \"hello again\";\n                    foo($a);\n                    bar($b);\n                    bat($c);',\n                'assertions' => [\n                    '$a' => 'string',\n                    '$b' => 'string',\n                    '$c' => 'string',\n                ],\n            ],\n            'valueReturnType' => [\n                '<?php\n                    /**\n                     * @param \"a\"|\"b\" $_p\n                     */\n                    function acceptsLiteral($_p): void {}\n\n                    /**\n                     * @return \"a\"|\"b\"\n                     */\n                    function returnsLiteral(): string {\n                        return rand(0,1) ? \"a\" : \"b\";\n                    }\n\n                    acceptsLiteral(returnsLiteral());',\n            ],\n            'typeAliasBeforeClass' => [\n                '<?php\n                    /**\n                     * @psalm-type CoolType = A|B|null\n                     */\n\n                    class A {}\n                    class B {}\n\n                    /** @return CoolType */\n                    function foo() {\n                        if (rand(0, 1)) {\n                            return new A();\n                        }\n\n                        if (rand(0, 1)) {\n                            return new B();\n                        }\n\n                        return null;\n                    }\n\n                    /** @param CoolType $_a **/\n                    function bar ($_a) : void { }\n\n                    bar(foo());',\n            ],\n            'typeAliasBeforeFunction' => [\n                '<?php\n                    /**\n                     * @psalm-type A_OR_B = A|B\n                     * @psalm-type CoolType = A_OR_B|null\n                     * @return CoolType\n                     */\n                    function foo() {\n                        if (rand(0, 1)) {\n                            return new A();\n                        }\n\n                        if (rand(0, 1)) {\n                            return new B();\n                        }\n\n                        return null;\n                    }\n\n                    class A {}\n                    class B {}\n\n                    /** @param CoolType $_a **/\n                    function bar ($_a) : void { }\n\n                    bar(foo());',\n            ],\n            'typeAliasInSeparateBlockBeforeFunction' => [\n                '<?php\n                    /**\n                     * @psalm-type CoolType = A|B|null\n                     */\n                    /**\n                     * @return CoolType\n                     */\n                    function foo() {\n                        if (rand(0, 1)) {\n                            return new A();\n                        }\n\n                        if (rand(0, 1)) {\n                            return new B();\n                        }\n\n                        return null;\n                    }\n\n                    class A {}\n                    class B {}\n\n                    /** @param CoolType $_a **/\n                    function bar ($_a) : void { }\n\n                    bar(foo());',\n            ],\n            'almostFreeStandingTypeAlias' => [\n                '<?php\n                    /**\n                     * @psalm-type CoolType = A|B|null\n                     */\n\n                    // this breaks up the line\n\n                    class A {}\n                    class B {}\n\n                    /** @return CoolType */\n                    function foo() {\n                        if (rand(0, 1)) {\n                            return new A();\n                        }\n\n                        if (rand(0, 1)) {\n                            return new B();\n                        }\n\n                        return null;\n                    }\n\n                    /** @param CoolType $_a **/\n                    function bar ($_a) : void { }\n\n                    bar(foo());',\n            ],\n            'typeAliasUsedTwice' => [\n                '<?php\n                    /** @psalm-type TA = array<int, string> */\n\n                    class Bar {\n                        public function foo() : void {\n                            $bar =\n                                /** @return TA */\n                                function() {\n                                    return [\"hello\"];\n                            };\n\n                            /** @var array<int, TA> */\n                            $bat = [$bar(), $bar()];\n\n                            foreach ($bat as $b) {\n                                echo $b[0];\n                            }\n                        }\n                    }\n\n                    /**\n                     * @psalm-type _A=array{elt:int}\n                     * @param _A $p\n                     * @return _A\n                     */\n                    function f($p) {\n                        /**\n                         * @psalm-suppress UnnecessaryVarAnnotation\n                         * @var _A\n                         */\n                        $r = $p;\n                        return $r;\n                    }',\n            ],\n            'listUnpackWithDocblock' => [\n                '<?php\n                    interface I {}\n\n                    class A implements I {\n                        public function bar() : void {}\n                    }\n\n                    /** @return I[] */\n                    function foo() : array {\n                        return [new A()];\n                    }\n\n                    /** @var A $a1 */\n                    [$a1, $_a2] = foo();\n\n                    $a1->bar();',\n            ],\n            'spaceInType' => [\n                '<?php\n                    /** @return string | null */\n                    function foo(string $s = null) {\n                        return $s;\n                    }',\n            ],\n            'missingReturnTypeWithBadDocblockIgnoreBoth' => [\n                '<?php\n                    /**\n                     * @return [bad]\n                     */\n                    function fooBar() {\n                    }',\n                [],\n                [\n                    'InvalidDocblock' => Config::REPORT_INFO,\n                    'MissingReturnType' => Config::REPORT_INFO,\n                ],\n            ],\n            'objectWithPropertiesAnnotation' => [\n                '<?php\n                    /** @param object{foo:string} $o */\n                    function foo(object $o) : string {\n                        return $o->foo;\n                    }\n\n                    $s = new \\stdClass();\n                    $s->foo = \"hello\";\n                    foo($s);\n\n                    class A {\n                        /** @var string */\n                        public $foo = \"hello\";\n                    }\n\n                    foo(new A);',\n            ],\n            'refineTypeInNestedCall' => [\n                '<?php\n                    function foo(array $arr): \\Generator {\n                        /** @var array<string, mixed> $arr */\n                        foreach (array_filter(array_keys($arr), function (string $key) : bool {\n                            return strpos($key, \"BAR\") === 0;\n                        }) as $envVar) {\n                            yield $envVar => [getenv($envVar)];\n                        }\n                    }',\n            ],\n            'allowAnnotationOnServer' => [\n                '<?php\n                    function foo(): \\Generator {\n                        /** @var array<string, mixed> $_SERVER */\n                        foreach (array_filter(array_keys($_SERVER), function (string $key) : bool {\n                            return strpos($key, \"BAR\") === 0;\n                        }) as $envVar) {\n                            yield $envVar => [getenv($envVar)];\n                        }\n                    }',\n            ],\n            'annotationOnForeachItems' => [\n                '<?php\n                    function foo(array $arr) : void {\n                        $item = null;\n\n                        /** @var string $item */\n                        foreach ($arr as $item) {}\n\n                        if (is_null($item)) {}\n                    }\n\n                    function bar(array $arr) : void {\n                        $item = null;\n\n                        /** @var string $item */\n                        foreach ($arr as $item => $_) {}\n\n                        if (is_null($item)) {}\n                    }\n\n                    function bat(array $arr) : void {\n                        $item = null;\n\n                        /**\n                         * @psalm-suppress MixedArrayAccess\n                         * @var string $item\n                         */\n                        foreach ($arr as list($item)) {}\n\n                        if (is_null($item)) {}\n                    }\n\n                    function baz(array $arr) : void {\n                        $item = null;\n\n                        /**\n                         * @psalm-suppress MixedArrayAccess\n                         * @var string $item\n                         */\n                        foreach ($arr as list($item => $_)) {}\n\n                        if (is_null($item)) {}\n                    }',\n                [],\n                [\n                    'MixedAssignment',\n                ],\n            ],\n            'extraneousDocblockParamName' => [\n                '<?php\n                    /**\n                     * @param string $_foo\n                     * @param string[] $bar\n                     * @param string[] $_barb\n                     */\n                    function f(string $_foo, array $_barb): void {}',\n            ],\n            'nonEmptyArray' => [\n                '<?php\n                    /** @param non-empty-array<string> $arr */\n                    function foo(array $arr) : void {\n                        foreach ($arr as $a) {}\n                        echo $a;\n                    }\n\n                    foo([\"a\", \"b\", \"c\"]);\n\n                    /** @param array<string> $arr */\n                    function bar(array $arr) : void {\n                        if (!$arr) {\n                            return;\n                        }\n\n                        foo($arr);\n                    }',\n            ],\n            'nonEmptyArrayInNamespace' => [\n                '<?php\n                    namespace ns;\n\n                    /** @param non-empty-array<string> $arr */\n                    function foo(array $arr) : void {\n                        foreach ($arr as $a) {}\n                        echo $a;\n                    }\n\n                    foo([\"a\", \"b\", \"c\"]);\n\n                    /** @param array<string> $arr */\n                    function bar(array $arr) : void {\n                        if (!$arr) {\n                            return;\n                        }\n\n                        foo($arr);\n                    }',\n            ],\n            'noExceptionOnIntersection' => [\n                '<?php\n                    class Foo {\n                        /** @var null|\\DateTime&\\DateTimeImmutable */\n                        private $s = null;\n                    }',\n            ],\n            'intersectionWithSpace' => [\n                '<?php\n                    interface A {\n                        public function foo() : void;\n                    }\n                    interface B {\n                        public function bar() : void;\n                    }\n\n                    /** @param A & B $a */\n                    function f(A $a) : void {\n                        $a->foo();\n                        $a->bar();\n                    }',\n            ],\n            'allowClosingComma' => [\n                '<?php\n                    /**\n                     * @psalm-type _Alias=array{\n                     *    foo: string,\n                     *    bar: string,\n                     *    baz: array{\n                     *       a: int,\n                     *    },\n                     * }\n                     */\n                    class Foo { }\n\n                    /**\n                     * @param array{\n                     *    foo: string,\n                     *    bar: string,\n                     *    baz: array{\n                     *       a: int,\n                     *    },\n                     * } $foo\n                     */\n                    function foo(array $foo) : int {\n                        return count($foo);\n                    }\n\n                    /**\n                     * @var array{\n                     *    foo:string,\n                     *    bar:string,\n                     *    baz:string,\n                     * } $_foo\n                     */\n                    $_foo = [\"foo\" => \"\", \"bar\" => \"\", \"baz\" => \"\"];',\n            ],\n            'returnNumber' => [\n                '<?php\n                    class C {\n                        /**\n                         * @return 1\n                         */\n                        public static function barBar() {\n                            return 1;\n                        }\n                    }',\n            ],\n            'returnNumberForInterface' => [\n                '<?php\n                    interface I {\n                        /**\n                         * @return 1\n                         */\n                        public static function barBar();\n                    }',\n            ],\n            'psalmTypeAnnotationAboveReturn' => [\n                '<?php\n                    /**\n                     * @psalm-type Person = array{name: string, age: int}\n                     */\n\n                    /**\n                     * @psalm-return Person\n                     */\n                    function getPerson_error(): array {\n                        $json = \\'{\"name\": \"John\", \"age\": 44}\\';\n                        /** @psalm-var Person */\n                        return json_decode($json, true);\n                    }'\n            ],\n            'allowDocblockDefinedTKeyedArrayIntoNonEmpty' => [\n                '<?php\n                    /** @param non-empty-array $_bar */\n                    function foo(array $_bar) : void { }\n\n                    /** @var array{0:list<string>, 1:list<int>} */\n                    $bar = [[], []];\n\n                    foo($bar);'\n            ],\n            'allowResourceInList' => [\n                '<?php\n                    /** @param list<scalar|array|object|resource|null> $_s */\n                    function foo(array $_s) : void { }'\n            ],\n            'possiblyUndefinedObjectProperty' => [\n                '<?php\n                    function consume(string $value): void {\n                        echo $value;\n                    }\n\n                    /** @var object{value?: string} $data */\n                    $data = json_decode(\"{}\", false);\n                    consume($data->value ?? \"\");'\n            ],\n            'throwSelf' => [\n                '<?php\n                    namespace Foo;\n\n                    class MyException extends \\Exception {\n                        /**\n                         * @throws self\n                         */\n                        public static function create(): void {\n                            throw new self();\n                        }\n                    }'\n            ],\n            'parseTrailingCommaInReturn' => [\n                '<?php\n                    /**\n                     * @psalm-return array{\n                     *     a: int,\n                     *     b: string,\n                     * }\n                     */\n                    function foo(): array {\n                        return [\"a\" => 1, \"b\" => \"two\"];\n                    }'\n            ],\n            'falsableFunctionAllowedWhenBooleanExpected' => [\n                '<?php\n\n                    /** @psalm-return bool */\n                    function alwaysFalse1()\n                    {\n                        return false;\n                    }\n\n                    function alwaysFalse2(): bool\n                    {\n                        return false;\n                    }'\n            ],\n            'dontInheritDocblockReturnWhenRedeclared' => [\n                '<?php\n                    interface Id {}\n\n                    class UserId implements Id {}\n\n                    interface Entity {\n                        /** @psalm-return Id */\n                        function id(): Id;\n                    }\n\n                    class User implements Entity {\n                        public function id(): UserId {\n                            return new UserId();\n                        }\n                    }',\n                [],\n                [],\n                '7.4'\n            ],\n            'arrayWithKeySlashesAndNewline' => [\n                '<?php\n                    $_arr = [\"foo\\\\bar\\nbaz\" => \"literal\"];',\n                [\n                    '$_arr' => 'array{\\'foo\\\\\\\\bar\\nbaz\\': string}'\n                ]\n            ],\n            'doubleSpaceBeforeAt' => [\n                '<?php\n                    /**\n                     *  @param string $_c\n                     */\n                    function foo($_c) : void {}'\n            ],\n            'throwsAnnotationWithBarAndSpace' => [\n                '<?php\n                    /**\n                     * @throws \\Exception| \\InvalidArgumentException\n                     */\n                    function bar() : void {}'\n            ],\n            'varDocblockAboveCall' => [\n                '<?php\n\n                    function example(string $s): void {\n                        if (preg_match(\\'{foo-(\\w+)}\\', $s, $m)) {\n                          /** @var array{string, string} $m */\n                          takesString($m[1]);\n                        }\n                    }\n\n                    function takesString(string $_s): void {}'\n            ],\n            'noCrashWithoutAssignment' => [\n                '<?php\n                    /** @var DateTime $obj */\n                    echo $obj->format(\"Y\");'\n            ],\n            'intMaskWithClassConstants' => [\n                '<?php\n                    class FileFlag {\n                        public const OPEN = 1;\n                        public const MODIFIED = 2;\n                        public const NEW = 4;\n                    }\n\n                    /**\n                     * @param int-mask<FileFlag::OPEN, FileFlag::MODIFIED, FileFlag::NEW> $flags\n                     */\n                    function takesFlags(int $flags) : void {\n                        echo $flags;\n                    }\n\n                    takesFlags(FileFlag::MODIFIED | FileFlag::NEW);'\n            ],\n            'intMaskWithZero' => [\n                '<?php\n                    /** @param int-mask<1,2> $_flags */\n                    function takesFlags(int $_flags): void {}\n\n                    takesFlags(0);\n                '\n            ],\n            'intMaskOfWithClassWildcard' => [\n                '<?php\n                    class FileFlag {\n                        public const OPEN = 1;\n                        public const MODIFIED = 2;\n                        public const NEW = 4;\n                    }\n\n                    /**\n                     * @param int-mask-of<FileFlag::*> $flags\n                     */\n                    function takesFlags(int $flags) : void {\n                        echo $flags;\n                    }\n\n                    takesFlags(FileFlag::MODIFIED | FileFlag::NEW);'\n            ],\n            'intMaskOfWithZero' => [\n                '<?php\n                    class FileFlag {\n                        public const OPEN = 1;\n                        public const MODIFIED = 2;\n                        public const NEW = 4;\n                    }\n\n                    /** @param int-mask-of<FileFlag::*> $_flags */\n                    function takesFlags(int $_flags): void {}\n\n                    takesFlags(0);\n                '\n            ],\n            'emptyStringFirst' => [\n                '<?php\n                    /**\n                     * @param \\'\\'|\\'a\\'|\\'b\\' $v\n                     */\n                    function testBad(string $v): void {\n                        echo $v;\n                    }'\n            ],\n            'UnnecessaryVarAnnotationSuppress' => [\n                '<?php\n                    /** @psalm-consistent-constructor */\n                    final class Foo{}\n                    /**\n                     * @param class-string $class\n                     */\n                    function foo(string $class): Foo {\n                        if (!is_subclass_of($class, Foo::class)) {\n                            throw new \\LogicException();\n                        }\n\n                        /**\n                         * @psalm-suppress UnnecessaryVarAnnotation\n                         * @var Foo $instance\n                         */\n                        $instance = new $class();\n\n                        return $instance;\n                    }',\n            ],\n            'suppressNonInvariantDocblockPropertyType' => [\n                '<?php\n                    class Vendor\n                    {\n                        /**\n                         * @var array\n                         */\n                        public array $config = [];\n                        public function getConfig(): array {return $this->config;}\n                    }\n\n                    class A extends Vendor\n                    {\n                        /**\n                         * @var string[]\n                         * @psalm-suppress NonInvariantDocblockPropertyType\n                         */\n                        public array $config = [];\n                    }\n                    $a = new Vendor();\n                    $_b = new A();\n                    echo (string)($a->getConfig()[0]??\"\");'\n            ],\n            'promotedPropertiesDocumentationEitherForParamOrForProperty' => [\n                '<?php\n                    final class UserRole\n                    {\n                        /** @psalm-param stdClass $id */\n                        public function __construct(\n                            protected $id,\n                            /** @psalm-var stdClass */\n                            protected $id2\n                        ) {\n                        }\n                    }\n\n                    new UserRole(new stdClass(), new stdClass());\n                    '\n            ],\n            'promotedPropertiesDocumentationForPropertyAndSignature' => [\n                '<?php\n                    final class A\n                    {\n                        public function __construct(\n                            /**\n                             * @var iterable<string>\n                             */\n                            private iterable $strings,\n                        ) {\n                        }\n                    }'\n            ],\n        ];\n    }\n\n    /**\n     * @return iterable<string,array{string,error_message:string,1?:string[],2?:bool,3?:string}>\n     */\n    public function providerInvalidCodeParse(): iterable\n    {\n        return [\n            'invalidClassMethodReturn' => [\n                '<?php\n                    class C {\n                        /**\n                         * @return $thus\n                         */\n                        public function barBar() {\n                            return $this;\n                        }\n                    }',\n                'error_message' => 'MissingDocblockType',\n            ],\n\n            'invalidClassMethodReturnBrackets' => [\n                '<?php\n                    class C {\n                        /**\n                         * @return []\n                         */\n                        public static function barBar() {\n                            return [];\n                        }\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'invalidInterfaceMethodReturn' => [\n                '<?php\n                    interface I {\n                        /**\n                         * @return $thus\n                         */\n                        public static function barBar();\n                    }',\n                'error_message' => 'MissingDocblockType',\n            ],\n            'invalidInterfaceMethodReturnBrackets' => [\n                '<?php\n                    interface I {\n                        /**\n                         * @return []\n                         */\n                        public static function barBar();\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'invalidPropertyBrackets' => [\n                '<?php\n                    class A {\n                        /**\n                         * @var []\n                         */\n                        public $bar;\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'invalidReturnClassWithComma' => [\n                '<?php\n                    interface I {\n                        /**\n                         * @return 1,\n                         */\n                        public static function barBar();\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'returnClassWithComma' => [\n                '<?php\n                    interface I {\n                        /**\n                         * @return a,\n                         */\n                        public static function barBar();\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'missingParamType' => [\n                '<?php\n                    /**\n                     * @param string $bar\n                     */\n                    function fooBar(): void {\n                    }\n\n                    fooBar(\"hello\");',\n                'error_message' => 'TooManyArguments - src' . DIRECTORY_SEPARATOR . 'somefile.php:8:21 - Too many arguments for fooBar '\n                    . '- expecting 0 but saw 1',\n            ],\n            'missingParamVar' => [\n                '<?php\n                    /**\n                     * @param string\n                     */\n                    function fooBar(): void {\n                    }',\n                'error_message' => 'InvalidDocblock - src' . DIRECTORY_SEPARATOR . 'somefile.php:5:21 - Badly-formatted @param',\n            ],\n            'invalidSlashWithString' => [\n                '<?php\n                    /**\n                     * @return \\?string\n                     */\n                    function foo() {\n                        return rand(0, 1) ? \"hello\" : null;\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'missingReturnTypeWithBadDocblock' => [\n                '<?php\n                    /**\n                     * @return [bad]\n                     */\n                    function fooBar() {\n                    }',\n                'error_message' => 'MissingReturnType',\n                [\n                    'InvalidDocblock' => Config::REPORT_INFO,\n                ],\n            ],\n            'invalidDocblockReturn' => [\n                '<?php\n                    /**\n                     * @return string\n                     */\n                    function fooFoo(): int {\n                        return 5;\n                    }',\n                'error_message' => 'MismatchingDocblockReturnType',\n            ],\n            'intParamTypeDefinedInParent' => [\n                '<?php\n                    class A {\n                        public function foo(int $a): void {}\n                    }\n\n                    class B extends A {\n                        public function foo($a): void {}\n                    }',\n                'error_message' => 'MissingParamType',\n                'error_levels' => ['MethodSignatureMismatch'],\n            ],\n            'psalmInvalidVar' => [\n                '<?php\n                    class A\n                    {\n                        /** @psalm-var array<int, string> */\n                        public $foo = [];\n\n                        public function updateFoo(): void {\n                            $this->foo[\"boof\"] = \"hello\";\n                        }\n                    }',\n                'error_message' => 'InvalidPropertyAssignmentValue',\n            ],\n            'incorrectDocblockOrder' => [\n                '<?php\n                    class MyClass {\n                        /**\n                         * Comment\n                         * @var $fooPropTypo string\n                         */\n                        public $fooProp = \"/tmp/file.txt\";\n                    }',\n                'error_message' => 'MissingDocblockType',\n            ],\n            'badlyFormattedVar' => [\n                '<?php\n                    /**\n                     * @return string[]\n                     */\n                    function returns_strings() {\n                        /** @var array(string) $result */\n                        $result = [\"example\"];\n                        return $result;\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'badlyWrittenVar' => [\n                '<?php\n                    /** @param mixed $x */\n                    function myvalue($x): void {\n                        /** @var $myVar MyNS\\OtherClass */\n                        $myVar = $x->conn()->method();\n                        $myVar->otherMethod();\n                    }',\n                'error_message' => 'MissingDocblockType',\n            ],\n            'dontOverrideSameType' => [\n                '<?php\n                    class A {\n                        /** @return ?int */\n                        public function foo(): ?int {\n                            if (rand(0, 1)) return 5;\n                        }\n                    }',\n                'error_message' => 'InvalidReturnType',\n            ],\n            'alwaysCheckReturnType' => [\n                '<?php\n                    class A {}\n\n                    /**\n                     * @return A\n                     * @psalm-suppress MismatchingDocblockReturnType\n                     */\n                    function foo(): B {\n                        return new A;\n                    }',\n                'error_message' => 'UndefinedClass',\n            ],\n            'preventBadBoolean' => [\n                '<?php\n                    function foo(): boolean {\n                        return true;\n                    }',\n                'error_message' => 'UndefinedClass',\n            ],\n            'undefinedDocblockClassCall' => [\n                '<?php\n                    class B {\n                        /**\n                         * @return A\n                         * @psalm-suppress UndefinedDocblockClass\n                         * @psalm-suppress InvalidReturnStatement\n                         * @psalm-suppress InvalidReturnType\n                         */\n                        public function foo() {\n                            return new stdClass();\n                        }\n\n                        public function bar() {\n                            $this->foo()->bar();\n                        }\n                    }\n                    ',\n                'error_message' => 'UndefinedDocblockClass',\n            ],\n            'noPhpStormAnnotationsThankYou' => [\n                '<?php\n                    /** @param ArrayIterator|string[] $i */\n                    function takesArrayIteratorOfString(ArrayIterator $i): void {}',\n                'error_message' => 'MismatchingDocblockParamType',\n            ],\n            'noPhpStormAnnotationsPossiblyInvalid' => [\n                '<?php\n                    /** @param ArrayIterator|string[] $i */\n                    function takesArrayIteratorOfString($i): void {\n                        $s = $i->offsetGet(\"a\");\n                    }',\n                'error_message' => 'PossiblyInvalidMethodCall',\n            ],\n            'doubleBar' => [\n                '<?php\n                    /** @param PDO||Closure|numeric $a */\n                    function foo($a) : void {}',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'badStringVar' => [\n                '<?php\n                    /** @var string; */\n                    $a = \"hello\";',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'badCallableVar' => [\n                '<?php\n                    /** @return Closure(int): */\n                    function foo() : callable {\n                        return function () : void {};\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'hyphenInType' => [\n                '<?php\n                    /**\n                     * @return - Description\n                     */\n                    function example() {\n                        return \"placeholder\";\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'badAmpersand' => [\n                '<?php\n                    /** @return &array */\n                    function foo() : array {\n                        return [];\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'invalidTypeAlias' => [\n                '<?php\n                    /**\n                     * @psalm-type CoolType = A|B>\n                     */\n\n                    class A {}',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'typeAliasInTKeyedArray' => [\n                '<?php\n                    /**\n                     * @psalm-type aType null|\"a\"|\"b\"|\"c\"|\"d\"\n                     */\n\n                    /** @psalm-return array{0:bool,1:aType} */\n                    function f(): array {\n                        return [(bool)rand(0,1), rand(0,1) ? \"z\" : null];\n                    }',\n                'error_message' => 'InvalidReturnStatement',\n            ],\n            'noCrashOnHalfDoneArrayPropertyType' => [\n                '<?php\n                    class A {\n                        /** @var array< */\n                        private $foo = [];\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'noCrashOnHalfDoneTKeyedArrayPropertyType' => [\n                '<?php\n                    class A {\n                        /** @var array{ */\n                        private $foo = [];\n                    }',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'noCrashOnInvalidClassTemplateAsType' => [\n                '<?php\n                    /**\n                     * @template T as ' . '\n                     */\n                    class A {}',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'noCrashOnInvalidFunctionTemplateAsType' => [\n                '<?php\n                    /**\n                     * @template T as ' . '\n                     */\n                    function foo() : void {}',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'returnTypeNewLineIsIgnored' => [\n                '<?php\n                    /**\n                     * @return\n                     *     Some text\n                     */\n                    function foo() {}',\n                'error_message' => 'MissingReturnType',\n            ],\n            'objectWithPropertiesAnnotationNoMatchingProperty' => [\n                '<?php\n                    /** @param object{foo:string} $o */\n                    function foo(object $o) : string {\n                        return $o->foo;\n                    }\n\n                    class A {}\n\n                    foo(new A);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'badVar' => [\n                '<?php\n                    /** @var Foo */\n                    $a = $_GET[\"foo\"];',\n                'error_message' => 'UndefinedDocblockClass',\n            ],\n            'badPsalmType' => [\n                '<?php\n                    /**\n                     * @psalm-type Foo = array{a:}\n                     */',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'mismatchingDocblockParamName' => [\n                '<?php\n                    /** @param string[] $bar */\n                    function f(array $barb): void {}',\n                'error_message' => 'InvalidDocblockParamName - src' . DIRECTORY_SEPARATOR . 'somefile.php:2:41',\n            ],\n            'nonEmptyArrayCalledWithEmpty' => [\n                '<?php\n                    /** @param non-empty-array<string> $arr */\n                    function foo(array $arr) : void {\n                        foreach ($arr as $a) {}\n                        echo $a;\n                    }\n\n                    foo([]);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'nonEmptyArrayCalledWithEmptyInNamespace' => [\n                '<?php\n                    namespace ns;\n\n                    /** @param non-empty-array<string> $arr */\n                    function foo(array $arr) : void {\n                        foreach ($arr as $a) {}\n                        echo $a;\n                    }\n\n                    foo([]);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'nonEmptyArrayCalledWithArray' => [\n                '<?php\n                    /** @param non-empty-array<string> $arr */\n                    function foo(array $arr) : void {\n                        foreach ($arr as $a) {}\n                        echo $a;\n                    }\n\n                    /** @param array<string> $arr */\n                    function bar(array $arr) {\n                        foo($arr);\n                    }',\n                'error_message' => 'ArgumentTypeCoercion',\n            ],\n            'spreadOperatorArrayAnnotationBadArg' => [\n                '<?php\n                    /** @param string[] $_s */\n                    function foo(string ...$_s) : void {}\n                    foo(5);',\n                'error_message' => 'InvalidScalarArgument',\n            ],\n            'spreadOperatorArrayAnnotationBadSpreadArg' => [\n                '<?php\n                    /** @param string[] $_s */\n                    function foo(string ...$_s) : void {}\n                    foo(...[5]);',\n                'error_message' => 'InvalidScalarArgument',\n            ],\n            'spreadOperatorByRefAnnotationBadCall1' => [\n                '<?php\n                    /** @param string &...$s */\n                    function foo(&...$s) : void {}\n\n                    $a = 1;\n                    foo($a);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'spreadOperatorByRefAnnotationBadCall2' => [\n                '<?php\n                    /** @param string ...&$s */\n                    function foo(&...$s) : void {}\n\n                    $b = 2;\n                    foo($b);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'spreadOperatorByRefAnnotationBadCall3' => [\n                '<?php\n                    /** @param string[] &$s */\n                    function foo(&...$s) : void {}\n\n                    $c = 3;\n                    foo($c);',\n                'error_message' => 'InvalidArgument',\n            ],\n            'identifyReturnType' => [\n                '<?php\n                    /** @return array{hello: string} */\n                    function foo() {}',\n                'error_message' => 'InvalidReturnType - src' . DIRECTORY_SEPARATOR . 'somefile.php:2:33',\n            ],\n            'invalidParamDocblockAsterisk' => [\n                '<?php\n                    /**\n                     * @param    *   $reference\n                     */\n                    function f($reference) {}',\n                'error_message' => 'MissingDocblockType',\n            ],\n            'canNeverReturnDeclaredType' => [\n                '<?php\n\n                    /** @psalm-return false */\n                    function alwaysFalse() : bool\n                    {\n                        return true;\n                    }',\n                'error_message' => 'InvalidReturnStatement - src' . DIRECTORY_SEPARATOR . 'somefile.php:6:32',\n            ],\n            'falsableWithExpectedTypeTrue' => [\n                '<?php\n\n                    /** @psalm-return true */\n                    function alwaysFalse()\n                    {\n                        return false;\n                    }',\n                'error_message' => 'FalsableReturnStatement - src' . DIRECTORY_SEPARATOR . 'somefile.php:6:32',\n            ],\n            'DuplicatedParam' => [\n                '<?php\n                    /**\n                     * @psalm-param array $arr\n                     * @psalm-param array $arr\n                     */\n                    function bar(array $arr): void {}',\n                'error_message' => 'InvalidDocblock - src' . DIRECTORY_SEPARATOR . 'somefile.php:6:21 - Found duplicated @param or prefixed @param tag in docblock for bar',\n            ],\n            'DuplicatedReturn' => [\n                '<?php\n                    /**\n                     * @return void\n                     * @return void\n                     */\n                    function bar(array $arr): void {}',\n                'error_message' => 'InvalidDocblock - src' . DIRECTORY_SEPARATOR . 'somefile.php:6:21 - Found duplicated @return or prefixed @return tag in docblock for bar',\n            ],\n            'missingClassForTKeyedArray' => [\n                '<?php\n                    interface I {\n                        /** @return object{id: int, a: int} */\n                        public function run();\n                    }\n\n                    class C implements I {\n                        /** @return X */\n                        public function run() {}\n                    }',\n                'error_message' => 'ImplementedReturnTypeMismatch'\n            ],\n            'unexpectedImportType' => [\n                '<?php\n                    /** @psalm-import-type asd */\n                    function f(): void {}\n                ',\n                'error_message' => 'PossiblyInvalidDocblockTag',\n            ],\n            'unexpectedVarOnFunction' => [\n                '<?php\n                    /** @var int $p */\n                    function f($p): void {}\n                ',\n                'error_message' => 'PossiblyInvalidDocblockTag',\n            ],\n            'unterminatedParentheses' => [\n                '<?php\n                    /** @return ( */\n                    function f() {}\n                ',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'emptyParentheses' => [\n                '<?php\n                    /** @return () */\n                    function f() {}\n                ',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'unbalancedParentheses' => [\n                \"<?php\n                    /** @return ((string) */\n                    function f(): string {\n                        return '';\n                    }\n                \",\n                'error_message' => 'InvalidDocblock',\n            ],\n            'promotedPropertiesDocumentationFailsWhenSendingBadTypeAgainstParam' => [\n                '<?php\n                    final class UserRole\n                    {\n                        /** @psalm-param stdClass $id */\n                        public function __construct(\n                            protected $id\n                        ) {\n                        }\n\n                    }\n                    new UserRole(\"a\");\n                    ',\n                'error_message' => 'InvalidArgument',\n            ],\n            'promotedPropertiesDocumentationFailsWhenSendingBadTypeAgainstProperty' => [\n                '<?php\n                    final class UserRole\n                    {\n                        public function __construct(\n                            /** @psalm-var stdClass */\n                            protected $id2\n                        ) {\n                        }\n                    }\n\n                    new UserRole(\"a\");\n                    ',\n                'error_message' => 'InvalidArgument',\n            ],\n            'promotedPropertyDuplicateDoc' => [\n                '<?php\n                    final class UserRole\n                    {\n                        /** @psalm-param string $id */\n                        public function __construct(\n                            /** @psalm-var stdClass */\n                            protected $id\n                        ) {\n                        }\n                    }\n                    ',\n                'error_message' => 'InvalidDocblock',\n            ],\n            'promotedPropertyWithParamDocblockAndSignatureType' => [\n                '<?php\n\n                    class A\n                    {\n                        public function __construct(\n                            /** @var \"cti\"|\"basic\"|\"teams\"|\"\" */\n                            public string $licenseType = \"\",\n                        ) {\n                        }\n                    }\n\n                    $a = new A(\"ladida\");\n                    $a->licenseType = \"dudidu\";\n\n                    echo $a->licenseType;',\n                'error_message' => 'InvalidArgument',\n            ],\n        ];\n    }\n}\n", "idx": 49, "id": 12632, "msg": "", "proj": "vimeo-psalm", "lang": "php"}
{"patch": "@@ -147,7 +147,8 @@ public final class RowIDAllocator implements Serializable {\n     }\n   }\n \n-  private void updateMeta(ByteString key, byte[] oldVal, Snapshot snapshot) {\n+  private Optional<BytePairWrapper> getMetaToUpdate(\n+      ByteString key, byte[] oldVal, Snapshot snapshot) {\n     // 1. encode hash meta key\n     // 2. load meta via hash meta key from TiKV\n     // 3. update meta's filed count and set it back to TiKV", "y": 0, "oldf": "/*\n * Copyright 2019 PingCAP, Inc.\n *\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n *      http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\npackage com.pingcap.tikv.allocator;\n\nimport static com.pingcap.tikv.util.BackOffer.ROW_ID_ALLOCATOR_BACKOFF;\n\nimport com.google.common.primitives.UnsignedLongs;\nimport com.google.protobuf.ByteString;\nimport com.pingcap.tikv.Snapshot;\nimport com.pingcap.tikv.TiConfiguration;\nimport com.pingcap.tikv.TiSession;\nimport com.pingcap.tikv.TwoPhaseCommitter;\nimport com.pingcap.tikv.codec.Codec.IntegerCodec;\nimport com.pingcap.tikv.codec.CodecDataInput;\nimport com.pingcap.tikv.codec.CodecDataOutput;\nimport com.pingcap.tikv.codec.KeyUtils;\nimport com.pingcap.tikv.codec.MetaCodec;\nimport com.pingcap.tikv.exception.AllocateRowIDOverflowException;\nimport com.pingcap.tikv.exception.TiBatchWriteException;\nimport com.pingcap.tikv.meta.TiTableInfo;\nimport com.pingcap.tikv.util.BackOffFunction;\nimport com.pingcap.tikv.util.BackOffer;\nimport com.pingcap.tikv.util.ConcreteBackOffer;\nimport java.io.Serializable;\nimport java.util.Arrays;\nimport java.util.function.Function;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\n/**\n * RowIDAllocator read current start from TiKV and write back 'start+step' back to TiKV. It designs\n * to allocate all id for data to be written at once, hence it does not need run inside a txn.\n *\n * <p>(start, end] is allocated\n */\npublic final class RowIDAllocator implements Serializable {\n  private final long maxShardRowIDBits;\n  private final long dbId;\n  private final TiConfiguration conf;\n  private final long step;\n  private long end;\n\n  private static final Logger LOG = LoggerFactory.getLogger(RowIDAllocator.class);\n\n  private RowIDAllocator(long maxShardRowIDBits, long dbId, long step, TiConfiguration conf) {\n    this.maxShardRowIDBits = maxShardRowIDBits;\n    this.dbId = dbId;\n    this.step = step;\n    this.conf = conf;\n  }\n\n  public long getAutoIncId(long index) {\n    return index + getStart();\n  }\n\n  /**\n   * @param index should >= 1\n   * @return\n   */\n  public long getShardRowId(long index) {\n    return getShardRowId(maxShardRowIDBits, index, index + getStart());\n  }\n\n  static long getShardRowId(long maxShardRowIDBits, long partitionIndex, long rowID) {\n    if (maxShardRowIDBits <= 0 || maxShardRowIDBits >= 16) {\n      return rowID;\n    }\n\n    // assert rowID < Math.pow(2, 64 - maxShardRowIDBits)\n\n    long partition = partitionIndex & ((1L << maxShardRowIDBits) - 1);\n    return rowID | (partition << (64 - maxShardRowIDBits - 1));\n  }\n\n  public static RowIDAllocator create(\n      long dbId, TiTableInfo table, TiConfiguration conf, boolean unsigned, long step) {\n    BackOffer backOffer = ConcreteBackOffer.newCustomBackOff(ROW_ID_ALLOCATOR_BACKOFF);\n    while (true) {\n      try {\n        return doCreate(dbId, table, conf, unsigned, step);\n      } catch (AllocateRowIDOverflowException | IllegalArgumentException e) {\n        throw e;\n      } catch (Exception e) {\n        LOG.warn(\"error during allocating row id\", e);\n        backOffer.doBackOff(BackOffFunction.BackOffFuncType.BoServerBusy, e);\n      }\n    }\n  }\n\n  private static RowIDAllocator doCreate(\n      long dbId, TiTableInfo table, TiConfiguration conf, boolean unsigned, long step) {\n    RowIDAllocator allocator = new RowIDAllocator(table.getMaxShardRowIDBits(), dbId, step, conf);\n    if (unsigned) {\n      allocator.initUnsigned(\n          TiSession.getInstance(conf).createSnapshot(),\n          table.getId(),\n          table.getMaxShardRowIDBits());\n    } else {\n      allocator.initSigned(\n          TiSession.getInstance(conf).createSnapshot(),\n          table.getId(),\n          table.getMaxShardRowIDBits());\n    }\n\n    return allocator;\n  }\n\n  public long getStart() {\n    return end - step;\n  }\n\n  public long getEnd() {\n    return end;\n  }\n\n  // set key value pair to tikv via two phase committer protocol.\n  private void set(ByteString key, byte[] value) {\n    TiSession session = TiSession.getInstance(conf);\n    TwoPhaseCommitter twoPhaseCommitter =\n        new TwoPhaseCommitter(conf, session.getTimestamp().getVersion());\n\n    twoPhaseCommitter.prewritePrimaryKey(\n        ConcreteBackOffer.newCustomBackOff(BackOffer.PREWRITE_MAX_BACKOFF),\n        key.toByteArray(),\n        value);\n\n    twoPhaseCommitter.commitPrimaryKey(\n        ConcreteBackOffer.newCustomBackOff(BackOffer.BATCH_COMMIT_BACKOFF),\n        key.toByteArray(),\n        session.getTimestamp().getVersion());\n\n    try {\n      twoPhaseCommitter.close();\n    } catch (Throwable ignored) {\n    }\n  }\n\n  private void updateMeta(ByteString key, byte[] oldVal, Snapshot snapshot) {\n    // 1. encode hash meta key\n    // 2. load meta via hash meta key from TiKV\n    // 3. update meta's filed count and set it back to TiKV\n    CodecDataOutput cdo = new CodecDataOutput();\n    ByteString metaKey = MetaCodec.encodeHashMetaKey(cdo, key.toByteArray());\n    long fieldCount = 0;\n    ByteString metaVal = snapshot.get(metaKey);\n\n    // decode long from bytes\n    // big endian the 8 bytes\n    if (!metaVal.isEmpty()) {\n      try {\n        fieldCount = IntegerCodec.readULong(new CodecDataInput(metaVal.toByteArray()));\n      } catch (Exception ignored) {\n        LOG.warn(\"metaDecode failed, field is ignored.\" + KeyUtils.formatBytesUTF8(metaVal));\n      }\n    }\n\n    // update meta field count only oldVal is null\n    if (oldVal == null || oldVal.length == 0) {\n      fieldCount++;\n      cdo.reset();\n      cdo.writeLong(fieldCount);\n\n      set(metaKey, cdo.toBytes());\n    }\n  }\n\n  private long updateHash(\n      ByteString key,\n      ByteString field,\n      Function<byte[], byte[]> calculateNewVal,\n      Snapshot snapshot) {\n    // 1. encode hash data key\n    // 2. get value in byte from get operation\n    // 3. calculate new value via calculateNewVal\n    // 4. check old value equals to new value or not\n    // 5. set the new value back to TiKV via 2pc\n    // 6. encode a hash meta key\n    // 7. update a hash meta field count if needed\n\n    CodecDataOutput cdo = new CodecDataOutput();\n    MetaCodec.encodeHashDataKey(cdo, key.toByteArray(), field.toByteArray());\n    ByteString dataKey = cdo.toByteString();\n    byte[] oldVal = snapshot.get(dataKey.toByteArray());\n\n    byte[] newVal = calculateNewVal.apply(oldVal);\n    if (Arrays.equals(newVal, oldVal)) {\n      // not need to update\n      return 0L;\n    }\n\n    set(dataKey, newVal);\n    updateMeta(key, oldVal, snapshot);\n    return Long.parseLong(new String(newVal));\n  }\n\n  private static boolean isDBExisted(long dbId, Snapshot snapshot) {\n    ByteString dbKey = MetaCodec.encodeDatabaseID(dbId);\n    ByteString json = MetaCodec.hashGet(MetaCodec.KEY_DBs, dbKey, snapshot);\n    return json != null && !json.isEmpty();\n  }\n\n  private static boolean isTableExisted(long dbId, long tableId, Snapshot snapshot) {\n    ByteString dbKey = MetaCodec.encodeDatabaseID(dbId);\n    ByteString tableKey = MetaCodec.tableKey(tableId);\n    return !MetaCodec.hashGet(dbKey, tableKey, snapshot).isEmpty();\n  }\n\n  public static boolean shardRowBitsOverflow(\n      long base, long step, long shardRowBits, boolean reservedSignBit) {\n    long signBit = reservedSignBit ? 1 : 0;\n    long mask = ((1L << shardRowBits) - 1) << (64 - shardRowBits - signBit);\n    if (reservedSignBit) {\n      return ((base + step) & mask) > 0;\n    } else {\n      return Long.compareUnsigned((base + step) & mask, 0) > 0;\n    }\n  }\n\n  /**\n   * read current row id from TiKV and write the calculated value back to TiKV. The calculation rule\n   * is start(read from TiKV) + step.\n   */\n  public long udpateAllocateId(\n      long dbId, long tableId, long step, Snapshot snapshot, long shard, boolean hasSignedBit) {\n    if (isDBExisted(dbId, snapshot) && isTableExisted(dbId, tableId, snapshot)) {\n      return updateHash(\n          MetaCodec.encodeDatabaseID(dbId),\n          MetaCodec.autoTableIDKey(tableId),\n          (oldVal) -> {\n            long base = 0;\n            if (oldVal != null && oldVal.length != 0) {\n              base = Long.parseLong(new String(oldVal));\n            }\n            if (shard >= 1 && shardRowBitsOverflow(base, step, shard, hasSignedBit)) {\n              throw new AllocateRowIDOverflowException(base, step, shard);\n            }\n            base += step;\n            return String.valueOf(base).getBytes();\n          },\n          snapshot);\n    }\n\n    throw new IllegalArgumentException(\"table or database is not existed\");\n  }\n\n  /** read current row id from TiKV according to database id and table id. */\n  public static long getAllocateId(long dbId, long tableId, Snapshot snapshot) {\n    if (isDBExisted(dbId, snapshot) && isTableExisted(dbId, tableId, snapshot)) {\n      ByteString dbKey = MetaCodec.encodeDatabaseID(dbId);\n      ByteString tblKey = MetaCodec.autoTableIDKey(tableId);\n      ByteString val = MetaCodec.hashGet(dbKey, tblKey, snapshot);\n      if (val.isEmpty()) return 0L;\n      return Long.parseLong(val.toStringUtf8());\n    }\n\n    throw new IllegalArgumentException(\"table or database is not existed\");\n  }\n\n  private void initSigned(Snapshot snapshot, long tableId, long shard) {\n    // get new start from TiKV, and calculate new end and set it back to TiKV.\n    long newStart = getAllocateId(dbId, tableId, snapshot);\n    long tmpStep = Math.min(Long.MAX_VALUE - newStart, step);\n    if (tmpStep != step) {\n      throw new TiBatchWriteException(\"cannot allocate ids for this write\");\n    }\n    if (newStart == Long.MAX_VALUE) {\n      throw new TiBatchWriteException(\"cannot allocate more ids since it \");\n    }\n    end = udpateAllocateId(dbId, tableId, tmpStep, snapshot, shard, true);\n  }\n\n  private void initUnsigned(Snapshot snapshot, long tableId, long shard) {\n    // get new start from TiKV, and calculate new end and set it back to TiKV.\n    long newStart = getAllocateId(dbId, tableId, snapshot);\n    // for unsigned long, -1L is max value.\n    long tmpStep = UnsignedLongs.min(-1L - newStart, step);\n    if (tmpStep != step) {\n      throw new TiBatchWriteException(\"cannot allocate ids for this write\");\n    }\n    // when compare unsigned long, the min value is largest value.\n    if (UnsignedLongs.compare(newStart, -1L) == 0) {\n      throw new TiBatchWriteException(\n          \"cannot allocate more ids since the start reaches \" + \"unsigned long's max value \");\n    }\n    end = udpateAllocateId(dbId, tableId, tmpStep, snapshot, shard, false);\n  }\n}\n", "idx": 4, "id": 13513, "msg": "", "proj": "pingcap-tispark", "lang": "java"}
{"patch": "@@ -1157,6 +1157,14 @@ class KoalasSeriesPlotMethods(PandasObject):\n         secondary_y=False,\n         **kwds\n     ):\n+        positional_args = locals()\n+        plot_backend = _get_plot_backend(kwds.pop(\"backend\", None))\n+        # when using another backend, let the backend take the charge\n+        if plot_backend.__name__ != \"databricks.koalas.plot\":\n+            args = {**positional_args, **kwds}\n+            plot_data, kwds = _get_args_map(plot_backend.__name__, self.data, kind, args)\n+            return plot_backend.plot(plot_data, kind=kind, **kwds)\n+\n         return plot_series(\n             self.data,\n             kind=kind,", "y": 0, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom distutils.version import LooseVersion\n\nimport matplotlib\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.axes._base import _process_plot_format\nfrom pandas.core.dtypes.inference import is_integer, is_list_like\nfrom pandas.io.formats.printing import pprint_thing\nfrom pandas.core.base import PandasObject\nfrom pyspark.ml.feature import Bucketizer\nfrom pyspark.mllib.stat import KernelDensity\nfrom pyspark.sql import functions as F\n\nfrom databricks.koalas.missing import unsupported_function\nfrom databricks.koalas.config import get_option\n\n\nif LooseVersion(pd.__version__) < LooseVersion(\"0.25\"):\n    from pandas.plotting._core import (\n        _all_kinds,\n        BarPlot,\n        BoxPlot,\n        HistPlot,\n        MPLPlot,\n        PiePlot,\n        AreaPlot,\n        LinePlot,\n        BarhPlot,\n        ScatterPlot,\n        KdePlot,\n    )\nelse:\n    from pandas.plotting._core import PlotAccessor\n    from pandas.plotting._matplotlib import (\n        BarPlot,\n        BoxPlot,\n        HistPlot,\n        PiePlot,\n        AreaPlot,\n        LinePlot,\n        BarhPlot,\n        ScatterPlot,\n        KdePlot,\n    )\n    from pandas.plotting._matplotlib.core import MPLPlot\n\n    _all_kinds = PlotAccessor._all_kinds\n\n\nclass TopNPlot:\n    def get_top_n(self, data):\n        from databricks.koalas import DataFrame, Series\n\n        max_rows = get_option(\"plotting.max_rows\")\n        # Simply use the first 1k elements and make it into a pandas dataframe\n        # For categorical variables, it is likely called from df.x.value_counts().plot.xxx().\n        if isinstance(data, (Series, DataFrame)):\n            data = data.head(max_rows + 1).to_pandas()\n        else:\n            raise ValueError(\"Only DataFrame and Series are supported for plotting.\")\n\n        self.partial = False\n        if len(data) > max_rows:\n            self.partial = True\n            data = data.iloc[:max_rows]\n        return data\n\n    def set_result_text(self, ax):\n        max_rows = get_option(\"plotting.max_rows\")\n        assert hasattr(self, \"partial\")\n\n        if self.partial:\n            ax.text(\n                1,\n                1,\n                \"showing top {} elements only\".format(max_rows),\n                size=6,\n                ha=\"right\",\n                va=\"bottom\",\n                transform=ax.transAxes,\n            )\n\n\nclass SampledPlot:\n    def get_sampled(self, data):\n        from databricks.koalas import DataFrame, Series\n\n        fraction = get_option(\"plotting.sample_ratio\")\n        if fraction is None:\n            fraction = 1 / (len(data) / get_option(\"plotting.max_rows\"))\n            fraction = min(1.0, fraction)\n        self.fraction = fraction\n\n        if isinstance(data, (DataFrame, Series)):\n            if isinstance(data, Series):\n                data = data.to_frame()\n            sampled = data._internal.resolved_copy.spark_frame.sample(fraction=self.fraction)\n            return DataFrame(data._internal.with_new_sdf(sampled)).to_pandas()\n        else:\n            raise ValueError(\"Only DataFrame and Series are supported for plotting.\")\n\n    def set_result_text(self, ax):\n        assert hasattr(self, \"fraction\")\n\n        if self.fraction < 1:\n            ax.text(\n                1,\n                1,\n                \"showing the sampled result by fraction %s\" % self.fraction,\n                size=6,\n                ha=\"right\",\n                va=\"bottom\",\n                transform=ax.transAxes,\n            )\n\n\nclass KoalasBarPlot(BarPlot, TopNPlot):\n    def __init__(self, data, **kwargs):\n        super(KoalasBarPlot, self).__init__(self.get_top_n(data), **kwargs)\n\n    def _plot(self, ax, x, y, w, start=0, log=False, **kwds):\n        self.set_result_text(ax)\n        return ax.bar(x, y, w, bottom=start, log=log, **kwds)\n\n\nclass KoalasBoxPlot(BoxPlot):\n    def boxplot(\n        self,\n        ax,\n        bxpstats,\n        notch=None,\n        sym=None,\n        vert=None,\n        whis=None,\n        positions=None,\n        widths=None,\n        patch_artist=None,\n        bootstrap=None,\n        usermedians=None,\n        conf_intervals=None,\n        meanline=None,\n        showmeans=None,\n        showcaps=None,\n        showbox=None,\n        showfliers=None,\n        boxprops=None,\n        labels=None,\n        flierprops=None,\n        medianprops=None,\n        meanprops=None,\n        capprops=None,\n        whiskerprops=None,\n        manage_xticks=True,\n        autorange=False,\n        zorder=None,\n        precision=None,\n    ):\n        def update_dict(dictionary, rc_name, properties):\n            \"\"\" Loads properties in the dictionary from rc file if not already\n            in the dictionary\"\"\"\n            rc_str = \"boxplot.{0}.{1}\"\n            if dictionary is None:\n                dictionary = dict()\n            for prop_dict in properties:\n                dictionary.setdefault(\n                    prop_dict, matplotlib.rcParams[rc_str.format(rc_name, prop_dict)]\n                )\n            return dictionary\n\n        # Common property dictionaries loading from rc\n        flier_props = [\n            \"color\",\n            \"marker\",\n            \"markerfacecolor\",\n            \"markeredgecolor\",\n            \"markersize\",\n            \"linestyle\",\n            \"linewidth\",\n        ]\n        default_props = [\"color\", \"linewidth\", \"linestyle\"]\n\n        boxprops = update_dict(boxprops, \"boxprops\", default_props)\n        whiskerprops = update_dict(whiskerprops, \"whiskerprops\", default_props)\n        capprops = update_dict(capprops, \"capprops\", default_props)\n        medianprops = update_dict(medianprops, \"medianprops\", default_props)\n        meanprops = update_dict(meanprops, \"meanprops\", default_props)\n        flierprops = update_dict(flierprops, \"flierprops\", flier_props)\n\n        if patch_artist:\n            boxprops[\"linestyle\"] = \"solid\"\n            boxprops[\"edgecolor\"] = boxprops.pop(\"color\")\n\n        # if non-default sym value, put it into the flier dictionary\n        # the logic for providing the default symbol ('b+') now lives\n        # in bxp in the initial value of final_flierprops\n        # handle all of the `sym` related logic here so we only have to pass\n        # on the flierprops dict.\n        if sym is not None:\n            # no-flier case, which should really be done with\n            # 'showfliers=False' but none-the-less deal with it to keep back\n            # compatibility\n            if sym == \"\":\n                # blow away existing dict and make one for invisible markers\n                flierprops = dict(linestyle=\"none\", marker=\"\", color=\"none\")\n                # turn the fliers off just to be safe\n                showfliers = False\n            # now process the symbol string\n            else:\n                # process the symbol string\n                # discarded linestyle\n                _, marker, color = _process_plot_format(sym)\n                # if we have a marker, use it\n                if marker is not None:\n                    flierprops[\"marker\"] = marker\n                # if we have a color, use it\n                if color is not None:\n                    # assume that if color is passed in the user want\n                    # filled symbol, if the users want more control use\n                    # flierprops\n                    flierprops[\"color\"] = color\n                    flierprops[\"markerfacecolor\"] = color\n                    flierprops[\"markeredgecolor\"] = color\n\n        # replace medians if necessary:\n        if usermedians is not None:\n            if len(np.ravel(usermedians)) != len(bxpstats) or np.shape(usermedians)[0] != len(\n                bxpstats\n            ):\n                raise ValueError(\"usermedians length not compatible with x\")\n            else:\n                # reassign medians as necessary\n                for stats, med in zip(bxpstats, usermedians):\n                    if med is not None:\n                        stats[\"med\"] = med\n\n        if conf_intervals is not None:\n            if np.shape(conf_intervals)[0] != len(bxpstats):\n                err_mess = \"conf_intervals length not compatible with x\"\n                raise ValueError(err_mess)\n            else:\n                for stats, ci in zip(bxpstats, conf_intervals):\n                    if ci is not None:\n                        if len(ci) != 2:\n                            raise ValueError(\"each confidence interval must \" \"have two values\")\n                        else:\n                            if ci[0] is not None:\n                                stats[\"cilo\"] = ci[0]\n                            if ci[1] is not None:\n                                stats[\"cihi\"] = ci[1]\n\n        artists = ax.bxp(\n            bxpstats,\n            positions=positions,\n            widths=widths,\n            vert=vert,\n            patch_artist=patch_artist,\n            shownotches=notch,\n            showmeans=showmeans,\n            showcaps=showcaps,\n            showbox=showbox,\n            boxprops=boxprops,\n            flierprops=flierprops,\n            medianprops=medianprops,\n            meanprops=meanprops,\n            meanline=meanline,\n            showfliers=showfliers,\n            capprops=capprops,\n            whiskerprops=whiskerprops,\n            manage_xticks=manage_xticks,\n            zorder=zorder,\n        )\n        return artists\n\n    def _plot(self, ax, bxpstats, column_num=None, return_type=\"axes\", **kwds):\n        bp = self.boxplot(ax, bxpstats, **kwds)\n\n        if return_type == \"dict\":\n            return bp, bp\n        elif return_type == \"both\":\n            return self.BP(ax=ax, lines=bp), bp\n        else:\n            return ax, bp\n\n    def _compute_plot_data(self):\n        colname = self.data.name\n        data = self.data\n\n        # Updates all props with the rc defaults from matplotlib\n        self.kwds.update(KoalasBoxPlot.rc_defaults(**self.kwds))\n\n        # Gets some important kwds\n        showfliers = self.kwds.get(\"showfliers\", False)\n        whis = self.kwds.get(\"whis\", 1.5)\n        labels = self.kwds.get(\"labels\", [colname])\n\n        # This one is Koalas specific to control precision for approx_percentile\n        precision = self.kwds.get(\"precision\", 0.01)\n\n        # # Computes mean, median, Q1 and Q3 with approx_percentile and precision\n        col_stats, col_fences = KoalasBoxPlot._compute_stats(data, colname, whis, precision)\n\n        # # Creates a column to flag rows as outliers or not\n        outliers = KoalasBoxPlot._outliers(data, colname, *col_fences)\n\n        # # Computes min and max values of non-outliers - the whiskers\n        whiskers = KoalasBoxPlot._calc_whiskers(colname, outliers)\n\n        if showfliers:\n            fliers = KoalasBoxPlot._get_fliers(colname, outliers)\n        else:\n            fliers = []\n\n        # Builds bxpstats dict\n        stats = []\n        item = {\n            \"mean\": col_stats[\"mean\"],\n            \"med\": col_stats[\"med\"],\n            \"q1\": col_stats[\"q1\"],\n            \"q3\": col_stats[\"q3\"],\n            \"whislo\": whiskers[0],\n            \"whishi\": whiskers[1],\n            \"fliers\": fliers,\n            \"label\": labels[0],\n        }\n        stats.append(item)\n\n        self.data = {labels[0]: stats}\n\n    def _make_plot(self):\n        bxpstats = list(self.data.values())[0]\n        ax = self._get_ax(0)\n        kwds = self.kwds.copy()\n\n        for stats in bxpstats:\n            if len(stats[\"fliers\"]) > 1000:\n                stats[\"fliers\"] = stats[\"fliers\"][:1000]\n                ax.text(\n                    1,\n                    1,\n                    \"showing top 1,000 fliers only\",\n                    size=6,\n                    ha=\"right\",\n                    va=\"bottom\",\n                    transform=ax.transAxes,\n                )\n\n        ret, bp = self._plot(ax, bxpstats, column_num=0, return_type=self.return_type, **kwds)\n        self.maybe_color_bp(bp)\n        self._return_obj = ret\n\n        labels = [l for l, _ in self.data.items()]\n        labels = [pprint_thing(l) for l in labels]\n        if not self.use_index:\n            labels = [pprint_thing(key) for key in range(len(labels))]\n        self._set_ticklabels(ax, labels)\n\n    @staticmethod\n    def rc_defaults(\n        notch=None,\n        vert=None,\n        whis=None,\n        patch_artist=None,\n        bootstrap=None,\n        meanline=None,\n        showmeans=None,\n        showcaps=None,\n        showbox=None,\n        showfliers=None,\n        **kwargs\n    ):\n        # Missing arguments default to rcParams.\n        if whis is None:\n            whis = matplotlib.rcParams[\"boxplot.whiskers\"]\n        if bootstrap is None:\n            bootstrap = matplotlib.rcParams[\"boxplot.bootstrap\"]\n\n        if notch is None:\n            notch = matplotlib.rcParams[\"boxplot.notch\"]\n        if vert is None:\n            vert = matplotlib.rcParams[\"boxplot.vertical\"]\n        if patch_artist is None:\n            patch_artist = matplotlib.rcParams[\"boxplot.patchartist\"]\n        if meanline is None:\n            meanline = matplotlib.rcParams[\"boxplot.meanline\"]\n        if showmeans is None:\n            showmeans = matplotlib.rcParams[\"boxplot.showmeans\"]\n        if showcaps is None:\n            showcaps = matplotlib.rcParams[\"boxplot.showcaps\"]\n        if showbox is None:\n            showbox = matplotlib.rcParams[\"boxplot.showbox\"]\n        if showfliers is None:\n            showfliers = matplotlib.rcParams[\"boxplot.showfliers\"]\n\n        return dict(\n            whis=whis,\n            bootstrap=bootstrap,\n            notch=notch,\n            vert=vert,\n            patch_artist=patch_artist,\n            meanline=meanline,\n            showmeans=showmeans,\n            showcaps=showcaps,\n            showbox=showbox,\n            showfliers=showfliers,\n        )\n\n    @staticmethod\n    def _compute_stats(data, colname, whis, precision):\n        # Computes mean, median, Q1 and Q3 with approx_percentile and precision\n        pdf = data._kdf._internal.resolved_copy.spark_frame.agg(\n            *[\n                F.expr(\n                    \"approx_percentile({}, {}, {})\".format(colname, q, int(1.0 / precision))\n                ).alias(\"{}_{}%\".format(colname, int(q * 100)))\n                for q in [0.25, 0.50, 0.75]\n            ],\n            F.mean(colname).alias(\"{}_mean\".format(colname))\n        ).toPandas()\n\n        # Computes IQR and Tukey's fences\n        iqr = \"{}_iqr\".format(colname)\n        p75 = \"{}_75%\".format(colname)\n        p25 = \"{}_25%\".format(colname)\n        pdf.loc[:, iqr] = pdf.loc[:, p75] - pdf.loc[:, p25]\n        pdf.loc[:, \"{}_lfence\".format(colname)] = pdf.loc[:, p25] - whis * pdf.loc[:, iqr]\n        pdf.loc[:, \"{}_ufence\".format(colname)] = pdf.loc[:, p75] + whis * pdf.loc[:, iqr]\n\n        qnames = [\"25%\", \"50%\", \"75%\", \"mean\", \"lfence\", \"ufence\"]\n        col_summ = pdf[[\"{}_{}\".format(colname, q) for q in qnames]]\n        col_summ.columns = qnames\n        lfence, ufence = col_summ[\"lfence\"], col_summ[\"ufence\"]\n\n        stats = {\n            \"mean\": col_summ[\"mean\"].values[0],\n            \"med\": col_summ[\"50%\"].values[0],\n            \"q1\": col_summ[\"25%\"].values[0],\n            \"q3\": col_summ[\"75%\"].values[0],\n        }\n\n        return stats, (lfence.values[0], ufence.values[0])\n\n    @staticmethod\n    def _outliers(data, colname, lfence, ufence):\n        # Builds expression to identify outliers\n        expression = F.col(colname).between(lfence, ufence)\n        # Creates a column to flag rows as outliers or not\n        return data._kdf._internal.resolved_copy.spark_frame.withColumn(\n            \"__{}_outlier\".format(colname), ~expression\n        )\n\n    @staticmethod\n    def _calc_whiskers(colname, outliers):\n        # Computes min and max values of non-outliers - the whiskers\n        minmax = (\n            outliers.filter(\"not __{}_outlier\".format(colname))\n            .agg(F.min(colname).alias(\"min\"), F.max(colname).alias(\"max\"))\n            .toPandas()\n        )\n        return minmax.iloc[0][[\"min\", \"max\"]].values\n\n    @staticmethod\n    def _get_fliers(colname, outliers):\n        # Filters only the outliers, should \"showfliers\" be True\n        fliers_df = outliers.filter(\"__{}_outlier\".format(colname))\n\n        # If shows fliers, takes the top 1k with highest absolute values\n        fliers = (\n            fliers_df.select(F.abs(F.col(\"`{}`\".format(colname))).alias(colname))\n            .orderBy(F.desc(\"`{}`\".format(colname)))\n            .limit(1001)\n            .toPandas()[colname]\n            .values\n        )\n\n        return fliers\n\n\nclass KoalasHistPlot(HistPlot):\n    def _args_adjust(self):\n        if is_list_like(self.bottom):\n            self.bottom = np.array(self.bottom)\n\n    def _compute_plot_data(self):\n        # TODO: this logic is same with KdePlot. Might have to deduplicate it.\n        from databricks.koalas.series import Series\n\n        data = self.data\n        if isinstance(data, Series):\n            data = data.to_frame()\n\n        numeric_data = data.select_dtypes(\n            include=[\"byte\", \"decimal\", \"integer\", \"float\", \"long\", \"double\", np.datetime64]\n        )\n\n        # no empty frames or series allowed\n        if len(numeric_data.columns) == 0:\n            raise TypeError(\n                \"Empty {0!r}: no numeric data to \" \"plot\".format(numeric_data.__class__.__name__)\n            )\n\n        if is_integer(self.bins):\n            # computes boundaries for the column\n            self.bins = self._get_bins(data.to_spark(), self.bins)\n\n        self.data = numeric_data\n\n    def _make_plot(self):\n        # TODO: this logic is similar with KdePlot. Might have to deduplicate it.\n        # 'num_colors' requires to calculate `shape` which has to count all.\n        # Use 1 for now to save the computation.\n        colors = self._get_colors(num_colors=1)\n        stacking_id = self._get_stacking_id()\n\n        sdf = self.data._internal.spark_frame\n\n        for i, label in enumerate(self.data._internal.column_labels):\n            # 'y' is a Spark DataFrame that selects one column.\n            y = sdf.select(self.data._internal.spark_column_for(label))\n            ax = self._get_ax(i)\n\n            kwds = self.kwds.copy()\n\n            label = pprint_thing(label if len(label) > 1 else label[0])\n            kwds[\"label\"] = label\n\n            style, kwds = self._apply_style_colors(colors, kwds, i, label)\n            if style is not None:\n                kwds[\"style\"] = style\n\n            # 'y' is a Spark DataFrame that selects one column.\n            # here, we manually calculates the weights separately via Spark\n            # and assign it directly to histogram plot.\n            y = KoalasHistPlot._compute_hist(y, self.bins)  # now y is a pandas Series.\n\n            kwds = self._make_plot_keywords(kwds, y)\n            artists = self._plot(ax, y, column_num=i, stacking_id=stacking_id, **kwds)\n            self._add_legend_handle(artists[0], label, index=i)\n\n    @classmethod\n    def _plot(cls, ax, y, style=None, bins=None, bottom=0, column_num=0, stacking_id=None, **kwds):\n        if column_num == 0:\n            cls._initialize_stacker(ax, stacking_id, len(bins) - 1)\n\n        base = np.zeros(len(bins) - 1)\n        bottom = bottom + cls._get_stacked_values(ax, stacking_id, base, kwds[\"label\"])\n\n        # Since the counts were computed already, we use them as weights and just generate\n        # one entry for each bin\n        n, bins, patches = ax.hist(bins[:-1], bins=bins, bottom=bottom, weights=y, **kwds)\n\n        cls._update_stacker(ax, stacking_id, n)\n        return patches\n\n    @staticmethod\n    def _get_bins(sdf, bins):\n        # 'data' is a Spark DataFrame that selects all columns.\n        if len(sdf.columns) > 1:\n            min_col = F.least(*map(F.min, sdf))\n            max_col = F.greatest(*map(F.max, sdf))\n        else:\n            min_col = F.min(sdf.columns[-1])\n            max_col = F.max(sdf.columns[-1])\n        boundaries = sdf.select(min_col, max_col).first()\n\n        # divides the boundaries into bins\n        if boundaries[0] == boundaries[1]:\n            boundaries = (boundaries[0] - 0.5, boundaries[1] + 0.5)\n\n        return np.linspace(boundaries[0], boundaries[1], bins + 1)\n\n    @staticmethod\n    def _compute_hist(sdf, bins):\n        # 'data' is a Spark DataFrame that selects one column.\n        assert isinstance(bins, (np.ndarray, np.generic))\n\n        colname = sdf.columns[-1]\n\n        bucket_name = \"__{}_bucket\".format(colname)\n        # creates a Bucketizer to get corresponding bin of each value\n        bucketizer = Bucketizer(\n            splits=bins, inputCol=colname, outputCol=bucket_name, handleInvalid=\"skip\"\n        )\n        # after bucketing values, groups and counts them\n        result = (\n            bucketizer.transform(sdf)\n            .select(bucket_name)\n            .groupby(bucket_name)\n            .agg(F.count(\"*\").alias(\"count\"))\n            .toPandas()\n            .sort_values(by=bucket_name)\n        )\n\n        # generates a pandas DF with one row for each bin\n        # we need this as some of the bins may be empty\n        indexes = pd.DataFrame({bucket_name: np.arange(0, len(bins) - 1), \"bucket\": bins[:-1]})\n        # merges the bins with counts on it and fills remaining ones with zeros\n        pdf = indexes.merge(result, how=\"left\", on=[bucket_name]).fillna(0)[[\"count\"]]\n        pdf.columns = [bucket_name]\n\n        return pdf[bucket_name]\n\n\nclass KoalasPiePlot(PiePlot, TopNPlot):\n    def __init__(self, data, **kwargs):\n        super(KoalasPiePlot, self).__init__(self.get_top_n(data), **kwargs)\n\n    def _make_plot(self):\n        self.set_result_text(self._get_ax(0))\n        super(KoalasPiePlot, self)._make_plot()\n\n\nclass KoalasAreaPlot(AreaPlot, SampledPlot):\n    def __init__(self, data, **kwargs):\n        super(KoalasAreaPlot, self).__init__(self.get_sampled(data), **kwargs)\n\n    def _make_plot(self):\n        self.set_result_text(self._get_ax(0))\n        super(KoalasAreaPlot, self)._make_plot()\n\n\nclass KoalasLinePlot(LinePlot, SampledPlot):\n    def __init__(self, data, **kwargs):\n        super(KoalasLinePlot, self).__init__(self.get_sampled(data), **kwargs)\n\n    def _make_plot(self):\n        self.set_result_text(self._get_ax(0))\n        super(KoalasLinePlot, self)._make_plot()\n\n\nclass KoalasBarhPlot(BarhPlot, TopNPlot):\n    def __init__(self, data, **kwargs):\n        super(KoalasBarhPlot, self).__init__(self.get_top_n(data), **kwargs)\n\n    def _make_plot(self):\n        self.set_result_text(self._get_ax(0))\n        super(KoalasBarhPlot, self)._make_plot()\n\n\nclass KoalasScatterPlot(ScatterPlot, TopNPlot):\n    def __init__(self, data, x, y, **kwargs):\n        super().__init__(self.get_top_n(data), x, y, **kwargs)\n\n    def _make_plot(self):\n        self.set_result_text(self._get_ax(0))\n        super(KoalasScatterPlot, self)._make_plot()\n\n\nclass KoalasKdePlot(KdePlot):\n    def _compute_plot_data(self):\n        from databricks.koalas.series import Series\n\n        data = self.data\n        if isinstance(data, Series):\n            data = data.to_frame()\n\n        numeric_data = data.select_dtypes(\n            include=[\"byte\", \"decimal\", \"integer\", \"float\", \"long\", \"double\", np.datetime64]\n        )\n\n        # no empty frames or series allowed\n        if len(numeric_data.columns) == 0:\n            raise TypeError(\n                \"Empty {0!r}: no numeric data to \" \"plot\".format(numeric_data.__class__.__name__)\n            )\n\n        self.data = numeric_data\n\n    def _make_plot(self):\n        # 'num_colors' requires to calculate `shape` which has to count all.\n        # Use 1 for now to save the computation.\n        colors = self._get_colors(num_colors=1)\n        stacking_id = self._get_stacking_id()\n\n        sdf = self.data._internal.spark_frame\n\n        for i, label in enumerate(self.data._internal.column_labels):\n            # 'y' is a Spark DataFrame that selects one column.\n            y = sdf.select(self.data._internal.spark_column_for(label))\n            ax = self._get_ax(i)\n\n            kwds = self.kwds.copy()\n\n            label = pprint_thing(label if len(label) > 1 else label[0])\n            kwds[\"label\"] = label\n\n            style, kwds = self._apply_style_colors(colors, kwds, i, label)\n            if style is not None:\n                kwds[\"style\"] = style\n\n            kwds = self._make_plot_keywords(kwds, y)\n            artists = self._plot(ax, y, column_num=i, stacking_id=stacking_id, **kwds)\n            self._add_legend_handle(artists[0], label, index=i)\n\n    def _get_ind(self, y):\n        # 'y' is a Spark DataFrame that selects one column.\n        if self.ind is None:\n            min_val, max_val = y.select(F.min(y.columns[-1]), F.max(y.columns[-1])).first()\n\n            sample_range = max_val - min_val\n            ind = np.linspace(min_val - 0.5 * sample_range, max_val + 0.5 * sample_range, 1000,)\n        elif is_integer(self.ind):\n            min_val, max_val = y.select(F.min(y.columns[-1]), F.max(y.columns[-1])).first()\n\n            sample_range = np.nanmax(y) - np.nanmin(y)\n            ind = np.linspace(min_val - 0.5 * sample_range, max_val + 0.5 * sample_range, self.ind,)\n        else:\n            ind = self.ind\n        return ind\n\n    @classmethod\n    def _plot(\n        cls, ax, y, style=None, bw_method=None, ind=None, column_num=None, stacking_id=None, **kwds\n    ):\n        # 'y' is a Spark DataFrame that selects one column.\n\n        # Using RDD is slow so we might have to change it to Dataset based implementation\n        # once Spark has that implementation.\n        sample = y.rdd.map(lambda x: float(x[0]))\n        kd = KernelDensity()\n        kd.setSample(sample)\n\n        assert isinstance(bw_method, (int, float)), \"'bw_method' must be set as a scalar number.\"\n\n        if bw_method is not None:\n            # Match the bandwidth with Spark.\n            kd.setBandwidth(float(bw_method))\n        y = kd.estimate(list(map(float, ind)))\n        lines = MPLPlot._plot(ax, ind, y, style=style, **kwds)\n        return lines\n\n\n_klasses = [\n    KoalasHistPlot,\n    KoalasBarPlot,\n    KoalasBoxPlot,\n    KoalasPiePlot,\n    KoalasAreaPlot,\n    KoalasLinePlot,\n    KoalasBarhPlot,\n    KoalasScatterPlot,\n    KoalasKdePlot,\n]\n_plot_klass = {getattr(klass, \"_kind\"): klass for klass in _klasses}\n\n\ndef plot_series(\n    data,\n    kind=\"line\",\n    ax=None,  # Series unique\n    figsize=None,\n    use_index=True,\n    title=None,\n    grid=None,\n    legend=False,\n    style=None,\n    logx=False,\n    logy=False,\n    loglog=False,\n    xticks=None,\n    yticks=None,\n    xlim=None,\n    ylim=None,\n    rot=None,\n    fontsize=None,\n    colormap=None,\n    table=False,\n    yerr=None,\n    xerr=None,\n    label=None,\n    secondary_y=False,  # Series unique\n    **kwds\n):\n    \"\"\"\n    Make plots of Series using matplotlib / pylab.\n\n    Each plot kind has a corresponding method on the\n    ``Series.plot`` accessor:\n    ``s.plot(kind='line')`` is equivalent to\n    ``s.plot.line()``.\n\n    Parameters\n    ----------\n    data : Series\n\n    kind : str\n        - 'line' : line plot (default)\n        - 'bar' : vertical bar plot\n        - 'barh' : horizontal bar plot\n        - 'hist' : histogram\n        - 'box' : boxplot\n        - 'kde' : Kernel Density Estimation plot\n        - 'density' : same as 'kde'\n        - 'area' : area plot\n        - 'pie' : pie plot\n\n    ax : matplotlib axes object\n        If not passed, uses gca()\n    figsize : a tuple (width, height) in inches\n    use_index : boolean, default True\n        Use index as ticks for x axis\n    title : string or list\n        Title to use for the plot. If a string is passed, print the string at\n        the top of the figure. If a list is passed and `subplots` is True,\n        print each item in the list above the corresponding subplot.\n    grid : boolean, default None (matlab style default)\n        Axis grid lines\n    legend : False/True/'reverse'\n        Place legend on axis subplots\n    style : list or dict\n        matplotlib line style per column\n    logx : boolean, default False\n        Use log scaling on x axis\n    logy : boolean, default False\n        Use log scaling on y axis\n    loglog : boolean, default False\n        Use log scaling on both x and y axes\n    xticks : sequence\n        Values to use for the xticks\n    yticks : sequence\n        Values to use for the yticks\n    xlim : 2-tuple/list\n    ylim : 2-tuple/list\n    rot : int, default None\n        Rotation for ticks (xticks for vertical, yticks for horizontal plots)\n    fontsize : int, default None\n        Font size for xticks and yticks\n    colormap : str or matplotlib colormap object, default None\n        Colormap to select colors from. If string, load colormap with that name\n        from matplotlib.\n    colorbar : boolean, optional\n        If True, plot colorbar (only relevant for 'scatter' and 'hexbin' plots)\n    position : float\n        Specify relative alignments for bar plot layout.\n        From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    table : boolean, Series or DataFrame, default False\n        If True, draw a table using the data in the DataFrame and the data will\n        be transposed to meet matplotlib's default layout.\n        If a Series or DataFrame is passed, use passed data to draw a table.\n    yerr : DataFrame, Series, array-like, dict and str\n        See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n        detail.\n    xerr : same types as yerr.\n    label : label argument to provide to plot\n    secondary_y : boolean or sequence of ints, default False\n        If True then y-axis will be on the right\n    mark_right : boolean, default True\n        When using a secondary_y axis, automatically mark the column\n        labels with \"(right)\" in the legend\n    **kwds : keywords\n        Options to pass to matplotlib plotting method\n\n    Returns\n    -------\n    axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n    Notes\n    -----\n\n    - See matplotlib documentation online for more on this subject\n    - If `kind` = 'bar' or 'barh', you can specify relative alignments\n      for bar plot layout by `position` keyword.\n      From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    \"\"\"\n\n    # function copied from pandas.plotting._core\n    # so it calls modified _plot below\n\n    import matplotlib.pyplot as plt\n\n    if ax is None and len(plt.get_fignums()) > 0:\n        ax = None\n        with plt.rc_context():\n            ax = plt.gca()\n        ax = MPLPlot._get_ax_layer(ax)\n    return _plot(\n        data,\n        kind=kind,\n        ax=ax,\n        figsize=figsize,\n        use_index=use_index,\n        title=title,\n        grid=grid,\n        legend=legend,\n        style=style,\n        logx=logx,\n        logy=logy,\n        loglog=loglog,\n        xticks=xticks,\n        yticks=yticks,\n        xlim=xlim,\n        ylim=ylim,\n        rot=rot,\n        fontsize=fontsize,\n        colormap=colormap,\n        table=table,\n        yerr=yerr,\n        xerr=xerr,\n        label=label,\n        secondary_y=secondary_y,\n        **kwds\n    )\n\n\ndef plot_frame(\n    data,\n    x=None,\n    y=None,\n    kind=\"line\",\n    ax=None,\n    subplots=None,\n    sharex=None,\n    sharey=False,\n    layout=None,\n    figsize=None,\n    use_index=True,\n    title=None,\n    grid=None,\n    legend=True,\n    style=None,\n    logx=False,\n    logy=False,\n    loglog=False,\n    xticks=None,\n    yticks=None,\n    xlim=None,\n    ylim=None,\n    rot=None,\n    fontsize=None,\n    colormap=None,\n    table=False,\n    yerr=None,\n    xerr=None,\n    secondary_y=False,\n    sort_columns=False,\n    **kwds\n):\n    \"\"\"\n    Make plots of DataFrames using matplotlib / pylab.\n\n    Each plot kind has a corresponding method on the\n    ``DataFrame.plot`` accessor:\n    ``kdf.plot(kind='line')`` is equivalent to\n    ``kdf.plot.line()``.\n\n    Parameters\n    ----------\n    data : DataFrame\n\n    kind : str\n        - 'line' : line plot (default)\n        - 'bar' : vertical bar plot\n        - 'barh' : horizontal bar plot\n        - 'hist' : histogram\n        - 'box' : boxplot\n        - 'kde' : Kernel Density Estimation plot\n        - 'density' : same as 'kde'\n        - 'area' : area plot\n        - 'pie' : pie plot\n        - 'scatter' : scatter plot\n    ax : matplotlib axes object\n        If not passed, uses gca()\n    x : label or position, default None\n    y : label, position or list of label, positions, default None\n        Allows plotting of one column versus another.\n    figsize : a tuple (width, height) in inches\n    use_index : boolean, default True\n        Use index as ticks for x axis\n    title : string or list\n        Title to use for the plot. If a string is passed, print the string at\n        the top of the figure. If a list is passed and `subplots` is True,\n        print each item in the list above the corresponding subplot.\n    grid : boolean, default None (matlab style default)\n        Axis grid lines\n    legend : False/True/'reverse'\n        Place legend on axis subplots\n    style : list or dict\n        matplotlib line style per column\n    logx : boolean, default False\n        Use log scaling on x axis\n    logy : boolean, default False\n        Use log scaling on y axis\n    loglog : boolean, default False\n        Use log scaling on both x and y axes\n    xticks : sequence\n        Values to use for the xticks\n    yticks : sequence\n        Values to use for the yticks\n    xlim : 2-tuple/list\n    ylim : 2-tuple/list\n    sharex: bool or None, default is None\n        Whether to share x axis or not.\n    sharey: bool, default is False\n        Whether to share y axis or not.\n    rot : int, default None\n        Rotation for ticks (xticks for vertical, yticks for horizontal plots)\n    fontsize : int, default None\n        Font size for xticks and yticks\n    colormap : str or matplotlib colormap object, default None\n        Colormap to select colors from. If string, load colormap with that name\n        from matplotlib.\n    colorbar : boolean, optional\n        If True, plot colorbar (only relevant for 'scatter' and 'hexbin' plots)\n    position : float\n        Specify relative alignments for bar plot layout.\n        From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    table : boolean, Series or DataFrame, default False\n        If True, draw a table using the data in the DataFrame and the data will\n        be transposed to meet matplotlib's default layout.\n        If a Series or DataFrame is passed, use passed data to draw a table.\n    yerr : DataFrame, Series, array-like, dict and str\n        See :ref:`Plotting with Error Bars <visualization.errorbars>` for\n        detail.\n    xerr : same types as yerr.\n    label : label argument to provide to plot\n    secondary_y : boolean or sequence of ints, default False\n        If True then y-axis will be on the right\n    mark_right : boolean, default True\n        When using a secondary_y axis, automatically mark the column\n        labels with \"(right)\" in the legend\n    sort_columns: bool, default is False\n        When True, will sort values on plots.\n    **kwds : keywords\n        Options to pass to matplotlib plotting method\n\n    Returns\n    -------\n    axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n    Notes\n    -----\n\n    - See matplotlib documentation online for more on this subject\n    - If `kind` = 'bar' or 'barh', you can specify relative alignments\n      for bar plot layout by `position` keyword.\n      From 0 (left/bottom-end) to 1 (right/top-end). Default is 0.5 (center)\n    \"\"\"\n\n    return _plot(\n        data,\n        kind=kind,\n        x=x,\n        y=y,\n        ax=ax,\n        figsize=figsize,\n        use_index=use_index,\n        title=title,\n        grid=grid,\n        legend=legend,\n        subplots=subplots,\n        style=style,\n        logx=logx,\n        logy=logy,\n        loglog=loglog,\n        xticks=xticks,\n        yticks=yticks,\n        xlim=xlim,\n        ylim=ylim,\n        rot=rot,\n        fontsize=fontsize,\n        colormap=colormap,\n        table=table,\n        yerr=yerr,\n        xerr=xerr,\n        sharex=sharex,\n        sharey=sharey,\n        secondary_y=secondary_y,\n        layout=layout,\n        sort_columns=sort_columns,\n        **kwds\n    )\n\n\ndef _plot(data, x=None, y=None, subplots=False, ax=None, kind=\"line\", **kwds):\n    from databricks.koalas import DataFrame\n\n    # function copied from pandas.plotting._core\n    # and adapted to handle Koalas DataFrame and Series\n\n    kind = kind.lower().strip()\n    kind = {\"density\": \"kde\"}.get(kind, kind)\n    if kind in _all_kinds:\n        klass = _plot_klass[kind]\n    else:\n        raise ValueError(\"%r is not a valid plot kind\" % kind)\n\n    # scatter and hexbin are inherited from PlanePlot which require x and y\n    if kind in (\"scatter\", \"hexbin\"):\n        plot_obj = klass(data, x, y, subplots=subplots, ax=ax, kind=kind, **kwds)\n    else:\n\n        # check data type and do preprocess before applying plot\n        if isinstance(data, DataFrame):\n            if x is not None:\n                data = data.set_index(x)\n            # TODO: check if value of y is plottable\n            if y is not None:\n                data = data[y]\n\n        plot_obj = klass(data, subplots=subplots, ax=ax, kind=kind, **kwds)\n    plot_obj.generate()\n    plot_obj.draw()\n    return plot_obj.result\n\n\nclass KoalasSeriesPlotMethods(PandasObject):\n    \"\"\"\n    Series plotting accessor and method.\n\n    Plotting methods can also be accessed by calling the accessor as a method\n    with the ``kind`` argument:\n    ``s.plot(kind='hist')`` is equivalent to ``s.plot.hist()``\n    \"\"\"\n\n    def __init__(self, data):\n        self.data = data\n\n    def __call__(\n        self,\n        kind=\"line\",\n        ax=None,\n        figsize=None,\n        use_index=True,\n        title=None,\n        grid=None,\n        legend=False,\n        style=None,\n        logx=False,\n        logy=False,\n        loglog=False,\n        xticks=None,\n        yticks=None,\n        xlim=None,\n        ylim=None,\n        rot=None,\n        fontsize=None,\n        colormap=None,\n        table=False,\n        yerr=None,\n        xerr=None,\n        label=None,\n        secondary_y=False,\n        **kwds\n    ):\n        return plot_series(\n            self.data,\n            kind=kind,\n            ax=ax,\n            figsize=figsize,\n            use_index=use_index,\n            title=title,\n            grid=grid,\n            legend=legend,\n            style=style,\n            logx=logx,\n            logy=logy,\n            loglog=loglog,\n            xticks=xticks,\n            yticks=yticks,\n            xlim=xlim,\n            ylim=ylim,\n            rot=rot,\n            fontsize=fontsize,\n            colormap=colormap,\n            table=table,\n            yerr=yerr,\n            xerr=xerr,\n            label=label,\n            secondary_y=secondary_y,\n            **kwds\n        )\n\n    __call__.__doc__ = plot_series.__doc__\n\n    def line(self, x=None, y=None, **kwargs):\n        \"\"\"\n        Plot Series as lines.\n\n        This function is useful to plot lines using Series's values\n        as coordinates.\n\n        Parameters\n        ----------\n        x : int or str, optional\n            Columns to use for the horizontal axis.\n            Either the location or the label of the columns to be used.\n            By default, it will use the DataFrame indices.\n        y : int, str, or list of them, optional\n            The values to be plotted.\n            Either the location or the label of the columns to be used.\n            By default, it will use the remaining DataFrame numeric columns.\n        **kwds\n            Keyword arguments to pass on to :meth:`Series.plot`.\n\n        Returns\n        -------\n        :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\n            Return an ndarray when ``subplots=True``.\n\n        See Also\n        --------\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = ks.Series([1, 3, 2])\n            >>> ax = s.plot.line()\n        \"\"\"\n        return self(kind=\"line\", x=x, y=y, **kwargs)\n\n    def bar(self, **kwds):\n        \"\"\"\n        Vertical bar plot.\n\n        Parameters\n        ----------\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`Koalas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = ks.Series([1, 3, 2])\n            >>> ax = s.plot.bar()\n        \"\"\"\n        return self(kind=\"bar\", **kwds)\n\n    def barh(self, **kwds):\n        \"\"\"\n        Make a horizontal bar plot.\n\n        A horizontal bar plot is a plot that presents quantitative data with\n        rectangular bars with lengths proportional to the values that they\n        represent. A bar plot shows comparisons among discrete categories. One\n        axis of the plot shows the specific categories being compared, and the\n        other axis represents a measured value.\n\n        Parameters\n        ----------\n        x : label or position, default DataFrame.index\n            Column to be used for categories.\n        y : label or position, default All numeric columns in dataframe\n            Columns to be plotted from the DataFrame.\n        **kwds\n            Keyword arguments to pass on to :meth:`databricks.koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        See Also\n        --------\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\n\n        Examples\n        --------\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n            >>> plot = df.val.plot.barh()\n        \"\"\"\n        return self(kind=\"barh\", **kwds)\n\n    def box(self, **kwds):\n        \"\"\"\n        Make a box plot of the DataFrame columns.\n\n        Parameters\n        ----------\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`Koalas.Series.plot`.\n\n        precision: scalar, default = 0.01\n            This argument is used by Koalas to compute approximate statistics\n            for building a boxplot. Use *smaller* values to get more precise\n            statistics.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        Notes\n        -----\n        There are behavior differences between Koalas and pandas.\n\n        * Koalas computes approximate statistics - expect differences between\n          pandas and Koalas boxplots, especially regarding 1st and 3rd quartiles.\n        * The `whis` argument is only supported as a single number.\n        * Koalas doesn't support the following argument(s).\n\n          * `bootstrap` argument is not supported\n          * `autorange` argument is not supported\n\n        Examples\n        --------\n        Draw a box plot from a DataFrame with four columns of randomly\n        generated data.\n\n        .. plot::\n            :context: close-figs\n\n            >>> data = np.random.randn(25, 4)\n            >>> df = ks.DataFrame(data, columns=list('ABCD'))\n            >>> ax = df['A'].plot.box()\n        \"\"\"\n        return self(kind=\"box\", **kwds)\n\n    def hist(self, bins=10, **kwds):\n        \"\"\"\n        Draw one histogram of the DataFrame\u2019s columns.\n\n        Parameters\n        ----------\n        bins : integer, default 10\n            Number of histogram bins to be used\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`Koalas.Series.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = ks.Series([1, 3, 2])\n            >>> ax = s.plot.hist()\n        \"\"\"\n        return self(kind=\"hist\", bins=bins, **kwds)\n\n    def kde(self, bw_method=None, ind=None, **kwargs):\n        \"\"\"\n        Generate Kernel Density Estimate plot using Gaussian kernels.\n\n        Parameters\n        ----------\n        bw_method : scalar\n            The method used to calculate the estimator bandwidth.\n            See KernelDensity in PySpark for more information.\n        ind : NumPy array or integer, optional\n            Evaluation points for the estimated PDF. If None (default),\n            1000 equally spaced points are used. If `ind` is a NumPy array, the\n            KDE is evaluated at the points passed. If `ind` is an integer,\n            `ind` number of equally spaced points are used.\n        **kwargs : optional\n            Keyword arguments to pass on to :meth:`Koalas.Series.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or numpy.ndarray of them\n\n        Examples\n        --------\n        A scalar bandwidth should be specified. Using a small bandwidth value can\n        lead to over-fitting, while using a large bandwidth value may result\n        in under-fitting:\n\n        .. plot::\n            :context: close-figs\n\n            >>> s = ks.Series([1, 2, 2.5, 3, 3.5, 4, 5])\n            >>> ax = s.plot.kde(bw_method=0.3)\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = s.plot.kde(bw_method=3)\n\n        The `ind` parameter determines the evaluation points for the\n        plot of the estimated KDF:\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = s.plot.kde(ind=[1, 2, 3, 4, 5], bw_method=0.3)\n        \"\"\"\n        return self(kind=\"kde\", bw_method=bw_method, ind=ind, **kwargs)\n\n    density = kde\n\n    def area(self, **kwds):\n        \"\"\"\n        Draw a stacked area plot.\n\n        An area plot displays quantitative data visually.\n        This function wraps the matplotlib area function.\n\n        Parameters\n        ----------\n        x : label or position, optional\n            Coordinates for the X axis. By default uses the index.\n        y : label or position, optional\n            Column to plot. By default uses all columns.\n        stacked : bool, default True\n            Area plots are stacked by default. Set to False to create a\n            unstacked plot.\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or numpy.ndarray\n            Area plot, or array of area plots if subplots is True.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({\n            ...     'sales': [3, 2, 3, 9, 10, 6],\n            ...     'signups': [5, 5, 6, 12, 14, 13],\n            ...     'visits': [20, 42, 28, 62, 81, 50],\n            ... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\n            ...                        freq='M'))\n            >>> plot = df.sales.plot.area()\n        \"\"\"\n        return self(kind=\"area\", **kwds)\n\n    def pie(self, **kwds):\n        \"\"\"\n        Generate a pie plot.\n\n        A pie plot is a proportional representation of the numerical data in a\n        column. This function wraps :meth:`matplotlib.pyplot.pie` for the\n        specified column. If no column reference is passed and\n        ``subplots=True`` a pie plot is drawn for each numerical column\n        independently.\n\n        Parameters\n        ----------\n        y : int or label, optional\n            Label or position of the column to plot.\n            If not provided, ``subplots=True`` argument must be passed.\n        **kwds\n            Keyword arguments to pass on to :meth:`Koalas.Series.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or np.ndarray of them\n            A NumPy array is returned when `subplots` is True.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({'mass': [0.330, 4.87, 5.97],\n            ...                    'radius': [2439.7, 6051.8, 6378.1]},\n            ...                   index=['Mercury', 'Venus', 'Earth'])\n            >>> plot = df.mass.plot.pie(figsize=(5, 5))\n\n        .. plot::\n            :context: close-figs\n\n            >>> plot = df.mass.plot.pie(subplots=True, figsize=(6, 3))\n        \"\"\"\n        return self(kind=\"pie\", **kwds)\n\n\nclass KoalasFramePlotMethods(PandasObject):\n    # TODO: not sure if Koalas wants to combine plot method for Series and DataFrame\n    \"\"\"\n    DataFrame plotting accessor and method.\n\n    Plotting methods can also be accessed by calling the accessor as a method\n    with the ``kind`` argument:\n    ``df.plot(kind='hist')`` is equivalent to ``df.plot.hist()``\n    \"\"\"\n\n    def __init__(self, data):\n        self.data = data\n\n    def __call__(\n        self,\n        x=None,\n        y=None,\n        kind=\"line\",\n        ax=None,\n        subplots=None,\n        sharex=None,\n        sharey=False,\n        layout=None,\n        figsize=None,\n        use_index=True,\n        title=None,\n        grid=None,\n        legend=True,\n        style=None,\n        logx=False,\n        logy=False,\n        loglog=False,\n        xticks=None,\n        yticks=None,\n        xlim=None,\n        ylim=None,\n        rot=None,\n        fontsize=None,\n        colormap=None,\n        table=False,\n        yerr=None,\n        xerr=None,\n        secondary_y=False,\n        sort_columns=False,\n        **kwds\n    ):\n        return plot_frame(\n            self.data,\n            x=x,\n            y=y,\n            kind=kind,\n            ax=ax,\n            subplots=subplots,\n            sharex=sharex,\n            sharey=sharey,\n            layout=layout,\n            figsize=figsize,\n            use_index=use_index,\n            title=title,\n            grid=grid,\n            legend=legend,\n            style=style,\n            logx=logx,\n            logy=logy,\n            loglog=loglog,\n            xticks=xticks,\n            yticks=yticks,\n            xlim=xlim,\n            ylim=ylim,\n            rot=rot,\n            fontsize=fontsize,\n            colormap=colormap,\n            table=table,\n            yerr=yerr,\n            xerr=xerr,\n            secondary_y=secondary_y,\n            sort_columns=sort_columns,\n            **kwds\n        )\n\n    def line(self, x=None, y=None, **kwargs):\n        \"\"\"\n        Plot DataFrame as lines.\n\n        Parameters\n        ----------\n        x: int or str, optional\n            Columns to use for the horizontal axis.\n        y : int, str, or list of them, optional\n            The values to be plotted.\n        **kwargs\n            Keyword arguments to pass on to :meth:`DataFrame.plot`.\n\n        Returns\n        -------\n        :class:`matplotlib.axes.Axes` or :class:`numpy.ndarray`\n            Return an ndarray when ``subplots=True``.\n\n        See Also\n        --------\n        matplotlib.pyplot.plot : Plot y versus x as lines and/or markers.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the populations for some animals\n            over the years.\n\n            >>> df = ks.DataFrame({'pig': [20, 18, 489, 675, 1776],\n            ...                    'horse': [4, 25, 281, 600, 1900]},\n            ...                   index=[1990, 1997, 2003, 2009, 2014])\n            >>> lines = df.plot.line()\n\n        .. plot::\n            :context: close-figs\n\n            An example with subplots, so an array of axes is returned.\n\n            >>> axes = df.plot.line(subplots=True)\n            >>> type(axes)\n            <class 'numpy.ndarray'>\n\n        .. plot::\n            :context: close-figs\n\n            The following example shows the relationship between both\n            populations.\n\n            >>> lines = df.plot.line(x='pig', y='horse')\n        \"\"\"\n        return self(kind=\"line\", x=x, y=y, **kwargs)\n\n    def kde(self, bw_method=None, ind=None, **kwargs):\n        \"\"\"\n        Generate Kernel Density Estimate plot using Gaussian kernels.\n\n        Parameters\n        ----------\n        bw_method : scalar\n            The method used to calculate the estimator bandwidth.\n            See KernelDensity in PySpark for more information.\n        ind : NumPy array or integer, optional\n            Evaluation points for the estimated PDF. If None (default),\n            1000 equally spaced points are used. If `ind` is a NumPy array, the\n            KDE is evaluated at the points passed. If `ind` is an integer,\n            `ind` number of equally spaced points are used.\n        **kwargs : optional\n            Keyword arguments to pass on to :meth:`Koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or numpy.ndarray of them\n\n        Examples\n        --------\n        For DataFrame, it works in the same way as Series:\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({\n            ...     'x': [1, 2, 2.5, 3, 3.5, 4, 5],\n            ...     'y': [4, 4, 4.5, 5, 5.5, 6, 6],\n            ... })\n            >>> ax = df.plot.kde(bw_method=0.3)\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.kde(bw_method=3)\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.kde(ind=[1, 2, 3, 4, 5, 6], bw_method=0.3)\n        \"\"\"\n        return self(kind=\"kde\", bw_method=bw_method, ind=ind, **kwargs)\n\n    density = kde\n\n    def pie(self, y=None, **kwds):\n        \"\"\"\n        Generate a pie plot.\n        A pie plot is a proportional representation of the numerical data in a\n        column. This function wraps :meth:`matplotlib.pyplot.pie` for the\n        specified column. If no column reference is passed and\n        ``subplots=True`` a pie plot is drawn for each numerical column\n        independently.\n\n        Parameters\n        ----------\n        y : int or label, optional\n            Label or position of the column to plot.\n            If not provided, ``subplots=True`` argument must be passed.\n        **kwds\n            Keyword arguments to pass on to :meth:`Koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or np.ndarray of them\n            A NumPy array is returned when `subplots` is True.\n\n        Examples\n        --------\n        In the example below we have a DataFrame with the information about\n        planet's mass and radius. We pass the the 'mass' column to the\n        pie function to get a pie plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({'mass': [0.330, 4.87, 5.97],\n            ...                    'radius': [2439.7, 6051.8, 6378.1]},\n            ...                   index=['Mercury', 'Venus', 'Earth'])\n            >>> plot = df.plot.pie(y='mass', figsize=(5, 5))\n\n        .. plot::\n            :context: close-figs\n\n            >>> plot = df.plot.pie(subplots=True, figsize=(6, 3))\n        \"\"\"\n        from databricks.koalas import DataFrame\n\n        # pandas will raise an error if y is None and subplots if not True\n        if isinstance(self.data, DataFrame) and y is None and not kwds.get(\"subplots\", False):\n            raise ValueError(\"pie requires either y column or 'subplots=True'\")\n        return self(kind=\"pie\", y=y, **kwds)\n\n    def area(self, x=None, y=None, stacked=True, **kwds):\n        \"\"\"\n        Draw a stacked area plot.\n\n        An area plot displays quantitative data visually.\n        This function wraps the matplotlib area function.\n\n        Parameters\n        ----------\n        x : label or position, optional\n            Coordinates for the X axis. By default uses the index.\n        y : label or position, optional\n            Column to plot. By default uses all columns.\n        stacked : bool, default True\n            Area plots are stacked by default. Set to False to create a\n            unstacked plot.\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`DataFrame.plot`.\n\n        Returns\n        -------\n        matplotlib.axes.Axes or numpy.ndarray\n            Area plot, or array of area plots if subplots is True.\n\n        Examples\n        --------\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({\n            ...     'sales': [3, 2, 3, 9, 10, 6],\n            ...     'signups': [5, 5, 6, 12, 14, 13],\n            ...     'visits': [20, 42, 28, 62, 81, 50],\n            ... }, index=pd.date_range(start='2018/01/01', end='2018/07/01',\n            ...                        freq='M'))\n            >>> plot = df.plot.area()\n        \"\"\"\n        return self(kind=\"area\", x=x, y=y, stacked=stacked, **kwds)\n\n    def bar(self, x=None, y=None, **kwds):\n        \"\"\"\n        Vertical bar plot.\n\n        Parameters\n        ----------\n        x : label or position, optional\n            Allows plotting of one column versus another.\n            If not specified, the index of the DataFrame is used.\n        y : label or position, optional\n            Allows plotting of one column versus another.\n            If not specified, all numerical columns are used.\n        **kwds : optional\n            Additional keyword arguments are documented in\n            :meth:`Koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        axes : :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        Examples\n        --------\n        Basic plot.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n            >>> ax = df.plot.bar(x='lab', y='val', rot=0)\n\n        Plot a whole dataframe to a bar plot. Each column is assigned a\n        distinct color, and each row is nested in a group along the\n        horizontal axis.\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = ks.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.bar(rot=0)\n\n        Instead of nesting, the figure can be split by column with\n        ``subplots=True``. In this case, a :class:`numpy.ndarray` of\n        :class:`matplotlib.axes.Axes` are returned.\n\n        .. plot::\n            :context: close-figs\n\n            >>> axes = df.plot.bar(rot=0, subplots=True)\n            >>> axes[1].legend(loc=2)  # doctest: +SKIP\n\n        Plot a single column.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(y='speed', rot=0)\n\n        Plot only selected categories for the DataFrame.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax = df.plot.bar(x='lifespan', rot=0)\n        \"\"\"\n        return self(kind=\"bar\", x=x, y=y, **kwds)\n\n    def barh(self, x=None, y=None, **kwargs):\n        \"\"\"\n        Make a horizontal bar plot.\n\n        A horizontal bar plot is a plot that presents quantitative data with rectangular\n        bars with lengths proportional to the values that they represent. A bar plot shows\n        comparisons among discrete categories. One axis of the plot shows the specific\n        categories being compared, and the other axis represents a measured value.\n\n        Parameters\n        ----------\n        x : label or position, default DataFrame.index\n            Column to be used for categories.\n        y : label or position, default All numeric columns in dataframe\n            Columns to be plotted from the DataFrame.\n        **kwds:\n            Keyword arguments to pass on to :meth:`databricks.koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        See Also\n        --------\n        matplotlib.axes.Axes.bar : Plot a vertical bar plot using matplotlib.\n\n        Examples\n        --------\n        Basic example\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame({'lab': ['A', 'B', 'C'], 'val': [10, 30, 20]})\n            >>> ax = df.plot.barh(x='lab', y='val')\n\n        Plot a whole DataFrame to a horizontal bar plot\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = ks.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh()\n\n        Plot a column of the DataFrame to a horizontal bar plot\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = ks.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh(y='speed')\n\n        Plot DataFrame versus the desired column\n\n        .. plot::\n            :context: close-figs\n\n            >>> speed = [0.1, 17.5, 40, 48, 52, 69, 88]\n            >>> lifespan = [2, 8, 70, 1.5, 25, 12, 28]\n            >>> index = ['snail', 'pig', 'elephant',\n            ...          'rabbit', 'giraffe', 'coyote', 'horse']\n            >>> df = ks.DataFrame({'speed': speed,\n            ...                    'lifespan': lifespan}, index=index)\n            >>> ax = df.plot.barh(x='lifespan')\n        \"\"\"\n        return self(kind=\"barh\", x=x, y=y, **kwargs)\n\n    def hexbin(self, **kwds):\n        return unsupported_function(class_name=\"pd.DataFrame\", method_name=\"hexbin\")()\n\n    def box(self, **kwds):\n        return unsupported_function(class_name=\"pd.DataFrame\", method_name=\"box\")()\n\n    def hist(self, bins=10, **kwds):\n        \"\"\"\n        Make a histogram of the DataFrame's.\n        A `histogram`_ is a representation of the distribution of data.\n        This function calls :meth:`matplotlib.pyplot.hist`, on each series in\n        the DataFrame, resulting in one histogram per column.\n\n        .. _histogram: https://en.wikipedia.org/wiki/Histogram\n\n        Parameters\n        ----------\n        bins : integer or sequence, default 10\n            Number of histogram bins to be used. If an integer is given, bins + 1\n            bin edges are calculated and returned. If bins is a sequence, gives\n            bin edges, including left edge of first bin and right edge of last\n            bin. In this case, bins is returned unmodified.\n        **kwds\n            All other plotting keyword arguments to be passed to\n            :meth:`matplotlib.pyplot.hist`.\n\n        Returns\n        -------\n        matplotlib.AxesSubplot or numpy.ndarray of them\n\n        See Also\n        --------\n        matplotlib.pyplot.hist : Plot a histogram using matplotlib.\n\n        Examples\n        --------\n        When we draw a dice 6000 times, we expect to get each value around 1000\n        times. But when we draw two dices and sum the result, the distribution\n        is going to be quite different. A histogram illustrates those\n        distributions.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = pd.DataFrame(\n            ...     np.random.randint(1, 7, 6000),\n            ...     columns=['one'])\n            >>> df['two'] = df['one'] + np.random.randint(1, 7, 6000)\n            >>> df = ks.from_pandas(df)\n            >>> ax = df.plot.hist(bins=12, alpha=0.5)\n        \"\"\"\n        return self(kind=\"hist\", bins=bins, **kwds)\n\n    def scatter(self, x, y, s=None, c=None, **kwds):\n        \"\"\"\n        Create a scatter plot with varying marker point size and color.\n\n        The coordinates of each point are defined by two dataframe columns and\n        filled circles are used to represent each point. This kind of plot is\n        useful to see complex correlations between two variables. Points could\n        be for instance natural 2D coordinates like longitude and latitude in\n        a map or, in general, any pair of metrics that can be plotted against\n        each other.\n\n        Parameters\n        ----------\n        x : int or str\n            The column name or column position to be used as horizontal\n            coordinates for each point.\n        y : int or str\n            The column name or column position to be used as vertical\n            coordinates for each point.\n        s : scalar or array_like, optional\n        c : str, int or array_like, optional\n\n        **kwds: Optional\n            Keyword arguments to pass on to :meth:`databricks.koalas.DataFrame.plot`.\n\n        Returns\n        -------\n        :class:`matplotlib.axes.Axes` or numpy.ndarray of them\n\n        See Also\n        --------\n        matplotlib.pyplot.scatter : Scatter plot using multiple input data\n            formats.\n\n        Examples\n        --------\n        Let's see how to draw a scatter plot using coordinates from the values\n        in a DataFrame's columns.\n\n        .. plot::\n            :context: close-figs\n\n            >>> df = ks.DataFrame([[5.1, 3.5, 0], [4.9, 3.0, 0], [7.0, 3.2, 1],\n            ...                    [6.4, 3.2, 1], [5.9, 3.0, 2]],\n            ...                   columns=['length', 'width', 'species'])\n            >>> ax1 = df.plot.scatter(x='length',\n            ...                       y='width',\n            ...                       c='DarkBlue')\n\n        And now with the color determined by a column as well.\n\n        .. plot::\n            :context: close-figs\n\n            >>> ax2 = df.plot.scatter(x='length',\n            ...                       y='width',\n            ...                       c='species',\n            ...                       colormap='viridis')\n        \"\"\"\n        return self(kind=\"scatter\", x=x, y=y, s=s, c=c, **kwds)\n", "idx": 3, "id": 15779, "msg": "", "proj": "databricks-koalas", "lang": "py"}
{"patch": "@@ -31,6 +31,11 @@ func (h *Handler) Add(c Checker) {\n \th.checkers = append(h.checkers, c)\n }\n \n+// AddFunc adds a new check function to the handler.\n+func (h *Handler) AddFunc(f CheckerFunc) {\n+\th.Add(f)\n+}\n+\n // ServeHTTP returns 200 if it is healthy, 500 otherwise.\n func (h *Handler) ServeHTTP(w http.ResponseWriter, _ *http.Request) {\n \tfor _, c := range h.checkers {", "y": 1, "oldf": "// Copyright 2018 The Go Cloud Development Kit Authors\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\n// Package health provides health check handlers.\npackage health // import \"gocloud.dev/health\"\n\nimport (\n\t\"io\"\n\t\"net/http\"\n)\n\n// Handler is an HTTP handler that reports on the success of an\n// aggregate of Checkers.  The zero value is always healthy.\ntype Handler struct {\n\tcheckers []Checker\n}\n\n// Add adds a new check to the handler.\nfunc (h *Handler) Add(c Checker) {\n\th.checkers = append(h.checkers, c)\n}\n\n// ServeHTTP returns 200 if it is healthy, 500 otherwise.\nfunc (h *Handler) ServeHTTP(w http.ResponseWriter, _ *http.Request) {\n\tfor _, c := range h.checkers {\n\t\tif err := c.CheckHealth(); err != nil {\n\t\t\twriteUnhealthy(w)\n\t\t\treturn\n\t\t}\n\t}\n\twriteHealthy(w)\n}\n\nfunc writeHeaders(statusLen string, w http.ResponseWriter) {\n\tw.Header().Set(\"Content-Length\", statusLen)\n\tw.Header().Set(\"Content-Type\", \"text/plain; charset=utf-8\")\n\tw.Header().Set(\"X-Content-Type-Options\", \"nosniff\")\n}\n\nfunc writeUnhealthy(w http.ResponseWriter) {\n\tconst (\n\t\tstatus    = \"unhealthy\"\n\t\tstatusLen = \"9\"\n\t)\n\n\twriteHeaders(statusLen, w)\n\tw.WriteHeader(http.StatusInternalServerError)\n\tio.WriteString(w, status)\n}\n\n// HandleLive is an http.HandleFunc that handles liveness checks by\n// immediately responding with an HTTP 200 status.\nfunc HandleLive(w http.ResponseWriter, _ *http.Request) {\n\twriteHealthy(w)\n}\n\nfunc writeHealthy(w http.ResponseWriter) {\n\tconst (\n\t\tstatus    = \"ok\"\n\t\tstatusLen = \"2\"\n\t)\n\n\twriteHeaders(statusLen, w)\n\tw.WriteHeader(http.StatusOK)\n\tio.WriteString(w, status)\n}\n\n// Checker wraps the CheckHealth method.\n//\n// CheckHealth returns nil if the resource is healthy, or a non-nil\n// error if the resource is not healthy.  CheckHealth must be safe to\n// call from multiple goroutines.\ntype Checker interface {\n\tCheckHealth() error\n}\n", "idx": 1, "id": 16656, "msg": "Why is this necessary, won't `Add` work directly?", "proj": "google-go-cloud", "lang": "go"}
{"patch": "@@ -311,6 +311,15 @@ func (m *endorsementManager) CollectionByBlockHash(blkHash []byte) *blockEndorse\n \treturn collections\n }\n \n+func (m *endorsementManager) IsMintedByBlockHash(blkHash []byte) bool {\n+\tencodedBlockHash := encodeToString(blkHash)\n+\tcollection, exists := m.collections[encodedBlockHash]\n+\tif !exists {\n+\t\treturn false\n+\t}\n+\treturn collection.isMinted\n+}\n+\n func (m *endorsementManager) Size() int {\n \treturn len(m.collections)\n }", "y": 0, "oldf": "// Copyright (c) 2019 IoTeX Foundation\n// This is an alpha (internal) release and is not suitable for production. This source code is provided 'as is' and no\n// warranties are given as to title or non-infringement, merchantability or fitness for purpose and, to the extent\n// permitted by law, all liability for your use of the code is disclaimed. This source code is governed by Apache\n// License 2.0 that can be found in the LICENSE file.\n\npackage rolldpos\n\nimport (\n\t\"encoding/hex\"\n\t\"time\"\n\n\t\"github.com/golang/protobuf/proto\"\n\t\"github.com/pkg/errors\"\n\t\"go.uber.org/zap\"\n\n\t\"github.com/iotexproject/iotex-core/blockchain/block\"\n\t\"github.com/iotexproject/iotex-core/consensus/scheme/rolldpos/endorsementpb\"\n\t\"github.com/iotexproject/iotex-core/db\"\n\t\"github.com/iotexproject/iotex-core/endorsement\"\n\t\"github.com/iotexproject/iotex-core/pkg/log\"\n)\n\nconst (\n\teManagerNS = \"edm\"\n)\n\nvar (\n\t// ErrExpiredEndorsement indicates that the endorsement is expired\n\tErrExpiredEndorsement = errors.New(\"the endorsement has been replaced or expired\")\n\tstatusKey             = []byte(\"status\")\n)\n\n//EndorsedByMajorityFunc defines a function to give an information of consensus status\ntype EndorsedByMajorityFunc func(blockHash []byte, topics []ConsensusVoteTopic) bool\n\ntype endorserEndorsementCollection struct {\n\tendorsements map[ConsensusVoteTopic]*endorsement.Endorsement\n}\n\nfunc newEndorserEndorsementCollection() *endorserEndorsementCollection {\n\treturn &endorserEndorsementCollection{\n\t\tendorsements: map[ConsensusVoteTopic]*endorsement.Endorsement{},\n\t}\n}\n\nfunc (ee *endorserEndorsementCollection) fromProto(endorserPro *endorsementpb.EndorserEndorsementCollection) error {\n\tee.endorsements = make(map[ConsensusVoteTopic]*endorsement.Endorsement)\n\tfor index := range endorserPro.Topics {\n\t\tendorse := &endorsement.Endorsement{}\n\t\tif err := endorse.LoadProto(endorserPro.Endorsements[index]); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tee.endorsements[ConsensusVoteTopic(endorserPro.Topics[index])] = endorse\n\t}\n\treturn nil\n}\n\nfunc (ee *endorserEndorsementCollection) toProto(endorser string) (*endorsementpb.EndorserEndorsementCollection, error) {\n\teeProto := &endorsementpb.EndorserEndorsementCollection{}\n\teeProto.Endorser = endorser\n\tfor topic, endorse := range ee.endorsements {\n\t\teeProto.Topics = append(eeProto.Topics, uint32(topic))\n\t\tioEndorsement, err := endorse.Proto()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\teeProto.Endorsements = append(eeProto.Endorsements, ioEndorsement)\n\t}\n\treturn eeProto, nil\n}\n\nfunc (ee *endorserEndorsementCollection) AddEndorsement(\n\ttopic ConsensusVoteTopic,\n\ten *endorsement.Endorsement,\n) error {\n\tif e, exists := ee.endorsements[topic]; exists {\n\t\tif e.Timestamp().After(en.Timestamp()) {\n\t\t\treturn ErrExpiredEndorsement\n\t\t}\n\t}\n\tee.endorsements[topic] = en\n\n\treturn nil\n}\n\nfunc (ee *endorserEndorsementCollection) Cleanup(timestamp time.Time) *endorserEndorsementCollection {\n\tcleaned := newEndorserEndorsementCollection()\n\tif e, exists := ee.endorsements[PROPOSAL]; exists {\n\t\tif !e.Timestamp().Before(timestamp) {\n\t\t\tcleaned.endorsements[PROPOSAL] = e\n\t\t}\n\t}\n\tif e, exists := ee.endorsements[LOCK]; exists {\n\t\tif !e.Timestamp().Before(timestamp) {\n\t\t\tcleaned.endorsements[LOCK] = e\n\t\t}\n\t}\n\tif e, exists := ee.endorsements[COMMIT]; exists {\n\t\tcleaned.endorsements[COMMIT] = e\n\t}\n\treturn cleaned\n}\n\nfunc (ee *endorserEndorsementCollection) Endorsement(\n\ttopic ConsensusVoteTopic,\n) *endorsement.Endorsement {\n\treturn ee.endorsements[topic]\n}\n\ntype blockEndorsementCollection struct {\n\tblk       *block.Block\n\tendorsers map[string]*endorserEndorsementCollection\n}\n\nfunc newBlockEndorsementCollection(blk *block.Block) *blockEndorsementCollection {\n\treturn &blockEndorsementCollection{\n\t\tblk:       blk,\n\t\tendorsers: map[string]*endorserEndorsementCollection{},\n\t}\n}\n\nfunc (bc *blockEndorsementCollection) fromProto(blockPro *endorsementpb.BlockEndorsementCollection) error {\n\tbc.endorsers = make(map[string]*endorserEndorsementCollection)\n\tif blockPro.Blk == nil {\n\t\tbc.blk = nil\n\t} else {\n\t\tblk := &block.Block{}\n\t\tif err := blk.ConvertFromBlockPb(blockPro.Blk); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbc.blk = blk\n\t}\n\tfor _, endorsement := range blockPro.BlockMap {\n\t\tee := &endorserEndorsementCollection{}\n\t\tif err := ee.fromProto(endorsement); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbc.endorsers[endorsement.Endorser] = ee\n\t}\n\treturn nil\n}\n\nfunc (bc *blockEndorsementCollection) toProto() (*endorsementpb.BlockEndorsementCollection, error) {\n\tbcProto := &endorsementpb.BlockEndorsementCollection{}\n\tif bc.blk != nil {\n\t\tbcProto.Blk = bc.blk.ConvertToBlockPb()\n\t}\n\n\tfor s, endorse := range bc.endorsers {\n\t\tioEndorsement, err := endorse.toProto(s)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tbcProto.BlockMap = append(bcProto.BlockMap, ioEndorsement)\n\t}\n\treturn bcProto, nil\n}\n\nfunc (bc *blockEndorsementCollection) SetBlock(blk *block.Block) error {\n\tbc.blk = blk\n\treturn nil\n}\n\nfunc (bc *blockEndorsementCollection) Block() *block.Block {\n\treturn bc.blk\n}\n\nfunc (bc *blockEndorsementCollection) AddEndorsement(\n\ttopic ConsensusVoteTopic,\n\ten *endorsement.Endorsement,\n) error {\n\tendorser := en.Endorser().HexString()\n\tee, exists := bc.endorsers[endorser]\n\tif !exists {\n\t\tee = newEndorserEndorsementCollection()\n\t}\n\tif err := ee.AddEndorsement(topic, en); err != nil {\n\t\treturn err\n\t}\n\tbc.endorsers[endorser] = ee\n\n\treturn nil\n}\n\nfunc (bc *blockEndorsementCollection) Endorsement(\n\tendorser string,\n\ttopic ConsensusVoteTopic,\n) *endorsement.Endorsement {\n\tee, exists := bc.endorsers[endorser]\n\tif !exists {\n\t\treturn nil\n\t}\n\treturn ee.Endorsement(topic)\n}\n\nfunc (bc *blockEndorsementCollection) Cleanup(timestamp time.Time) *blockEndorsementCollection {\n\tcleaned := newBlockEndorsementCollection(bc.blk)\n\tfor endorser, ee := range bc.endorsers {\n\t\tcleaned.endorsers[endorser] = ee.Cleanup(timestamp)\n\t}\n\treturn cleaned\n}\n\nfunc (bc *blockEndorsementCollection) Endorsements(\n\ttopics []ConsensusVoteTopic,\n) []*endorsement.Endorsement {\n\tendorsements := []*endorsement.Endorsement{}\n\tfor _, ee := range bc.endorsers {\n\t\tfor _, topic := range topics {\n\t\t\tif en := ee.Endorsement(topic); en != nil {\n\t\t\t\tendorsements = append(endorsements, en)\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t}\n\treturn endorsements\n}\n\ntype endorsementManager struct {\n\tisMajorityFunc EndorsedByMajorityFunc\n\teManagerDB     db.KVStore\n\tcollections    map[string]*blockEndorsementCollection\n}\n\nfunc newEndorsementManager(eManagerDB db.KVStore) (*endorsementManager, error) {\n\tif eManagerDB == nil {\n\t\treturn &endorsementManager{\n\t\t\teManagerDB:  nil,\n\t\t\tcollections: map[string]*blockEndorsementCollection{},\n\t\t}, nil\n\t}\n\tbytes, err := eManagerDB.Get(eManagerNS, statusKey)\n\tswitch errors.Cause(err) {\n\tcase nil:\n\t\t// Get from DB\n\t\tmanager := &endorsementManager{eManagerDB: eManagerDB}\n\t\tmanagerProto := &endorsementpb.EndorsementManager{}\n\t\tif err = proto.Unmarshal(bytes, managerProto); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif err = manager.fromProto(managerProto); err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmanager.eManagerDB = eManagerDB\n\t\treturn manager, nil\n\tcase db.ErrNotExist:\n\t\t// If DB doesn't have any information\n\t\tlog.L().Info(\"First initializing DB\")\n\t\treturn &endorsementManager{\n\t\t\teManagerDB:  eManagerDB,\n\t\t\tcollections: map[string]*blockEndorsementCollection{},\n\t\t}, nil\n\tdefault:\n\t\treturn nil, err\n\t}\n}\n\nfunc (m *endorsementManager) PutEndorsementManagerToDB() error {\n\tmanagerProto, err := m.toProto()\n\tif err != nil {\n\t\treturn err\n\t}\n\tvalBytes, err := proto.Marshal(managerProto)\n\tif err != nil {\n\t\treturn err\n\t}\n\terr = m.eManagerDB.Put(eManagerNS, statusKey, valBytes)\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn nil\n}\n\nfunc (m *endorsementManager) SetIsMarjorityFunc(isMajorityFunc EndorsedByMajorityFunc) {\n\tm.isMajorityFunc = isMajorityFunc\n\treturn\n}\n\nfunc (m *endorsementManager) fromProto(managerPro *endorsementpb.EndorsementManager) error {\n\tm.collections = make(map[string]*blockEndorsementCollection)\n\tfor i, block := range managerPro.BlockEndorsements {\n\t\tbc := &blockEndorsementCollection{}\n\t\tif err := bc.fromProto(block); err != nil {\n\t\t\treturn err\n\t\t}\n\t\tm.collections[managerPro.BlkHash[i]] = bc\n\t}\n\treturn nil\n}\n\nfunc (m *endorsementManager) toProto() (*endorsementpb.EndorsementManager, error) {\n\tmc := &endorsementpb.EndorsementManager{}\n\tfor encodedBlockHash, block := range m.collections {\n\t\tioBlockEndorsement, err := block.toProto()\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tmc.BlkHash = append(mc.BlkHash, encodedBlockHash)\n\t\tmc.BlockEndorsements = append(mc.BlockEndorsements, ioBlockEndorsement)\n\t}\n\treturn mc, nil\n}\n\nfunc (m *endorsementManager) CollectionByBlockHash(blkHash []byte) *blockEndorsementCollection {\n\tencodedBlockHash := encodeToString(blkHash)\n\tcollections, exists := m.collections[encodedBlockHash]\n\tif !exists {\n\t\treturn nil\n\t}\n\treturn collections\n}\n\nfunc (m *endorsementManager) Size() int {\n\treturn len(m.collections)\n}\n\nfunc (m *endorsementManager) SizeWithBlock() int {\n\tsize := 0\n\tfor _, c := range m.collections {\n\t\tif c.Block() != nil {\n\t\t\tsize++\n\t\t}\n\t}\n\treturn size\n}\n\nfunc (m *endorsementManager) RegisterBlock(blk *block.Block) error {\n\tblkHash := blk.HashBlock()\n\tencodedBlockHash := encodeToString(blkHash[:])\n\tif c, exists := m.collections[encodedBlockHash]; exists {\n\t\treturn c.SetBlock(blk)\n\t}\n\tm.collections[encodedBlockHash] = newBlockEndorsementCollection(blk)\n\n\tif m.eManagerDB != nil {\n\t\treturn m.PutEndorsementManagerToDB()\n\t}\n\treturn nil\n}\n\nfunc (m *endorsementManager) AddVoteEndorsement(\n\tvote *ConsensusVote,\n\ten *endorsement.Endorsement,\n) error {\n\tvar beforeVote, afterVote bool\n\tif m.isMajorityFunc != nil {\n\t\tbeforeVote = m.isMajorityFunc(vote.BlockHash(), []ConsensusVoteTopic{vote.Topic()})\n\t}\n\tencoded := encodeToString(vote.BlockHash())\n\tc, exists := m.collections[encoded]\n\tif !exists {\n\t\tc = newBlockEndorsementCollection(nil)\n\t}\n\tif err := c.AddEndorsement(vote.Topic(), en); err != nil {\n\t\treturn err\n\t}\n\tm.collections[encoded] = c\n\n\tif m.eManagerDB != nil && m.isMajorityFunc != nil {\n\t\tafterVote = m.isMajorityFunc(vote.BlockHash(), []ConsensusVoteTopic{vote.Topic()})\n\t\tif !beforeVote && afterVote {\n\t\t\t//put into DB only it changes the status of consensus\n\t\t\treturn m.PutEndorsementManagerToDB()\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (m *endorsementManager) Cleanup(timestamp time.Time) error {\n\tif !timestamp.IsZero() {\n\t\tfor encoded, c := range m.collections {\n\t\t\tm.collections[encoded] = c.Cleanup(timestamp)\n\t\t}\n\t} else {\n\t\tm.collections = map[string]*blockEndorsementCollection{}\n\t}\n\tif m.eManagerDB != nil {\n\t\treturn m.PutEndorsementManagerToDB()\n\t}\n\treturn nil\n}\n\nfunc (m *endorsementManager) Log(\n\tlogger *zap.Logger,\n\tdelegates []string,\n) *zap.Logger {\n\tfor encoded, c := range m.collections {\n\t\tproposalEndorsements := c.Endorsements(\n\t\t\t[]ConsensusVoteTopic{PROPOSAL},\n\t\t)\n\t\tlockEndorsements := c.Endorsements(\n\t\t\t[]ConsensusVoteTopic{LOCK},\n\t\t)\n\t\tcommitEndorsments := c.Endorsements(\n\t\t\t[]ConsensusVoteTopic{COMMIT},\n\t\t)\n\t\treturn logger.With(\n\t\t\tzap.Int(\"numProposals:\"+encoded, len(proposalEndorsements)),\n\t\t\tzap.Int(\"numLocks:\"+encoded, len(lockEndorsements)),\n\t\t\tzap.Int(\"numCommits:\"+encoded, len(commitEndorsments)),\n\t\t)\n\t}\n\treturn logger\n}\n\nfunc encodeToString(hash []byte) string {\n\treturn hex.EncodeToString(hash)\n}\n", "idx": 6, "id": 22486, "msg": "", "proj": "iotexproject-iotex-core", "lang": "go"}
{"patch": "@@ -188,6 +188,14 @@ class SeriesTest(ReusedSQLTestCase, SQLTestUtils):\n             self.kser.last(\"1D\")\n         self.assert_eq(ks_input.last(\"1D\"), pd_input.last(\"1D\"))\n \n+    def test_first(self):\n+        index = pd.date_range(\"2018-04-09\", periods=4, freq=\"2D\")\n+        pd_input = pd.Series([1, 2, 3, 4], index=index)\n+        ks_input = ks.Series([1, 2, 3, 4], index=index)\n+        with self.assertRaises(TypeError):\n+            self.kser.first(\"1D\")\n+        self.assert_eq(ks_input.first(\"1D\"), pd_input.first(\"1D\"))\n+\n     def test_rename(self):\n         pser = pd.Series([1, 2, 3, 4, 5, 6, 7], name=\"x\")\n         kser = ks.from_pandas(pser)", "y": 1, "oldf": "#\n# Copyright (C) 2019 Databricks, Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nimport unittest\nfrom collections import defaultdict\nfrom distutils.version import LooseVersion\nimport inspect\nfrom itertools import product\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nimport pandas as pd\nimport pyspark\nfrom pyspark.ml.linalg import SparseVector\nfrom pyspark.sql import functions as F\n\nfrom databricks import koalas as ks\nfrom databricks.koalas.testing.utils import (\n    ReusedSQLTestCase,\n    SQLTestUtils,\n    SPARK_CONF_ARROW_ENABLED,\n)\nfrom databricks.koalas.exceptions import PandasNotImplementedError\nfrom databricks.koalas.missing.series import MissingPandasLikeSeries\nfrom databricks.koalas.typedef.typehints import (\n    extension_dtypes,\n    extension_dtypes_available,\n    extension_float_dtypes_available,\n    extension_object_dtypes_available,\n)\n\n\nclass SeriesTest(ReusedSQLTestCase, SQLTestUtils):\n    @property\n    def pser(self):\n        return pd.Series([1, 2, 3, 4, 5, 6, 7], name=\"x\")\n\n    @property\n    def kser(self):\n        return ks.from_pandas(self.pser)\n\n    def test_series_ops(self):\n        pser = self.pser\n        kser = self.kser\n\n        self.assert_eq(kser + 1, pser + 1)\n        self.assert_eq(1 + kser, 1 + pser)\n        self.assert_eq(kser + 1 + 10 * kser, pser + 1 + 10 * pser)\n        self.assert_eq(kser + 1 + 10 * kser.index, pser + 1 + 10 * pser.index)\n        self.assert_eq(kser.index + 1 + 10 * kser, pser.index + 1 + 10 * pser)\n\n    def test_series_tuple_name(self):\n        pser = self.pser\n        pser.name = (\"x\", \"a\")\n\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser, pser)\n        self.assert_eq(kser.name, pser.name)\n\n        pser.name = (\"y\", \"z\")\n        kser.name = (\"y\", \"z\")\n\n        self.assert_eq(kser, pser)\n        self.assert_eq(kser.name, pser.name)\n\n    def test_repr_cache_invalidation(self):\n        # If there is any cache, inplace operations should invalidate it.\n        s = ks.range(10)[\"id\"]\n        s.__repr__()\n        s.rename(\"a\", inplace=True)\n        self.assertEqual(s.__repr__(), s.rename(\"a\").__repr__())\n\n    def _check_extension(self, kser, pser):\n        if LooseVersion(\"1.1\") <= LooseVersion(pd.__version__) < LooseVersion(\"1.2.2\"):\n            self.assert_eq(kser, pser, check_exact=False)\n            self.assertTrue(isinstance(kser.dtype, extension_dtypes))\n        else:\n            self.assert_eq(kser, pser)\n\n    @unittest.skipIf(not extension_dtypes_available, \"pandas extension dtypes are not available\")\n    def test_extension_dtypes(self):\n        for pser in [\n            pd.Series([1, 2, None, 4], dtype=\"Int8\"),\n            pd.Series([1, 2, None, 4], dtype=\"Int16\"),\n            pd.Series([1, 2, None, 4], dtype=\"Int32\"),\n            pd.Series([1, 2, None, 4], dtype=\"Int64\"),\n        ]:\n            kser = ks.from_pandas(pser)\n\n            self._check_extension(kser, pser)\n            self._check_extension(kser + F.lit(1).cast(\"byte\"), pser + 1)\n            self._check_extension(kser + kser, pser + pser)\n\n    @unittest.skipIf(\n        not extension_object_dtypes_available, \"pandas extension object dtypes are not available\"\n    )\n    def test_extension_object_dtypes(self):\n        # string\n        pser = pd.Series([\"a\", None, \"c\", \"d\"], dtype=\"string\")\n        kser = ks.from_pandas(pser)\n\n        self._check_extension(kser, pser)\n\n        # boolean\n        pser = pd.Series([True, False, True, None], dtype=\"boolean\")\n        kser = ks.from_pandas(pser)\n\n        self._check_extension(kser, pser)\n        self._check_extension(kser & kser, pser & pser)\n        self._check_extension(kser | kser, pser | pser)\n\n    @unittest.skipIf(\n        not extension_float_dtypes_available, \"pandas extension float dtypes are not available\"\n    )\n    def test_extension_float_dtypes(self):\n        for pser in [\n            pd.Series([1.0, 2.0, None, 4.0], dtype=\"Float32\"),\n            pd.Series([1.0, 2.0, None, 4.0], dtype=\"Float64\"),\n        ]:\n            kser = ks.from_pandas(pser)\n\n            self._check_extension(kser, pser)\n            self._check_extension(kser + 1, pser + 1)\n            self._check_extension(kser + kser, pser + pser)\n\n    def test_empty_series(self):\n        pser_a = pd.Series([], dtype=\"i1\")\n        pser_b = pd.Series([], dtype=\"str\")\n\n        self.assert_eq(ks.from_pandas(pser_a), pser_a)\n\n        kser_b = ks.from_pandas(pser_b)\n        if LooseVersion(pyspark.__version__) >= LooseVersion(\"2.4\"):\n            self.assert_eq(kser_b, pser_b)\n        else:\n            with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n                self.assert_eq(kser_b, pser_b)\n\n        with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n            self.assert_eq(ks.from_pandas(pser_a), pser_a)\n            self.assert_eq(ks.from_pandas(pser_b), pser_b)\n\n    def test_all_null_series(self):\n        pser_a = pd.Series([None, None, None], dtype=\"float64\")\n        pser_b = pd.Series([None, None, None], dtype=\"str\")\n\n        self.assert_eq(ks.from_pandas(pser_a), pser_a)\n\n        kser_b = ks.from_pandas(pser_b)\n        if LooseVersion(pyspark.__version__) >= LooseVersion(\"2.4\"):\n            self.assert_eq(kser_b, pser_b)\n        else:\n            with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n                self.assert_eq(kser_b, pser_b)\n\n        with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n            self.assert_eq(ks.from_pandas(pser_a), pser_a)\n            self.assert_eq(ks.from_pandas(pser_b), pser_b)\n\n    def test_head(self):\n        kser = self.kser\n        pser = self.pser\n\n        self.assert_eq(kser.head(3), pser.head(3))\n        self.assert_eq(kser.head(0), pser.head(0))\n        self.assert_eq(kser.head(-3), pser.head(-3))\n        self.assert_eq(kser.head(-10), pser.head(-10))\n\n    def test_last(self):\n        index = pd.date_range(\"2018-04-09\", periods=4, freq=\"2D\")\n        pd_input = pd.Series([1, 2, 3, 4], index=index)\n        ks_input = ks.Series([1, 2, 3, 4], index=index)\n        with self.assertRaises(TypeError):\n            self.kser.last(\"1D\")\n        self.assert_eq(ks_input.last(\"1D\"), pd_input.last(\"1D\"))\n\n    def test_rename(self):\n        pser = pd.Series([1, 2, 3, 4, 5, 6, 7], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        pser.name = \"renamed\"\n        kser.name = \"renamed\"\n        self.assertEqual(kser.name, \"renamed\")\n        self.assert_eq(kser, pser)\n\n        pser.name = None\n        kser.name = None\n        self.assertEqual(kser.name, None)\n        self.assert_eq(kser, pser)\n\n        pidx = pser.index\n        kidx = kser.index\n        pidx.name = \"renamed\"\n        kidx.name = \"renamed\"\n        self.assertEqual(kidx.name, \"renamed\")\n        self.assert_eq(kidx, pidx)\n\n        expected_error_message = \"Series.name must be a hashable type\"\n        with self.assertRaisesRegex(TypeError, expected_error_message):\n            kser.name = [\"renamed\"]\n        with self.assertRaisesRegex(TypeError, expected_error_message):\n            kser.name = [\"0\", \"1\"]\n        with self.assertRaisesRegex(TypeError, expected_error_message):\n            ks.Series([1, 2, 3], name=[\"0\", \"1\"])\n\n    def test_rename_method(self):\n        # Series name\n        pser = pd.Series([1, 2, 3, 4, 5, 6, 7], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.rename(\"y\"), pser.rename(\"y\"))\n        self.assertEqual(kser.name, \"x\")  # no mutation\n        self.assert_eq(kser.rename(), pser.rename())\n\n        self.assert_eq((kser.rename(\"y\") + 1).head(), (pser.rename(\"y\") + 1).head())\n\n        kser.rename(\"z\", inplace=True)\n        pser.rename(\"z\", inplace=True)\n        self.assertEqual(kser.name, \"z\")\n        self.assert_eq(kser, pser)\n\n        expected_error_message = \"Series.name must be a hashable type\"\n        with self.assertRaisesRegex(TypeError, expected_error_message):\n            kser.rename([\"0\", \"1\"])\n\n        # Series index\n        # pser = pd.Series(['a', 'b', 'c', 'd', 'e', 'f', 'g'], name='x')\n        # kser = ks.from_pandas(s)\n\n        # TODO: index\n        # res = kser.rename(lambda x: x ** 2)\n        # self.assert_eq(res, pser.rename(lambda x: x ** 2))\n\n        # res = kser.rename(pser)\n        # self.assert_eq(res, pser.rename(pser))\n\n        # res = kser.rename(kser)\n        # self.assert_eq(res, pser.rename(pser))\n\n        # res = kser.rename(lambda x: x**2, inplace=True)\n        # self.assertis(res, kser)\n        # s.rename(lambda x: x**2, inplace=True)\n        # self.assert_eq(kser, pser)\n\n    def test_rename_axis(self):\n        index = pd.Index([\"A\", \"B\", \"C\"], name=\"index\")\n        pser = pd.Series([1.0, 2.0, 3.0], index=index, name=\"name\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(\n            pser.rename_axis(\"index2\").sort_index(), kser.rename_axis(\"index2\").sort_index(),\n        )\n\n        self.assert_eq(\n            (pser + 1).rename_axis(\"index2\").sort_index(),\n            (kser + 1).rename_axis(\"index2\").sort_index(),\n        )\n\n        pser2 = pser.copy()\n        kser2 = kser.copy()\n        pser2.rename_axis(\"index2\", inplace=True)\n        kser2.rename_axis(\"index2\", inplace=True)\n        self.assert_eq(pser2.sort_index(), kser2.sort_index())\n\n        self.assertRaises(ValueError, lambda: kser.rename_axis([\"index2\", \"index3\"]))\n        self.assertRaises(TypeError, lambda: kser.rename_axis(mapper=[\"index2\"], index=[\"index3\"]))\n\n        # index/columns parameters and dict_like/functions mappers introduced in pandas 0.24.0\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24.0\"):\n            self.assert_eq(\n                pser.rename_axis(index={\"index\": \"index2\", \"missing\": \"index4\"}).sort_index(),\n                kser.rename_axis(index={\"index\": \"index2\", \"missing\": \"index4\"}).sort_index(),\n            )\n\n            self.assert_eq(\n                pser.rename_axis(index=str.upper).sort_index(),\n                kser.rename_axis(index=str.upper).sort_index(),\n            )\n        else:\n            expected = kser\n            expected.index.name = \"index2\"\n            result = kser.rename_axis(index={\"index\": \"index2\", \"missing\": \"index4\"}).sort_index()\n            self.assert_eq(expected, result)\n\n            expected = kser\n            expected.index.name = \"INDEX\"\n            result = kser.rename_axis(index=str.upper).sort_index()\n            self.assert_eq(expected, result)\n\n        index = pd.MultiIndex.from_tuples(\n            [(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\")], names=[\"index1\", \"index2\"]\n        )\n        pser = pd.Series([1.0, 2.0, 3.0], index=index, name=\"name\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(\n            pser.rename_axis([\"index3\", \"index4\"]).sort_index(),\n            kser.rename_axis([\"index3\", \"index4\"]).sort_index(),\n        )\n\n        self.assertRaises(ValueError, lambda: kser.rename_axis([\"index3\", \"index4\", \"index5\"]))\n\n        # index/columns parameters and dict_like/functions mappers introduced in pandas 0.24.0\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24.0\"):\n            self.assert_eq(\n                pser.rename_axis(\n                    index={\"index1\": \"index3\", \"index2\": \"index4\", \"missing\": \"index5\"}\n                ).sort_index(),\n                kser.rename_axis(\n                    index={\"index1\": \"index3\", \"index2\": \"index4\", \"missing\": \"index5\"}\n                ).sort_index(),\n            )\n\n            self.assert_eq(\n                pser.rename_axis(index=str.upper).sort_index(),\n                kser.rename_axis(index=str.upper).sort_index(),\n            )\n        else:\n            expected = kser\n            expected.index.names = [\"index3\", \"index4\"]\n            result = kser.rename_axis(\n                index={\"index1\": \"index3\", \"index2\": \"index4\", \"missing\": \"index5\"}\n            ).sort_index()\n            self.assert_eq(expected, result)\n\n            expected.index.names = [\"INDEX1\", \"INDEX2\"]\n            result = kser.rename_axis(index=str.upper).sort_index()\n            self.assert_eq(expected, result)\n\n    def test_or(self):\n        pdf = pd.DataFrame(\n            {\n                \"left\": [True, False, True, False, np.nan, np.nan, True, False, np.nan],\n                \"right\": [True, False, False, True, True, False, np.nan, np.nan, np.nan],\n            }\n        )\n        kdf = ks.from_pandas(pdf)\n\n        self.assert_eq(kdf[\"left\"] | kdf[\"right\"], pdf[\"left\"] | pdf[\"right\"])\n        self.assert_eq(kdf[\"left\"] | True, pdf[\"left\"] | True)\n        self.assert_eq(kdf[\"left\"] | False, pdf[\"left\"] | False)\n        self.assert_eq(kdf[\"left\"] | None, pdf[\"left\"] | None)\n        self.assert_eq(True | kdf[\"right\"], True | pdf[\"right\"])\n        self.assert_eq(False | kdf[\"right\"], False | pdf[\"right\"])\n        self.assert_eq(None | kdf[\"right\"], None | pdf[\"right\"])\n\n    @unittest.skipIf(\n        not extension_object_dtypes_available, \"pandas extension object dtypes are not available\"\n    )\n    def test_or_extenstion_dtypes(self):\n        pdf = pd.DataFrame(\n            {\n                \"left\": [True, False, True, False, np.nan, np.nan, True, False, np.nan],\n                \"right\": [True, False, False, True, True, False, np.nan, np.nan, np.nan],\n            }\n        ).astype(\"boolean\")\n        kdf = ks.from_pandas(pdf)\n\n        self._check_extension(kdf[\"left\"] | kdf[\"right\"], pdf[\"left\"] | pdf[\"right\"])\n        self._check_extension(kdf[\"left\"] | True, pdf[\"left\"] | True)\n        self._check_extension(kdf[\"left\"] | False, pdf[\"left\"] | False)\n        self._check_extension(kdf[\"left\"] | pd.NA, pdf[\"left\"] | pd.NA)\n        self._check_extension(True | kdf[\"right\"], True | pdf[\"right\"])\n        self._check_extension(False | kdf[\"right\"], False | pdf[\"right\"])\n        self._check_extension(pd.NA | kdf[\"right\"], pd.NA | pdf[\"right\"])\n\n    def test_and(self):\n        pdf = pd.DataFrame(\n            {\n                \"left\": [True, False, True, False, np.nan, np.nan, True, False, np.nan],\n                \"right\": [True, False, False, True, True, False, np.nan, np.nan, np.nan],\n            }\n        )\n        kdf = ks.from_pandas(pdf)\n\n        self.assert_eq(kdf[\"left\"] & kdf[\"right\"], pdf[\"left\"] & pdf[\"right\"])\n        self.assert_eq(kdf[\"left\"] & True, pdf[\"left\"] & True)\n        self.assert_eq(kdf[\"left\"] & False, pdf[\"left\"] & False)\n        self.assert_eq(kdf[\"left\"] & None, pdf[\"left\"] & None)\n        self.assert_eq(True & kdf[\"right\"], True & pdf[\"right\"])\n        self.assert_eq(False & kdf[\"right\"], False & pdf[\"right\"])\n        self.assert_eq(None & kdf[\"right\"], None & pdf[\"right\"])\n\n    @unittest.skipIf(\n        not extension_object_dtypes_available, \"pandas extension object dtypes are not available\"\n    )\n    def test_and_extenstion_dtypes(self):\n        pdf = pd.DataFrame(\n            {\n                \"left\": [True, False, True, False, np.nan, np.nan, True, False, np.nan],\n                \"right\": [True, False, False, True, True, False, np.nan, np.nan, np.nan],\n            }\n        ).astype(\"boolean\")\n        kdf = ks.from_pandas(pdf)\n\n        self._check_extension(kdf[\"left\"] & kdf[\"right\"], pdf[\"left\"] & pdf[\"right\"])\n        self._check_extension(kdf[\"left\"] & True, pdf[\"left\"] & True)\n        self._check_extension(kdf[\"left\"] & False, pdf[\"left\"] & False)\n        self._check_extension(kdf[\"left\"] & pd.NA, pdf[\"left\"] & pd.NA)\n        self._check_extension(True & kdf[\"right\"], True & pdf[\"right\"])\n        self._check_extension(False & kdf[\"right\"], False & pdf[\"right\"])\n        self._check_extension(pd.NA & kdf[\"right\"], pd.NA & pdf[\"right\"])\n\n    def test_to_numpy(self):\n        pser = pd.Series([1, 2, 3, 4, 5, 6, 7], name=\"x\")\n\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.to_numpy(), pser.values)\n\n    def test_isin(self):\n        pser = pd.Series([\"lama\", \"cow\", \"lama\", \"beetle\", \"lama\", \"hippo\"], name=\"animal\")\n\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.isin([\"cow\", \"lama\"]), pser.isin([\"cow\", \"lama\"]))\n        self.assert_eq(kser.isin(np.array([\"cow\", \"lama\"])), pser.isin(np.array([\"cow\", \"lama\"])))\n        self.assert_eq(kser.isin({\"cow\"}), pser.isin({\"cow\"}))\n\n        msg = \"only list-like objects are allowed to be passed to isin()\"\n        with self.assertRaisesRegex(TypeError, msg):\n            kser.isin(1)\n\n    def test_drop_duplicates(self):\n        pdf = pd.DataFrame({\"animal\": [\"lama\", \"cow\", \"lama\", \"beetle\", \"lama\", \"hippo\"]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.animal\n        kser = kdf.animal\n\n        self.assert_eq(kser.drop_duplicates().sort_index(), pser.drop_duplicates().sort_index())\n        self.assert_eq(\n            kser.drop_duplicates(keep=\"last\").sort_index(),\n            pser.drop_duplicates(keep=\"last\").sort_index(),\n        )\n\n        # inplace\n        kser.drop_duplicates(keep=False, inplace=True)\n        pser.drop_duplicates(keep=False, inplace=True)\n        self.assert_eq(kser.sort_index(), pser.sort_index())\n        self.assert_eq(kdf, pdf)\n\n    def test_reindex(self):\n        index = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n        pser = pd.Series([1.0, 2.0, 3.0, 4.0, None], index=index, name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser, kser)\n\n        self.assert_eq(\n            pser.reindex([\"A\", \"B\"]).sort_index(), kser.reindex([\"A\", \"B\"]).sort_index(),\n        )\n\n        self.assert_eq(\n            pser.reindex([\"A\", \"B\", \"2\", \"3\"]).sort_index(),\n            kser.reindex([\"A\", \"B\", \"2\", \"3\"]).sort_index(),\n        )\n\n        self.assert_eq(\n            pser.reindex([\"A\", \"E\", \"2\"], fill_value=0).sort_index(),\n            kser.reindex([\"A\", \"E\", \"2\"], fill_value=0).sort_index(),\n        )\n\n        self.assertRaises(TypeError, lambda: kser.reindex(index=123))\n\n    def test_reindex_like(self):\n        data = [1.0, 2.0, None]\n        index = pd.Index([\"A\", \"B\", \"C\"], name=\"index1\")\n        pser = pd.Series(data=data, index=index, name=\"name1\")\n        kser = ks.from_pandas(pser)\n\n        # Reindexing single Index on single Index\n        data2 = [3.0, None, 4.0]\n        index2 = pd.Index([\"A\", \"C\", \"D\"], name=\"index2\")\n        pser2 = pd.Series(data=data2, index=index2, name=\"name2\")\n        kser2 = ks.from_pandas(pser2)\n\n        self.assert_eq(\n            pser.reindex_like(pser2).sort_index(), kser.reindex_like(kser2).sort_index(),\n        )\n\n        self.assert_eq(\n            (pser + 1).reindex_like(pser2).sort_index(),\n            (kser + 1).reindex_like(kser2).sort_index(),\n        )\n\n        # Reindexing MultiIndex on single Index\n        index2 = pd.MultiIndex.from_tuples(\n            [(\"A\", \"G\"), (\"C\", \"D\"), (\"I\", \"J\")], names=[\"index3\", \"index4\"]\n        )\n        pser2 = pd.Series(data=data2, index=index2, name=\"name2\")\n        kser2 = ks.from_pandas(pser2)\n\n        self.assert_eq(\n            pser.reindex_like(pser2).sort_index(), kser.reindex_like(kser2).sort_index(),\n        )\n\n        self.assertRaises(TypeError, lambda: kser.reindex_like(index2))\n        self.assertRaises(AssertionError, lambda: kser2.reindex_like(kser))\n\n        # Reindexing MultiIndex on MultiIndex\n        index = pd.MultiIndex.from_tuples(\n            [(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\")], names=[\"index1\", \"index2\"]\n        )\n        pser = pd.Series(data=data, index=index, name=\"name1\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(\n            pser.reindex_like(pser2).sort_index(), kser.reindex_like(kser2).sort_index(),\n        )\n\n        # Reindexing with DataFrame\n        index2 = pd.MultiIndex.from_tuples(\n            [(\"A\", \"B\"), (\"C\", \"D\"), (\"E\", \"F\")], names=[\"name3\", \"name4\"]\n        )\n        pdf = pd.DataFrame(data=data, index=index2)\n        kdf = ks.from_pandas(pdf)\n\n        self.assert_eq(\n            pser.reindex_like(pdf).sort_index(), kser.reindex_like(kdf).sort_index(),\n        )\n\n    def test_fillna(self):\n        pdf = pd.DataFrame({\"x\": [np.nan, 2, 3, 4, np.nan, 6], \"y\": [np.nan, 2, 3, 4, np.nan, 6]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.fillna(0), pser.fillna(0))\n        self.assert_eq(kser.fillna(np.nan).fillna(0), pser.fillna(np.nan).fillna(0))\n\n        kser.fillna(0, inplace=True)\n        pser.fillna(0, inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n        # test considering series does not have NA/NaN values\n        kser.fillna(0, inplace=True)\n        pser.fillna(0, inplace=True)\n        self.assert_eq(kser, pser)\n\n        kser = kdf.x.rename(\"y\")\n        pser = pdf.x.rename(\"y\")\n        kser.fillna(0, inplace=True)\n        pser.fillna(0, inplace=True)\n        self.assert_eq(kser.head(), pser.head())\n\n        pser = pd.Series([1, 2, 3, 4, 5, 6], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        pser.loc[3] = np.nan\n        kser.loc[3] = np.nan\n\n        self.assert_eq(kser.fillna(0), pser.fillna(0))\n        self.assert_eq(kser.fillna(method=\"ffill\"), pser.fillna(method=\"ffill\"))\n        self.assert_eq(kser.fillna(method=\"bfill\"), pser.fillna(method=\"bfill\"))\n\n        # inplace fillna on non-nullable column\n        pdf = pd.DataFrame({\"a\": [1, 2, None], \"b\": [1, 2, 3]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.b\n        kser = kdf.b\n\n        self.assert_eq(kser.fillna(0), pser.fillna(0))\n        self.assert_eq(kser.fillna(np.nan).fillna(0), pser.fillna(np.nan).fillna(0))\n\n        kser.fillna(0, inplace=True)\n        pser.fillna(0, inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n    def test_dropna(self):\n        pdf = pd.DataFrame({\"x\": [np.nan, 2, 3, 4, np.nan, 6]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.dropna(), pser.dropna())\n\n        pser.dropna(inplace=True)\n        kser.dropna(inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n    def test_nunique(self):\n        pser = pd.Series([1, 2, 1, np.nan])\n        kser = ks.from_pandas(pser)\n\n        # Assert NaNs are dropped by default\n        nunique_result = kser.nunique()\n        self.assertEqual(nunique_result, 2)\n        self.assert_eq(nunique_result, pser.nunique())\n\n        # Assert including NaN values\n        nunique_result = kser.nunique(dropna=False)\n        self.assertEqual(nunique_result, 3)\n        self.assert_eq(nunique_result, pser.nunique(dropna=False))\n\n        # Assert approximate counts\n        self.assertEqual(ks.Series(range(100)).nunique(approx=True), 103)\n        self.assertEqual(ks.Series(range(100)).nunique(approx=True, rsd=0.01), 100)\n\n    def _test_value_counts(self):\n        # this is also containing test for Index & MultiIndex\n        pser = pd.Series(\n            [1, 2, 1, 3, 3, np.nan, 1, 4, 2, np.nan, 3, np.nan, 3, 1, 3],\n            index=[1, 2, 1, 3, 3, np.nan, 1, 4, 2, np.nan, 3, np.nan, 3, 1, 3],\n            name=\"x\",\n        )\n        kser = ks.from_pandas(pser)\n\n        exp = pser.value_counts()\n        res = kser.value_counts()\n        self.assertEqual(res.name, exp.name)\n        self.assert_eq(res, exp)\n\n        self.assert_eq(kser.value_counts(normalize=True), pser.value_counts(normalize=True))\n        self.assert_eq(kser.value_counts(ascending=True), pser.value_counts(ascending=True))\n        self.assert_eq(\n            kser.value_counts(normalize=True, dropna=False),\n            pser.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.value_counts(ascending=True, dropna=False),\n            pser.value_counts(ascending=True, dropna=False),\n        )\n\n        self.assert_eq(\n            kser.index.value_counts(normalize=True), pser.index.value_counts(normalize=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True), pser.index.value_counts(ascending=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(normalize=True, dropna=False),\n            pser.index.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True, dropna=False),\n            pser.index.value_counts(ascending=True, dropna=False),\n        )\n\n        with self.assertRaisesRegex(\n            NotImplementedError, \"value_counts currently does not support bins\"\n        ):\n            kser.value_counts(bins=3)\n\n        pser.name = \"index\"\n        kser.name = \"index\"\n        self.assert_eq(kser.value_counts(), pser.value_counts())\n\n        # Series from DataFrame\n        pdf = pd.DataFrame({\"a\": [2, 2, 3], \"b\": [None, 1, None]})\n        kdf = ks.from_pandas(pdf)\n\n        self.assert_eq(kdf.a.value_counts(normalize=True), pdf.a.value_counts(normalize=True))\n        self.assert_eq(kdf.a.value_counts(ascending=True), pdf.a.value_counts(ascending=True))\n        self.assert_eq(\n            kdf.a.value_counts(normalize=True, dropna=False),\n            pdf.a.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kdf.a.value_counts(ascending=True, dropna=False),\n            pdf.a.value_counts(ascending=True, dropna=False),\n        )\n\n        self.assert_eq(\n            kser.index.value_counts(normalize=True), pser.index.value_counts(normalize=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True), pser.index.value_counts(ascending=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(normalize=True, dropna=False),\n            pser.index.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True, dropna=False),\n            pser.index.value_counts(ascending=True, dropna=False),\n        )\n\n        # Series with NaN index\n        pser = pd.Series([3, 2, 3, 1, 2, 3], index=[2.0, None, 5.0, 5.0, None, 5.0])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.value_counts(normalize=True), pser.value_counts(normalize=True))\n        self.assert_eq(kser.value_counts(ascending=True), pser.value_counts(ascending=True))\n        self.assert_eq(\n            kser.value_counts(normalize=True, dropna=False),\n            pser.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.value_counts(ascending=True, dropna=False),\n            pser.value_counts(ascending=True, dropna=False),\n        )\n\n        self.assert_eq(\n            kser.index.value_counts(normalize=True), pser.index.value_counts(normalize=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True), pser.index.value_counts(ascending=True)\n        )\n        self.assert_eq(\n            kser.index.value_counts(normalize=True, dropna=False),\n            pser.index.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True, dropna=False),\n            pser.index.value_counts(ascending=True, dropna=False),\n        )\n\n        # Series with MultiIndex\n        pser.index = pd.MultiIndex.from_tuples(\n            [(\"x\", \"a\"), (\"x\", \"b\"), (\"y\", \"c\"), (\"x\", \"a\"), (\"y\", \"c\"), (\"x\", \"a\")]\n        )\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.value_counts(normalize=True), pser.value_counts(normalize=True))\n        self.assert_eq(kser.value_counts(ascending=True), pser.value_counts(ascending=True))\n        self.assert_eq(\n            kser.value_counts(normalize=True, dropna=False),\n            pser.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.value_counts(ascending=True, dropna=False),\n            pser.value_counts(ascending=True, dropna=False),\n        )\n\n        # FIXME: MultiIndex.value_counts returns wrong indices.\n        self.assert_eq(\n            kser.index.value_counts(normalize=True),\n            pser.index.value_counts(normalize=True),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True),\n            pser.index.value_counts(ascending=True),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(normalize=True, dropna=False),\n            pser.index.value_counts(normalize=True, dropna=False),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True, dropna=False),\n            pser.index.value_counts(ascending=True, dropna=False),\n            almost=True,\n        )\n\n        # Series with MultiIndex some of index has NaN\n        pser.index = pd.MultiIndex.from_tuples(\n            [(\"x\", \"a\"), (\"x\", None), (\"y\", \"c\"), (\"x\", \"a\"), (\"y\", \"c\"), (\"x\", \"a\")]\n        )\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.value_counts(normalize=True), pser.value_counts(normalize=True))\n        self.assert_eq(kser.value_counts(ascending=True), pser.value_counts(ascending=True))\n        self.assert_eq(\n            kser.value_counts(normalize=True, dropna=False),\n            pser.value_counts(normalize=True, dropna=False),\n        )\n        self.assert_eq(\n            kser.value_counts(ascending=True, dropna=False),\n            pser.value_counts(ascending=True, dropna=False),\n        )\n\n        # FIXME: MultiIndex.value_counts returns wrong indices.\n        self.assert_eq(\n            kser.index.value_counts(normalize=True),\n            pser.index.value_counts(normalize=True),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True),\n            pser.index.value_counts(ascending=True),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(normalize=True, dropna=False),\n            pser.index.value_counts(normalize=True, dropna=False),\n            almost=True,\n        )\n        self.assert_eq(\n            kser.index.value_counts(ascending=True, dropna=False),\n            pser.index.value_counts(ascending=True, dropna=False),\n            almost=True,\n        )\n\n        # Series with MultiIndex some of index is NaN.\n        # This test only available for pandas >= 0.24.\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24\"):\n            pser.index = pd.MultiIndex.from_tuples(\n                [(\"x\", \"a\"), None, (\"y\", \"c\"), (\"x\", \"a\"), (\"y\", \"c\"), (\"x\", \"a\")]\n            )\n            kser = ks.from_pandas(pser)\n\n            self.assert_eq(kser.value_counts(normalize=True), pser.value_counts(normalize=True))\n            self.assert_eq(kser.value_counts(ascending=True), pser.value_counts(ascending=True))\n            self.assert_eq(\n                kser.value_counts(normalize=True, dropna=False),\n                pser.value_counts(normalize=True, dropna=False),\n            )\n            self.assert_eq(\n                kser.value_counts(ascending=True, dropna=False),\n                pser.value_counts(ascending=True, dropna=False),\n            )\n\n            # FIXME: MultiIndex.value_counts returns wrong indices.\n            self.assert_eq(\n                kser.index.value_counts(normalize=True),\n                pser.index.value_counts(normalize=True),\n                almost=True,\n            )\n            self.assert_eq(\n                kser.index.value_counts(ascending=True),\n                pser.index.value_counts(ascending=True),\n                almost=True,\n            )\n            self.assert_eq(\n                kser.index.value_counts(normalize=True, dropna=False),\n                pser.index.value_counts(normalize=True, dropna=False),\n                almost=True,\n            )\n            self.assert_eq(\n                kser.index.value_counts(ascending=True, dropna=False),\n                pser.index.value_counts(ascending=True, dropna=False),\n                almost=True,\n            )\n\n    def test_value_counts(self):\n        if LooseVersion(pyspark.__version__) < LooseVersion(\"2.4\"):\n            with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n                self._test_value_counts()\n            self.assertRaises(\n                RuntimeError,\n                lambda: ks.MultiIndex.from_tuples([(\"x\", \"a\"), (\"x\", \"b\")]).value_counts(),\n            )\n        else:\n            self._test_value_counts()\n\n    def test_nsmallest(self):\n        sample_lst = [1, 2, 3, 4, np.nan, 6]\n        pser = pd.Series(sample_lst, name=\"x\")\n        kser = ks.Series(sample_lst, name=\"x\")\n        self.assert_eq(kser.nsmallest(n=3), pser.nsmallest(n=3))\n        self.assert_eq(kser.nsmallest(), pser.nsmallest())\n        self.assert_eq((kser + 1).nsmallest(), (pser + 1).nsmallest())\n\n    def test_nlargest(self):\n        sample_lst = [1, 2, 3, 4, np.nan, 6]\n        pser = pd.Series(sample_lst, name=\"x\")\n        kser = ks.Series(sample_lst, name=\"x\")\n        self.assert_eq(kser.nlargest(n=3), pser.nlargest(n=3))\n        self.assert_eq(kser.nlargest(), pser.nlargest())\n        self.assert_eq((kser + 1).nlargest(), (pser + 1).nlargest())\n\n    def test_isnull(self):\n        pser = pd.Series([1, 2, 3, 4, np.nan, 6], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.notnull(), pser.notnull())\n        self.assert_eq(kser.isnull(), pser.isnull())\n\n        pser = self.pser\n        kser = self.kser\n\n        self.assert_eq(kser.notnull(), pser.notnull())\n        self.assert_eq(kser.isnull(), pser.isnull())\n\n    def test_all(self):\n        for pser in [\n            pd.Series([True, True], name=\"x\"),\n            pd.Series([True, False], name=\"x\"),\n            pd.Series([0, 1], name=\"x\"),\n            pd.Series([1, 2, 3], name=\"x\"),\n            pd.Series([True, True, None], name=\"x\"),\n            pd.Series([True, False, None], name=\"x\"),\n            pd.Series([], name=\"x\"),\n            pd.Series([np.nan], name=\"x\"),\n        ]:\n            kser = ks.from_pandas(pser)\n            self.assert_eq(kser.all(), pser.all())\n\n        pser = pd.Series([1, 2, 3, 4], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq((kser % 2 == 0).all(), (pser % 2 == 0).all())\n\n        with self.assertRaisesRegex(\n            NotImplementedError, 'axis should be either 0 or \"index\" currently.'\n        ):\n            kser.all(axis=1)\n\n    def test_any(self):\n        for pser in [\n            pd.Series([False, False], name=\"x\"),\n            pd.Series([True, False], name=\"x\"),\n            pd.Series([0, 1], name=\"x\"),\n            pd.Series([1, 2, 3], name=\"x\"),\n            pd.Series([True, True, None], name=\"x\"),\n            pd.Series([True, False, None], name=\"x\"),\n            pd.Series([], name=\"x\"),\n            pd.Series([np.nan], name=\"x\"),\n        ]:\n            kser = ks.from_pandas(pser)\n            self.assert_eq(kser.any(), pser.any())\n\n        pser = pd.Series([1, 2, 3, 4], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq((kser % 2 == 0).any(), (pser % 2 == 0).any())\n\n        with self.assertRaisesRegex(\n            NotImplementedError, 'axis should be either 0 or \"index\" currently.'\n        ):\n            kser.any(axis=1)\n\n    def test_reset_index(self):\n        pdf = pd.DataFrame({\"foo\": [1, 2, 3, 4]}, index=pd.Index([\"a\", \"b\", \"c\", \"d\"], name=\"idx\"))\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.foo\n        kser = kdf.foo\n\n        self.assert_eq(kser.reset_index(), pser.reset_index())\n        self.assert_eq(kser.reset_index(name=\"values\"), pser.reset_index(name=\"values\"))\n        self.assert_eq(kser.reset_index(drop=True), pser.reset_index(drop=True))\n\n        # inplace\n        kser.reset_index(drop=True, inplace=True)\n        pser.reset_index(drop=True, inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n    def test_reset_index_with_default_index_types(self):\n        pser = pd.Series([1, 2, 3], name=\"0\", index=np.random.rand(3))\n        kser = ks.from_pandas(pser)\n\n        with ks.option_context(\"compute.default_index_type\", \"sequence\"):\n            self.assert_eq(kser.reset_index(), pser.reset_index())\n\n        with ks.option_context(\"compute.default_index_type\", \"distributed-sequence\"):\n            # the order might be changed.\n            self.assert_eq(kser.reset_index().sort_index(), pser.reset_index())\n\n        with ks.option_context(\"compute.default_index_type\", \"distributed\"):\n            # the index is different.\n            self.assert_eq(\n                kser.reset_index().to_pandas().reset_index(drop=True), pser.reset_index()\n            )\n\n    def test_index_to_series_reset_index(self):\n        def check(kser, pser):\n            self.assert_eq(kser.reset_index(), pser.reset_index())\n            self.assert_eq(kser.reset_index(drop=True), pser.reset_index(drop=True))\n\n            pser.reset_index(drop=True, inplace=True)\n            kser.reset_index(drop=True, inplace=True)\n            self.assert_eq(kser, pser)\n\n        pdf = pd.DataFrame(\n            {\"a\": [1, 2, 3, 4, 5, 6, 7, 8, 9], \"b\": [4, 5, 6, 3, 2, 1, 0, 0, 0]},\n            index=np.random.rand(9),\n        )\n        kdf = ks.from_pandas(pdf)\n        check(kdf.index.to_series(), pdf.index.to_series())\n        check(kdf.index.to_series(name=\"a\"), pdf.index.to_series(name=\"a\"))\n        check(kdf.index.to_series(name=(\"x\", \"a\")), pdf.index.to_series(name=(\"x\", \"a\")))\n\n    def test_sort_values(self):\n        pdf = pd.DataFrame({\"x\": [1, 2, 3, 4, 5, None, 7]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.sort_values(), pser.sort_values())\n        self.assert_eq(kser.sort_values(ascending=False), pser.sort_values(ascending=False))\n        self.assert_eq(kser.sort_values(na_position=\"first\"), pser.sort_values(na_position=\"first\"))\n\n        self.assertRaises(ValueError, lambda: kser.sort_values(na_position=\"invalid\"))\n\n        # inplace\n        # pandas raises an exception when the Series is derived from DataFrame\n        kser.sort_values(inplace=True)\n        self.assert_eq(kser, pser.sort_values())\n        self.assert_eq(kdf, pdf)\n\n        pser = pdf.x.copy()\n        kser = kdf.x.copy()\n\n        kser.sort_values(inplace=True)\n        pser.sort_values(inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n    def test_sort_index(self):\n        pdf = pd.DataFrame({\"x\": [2, 1, np.nan]}, index=[\"b\", \"a\", np.nan])\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        # Assert invalid parameters\n        self.assertRaises(NotImplementedError, lambda: kser.sort_index(axis=1))\n        self.assertRaises(NotImplementedError, lambda: kser.sort_index(kind=\"mergesort\"))\n        self.assertRaises(ValueError, lambda: kser.sort_index(na_position=\"invalid\"))\n\n        # Assert default behavior without parameters\n        self.assert_eq(kser.sort_index(), pser.sort_index())\n        # Assert sorting descending\n        self.assert_eq(kser.sort_index(ascending=False), pser.sort_index(ascending=False))\n        # Assert sorting NA indices first\n        self.assert_eq(kser.sort_index(na_position=\"first\"), pser.sort_index(na_position=\"first\"))\n\n        # Assert sorting inplace\n        # pandas sorts pdf.x by the index and update the column only\n        # when the Series is derived from DataFrame.\n        kser.sort_index(inplace=True)\n        self.assert_eq(kser, pser.sort_index())\n        self.assert_eq(kdf, pdf)\n\n        pser = pdf.x.copy()\n        kser = kdf.x.copy()\n\n        kser.sort_index(inplace=True)\n        pser.sort_index(inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n        # Assert multi-indices\n        pser = pd.Series(range(4), index=[[\"b\", \"b\", \"a\", \"a\"], [1, 0, 1, 0]], name=\"0\")\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.sort_index(), pser.sort_index())\n        self.assert_eq(kser.sort_index(level=[1, 0]), pser.sort_index(level=[1, 0]))\n\n        self.assert_eq(kser.reset_index().sort_index(), pser.reset_index().sort_index())\n\n    def test_to_datetime(self):\n        pser = pd.Series([\"3/11/2000\", \"3/12/2000\", \"3/13/2000\"] * 100)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(\n            pd.to_datetime(pser, infer_datetime_format=True),\n            ks.to_datetime(kser, infer_datetime_format=True),\n        )\n\n    def test_missing(self):\n        kser = self.kser\n\n        missing_functions = inspect.getmembers(MissingPandasLikeSeries, inspect.isfunction)\n        unsupported_functions = [\n            name for (name, type_) in missing_functions if type_.__name__ == \"unsupported_function\"\n        ]\n        for name in unsupported_functions:\n            with self.assertRaisesRegex(\n                PandasNotImplementedError,\n                \"method.*Series.*{}.*not implemented( yet\\\\.|\\\\. .+)\".format(name),\n            ):\n                getattr(kser, name)()\n\n        deprecated_functions = [\n            name for (name, type_) in missing_functions if type_.__name__ == \"deprecated_function\"\n        ]\n        for name in deprecated_functions:\n            with self.assertRaisesRegex(\n                PandasNotImplementedError, \"method.*Series.*{}.*is deprecated\".format(name)\n            ):\n                getattr(kser, name)()\n\n        missing_properties = inspect.getmembers(\n            MissingPandasLikeSeries, lambda o: isinstance(o, property)\n        )\n        unsupported_properties = [\n            name\n            for (name, type_) in missing_properties\n            if type_.fget.__name__ == \"unsupported_property\"\n        ]\n        for name in unsupported_properties:\n            with self.assertRaisesRegex(\n                PandasNotImplementedError,\n                \"property.*Series.*{}.*not implemented( yet\\\\.|\\\\. .+)\".format(name),\n            ):\n                getattr(kser, name)\n        deprecated_properties = [\n            name\n            for (name, type_) in missing_properties\n            if type_.fget.__name__ == \"deprecated_property\"\n        ]\n        for name in deprecated_properties:\n            with self.assertRaisesRegex(\n                PandasNotImplementedError, \"property.*Series.*{}.*is deprecated\".format(name)\n            ):\n                getattr(kser, name)\n\n    def test_clip(self):\n        pser = pd.Series([0, 2, 4], index=np.random.rand(3))\n        kser = ks.from_pandas(pser)\n\n        # Assert list-like values are not accepted for 'lower' and 'upper'\n        msg = \"List-like value are not supported for 'lower' and 'upper' at the moment\"\n        with self.assertRaises(ValueError, msg=msg):\n            kser.clip(lower=[1])\n        with self.assertRaises(ValueError, msg=msg):\n            kser.clip(upper=[1])\n\n        # Assert no lower or upper\n        self.assert_eq(kser.clip(), pser.clip())\n        # Assert lower only\n        self.assert_eq(kser.clip(1), pser.clip(1))\n        # Assert upper only\n        self.assert_eq(kser.clip(upper=3), pser.clip(upper=3))\n        # Assert lower and upper\n        self.assert_eq(kser.clip(1, 3), pser.clip(1, 3))\n\n        # Assert behavior on string values\n        str_kser = ks.Series([\"a\", \"b\", \"c\"])\n        self.assert_eq(str_kser.clip(1, 3), str_kser)\n\n    def test_compare(self):\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.1\"):\n            pser = pd.Series([1, 2])\n            kser = ks.from_pandas(pser)\n\n            res_kdf = kser.compare(kser)\n            self.assertTrue(res_kdf.empty)\n            self.assert_eq(res_kdf.columns, pd.Index([\"self\", \"other\"]))\n\n            self.assert_eq(pser.compare(pser + 1).sort_index(), kser.compare(kser + 1).sort_index())\n\n            pser = pd.Series([1, 2], index=[\"x\", \"y\"])\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.compare(pser + 1).sort_index(), kser.compare(kser + 1).sort_index())\n        else:\n            kser = ks.Series([1, 2])\n            res_kdf = kser.compare(kser)\n            self.assertTrue(res_kdf.empty)\n            self.assert_eq(res_kdf.columns, pd.Index([\"self\", \"other\"]))\n            expected = ks.DataFrame([[1, 2], [2, 3]], columns=[\"self\", \"other\"])\n            self.assert_eq(expected, kser.compare(kser + 1).sort_index())\n\n            kser = ks.Series([1, 2], index=[\"x\", \"y\"])\n            expected = ks.DataFrame([[1, 2], [2, 3]], index=[\"x\", \"y\"], columns=[\"self\", \"other\"])\n            self.assert_eq(expected, kser.compare(kser + 1).sort_index())\n\n    def test_is_unique(self):\n        # We can't use pandas' is_unique for comparison. pandas 0.23 ignores None\n        pser = pd.Series([1, 2, 2, None, None])\n        kser = ks.from_pandas(pser)\n        self.assertEqual(False, kser.is_unique)\n        self.assertEqual(False, (kser + 1).is_unique)\n\n        pser = pd.Series([1, None, None])\n        kser = ks.from_pandas(pser)\n        self.assertEqual(False, kser.is_unique)\n        self.assertEqual(False, (kser + 1).is_unique)\n\n        pser = pd.Series([1])\n        kser = ks.from_pandas(pser)\n        self.assertEqual(pser.is_unique, kser.is_unique)\n        self.assertEqual((pser + 1).is_unique, (kser + 1).is_unique)\n\n        pser = pd.Series([1, 1, 1])\n        kser = ks.from_pandas(pser)\n        self.assertEqual(pser.is_unique, kser.is_unique)\n        self.assertEqual((pser + 1).is_unique, (kser + 1).is_unique)\n\n    def test_to_list(self):\n        self.assert_eq(self.kser.tolist(), self.pser.tolist())\n\n    def test_append(self):\n        pser1 = pd.Series([1, 2, 3], name=\"0\")\n        pser2 = pd.Series([4, 5, 6], name=\"0\")\n        pser3 = pd.Series([4, 5, 6], index=[3, 4, 5], name=\"0\")\n        kser1 = ks.from_pandas(pser1)\n        kser2 = ks.from_pandas(pser2)\n        kser3 = ks.from_pandas(pser3)\n\n        self.assert_eq(kser1.append(kser2), pser1.append(pser2))\n        self.assert_eq(kser1.append(kser3), pser1.append(pser3))\n        self.assert_eq(\n            kser1.append(kser2, ignore_index=True), pser1.append(pser2, ignore_index=True)\n        )\n\n        kser1.append(kser3, verify_integrity=True)\n        msg = \"Indices have overlapping values\"\n        with self.assertRaises(ValueError, msg=msg):\n            kser1.append(kser2, verify_integrity=True)\n\n    def test_map(self):\n        pser = pd.Series([\"cat\", \"dog\", None, \"rabbit\"])\n        kser = ks.from_pandas(pser)\n        # Currently Koalas doesn't return NaN as pandas does.\n        self.assert_eq(kser.map({}), pser.map({}).replace({pd.np.nan: None}))\n\n        d = defaultdict(lambda: \"abc\")\n        self.assertTrue(\"abc\" in repr(kser.map(d)))\n        self.assert_eq(kser.map(d), pser.map(d))\n\n        def tomorrow(date) -> datetime:\n            return date + timedelta(days=1)\n\n        pser = pd.Series([datetime(2019, 10, 24)])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.map(tomorrow), pser.map(tomorrow))\n\n    def test_add_prefix(self):\n        pser = pd.Series([1, 2, 3, 4], name=\"0\")\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.add_prefix(\"item_\"), kser.add_prefix(\"item_\"))\n\n        pser = pd.Series(\n            [1, 2, 3],\n            name=\"0\",\n            index=pd.MultiIndex.from_tuples([(\"A\", \"X\"), (\"A\", \"Y\"), (\"B\", \"X\")]),\n        )\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.add_prefix(\"item_\"), kser.add_prefix(\"item_\"))\n\n    def test_add_suffix(self):\n        pser = pd.Series([1, 2, 3, 4], name=\"0\")\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.add_suffix(\"_item\"), kser.add_suffix(\"_item\"))\n\n        pser = pd.Series(\n            [1, 2, 3],\n            name=\"0\",\n            index=pd.MultiIndex.from_tuples([(\"A\", \"X\"), (\"A\", \"Y\"), (\"B\", \"X\")]),\n        )\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.add_suffix(\"_item\"), kser.add_suffix(\"_item\"))\n\n    def test_cummin(self):\n        pser = pd.Series([1.0, None, 0.0, 4.0, 9.0])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cummin(), kser.cummin())\n        self.assert_eq(pser.cummin(skipna=False), kser.cummin(skipna=False))\n        self.assert_eq(pser.cummin().sum(), kser.cummin().sum())\n\n        # with reversed index\n        pser.index = [4, 3, 2, 1, 0]\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cummin(), kser.cummin())\n        self.assert_eq(pser.cummin(skipna=False), kser.cummin(skipna=False))\n\n    def test_cummax(self):\n        pser = pd.Series([1.0, None, 0.0, 4.0, 9.0])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cummax(), kser.cummax())\n        self.assert_eq(pser.cummax(skipna=False), kser.cummax(skipna=False))\n        self.assert_eq(pser.cummax().sum(), kser.cummax().sum())\n\n        # with reversed index\n        pser.index = [4, 3, 2, 1, 0]\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cummax(), kser.cummax())\n        self.assert_eq(pser.cummax(skipna=False), kser.cummax(skipna=False))\n\n    def test_cumsum(self):\n        pser = pd.Series([1.0, None, 0.0, 4.0, 9.0])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumsum(), kser.cumsum())\n        self.assert_eq(pser.cumsum(skipna=False), kser.cumsum(skipna=False))\n        self.assert_eq(pser.cumsum().sum(), kser.cumsum().sum())\n\n        # with reversed index\n        pser.index = [4, 3, 2, 1, 0]\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumsum(), kser.cumsum())\n        self.assert_eq(pser.cumsum(skipna=False), kser.cumsum(skipna=False))\n\n        # bool\n        pser = pd.Series([True, True, False, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumsum().astype(int), kser.cumsum())\n        self.assert_eq(pser.cumsum(skipna=False).astype(int), kser.cumsum(skipna=False))\n\n    def test_cumprod(self):\n        pser = pd.Series([1.0, None, 1.0, 4.0, 9.0])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False), kser.cumprod(skipna=False))\n        self.assert_eq(pser.cumprod().sum(), kser.cumprod().sum())\n\n        # with integer type\n        pser = pd.Series([1, 10, 1, 4, 9])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False), kser.cumprod(skipna=False))\n        self.assert_eq(pser.cumprod().sum(), kser.cumprod().sum())\n\n        # with reversed index\n        pser.index = [4, 3, 2, 1, 0]\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False), kser.cumprod(skipna=False))\n\n        # including zero\n        pser = pd.Series([1, 2, 0, 3])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False), kser.cumprod(skipna=False))\n\n        # including negative values\n        pser = pd.Series([1, -1, -2])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False), kser.cumprod(skipna=False))\n\n        # bool\n        pser = pd.Series([True, True, False, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.cumprod(), kser.cumprod())\n        self.assert_eq(pser.cumprod(skipna=False).astype(int), kser.cumprod(skipna=False))\n\n    def test_median(self):\n        with self.assertRaisesRegex(ValueError, \"accuracy must be an integer; however\"):\n            ks.Series([24.0, 21.0, 25.0, 33.0, 26.0]).median(accuracy=\"a\")\n\n    def test_rank(self):\n        pser = pd.Series([1, 2, 3, 1], name=\"x\")\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.rank(), kser.rank().sort_index())\n        self.assert_eq(pser.rank().sum(), kser.rank().sum())\n        self.assert_eq(pser.rank(ascending=False), kser.rank(ascending=False).sort_index())\n        self.assert_eq(pser.rank(method=\"min\"), kser.rank(method=\"min\").sort_index())\n        self.assert_eq(pser.rank(method=\"max\"), kser.rank(method=\"max\").sort_index())\n        self.assert_eq(pser.rank(method=\"first\"), kser.rank(method=\"first\").sort_index())\n        self.assert_eq(pser.rank(method=\"dense\"), kser.rank(method=\"dense\").sort_index())\n\n        msg = \"method must be one of 'average', 'min', 'max', 'first', 'dense'\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.rank(method=\"nothing\")\n\n    def test_round(self):\n        pser = pd.Series([0.028208, 0.038683, 0.877076], name=\"x\")\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.round(2), kser.round(2))\n        msg = \"decimals must be an integer\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.round(1.5)\n\n    def test_quantile(self):\n        pser = pd.Series([])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.quantile(0.5), pser.quantile(0.5))\n        self.assert_eq(kser.quantile([0.25, 0.5, 0.75]), pser.quantile([0.25, 0.5, 0.75]))\n\n        with self.assertRaisesRegex(ValueError, \"accuracy must be an integer; however\"):\n            ks.Series([24.0, 21.0, 25.0, 33.0, 26.0]).quantile(accuracy=\"a\")\n        with self.assertRaisesRegex(ValueError, \"q must be a float or an array of floats;\"):\n            ks.Series([24.0, 21.0, 25.0, 33.0, 26.0]).quantile(q=\"a\")\n        with self.assertRaisesRegex(ValueError, \"q must be a float or an array of floats;\"):\n            ks.Series([24.0, 21.0, 25.0, 33.0, 26.0]).quantile(q=[\"a\"])\n\n        with self.assertRaisesRegex(TypeError, \"Could not convert object \\\\(string\\\\) to numeric\"):\n            ks.Series([\"a\", \"b\", \"c\"]).quantile()\n        with self.assertRaisesRegex(TypeError, \"Could not convert object \\\\(string\\\\) to numeric\"):\n            ks.Series([\"a\", \"b\", \"c\"]).quantile([0.25, 0.5, 0.75])\n\n    def test_idxmax(self):\n        pser = pd.Series(data=[1, 4, 5], index=[\"A\", \"B\", \"C\"])\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmax(), pser.idxmax())\n        self.assertEqual(kser.idxmax(skipna=False), pser.idxmax(skipna=False))\n\n        index = pd.MultiIndex.from_arrays(\n            [[\"a\", \"a\", \"b\", \"b\"], [\"c\", \"d\", \"e\", \"f\"]], names=(\"first\", \"second\")\n        )\n        pser = pd.Series(data=[1, 2, 4, 5], index=index)\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmax(), pser.idxmax())\n        self.assertEqual(kser.idxmax(skipna=False), pser.idxmax(skipna=False))\n\n        kser = ks.Series([])\n        with self.assertRaisesRegex(ValueError, \"an empty sequence\"):\n            kser.idxmax()\n\n        pser = pd.Series([1, 100, None, 100, 1, 100], index=[10, 3, 5, 2, 1, 8])\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmax(), pser.idxmax())\n        self.assertEqual(repr(kser.idxmax(skipna=False)), repr(pser.idxmax(skipna=False)))\n\n    def test_idxmin(self):\n        pser = pd.Series(data=[1, 4, 5], index=[\"A\", \"B\", \"C\"])\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmin(), pser.idxmin())\n        self.assertEqual(kser.idxmin(skipna=False), pser.idxmin(skipna=False))\n\n        index = pd.MultiIndex.from_arrays(\n            [[\"a\", \"a\", \"b\", \"b\"], [\"c\", \"d\", \"e\", \"f\"]], names=(\"first\", \"second\")\n        )\n        pser = pd.Series(data=[1, 2, 4, 5], index=index)\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmin(), pser.idxmin())\n        self.assertEqual(kser.idxmin(skipna=False), pser.idxmin(skipna=False))\n\n        kser = ks.Series([])\n        with self.assertRaisesRegex(ValueError, \"an empty sequence\"):\n            kser.idxmin()\n\n        pser = pd.Series([1, 100, None, 100, 1, 100], index=[10, 3, 5, 2, 1, 8])\n        kser = ks.Series(pser)\n\n        self.assertEqual(kser.idxmin(), pser.idxmin())\n        self.assertEqual(repr(kser.idxmin(skipna=False)), repr(pser.idxmin(skipna=False)))\n\n    def test_shift(self):\n        pser = pd.Series([10, 20, 15, 30, 45], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.shift(2), pser.shift(2))\n        self.assert_eq(kser.shift().shift(-1), pser.shift().shift(-1))\n        self.assert_eq(kser.shift().sum(), pser.shift().sum())\n\n        if LooseVersion(pd.__version__) < LooseVersion(\"0.24.2\"):\n            self.assert_eq(kser.shift(periods=2), pser.shift(periods=2))\n        else:\n            self.assert_eq(kser.shift(periods=2, fill_value=0), pser.shift(periods=2, fill_value=0))\n        with self.assertRaisesRegex(ValueError, \"periods should be an int; however\"):\n            kser.shift(periods=1.5)\n\n    def test_diff(self):\n        pser = pd.Series([10, 20, 15, 30, 45], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.diff(2), pser.diff(2))\n        self.assert_eq(kser.diff().diff(-1), pser.diff().diff(-1))\n        self.assert_eq(kser.diff().sum(), pser.diff().sum())\n\n    def _test_numeric_astype(self, pser):\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.astype(int), pser.astype(int))\n        self.assert_eq(kser.astype(np.int), pser.astype(np.int))\n        self.assert_eq(kser.astype(np.int8), pser.astype(np.int8))\n        self.assert_eq(kser.astype(np.int16), pser.astype(np.int16))\n        self.assert_eq(kser.astype(np.int32), pser.astype(np.int32))\n        self.assert_eq(kser.astype(np.int64), pser.astype(np.int64))\n        self.assert_eq(kser.astype(np.byte), pser.astype(np.byte))\n        self.assert_eq(kser.astype(\"int\"), pser.astype(\"int\"))\n        self.assert_eq(kser.astype(\"int8\"), pser.astype(\"int8\"))\n        self.assert_eq(kser.astype(\"int16\"), pser.astype(\"int16\"))\n        self.assert_eq(kser.astype(\"int32\"), pser.astype(\"int32\"))\n        self.assert_eq(kser.astype(\"int64\"), pser.astype(\"int64\"))\n        self.assert_eq(kser.astype(\"b\"), pser.astype(\"b\"))\n        self.assert_eq(kser.astype(\"byte\"), pser.astype(\"byte\"))\n        self.assert_eq(kser.astype(\"i\"), pser.astype(\"i\"))\n        self.assert_eq(kser.astype(\"long\"), pser.astype(\"long\"))\n        self.assert_eq(kser.astype(\"short\"), pser.astype(\"short\"))\n        self.assert_eq(kser.astype(np.float), pser.astype(np.float))\n        self.assert_eq(kser.astype(np.float32), pser.astype(np.float32))\n        self.assert_eq(kser.astype(np.float64), pser.astype(np.float64))\n        self.assert_eq(kser.astype(\"float\"), pser.astype(\"float\"))\n        self.assert_eq(kser.astype(\"float32\"), pser.astype(\"float32\"))\n        self.assert_eq(kser.astype(\"float64\"), pser.astype(\"float64\"))\n        self.assert_eq(kser.astype(\"double\"), pser.astype(\"double\"))\n        self.assert_eq(kser.astype(\"f\"), pser.astype(\"f\"))\n        self.assert_eq(kser.astype(bool), pser.astype(bool))\n        self.assert_eq(kser.astype(\"bool\"), pser.astype(\"bool\"))\n        self.assert_eq(kser.astype(\"?\"), pser.astype(\"?\"))\n        self.assert_eq(kser.astype(np.unicode_), pser.astype(np.unicode_))\n        self.assert_eq(kser.astype(\"str\"), pser.astype(\"str\"))\n        self.assert_eq(kser.astype(\"U\"), pser.astype(\"U\"))\n\n        if extension_dtypes_available:\n            from pandas import Int8Dtype, Int16Dtype, Int32Dtype, Int64Dtype\n\n            self._check_extension(kser.astype(\"Int8\"), pser.astype(\"Int8\"))\n            self._check_extension(kser.astype(\"Int16\"), pser.astype(\"Int16\"))\n            self._check_extension(kser.astype(\"Int32\"), pser.astype(\"Int32\"))\n            self._check_extension(kser.astype(\"Int64\"), pser.astype(\"Int64\"))\n            self._check_extension(kser.astype(Int8Dtype()), pser.astype(Int8Dtype()))\n            self._check_extension(kser.astype(Int16Dtype()), pser.astype(Int16Dtype()))\n            self._check_extension(kser.astype(Int32Dtype()), pser.astype(Int32Dtype()))\n            self._check_extension(kser.astype(Int64Dtype()), pser.astype(Int64Dtype()))\n\n        if extension_object_dtypes_available:\n            from pandas import StringDtype\n\n            if LooseVersion(pd.__version__) >= LooseVersion(\"1.1\"):\n                self._check_extension(kser.astype(\"string\"), pser.astype(\"string\"))\n                self._check_extension(kser.astype(StringDtype()), pser.astype(StringDtype()))\n            else:\n                self._check_extension(\n                    kser.astype(\"string\"),\n                    pd.Series([\"10\", \"20\", \"15\", \"30\", \"45\"], name=\"x\", dtype=\"string\"),\n                )\n                self._check_extension(\n                    kser.astype(StringDtype()),\n                    pd.Series([\"10\", \"20\", \"15\", \"30\", \"45\"], name=\"x\", dtype=StringDtype()),\n                )\n\n        if extension_float_dtypes_available:\n            from pandas import Float32Dtype, Float64Dtype\n\n            self._check_extension(kser.astype(\"Float32\"), pser.astype(\"Float32\"))\n            self._check_extension(kser.astype(\"Float64\"), pser.astype(\"Float64\"))\n            self._check_extension(kser.astype(Float32Dtype()), pser.astype(Float32Dtype()))\n            self._check_extension(kser.astype(Float64Dtype()), pser.astype(Float64Dtype()))\n\n    def test_astype(self):\n        psers = [pd.Series([10, 20, 15, 30, 45], name=\"x\")]\n\n        if extension_dtypes_available:\n            psers.append(pd.Series([10, 20, 15, 30, 45], name=\"x\", dtype=\"Int64\"))\n        if extension_float_dtypes_available:\n            psers.append(pd.Series([10, 20, 15, 30, 45], name=\"x\", dtype=\"Float64\"))\n\n        for pser in psers:\n            self._test_numeric_astype(pser)\n\n        pser = pd.Series([10, 20, 15, 30, 45, None, np.nan], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.astype(bool), pser.astype(bool))\n        self.assert_eq(kser.astype(str), pser.astype(str))\n\n        pser = pd.Series([\"hi\", \"hi \", \" \", \" \\t\", \"\", None], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.astype(bool), pser.astype(bool))\n        if LooseVersion(\"1.1.1\") <= LooseVersion(pd.__version__) < LooseVersion(\"1.1.4\"):\n            # a pandas bug: https://github.com/databricks/koalas/pull/1818#issuecomment-703961980\n            self.assert_eq(kser.astype(str).tolist(), [\"hi\", \"hi \", \" \", \" \\t\", \"\", \"None\"])\n        else:\n            self.assert_eq(kser.astype(str), pser.astype(str))\n        self.assert_eq(kser.str.strip().astype(bool), pser.str.strip().astype(bool))\n\n        if extension_object_dtypes_available:\n            from pandas import StringDtype\n\n            self._check_extension(kser.astype(\"string\"), pser.astype(\"string\"))\n            self._check_extension(kser.astype(StringDtype()), pser.astype(StringDtype()))\n\n        pser = pd.Series([True, False, None], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.astype(bool), pser.astype(bool))\n        self.assert_eq(kser.astype(str), pser.astype(str))\n\n        if extension_object_dtypes_available:\n            from pandas import BooleanDtype, StringDtype\n\n            self._check_extension(kser.astype(\"boolean\"), pser.astype(\"boolean\"))\n            self._check_extension(kser.astype(BooleanDtype()), pser.astype(BooleanDtype()))\n\n            if LooseVersion(pd.__version__) >= LooseVersion(\"1.1\"):\n                self._check_extension(kser.astype(\"string\"), pser.astype(\"string\"))\n                self._check_extension(kser.astype(StringDtype()), pser.astype(StringDtype()))\n            else:\n                self._check_extension(\n                    kser.astype(\"string\"),\n                    pd.Series([\"True\", \"False\", None], name=\"x\", dtype=\"string\"),\n                )\n                self._check_extension(\n                    kser.astype(StringDtype()),\n                    pd.Series([\"True\", \"False\", None], name=\"x\", dtype=StringDtype()),\n                )\n\n        pser = pd.Series([\"2020-10-27 00:00:01\", None], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.astype(np.datetime64), pser.astype(np.datetime64))\n        self.assert_eq(kser.astype(\"datetime64[ns]\"), pser.astype(\"datetime64[ns]\"))\n        self.assert_eq(kser.astype(\"M\"), pser.astype(\"M\"))\n        self.assert_eq(kser.astype(\"M\").astype(str), pser.astype(\"M\").astype(str))\n        self.assert_eq(kser.astype(\"M\").dt.date.astype(str), pser.astype(\"M\").dt.date.astype(str))\n\n        if extension_object_dtypes_available:\n            from pandas import StringDtype\n\n            self._check_extension(\n                kser.astype(\"M\").astype(\"string\"), pser.astype(\"M\").astype(\"string\")\n            )\n            self._check_extension(\n                kser.astype(\"M\").astype(StringDtype()), pser.astype(\"M\").astype(StringDtype())\n            )\n\n        with self.assertRaisesRegex(TypeError, \"not understood\"):\n            kser.astype(\"int63\")\n\n    def test_aggregate(self):\n        pser = pd.Series([10, 20, 15, 30, 45], name=\"x\")\n        kser = ks.Series(pser)\n        msg = \"func must be a string or list of strings\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.aggregate({\"x\": [\"min\", \"max\"]})\n        msg = (\n            \"If the given function is a list, it \" \"should only contains function names as strings.\"\n        )\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.aggregate([\"min\", max])\n\n    def test_drop(self):\n        pser = pd.Series([10, 20, 15, 30, 45], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.drop(1), pser.drop(1))\n        self.assert_eq(kser.drop([1, 4]), pser.drop([1, 4]))\n\n        msg = \"Need to specify at least one of 'labels' or 'index'\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.drop()\n        self.assertRaises(KeyError, lambda: kser.drop((0, 1)))\n\n        # For MultiIndex\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.drop(\"lama\"), pser.drop(\"lama\"))\n        self.assert_eq(kser.drop(labels=\"weight\", level=1), pser.drop(labels=\"weight\", level=1))\n        self.assert_eq(kser.drop((\"lama\", \"weight\")), pser.drop((\"lama\", \"weight\")))\n        self.assert_eq(\n            kser.drop([(\"lama\", \"speed\"), (\"falcon\", \"weight\")]),\n            pser.drop([(\"lama\", \"speed\"), (\"falcon\", \"weight\")]),\n        )\n        self.assert_eq(kser.drop({\"lama\": \"speed\"}), pser.drop({\"lama\": \"speed\"}))\n\n        msg = \"'level' should be less than the number of indexes\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.drop(labels=\"weight\", level=2)\n\n        msg = (\n            \"If the given index is a list, it \"\n            \"should only contains names as all tuples or all non tuples \"\n            \"that contain index names\"\n        )\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.drop([\"lama\", [\"cow\", \"falcon\"]])\n\n        msg = \"Cannot specify both 'labels' and 'index'\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.drop(\"lama\", index=\"cow\")\n\n        msg = r\"'Key length \\(2\\) exceeds index depth \\(3\\)'\"\n        with self.assertRaisesRegex(KeyError, msg):\n            kser.drop((\"lama\", \"speed\", \"x\"))\n\n    def test_pop(self):\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pdf = pd.DataFrame({\"x\": [45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3]}, index=midx)\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.pop((\"lama\", \"speed\")), pser.pop((\"lama\", \"speed\")))\n        self.assert_eq(kser, pser)\n        self.assert_eq(kdf, pdf)\n\n        msg = r\"'Key length \\(3\\) exceeds index depth \\(2\\)'\"\n        with self.assertRaisesRegex(KeyError, msg):\n            kser.pop((\"lama\", \"speed\", \"x\"))\n\n    def test_replace(self):\n        pser = pd.Series([10, 20, 15, 30, np.nan], name=\"x\")\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser.replace(), pser.replace())\n        self.assert_eq(kser.replace({}), pser.replace({}))\n\n        self.assert_eq(kser.replace(np.nan, 45), pser.replace(np.nan, 45))\n        self.assert_eq(kser.replace([10, 15], 45), pser.replace([10, 15], 45))\n        self.assert_eq(kser.replace((10, 15), 45), pser.replace((10, 15), 45))\n        self.assert_eq(kser.replace([10, 15], [45, 50]), pser.replace([10, 15], [45, 50]))\n        self.assert_eq(kser.replace((10, 15), (45, 50)), pser.replace((10, 15), (45, 50)))\n\n        msg = \"'to_replace' should be one of str, list, tuple, dict, int, float\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.replace(ks.range(5))\n        msg = \"Replacement lists must match in length. Expecting 3 got 2\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.replace([10, 20, 30], [1, 2])\n        msg = \"replace currently not support for regex\"\n        with self.assertRaisesRegex(NotImplementedError, msg):\n            kser.replace(r\"^1.$\", regex=True)\n\n    def test_xs(self):\n        midx = pd.MultiIndex(\n            [[\"a\", \"b\", \"c\"], [\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.xs((\"a\", \"lama\", \"speed\")), pser.xs((\"a\", \"lama\", \"speed\")))\n\n    def test_duplicates(self):\n        psers = {\n            \"test on texts\": pd.Series(\n                [\"lama\", \"cow\", \"lama\", \"beetle\", \"lama\", \"hippo\"], name=\"animal\"\n            ),\n            \"test on numbers\": pd.Series([1, 1, 2, 4, 3]),\n        }\n        keeps = [\"first\", \"last\", False]\n\n        for (msg, pser), keep in product(psers.items(), keeps):\n            with self.subTest(msg, keep=keep):\n                kser = ks.Series(pser)\n\n                self.assert_eq(\n                    pser.drop_duplicates(keep=keep).sort_values(),\n                    kser.drop_duplicates(keep=keep).sort_values(),\n                )\n\n    def test_update(self):\n        pser = pd.Series([10, 20, 15, 30, 45], name=\"x\")\n        kser = ks.Series(pser)\n\n        msg = \"'other' must be a Series\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.update(10)\n\n    def test_where(self):\n        pser1 = pd.Series([0, 1, 2, 3, 4])\n        kser1 = ks.from_pandas(pser1)\n\n        self.assert_eq(pser1.where(pser1 > 3), kser1.where(kser1 > 3).sort_index())\n\n    def test_mask(self):\n        pser1 = pd.Series([0, 1, 2, 3, 4])\n        kser1 = ks.from_pandas(pser1)\n\n        self.assert_eq(pser1.mask(pser1 > 3), kser1.mask(kser1 > 3).sort_index())\n\n    def test_truncate(self):\n        pser1 = pd.Series([10, 20, 30, 40, 50, 60, 70], index=[1, 2, 3, 4, 5, 6, 7])\n        kser1 = ks.Series(pser1)\n        pser2 = pd.Series([10, 20, 30, 40, 50, 60, 70], index=[7, 6, 5, 4, 3, 2, 1])\n        kser2 = ks.Series(pser2)\n\n        self.assert_eq(kser1.truncate(), pser1.truncate())\n        self.assert_eq(kser1.truncate(before=2), pser1.truncate(before=2))\n        self.assert_eq(kser1.truncate(after=5), pser1.truncate(after=5))\n        self.assert_eq(kser1.truncate(copy=False), pser1.truncate(copy=False))\n        self.assert_eq(kser1.truncate(2, 5, copy=False), pser1.truncate(2, 5, copy=False))\n        # The bug for these tests has been fixed in pandas 1.1.0.\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.1.0\"):\n            self.assert_eq(kser2.truncate(4, 6), pser2.truncate(4, 6))\n            self.assert_eq(kser2.truncate(4, 6, copy=False), pser2.truncate(4, 6, copy=False))\n        else:\n            expected_kser = ks.Series([20, 30, 40], index=[6, 5, 4])\n            self.assert_eq(kser2.truncate(4, 6), expected_kser)\n            self.assert_eq(kser2.truncate(4, 6, copy=False), expected_kser)\n\n        kser = ks.Series([10, 20, 30, 40, 50, 60, 70], index=[1, 2, 3, 4, 3, 2, 1])\n        msg = \"truncate requires a sorted index\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.truncate()\n\n        kser = ks.Series([10, 20, 30, 40, 50, 60, 70], index=[1, 2, 3, 4, 5, 6, 7])\n        msg = \"Truncate: 2 must be after 5\"\n        with self.assertRaisesRegex(ValueError, msg):\n            kser.truncate(5, 2)\n\n    def test_getitem(self):\n        pser = pd.Series([10, 20, 15, 30, 45], [\"A\", \"A\", \"B\", \"C\", \"D\"])\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser[\"A\"], pser[\"A\"])\n        self.assert_eq(kser[\"B\"], pser[\"B\"])\n        self.assert_eq(kser[kser > 15], pser[pser > 15])\n\n        # for MultiIndex\n        midx = pd.MultiIndex(\n            [[\"a\", \"b\", \"c\"], [\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 0, 0, 0, 1, 1, 1], [0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 0, 0, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], name=\"0\", index=midx)\n        kser = ks.Series(pser)\n\n        self.assert_eq(kser[\"a\"], pser[\"a\"])\n        self.assert_eq(kser[\"a\", \"lama\"], pser[\"a\", \"lama\"])\n        self.assert_eq(kser[kser > 1.5], pser[pser > 1.5])\n\n        msg = r\"'Key length \\(4\\) exceeds index depth \\(3\\)'\"\n        with self.assertRaisesRegex(KeyError, msg):\n            kser[(\"a\", \"lama\", \"speed\", \"x\")]\n\n    def test_keys(self):\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.keys(), pser.keys())\n\n    def test_index(self):\n        # to check setting name of Index properly.\n        idx = pd.Index([1, 2, 3, 4, 5, 6, 7, 8, 9])\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=idx)\n        kser = ks.from_pandas(pser)\n\n        kser.name = \"koalas\"\n        pser.name = \"koalas\"\n        self.assert_eq(kser.index.name, pser.index.name)\n\n        # for check setting names of MultiIndex properly.\n        kser.names = [\"hello\", \"koalas\"]\n        pser.names = [\"hello\", \"koalas\"]\n        self.assert_eq(kser.index.names, pser.index.names)\n\n    def test_pct_change(self):\n        pser = pd.Series([90, 91, 85], index=[2, 4, 1])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.pct_change(), pser.pct_change(), check_exact=False)\n        self.assert_eq(kser.pct_change().sum(), pser.pct_change().sum(), almost=True)\n        self.assert_eq(kser.pct_change(periods=2), pser.pct_change(periods=2), check_exact=False)\n        self.assert_eq(kser.pct_change(periods=-1), pser.pct_change(periods=-1), check_exact=False)\n        self.assert_eq(kser.pct_change(periods=-100000000), pser.pct_change(periods=-100000000))\n        self.assert_eq(kser.pct_change(periods=100000000), pser.pct_change(periods=100000000))\n\n        # for MultiIndex\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.pct_change(), pser.pct_change(), check_exact=False)\n        self.assert_eq(kser.pct_change().sum(), pser.pct_change().sum(), almost=True)\n        self.assert_eq(kser.pct_change(periods=2), pser.pct_change(periods=2), check_exact=False)\n        self.assert_eq(kser.pct_change(periods=-1), pser.pct_change(periods=-1), check_exact=False)\n        self.assert_eq(kser.pct_change(periods=-100000000), pser.pct_change(periods=-100000000))\n        self.assert_eq(kser.pct_change(periods=100000000), pser.pct_change(periods=100000000))\n\n    def test_axes(self):\n        pser = pd.Series([90, 91, 85], index=[2, 4, 1])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.axes, pser.axes)\n\n        # for MultiIndex\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser = pd.Series([45, 200, 1.2, 30, 250, 1.5, 320, 1, 0.3], index=midx)\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.axes, pser.axes)\n\n    def test_udt(self):\n        sparse_values = {0: 0.1, 1: 1.1}\n        sparse_vector = SparseVector(len(sparse_values), sparse_values)\n        pser = pd.Series([sparse_vector])\n\n        if LooseVersion(pyspark.__version__) < LooseVersion(\"2.4\"):\n            with self.sql_conf({SPARK_CONF_ARROW_ENABLED: False}):\n                kser = ks.from_pandas(pser)\n                self.assert_eq(kser, pser)\n        else:\n            kser = ks.from_pandas(pser)\n            self.assert_eq(kser, pser)\n\n    def test_repeat(self):\n        pser = pd.Series([\"a\", \"b\", \"c\"], name=\"0\", index=np.random.rand(3))\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.repeat(3).sort_index(), pser.repeat(3).sort_index())\n        self.assert_eq(kser.repeat(0).sort_index(), pser.repeat(0).sort_index())\n\n        self.assertRaises(ValueError, lambda: kser.repeat(-1))\n        self.assertRaises(ValueError, lambda: kser.repeat(\"abc\"))\n\n        pdf = pd.DataFrame({\"a\": [\"a\", \"b\", \"c\"], \"rep\": [10, 20, 30]}, index=np.random.rand(3))\n        kdf = ks.from_pandas(pdf)\n\n        if LooseVersion(pyspark.__version__) < LooseVersion(\"2.4\"):\n            self.assertRaises(ValueError, lambda: kdf.a.repeat(kdf.rep))\n        else:\n            self.assert_eq(kdf.a.repeat(kdf.rep).sort_index(), pdf.a.repeat(pdf.rep).sort_index())\n\n    def test_take(self):\n        pser = pd.Series([100, 200, 300, 400, 500], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.take([0, 2, 4]).sort_values(), pser.take([0, 2, 4]).sort_values())\n        self.assert_eq(\n            kser.take(range(0, 5, 2)).sort_values(), pser.take(range(0, 5, 2)).sort_values()\n        )\n        self.assert_eq(kser.take([-4, -2, 0]).sort_values(), pser.take([-4, -2, 0]).sort_values())\n        self.assert_eq(\n            kser.take(range(-2, 1, 2)).sort_values(), pser.take(range(-2, 1, 2)).sort_values()\n        )\n\n        # Checking the type of indices.\n        self.assertRaises(ValueError, lambda: kser.take(1))\n        self.assertRaises(ValueError, lambda: kser.take(\"1\"))\n        self.assertRaises(ValueError, lambda: kser.take({1, 2}))\n        self.assertRaises(ValueError, lambda: kser.take({1: None, 2: None}))\n\n    def test_divmod(self):\n        pser = pd.Series([100, None, 300, None, 500], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.0.0\"):\n            kdiv, kmod = kser.divmod(-100)\n            pdiv, pmod = pser.divmod(-100)\n            self.assert_eq(kdiv, pdiv)\n            self.assert_eq(kmod, pmod)\n\n            kdiv, kmod = kser.divmod(100)\n            pdiv, pmod = pser.divmod(100)\n            self.assert_eq(kdiv, pdiv)\n            self.assert_eq(kmod, pmod)\n        elif LooseVersion(pd.__version__) < LooseVersion(\"1.0.0\"):\n            kdiv, kmod = kser.divmod(-100)\n            pdiv, pmod = pser.floordiv(-100), pser.mod(-100)\n            self.assert_eq(kdiv, pdiv)\n            self.assert_eq(kmod, pmod)\n\n            kdiv, kmod = kser.divmod(100)\n            pdiv, pmod = pser.floordiv(100), pser.mod(100)\n            self.assert_eq(kdiv, pdiv)\n            self.assert_eq(kmod, pmod)\n\n    def test_rdivmod(self):\n        pser = pd.Series([100, None, 300, None, 500])\n        kser = ks.from_pandas(pser)\n\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.0.0\"):\n            krdiv, krmod = kser.rdivmod(-100)\n            prdiv, prmod = pser.rdivmod(-100)\n            self.assert_eq(krdiv, prdiv)\n            self.assert_eq(krmod, prmod)\n\n            krdiv, krmod = kser.rdivmod(100)\n            prdiv, prmod = pser.rdivmod(100)\n            self.assert_eq(krdiv, prdiv)\n            self.assert_eq(krmod, prmod)\n        elif LooseVersion(pd.__version__) < LooseVersion(\"1.0.0\"):\n            krdiv, krmod = kser.rdivmod(-100)\n            prdiv, prmod = pser.rfloordiv(-100), pser.rmod(-100)\n            self.assert_eq(krdiv, prdiv)\n            self.assert_eq(krmod, prmod)\n\n            krdiv, krmod = kser.rdivmod(100)\n            prdiv, prmod = pser.rfloordiv(100), pser.rmod(100)\n            self.assert_eq(krdiv, prdiv)\n            self.assert_eq(krmod, prmod)\n\n    def test_mod(self):\n        pser = pd.Series([100, None, -300, None, 500, -700], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.mod(-150), pser.mod(-150))\n        self.assert_eq(kser.mod(0), pser.mod(0))\n        self.assert_eq(kser.mod(150), pser.mod(150))\n\n        pdf = pd.DataFrame({\"a\": [100, None, -300, None, 500, -700], \"b\": [150] * 6})\n        kdf = ks.from_pandas(pdf)\n        self.assert_eq(kdf.a.mod(kdf.b), pdf.a.mod(pdf.b))\n\n    def test_mode(self):\n        pser = pd.Series([0, 0, 1, 1, 1, np.nan, np.nan, np.nan])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.mode(), pser.mode())\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24\"):\n            # The `dropna` argument is added in pandas 0.24.\n            self.assert_eq(\n                kser.mode(dropna=False).sort_values().reset_index(drop=True),\n                pser.mode(dropna=False).sort_values().reset_index(drop=True),\n            )\n\n        pser.name = \"x\"\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.mode(), pser.mode())\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24\"):\n            # The `dropna` argument is added in pandas 0.24.\n            self.assert_eq(\n                kser.mode(dropna=False).sort_values().reset_index(drop=True),\n                pser.mode(dropna=False).sort_values().reset_index(drop=True),\n            )\n\n    def test_rmod(self):\n        pser = pd.Series([100, None, -300, None, 500, -700], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.rmod(-150), pser.rmod(-150))\n        self.assert_eq(kser.rmod(0), pser.rmod(0))\n        self.assert_eq(kser.rmod(150), pser.rmod(150))\n\n        pdf = pd.DataFrame({\"a\": [100, None, -300, None, 500, -700], \"b\": [150] * 6})\n        kdf = ks.from_pandas(pdf)\n        self.assert_eq(kdf.a.rmod(kdf.b), pdf.a.rmod(pdf.b))\n\n    def test_asof(self):\n        pser = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.asof(20), pser.asof(20))\n        self.assert_eq(kser.asof([5, 20]).sort_index(), pser.asof([5, 20]).sort_index())\n        self.assert_eq(kser.asof(100), pser.asof(100))\n        self.assert_eq(repr(kser.asof(-100)), repr(pser.asof(-100)))\n        self.assert_eq(kser.asof([-100, 100]).sort_index(), pser.asof([-100, 100]).sort_index())\n\n        # where cannot be an Index, Series or a DataFrame\n        self.assertRaises(ValueError, lambda: kser.asof(ks.Index([-100, 100])))\n        self.assertRaises(ValueError, lambda: kser.asof(ks.Series([-100, 100])))\n        self.assertRaises(ValueError, lambda: kser.asof(ks.DataFrame({\"A\": [1, 2, 3]})))\n        # asof is not supported for a MultiIndex\n        pser.index = pd.MultiIndex.from_tuples([(\"x\", \"a\"), (\"x\", \"b\"), (\"y\", \"c\"), (\"y\", \"d\")])\n        kser = ks.from_pandas(pser)\n        self.assertRaises(ValueError, lambda: kser.asof(20))\n        # asof requires a sorted index (More precisely, should be a monotonic increasing)\n        kser = ks.Series([1, 2, np.nan, 4], index=[10, 30, 20, 40], name=\"Koalas\")\n        self.assertRaises(ValueError, lambda: kser.asof(20))\n        kser = ks.Series([1, 2, np.nan, 4], index=[40, 30, 20, 10], name=\"Koalas\")\n        self.assertRaises(ValueError, lambda: kser.asof(20))\n\n        pidx = pd.DatetimeIndex([\"2013-12-31\", \"2014-01-02\", \"2014-01-03\"])\n        pser = pd.Series([1, 2, np.nan], index=pidx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.asof(\"2014-01-01\"), pser.asof(\"2014-01-01\"))\n        self.assert_eq(kser.asof(\"2014-01-02\"), pser.asof(\"2014-01-02\"))\n        self.assert_eq(repr(kser.asof(\"1999-01-02\")), repr(pser.asof(\"1999-01-02\")))\n\n    def test_squeeze(self):\n        # Single value\n        pser = pd.Series([90])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.squeeze(), pser.squeeze())\n\n        # Single value with MultiIndex\n        midx = pd.MultiIndex.from_tuples([(\"a\", \"b\", \"c\")])\n        pser = pd.Series([90], index=midx)\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.squeeze(), pser.squeeze())\n\n        # Multiple values\n        pser = pd.Series([90, 91, 85])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.squeeze(), pser.squeeze())\n\n        # Multiple values with MultiIndex\n        midx = pd.MultiIndex.from_tuples([(\"a\", \"x\"), (\"b\", \"y\"), (\"c\", \"z\")])\n        pser = pd.Series([90, 91, 85], index=midx)\n        kser = ks.from_pandas(pser)\n        self.assert_eq(kser.squeeze(), pser.squeeze())\n\n    def test_swaplevel(self):\n        # MultiIndex with two levels\n        arrays = [[1, 1, 2, 2], [\"red\", \"blue\", \"red\", \"blue\"]]\n        pidx = pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\"))\n        pser = pd.Series([\"a\", \"b\", \"c\", \"d\"], index=pidx)\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.swaplevel(), kser.swaplevel())\n        self.assert_eq(pser.swaplevel(0, 1), kser.swaplevel(0, 1))\n        self.assert_eq(pser.swaplevel(1, 1), kser.swaplevel(1, 1))\n        self.assert_eq(pser.swaplevel(\"number\", \"color\"), kser.swaplevel(\"number\", \"color\"))\n\n        # MultiIndex with more than two levels\n        arrays = [[1, 1, 2, 2], [\"red\", \"blue\", \"red\", \"blue\"], [\"l\", \"m\", \"s\", \"xs\"]]\n        pidx = pd.MultiIndex.from_arrays(arrays, names=(\"number\", \"color\", \"size\"))\n        pser = pd.Series([\"a\", \"b\", \"c\", \"d\"], index=pidx)\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.swaplevel(), kser.swaplevel())\n        self.assert_eq(pser.swaplevel(0, 1), kser.swaplevel(0, 1))\n        self.assert_eq(pser.swaplevel(0, 2), kser.swaplevel(0, 2))\n        self.assert_eq(pser.swaplevel(1, 2), kser.swaplevel(1, 2))\n        self.assert_eq(pser.swaplevel(1, 1), kser.swaplevel(1, 1))\n        self.assert_eq(pser.swaplevel(-1, -2), kser.swaplevel(-1, -2))\n        self.assert_eq(pser.swaplevel(\"number\", \"color\"), kser.swaplevel(\"number\", \"color\"))\n        self.assert_eq(pser.swaplevel(\"number\", \"size\"), kser.swaplevel(\"number\", \"size\"))\n        self.assert_eq(pser.swaplevel(\"color\", \"size\"), kser.swaplevel(\"color\", \"size\"))\n\n        # Error conditions\n        self.assertRaises(AssertionError, lambda: ks.Series([1, 2]).swaplevel())\n        self.assertRaises(IndexError, lambda: kser.swaplevel(0, 9))\n        self.assertRaises(KeyError, lambda: kser.swaplevel(\"not_number\", \"color\"))\n        self.assertRaises(AssertionError, lambda: kser.swaplevel(copy=False))\n\n    def test_swapaxes(self):\n        pser = pd.Series([1, 2, 3], index=[\"x\", \"y\", \"z\"], name=\"ser\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(kser.swapaxes(0, 0), pser.swapaxes(0, 0))\n        self.assert_eq(kser.swapaxes(\"index\", \"index\"), pser.swapaxes(\"index\", \"index\"))\n        self.assert_eq((kser + 1).swapaxes(0, 0), (pser + 1).swapaxes(0, 0))\n\n        self.assertRaises(AssertionError, lambda: kser.swapaxes(0, 1, copy=False))\n        self.assertRaises(ValueError, lambda: kser.swapaxes(0, 1))\n        self.assertRaises(ValueError, lambda: kser.swapaxes(\"index\", \"columns\"))\n\n    def test_div_zero_and_nan(self):\n        pser = pd.Series([100, None, -300, None, 500, -700, np.inf, -np.inf], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.div(0), kser.div(0))\n        self.assert_eq(pser.truediv(0), kser.truediv(0))\n        self.assert_eq(pser / 0, kser / 0)\n        self.assert_eq(pser.div(np.nan), kser.div(np.nan))\n        self.assert_eq(pser.truediv(np.nan), kser.truediv(np.nan))\n        self.assert_eq(pser / np.nan, kser / np.nan)\n\n        # floordiv has different behavior in pandas > 1.0.0 when divide by 0\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.0.0\"):\n            self.assert_eq(pser.floordiv(0), kser.floordiv(0))\n            self.assert_eq(pser // 0, kser // 0)\n        else:\n            result = pd.Series(\n                [np.inf, np.nan, -np.inf, np.nan, np.inf, -np.inf, np.inf, -np.inf], name=\"Koalas\"\n            )\n            self.assert_eq(kser.floordiv(0), result)\n            self.assert_eq(kser // 0, result)\n        self.assert_eq(pser.floordiv(np.nan), kser.floordiv(np.nan))\n\n    def test_mad(self):\n        pser = pd.Series([1, 2, 3, 4], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.mad(), kser.mad())\n\n        pser = pd.Series([None, -2, 5, 10, 50, np.nan, -20], name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.mad(), kser.mad())\n\n        pmidx = pd.MultiIndex.from_tuples(\n            [(\"a\", \"1\"), (\"a\", \"2\"), (\"b\", \"1\"), (\"b\", \"2\"), (\"c\", \"1\")]\n        )\n        pser = pd.Series([1, 2, 3, 4, 5], name=\"Koalas\")\n        pser.index = pmidx\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.mad(), kser.mad())\n\n        pmidx = pd.MultiIndex.from_tuples(\n            [(\"a\", \"1\"), (\"a\", \"2\"), (\"b\", \"1\"), (\"b\", \"2\"), (\"c\", \"1\")]\n        )\n        pser = pd.Series([None, -2, 5, 50, np.nan], name=\"Koalas\")\n        pser.index = pmidx\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.mad(), kser.mad())\n\n    def test_to_frame(self):\n        pser = pd.Series([\"a\", \"b\", \"c\"])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.to_frame(name=\"a\"), kser.to_frame(name=\"a\"))\n\n        # for MultiIndex\n        midx = pd.MultiIndex.from_tuples([(\"a\", \"x\"), (\"b\", \"y\"), (\"c\", \"z\")])\n        pser = pd.Series([\"a\", \"b\", \"c\"], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.to_frame(name=\"a\"), kser.to_frame(name=\"a\"))\n\n    def test_shape(self):\n        pser = pd.Series([\"a\", \"b\", \"c\"])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.shape, kser.shape)\n\n        # for MultiIndex\n        midx = pd.MultiIndex.from_tuples([(\"a\", \"x\"), (\"b\", \"y\"), (\"c\", \"z\")])\n        pser = pd.Series([\"a\", \"b\", \"c\"], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.shape, kser.shape)\n\n    def test_to_markdown(self):\n        pser = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\")\n        kser = ks.from_pandas(pser)\n\n        # `to_markdown()` is supported in pandas >= 1.0.0 since it's newly added in pandas 1.0.0.\n        if LooseVersion(pd.__version__) < LooseVersion(\"1.0.0\"):\n            self.assertRaises(NotImplementedError, lambda: kser.to_markdown())\n        else:\n            self.assert_eq(pser.to_markdown(), kser.to_markdown())\n\n    def test_unstack(self):\n        pser = pd.Series(\n            [10, -2, 4, 7],\n            index=pd.MultiIndex.from_tuples(\n                [(\"one\", \"a\", \"z\"), (\"one\", \"b\", \"x\"), (\"two\", \"a\", \"c\"), (\"two\", \"b\", \"v\")],\n                names=[\"A\", \"B\", \"C\"],\n            ),\n        )\n        kser = ks.from_pandas(pser)\n\n        levels = [-3, -2, -1, 0, 1, 2]\n        for level in levels:\n            pandas_result = pser.unstack(level=level)\n            koalas_result = kser.unstack(level=level).sort_index()\n            self.assert_eq(pandas_result, koalas_result)\n            self.assert_eq(pandas_result.index.names, koalas_result.index.names)\n            self.assert_eq(pandas_result.columns.names, koalas_result.columns.names)\n\n        # non-numeric datatypes\n        pser = pd.Series(\n            list(\"abcd\"), index=pd.MultiIndex.from_product([[\"one\", \"two\"], [\"a\", \"b\"]])\n        )\n        kser = ks.from_pandas(pser)\n\n        levels = [-2, -1, 0, 1]\n        for level in levels:\n            pandas_result = pser.unstack(level=level)\n            koalas_result = kser.unstack(level=level).sort_index()\n            self.assert_eq(pandas_result, koalas_result)\n            self.assert_eq(pandas_result.index.names, koalas_result.index.names)\n            self.assert_eq(pandas_result.columns.names, koalas_result.columns.names)\n\n        # Exceeding the range of level\n        self.assertRaises(IndexError, lambda: kser.unstack(level=3))\n        self.assertRaises(IndexError, lambda: kser.unstack(level=-4))\n        # Only support for MultiIndex\n        kser = ks.Series([10, -2, 4, 7])\n        self.assertRaises(ValueError, lambda: kser.unstack())\n\n    def test_item(self):\n        kser = ks.Series([10, 20])\n        self.assertRaises(ValueError, lambda: kser.item())\n\n    def test_filter(self):\n        pser = pd.Series([0, 1, 2], index=[\"one\", \"two\", \"three\"])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.filter(items=[\"one\", \"three\"]), kser.filter(items=[\"one\", \"three\"]))\n        self.assert_eq(pser.filter(regex=\"e$\"), kser.filter(regex=\"e$\"))\n        self.assert_eq(pser.filter(like=\"hre\"), kser.filter(like=\"hre\"))\n\n        with self.assertRaisesRegex(ValueError, \"Series does not support columns axis.\"):\n            kser.filter(like=\"hre\", axis=1)\n\n        # for MultiIndex\n        midx = pd.MultiIndex.from_tuples([(\"one\", \"x\"), (\"two\", \"y\"), (\"three\", \"z\")])\n        pser = pd.Series([0, 1, 2], index=midx)\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(\n            pser.filter(items=[(\"one\", \"x\"), (\"three\", \"z\")]),\n            kser.filter(items=[(\"one\", \"x\"), (\"three\", \"z\")]),\n        )\n\n        with self.assertRaisesRegex(TypeError, \"Unsupported type list\"):\n            kser.filter(items=[[\"one\", \"x\"], (\"three\", \"z\")])\n\n        with self.assertRaisesRegex(ValueError, \"The item should not be empty.\"):\n            kser.filter(items=[(), (\"three\", \"z\")])\n\n    def test_abs(self):\n        pser = pd.Series([-2, -1, 0, 1])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(abs(kser), abs(pser))\n        self.assert_eq(np.abs(kser), np.abs(pser))\n\n    def test_bfill(self):\n        pdf = pd.DataFrame({\"x\": [np.nan, 2, 3, 4, np.nan, 6], \"y\": [np.nan, 2, 3, 4, np.nan, 6]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.bfill(), pser.bfill())\n        self.assert_eq(kser.bfill()[0], pser.bfill()[0])\n\n        kser.bfill(inplace=True)\n        pser.bfill(inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kser[0], pser[0])\n        self.assert_eq(kdf, pdf)\n\n    def test_ffill(self):\n        pdf = pd.DataFrame({\"x\": [np.nan, 2, 3, 4, np.nan, 6], \"y\": [np.nan, 2, 3, 4, np.nan, 6]})\n        kdf = ks.from_pandas(pdf)\n\n        pser = pdf.x\n        kser = kdf.x\n\n        self.assert_eq(kser.ffill(), pser.ffill())\n        self.assert_eq(kser.ffill()[4], pser.ffill()[4])\n\n        kser.ffill(inplace=True)\n        pser.ffill(inplace=True)\n        self.assert_eq(kser, pser)\n        self.assert_eq(kser[4], pser[4])\n        self.assert_eq(kdf, pdf)\n\n    def test_iteritems(self):\n        pser = pd.Series([\"A\", \"B\", \"C\"])\n        kser = ks.from_pandas(pser)\n\n        for (p_name, p_items), (k_name, k_items) in zip(pser.iteritems(), kser.iteritems()):\n            self.assert_eq(p_name, k_name)\n            self.assert_eq(p_items, k_items)\n\n    def test_droplevel(self):\n        # droplevel is new in pandas 0.24.0\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.24.0\"):\n            pser = pd.Series(\n                [1, 2, 3],\n                index=pd.MultiIndex.from_tuples(\n                    [(\"x\", \"a\", \"q\"), (\"x\", \"b\", \"w\"), (\"y\", \"c\", \"e\")],\n                    names=[\"level_1\", \"level_2\", \"level_3\"],\n                ),\n            )\n            kser = ks.from_pandas(pser)\n\n            self.assert_eq(pser.droplevel(0), kser.droplevel(0))\n            self.assert_eq(pser.droplevel(\"level_1\"), kser.droplevel(\"level_1\"))\n            self.assert_eq(pser.droplevel(-1), kser.droplevel(-1))\n            self.assert_eq(pser.droplevel([0]), kser.droplevel([0]))\n            self.assert_eq(pser.droplevel([\"level_1\"]), kser.droplevel([\"level_1\"]))\n            self.assert_eq(pser.droplevel((0,)), kser.droplevel((0,)))\n            self.assert_eq(pser.droplevel((\"level_1\",)), kser.droplevel((\"level_1\",)))\n            self.assert_eq(pser.droplevel([0, 2]), kser.droplevel([0, 2]))\n            self.assert_eq(\n                pser.droplevel([\"level_1\", \"level_3\"]), kser.droplevel([\"level_1\", \"level_3\"])\n            )\n            self.assert_eq(pser.droplevel((1, 2)), kser.droplevel((1, 2)))\n            self.assert_eq(\n                pser.droplevel((\"level_2\", \"level_3\")), kser.droplevel((\"level_2\", \"level_3\"))\n            )\n\n            with self.assertRaisesRegex(KeyError, \"Level {0, 1, 2} not found\"):\n                kser.droplevel({0, 1, 2})\n            with self.assertRaisesRegex(KeyError, \"Level level_100 not found\"):\n                kser.droplevel([\"level_1\", \"level_100\"])\n            with self.assertRaisesRegex(\n                IndexError, \"Too many levels: Index has only 3 levels, not 11\"\n            ):\n                kser.droplevel(10)\n            with self.assertRaisesRegex(\n                IndexError,\n                \"Too many levels: Index has only 3 levels, -10 is not a valid level number\",\n            ):\n                kser.droplevel(-10)\n            with self.assertRaisesRegex(\n                ValueError,\n                \"Cannot remove 3 levels from an index with 3 levels: \"\n                \"at least one level must be left.\",\n            ):\n                kser.droplevel([0, 1, 2])\n            with self.assertRaisesRegex(\n                ValueError,\n                \"Cannot remove 5 levels from an index with 3 levels: \"\n                \"at least one level must be left.\",\n            ):\n                kser.droplevel([1, 1, 1, 1, 1])\n\n            # Tupled names\n            pser.index.names = [(\"a\", \"1\"), (\"b\", \"2\"), (\"c\", \"3\")]\n            kser = ks.from_pandas(pser)\n\n            self.assert_eq(\n                pser.droplevel([(\"a\", \"1\"), (\"c\", \"3\")]), kser.droplevel([(\"a\", \"1\"), (\"c\", \"3\")])\n            )\n\n    def test_dot(self):\n        pdf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [4, 5, 6]})\n        kdf = ks.from_pandas(pdf)\n\n        self.assert_eq((kdf[\"b\"] * 10).dot(kdf[\"a\"]), (pdf[\"b\"] * 10).dot(pdf[\"a\"]))\n        self.assert_eq((kdf[\"b\"] * 10).dot(kdf), (pdf[\"b\"] * 10).dot(pdf))\n        self.assert_eq((kdf[\"b\"] * 10).dot(kdf + 1), (pdf[\"b\"] * 10).dot(pdf + 1))\n\n    @unittest.skipIf(\n        LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"),\n        \"tail won't work properly with PySpark<3.0\",\n    )\n    def test_tail(self):\n        pser = pd.Series(range(1000), name=\"Koalas\")\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.tail(), kser.tail())\n        self.assert_eq(pser.tail(10), kser.tail(10))\n        self.assert_eq(pser.tail(-990), kser.tail(-990))\n        self.assert_eq(pser.tail(0), kser.tail(0))\n        self.assert_eq(pser.tail(1001), kser.tail(1001))\n        self.assert_eq(pser.tail(-1001), kser.tail(-1001))\n        self.assert_eq((pser + 1).tail(), (kser + 1).tail())\n        self.assert_eq((pser + 1).tail(10), (kser + 1).tail(10))\n        self.assert_eq((pser + 1).tail(-990), (kser + 1).tail(-990))\n        self.assert_eq((pser + 1).tail(0), (kser + 1).tail(0))\n        self.assert_eq((pser + 1).tail(1001), (kser + 1).tail(1001))\n        self.assert_eq((pser + 1).tail(-1001), (kser + 1).tail(-1001))\n        with self.assertRaisesRegex(TypeError, \"bad operand type for unary -: 'str'\"):\n            kser.tail(\"10\")\n\n    def test_product(self):\n        pser = pd.Series([10, 20, 30, 40, 50])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        # Containing NA values\n        pser = pd.Series([10, np.nan, 30, np.nan, 50])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod(), almost=True)\n\n        # All-NA values\n        pser = pd.Series([np.nan, np.nan, np.nan])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        # Empty Series\n        pser = pd.Series([])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        # Boolean Series\n        pser = pd.Series([True, True, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        pser = pd.Series([False, False, False])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        pser = pd.Series([True, False, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(), kser.prod())\n\n        # With `min_count` parameter\n        pser = pd.Series([10, 20, 30, 40, 50])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(min_count=5), kser.prod(min_count=5))\n        self.assert_eq(pser.prod(min_count=6), kser.prod(min_count=6))\n\n        pser = pd.Series([10, np.nan, 30, np.nan, 50])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(min_count=3), kser.prod(min_count=3), almost=True)\n        self.assert_eq(pser.prod(min_count=4), kser.prod(min_count=4))\n\n        pser = pd.Series([np.nan, np.nan, np.nan])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(min_count=1), kser.prod(min_count=1))\n\n        pser = pd.Series([])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.prod(min_count=1), kser.prod(min_count=1))\n\n        with self.assertRaisesRegex(TypeError, \"Could not convert object \\\\(string\\\\) to numeric\"):\n            ks.Series([\"a\", \"b\", \"c\"]).prod()\n        with self.assertRaisesRegex(\n            TypeError, \"Could not convert datetime64\\\\[ns\\\\] \\\\(timestamp\\\\) to numeric\"\n        ):\n            ks.Series([pd.Timestamp(\"2016-01-01\") for _ in range(3)]).prod()\n\n    def test_hasnans(self):\n        # BooleanType\n        pser = pd.Series([True, False, True, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.hasnans, kser.hasnans)\n\n        pser = pd.Series([True, False, np.nan, True])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.hasnans, kser.hasnans)\n\n        # TimestampType\n        pser = pd.Series([pd.Timestamp(\"2020-07-30\") for _ in range(3)])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.hasnans, kser.hasnans)\n\n        pser = pd.Series([pd.Timestamp(\"2020-07-30\"), np.nan, pd.Timestamp(\"2020-07-30\")])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.hasnans, kser.hasnans)\n\n    @unittest.skipIf(\n        LooseVersion(pyspark.__version__) < LooseVersion(\"3.0\"),\n        \"last_valid_index won't work properly with PySpark<3.0\",\n    )\n    def test_last_valid_index(self):\n        pser = pd.Series([250, 1.5, 320, 1, 0.3, None, None, None, None])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.last_valid_index(), kser.last_valid_index())\n\n        # MultiIndex columns\n        midx = pd.MultiIndex(\n            [[\"lama\", \"cow\", \"falcon\"], [\"speed\", \"weight\", \"length\"]],\n            [[0, 0, 0, 1, 1, 1, 2, 2, 2], [0, 1, 2, 0, 1, 2, 0, 1, 2]],\n        )\n        pser.index = midx\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.last_valid_index(), kser.last_valid_index())\n\n        # Empty Series\n        pser = pd.Series([])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.last_valid_index(), kser.last_valid_index())\n\n    def test_first_valid_index(self):\n        # Empty Series\n        pser = pd.Series([])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.first_valid_index(), kser.first_valid_index())\n\n    def test_factorize(self):\n        pser = pd.Series([\"a\", \"b\", \"a\", \"b\"])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series([5, 1, 5, 1])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = (pser + 1).factorize(sort=True)\n        kcodes, kuniques = (kser + 1).factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series([\"a\", \"b\", \"a\", \"b\"], name=\"ser\", index=[\"w\", \"x\", \"y\", \"z\"])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series(\n            [\"a\", \"b\", \"a\", \"b\"], index=pd.MultiIndex.from_arrays([[4, 3, 2, 1], [1, 2, 3, 4]])\n        )\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        #\n        # Deals with None and np.nan\n        #\n        pser = pd.Series([\"a\", \"b\", \"a\", np.nan])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series([1, None, 3, 2, 1])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series([\"a\", None, \"a\"])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize(sort=True)\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pser = pd.Series([None, np.nan])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize()\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes, kcodes.to_list())\n        # pandas: Float64Index([], dtype='float64')\n        self.assert_eq(pd.Index([]), kuniques)\n\n        pser = pd.Series([np.nan, np.nan])\n        kser = ks.from_pandas(pser)\n        pcodes, puniques = pser.factorize()\n        kcodes, kuniques = kser.factorize()\n        self.assert_eq(pcodes, kcodes.to_list())\n        # pandas: Float64Index([], dtype='float64')\n        self.assert_eq(pd.Index([]), kuniques)\n\n        #\n        # Deals with na_sentinel\n        #\n        # pandas >= 1.1.2 support na_sentinel=None\n        # pandas >= 0.24 support na_sentinel not to be -1\n        #\n        pd_below_1_1_2 = LooseVersion(pd.__version__) < LooseVersion(\"1.1.2\")\n        pd_below_0_24 = LooseVersion(pd.__version__) < LooseVersion(\"0.24\")\n\n        pser = pd.Series([\"a\", \"b\", \"a\", np.nan, None])\n        kser = ks.from_pandas(pser)\n\n        pcodes, puniques = pser.factorize(sort=True, na_sentinel=-2)\n        kcodes, kuniques = kser.factorize(na_sentinel=-2)\n        self.assert_eq([0, 1, 0, -2, -2] if pd_below_0_24 else pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        pcodes, puniques = pser.factorize(sort=True, na_sentinel=2)\n        kcodes, kuniques = kser.factorize(na_sentinel=2)\n        self.assert_eq([0, 1, 0, 2, 2] if pd_below_0_24 else pcodes.tolist(), kcodes.to_list())\n        self.assert_eq(puniques, kuniques)\n\n        if not pd_below_1_1_2:\n            pcodes, puniques = pser.factorize(sort=True, na_sentinel=None)\n            kcodes, kuniques = kser.factorize(na_sentinel=None)\n            self.assert_eq(pcodes.tolist(), kcodes.to_list())\n            # puniques is Index(['a', 'b', nan], dtype='object')\n            self.assert_eq(ks.Index([\"a\", \"b\", None]), kuniques)\n\n            kser = ks.Series([1, 2, np.nan, 4, 5])  # Arrow takes np.nan as null\n            kser.loc[3] = np.nan  # Spark takes np.nan as NaN\n            kcodes, kuniques = kser.factorize(na_sentinel=None)\n            pcodes, puniques = kser.to_pandas().factorize(sort=True, na_sentinel=None)\n            self.assert_eq(pcodes.tolist(), kcodes.to_list())\n            self.assert_eq(puniques, kuniques)\n\n    def test_pad(self):\n        pser = pd.Series([np.nan, 2, 3, 4, np.nan, 6], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.1\"):\n            self.assert_eq(pser.pad(), kser.pad())\n\n            # Test `inplace=True`\n            pser.pad(inplace=True)\n            kser.pad(inplace=True)\n            self.assert_eq(pser, kser)\n        else:\n            expected = ks.Series([np.nan, 2, 3, 4, 4, 6], name=\"x\")\n            self.assert_eq(expected, kser.pad())\n\n            # Test `inplace=True`\n            kser.pad(inplace=True)\n            self.assert_eq(expected, kser)\n\n    def test_explode(self):\n        if LooseVersion(pd.__version__) >= LooseVersion(\"0.25\"):\n            pser = pd.Series([[1, 2, 3], [], None, [3, 4]])\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.explode(), kser.explode(), almost=True)\n\n            # MultiIndex\n            pser.index = pd.MultiIndex.from_tuples([(\"a\", \"w\"), (\"b\", \"x\"), (\"c\", \"y\"), (\"d\", \"z\")])\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.explode(), kser.explode(), almost=True)\n\n            # non-array type Series\n            pser = pd.Series([1, 2, 3, 4])\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.explode(), kser.explode())\n        else:\n            pser = pd.Series([[1, 2, 3], [], None, [3, 4]])\n            kser = ks.from_pandas(pser)\n            expected = pd.Series([1.0, 2.0, 3.0, None, None, 3.0, 4.0], index=[0, 0, 0, 1, 2, 3, 3])\n            self.assert_eq(kser.explode(), expected)\n\n            # MultiIndex\n            pser.index = pd.MultiIndex.from_tuples([(\"a\", \"w\"), (\"b\", \"x\"), (\"c\", \"y\"), (\"d\", \"z\")])\n            kser = ks.from_pandas(pser)\n            expected = pd.Series(\n                [1.0, 2.0, 3.0, None, None, 3.0, 4.0],\n                index=pd.MultiIndex.from_tuples(\n                    [\n                        (\"a\", \"w\"),\n                        (\"a\", \"w\"),\n                        (\"a\", \"w\"),\n                        (\"b\", \"x\"),\n                        (\"c\", \"y\"),\n                        (\"d\", \"z\"),\n                        (\"d\", \"z\"),\n                    ]\n                ),\n            )\n            self.assert_eq(kser.explode(), expected)\n\n            # non-array type Series\n            pser = pd.Series([1, 2, 3, 4])\n            kser = ks.from_pandas(pser)\n            expected = pser\n            self.assert_eq(kser.explode(), expected)\n\n    def test_argsort(self):\n        # Without null values\n        pser = pd.Series([0, -100, 50, 100, 20], index=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # MultiIndex\n        pser.index = pd.MultiIndex.from_tuples(\n            [(\"a\", \"v\"), (\"b\", \"w\"), (\"c\", \"x\"), (\"d\", \"y\"), (\"e\", \"z\")]\n        )\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # With name\n        pser.name = \"Koalas\"\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # Series from Index\n        pidx = pd.Index([4.0, -6.0, 2.0, -100.0, 11.0, 20.0, 1.0, -99.0])\n        kidx = ks.from_pandas(pidx)\n        self.assert_eq(\n            pidx.to_series().argsort().sort_index(), kidx.to_series().argsort().sort_index()\n        )\n        self.assert_eq(\n            (-pidx.to_series()).argsort().sort_index(), (-kidx.to_series()).argsort().sort_index()\n        )\n\n        # Series from Index with name\n        pidx.name = \"Koalas\"\n        kidx = ks.from_pandas(pidx)\n        self.assert_eq(\n            pidx.to_series().argsort().sort_index(), kidx.to_series().argsort().sort_index()\n        )\n        self.assert_eq(\n            (-pidx.to_series()).argsort().sort_index(), (-kidx.to_series()).argsort().sort_index()\n        )\n\n        # Series from DataFrame\n        pdf = pd.DataFrame({\"A\": [4.0, -6.0, 2.0, np.nan, -100.0, 11.0, 20.0, np.nan, 1.0, -99.0]})\n        kdf = ks.from_pandas(pdf)\n        self.assert_eq(pdf.A.argsort().sort_index(), kdf.A.argsort().sort_index())\n        self.assert_eq((-pdf.A).argsort().sort_index(), (-kdf.A).argsort().sort_index())\n\n        # With null values\n        pser = pd.Series([0, -100, np.nan, 100, np.nan], index=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # MultiIndex with null values\n        pser.index = pd.MultiIndex.from_tuples(\n            [(\"a\", \"v\"), (\"b\", \"w\"), (\"c\", \"x\"), (\"d\", \"y\"), (\"e\", \"z\")]\n        )\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # With name with null values\n        pser.name = \"Koalas\"\n        kser = ks.from_pandas(pser)\n        self.assert_eq(pser.argsort().sort_index(), kser.argsort().sort_index())\n        self.assert_eq((-pser).argsort().sort_index(), (-kser).argsort().sort_index())\n\n        # Series from Index with null values\n        pidx = pd.Index([4.0, -6.0, 2.0, np.nan, -100.0, 11.0, 20.0, np.nan, 1.0, -99.0])\n        kidx = ks.from_pandas(pidx)\n        self.assert_eq(\n            pidx.to_series().argsort().sort_index(), kidx.to_series().argsort().sort_index()\n        )\n        self.assert_eq(\n            (-pidx.to_series()).argsort().sort_index(), (-kidx.to_series()).argsort().sort_index()\n        )\n\n        # Series from Index with name with null values\n        pidx.name = \"Koalas\"\n        kidx = ks.from_pandas(pidx)\n        self.assert_eq(\n            pidx.to_series().argsort().sort_index(), kidx.to_series().argsort().sort_index()\n        )\n        self.assert_eq(\n            (-pidx.to_series()).argsort().sort_index(), (-kidx.to_series()).argsort().sort_index()\n        )\n\n        # Series from DataFrame with null values\n        pdf = pd.DataFrame({\"A\": [4.0, -6.0, 2.0, np.nan, -100.0, 11.0, 20.0, np.nan, 1.0, -99.0]})\n        kdf = ks.from_pandas(pdf)\n        self.assert_eq(pdf.A.argsort().sort_index(), kdf.A.argsort().sort_index())\n        self.assert_eq((-pdf.A).argsort().sort_index(), (-kdf.A).argsort().sort_index())\n\n    def test_argmin_argmax(self):\n        pser = pd.Series(\n            {\n                \"Corn Flakes\": 100.0,\n                \"Almond Delight\": 110.0,\n                \"Cinnamon Toast Crunch\": 120.0,\n                \"Cocoa Puff\": 110.0,\n                \"Expensive Flakes\": 120.0,\n                \"Cheap Flakes\": 100.0,\n            },\n            name=\"Koalas\",\n        )\n        kser = ks.from_pandas(pser)\n\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.0\"):\n            self.assert_eq(pser.argmin(), kser.argmin())\n            self.assert_eq(pser.argmax(), kser.argmax())\n\n            # MultiIndex\n            pser.index = pd.MultiIndex.from_tuples(\n                [(\"a\", \"t\"), (\"b\", \"u\"), (\"c\", \"v\"), (\"d\", \"w\"), (\"e\", \"x\"), (\"f\", \"u\")]\n            )\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.argmin(), kser.argmin())\n            self.assert_eq(pser.argmax(), kser.argmax())\n\n            # Null Series\n            self.assert_eq(pd.Series([np.nan]).argmin(), ks.Series([np.nan]).argmin())\n            self.assert_eq(pd.Series([np.nan]).argmax(), ks.Series([np.nan]).argmax())\n        else:\n            self.assert_eq(pser.values.argmin(), kser.argmin())\n            self.assert_eq(pser.values.argmax(), kser.argmax())\n\n            # MultiIndex\n            pser.index = pd.MultiIndex.from_tuples(\n                [(\"a\", \"t\"), (\"b\", \"u\"), (\"c\", \"v\"), (\"d\", \"w\"), (\"e\", \"x\"), (\"f\", \"u\")]\n            )\n            kser = ks.from_pandas(pser)\n            self.assert_eq(pser.values.argmin(), kser.argmin())\n            self.assert_eq(pser.values.argmax(), kser.argmax())\n\n            # Null Series\n            self.assert_eq(-1, ks.Series([np.nan]).argmin())\n            self.assert_eq(-1, ks.Series([np.nan]).argmax())\n\n        with self.assertRaisesRegex(ValueError, \"attempt to get argmin of an empty sequence\"):\n            ks.Series([]).argmin()\n        with self.assertRaisesRegex(ValueError, \"attempt to get argmax of an empty sequence\"):\n            ks.Series([]).argmax()\n\n    def test_backfill(self):\n        pser = pd.Series([np.nan, 2, 3, 4, np.nan, 6], name=\"x\")\n        kser = ks.from_pandas(pser)\n\n        if LooseVersion(pd.__version__) >= LooseVersion(\"1.1\"):\n            self.assert_eq(pser.backfill(), kser.backfill())\n\n            # Test `inplace=True`\n            pser.backfill(inplace=True)\n            kser.backfill(inplace=True)\n            self.assert_eq(pser, kser)\n        else:\n            expected = ks.Series([2.0, 2.0, 3.0, 4.0, 6.0, 6.0], name=\"x\")\n            self.assert_eq(expected, kser.backfill())\n\n            # Test `inplace=True`\n            kser.backfill(inplace=True)\n            self.assert_eq(expected, kser)\n\n    def test_align(self):\n        pdf = pd.DataFrame({\"a\": [1, 2, 3], \"b\": [\"a\", \"b\", \"c\"]})\n        kdf = ks.from_pandas(pdf)\n\n        for join in [\"outer\", \"inner\", \"left\", \"right\"]:\n            for axis in [None, 0]:\n                kser_l, kser_r = kdf.a.align(kdf.b, join=join, axis=axis)\n                pser_l, pser_r = pdf.a.align(pdf.b, join=join, axis=axis)\n                self.assert_eq(kser_l, pser_l)\n                self.assert_eq(kser_r, pser_r)\n\n                kser_l, kdf_r = kdf.b.align(kdf[[\"b\", \"a\"]], join=join, axis=axis)\n                pser_l, pdf_r = pdf.b.align(pdf[[\"b\", \"a\"]], join=join, axis=axis)\n                self.assert_eq(kser_l, pser_l)\n                self.assert_eq(kdf_r, pdf_r)\n\n        self.assertRaises(ValueError, lambda: kdf.a.align(kdf.b, axis=1))\n\n    def test_pow_and_rpow(self):\n        pser = pd.Series([1, 2, np.nan])\n        kser = ks.from_pandas(pser)\n\n        self.assert_eq(pser.pow(np.nan), kser.pow(np.nan))\n        self.assert_eq(pser ** np.nan, kser ** np.nan)\n        self.assert_eq(pser.rpow(np.nan), kser.rpow(np.nan))\n        self.assert_eq(1 ** pser, 1 ** kser)\n", "idx": 1, "id": 18120, "msg": "Shall we use `ks.from_pandas`? Could you also update it in `test_last`?", "proj": "databricks-koalas", "lang": "py"}
{"patch": "@@ -95,16 +95,12 @@ var (\n func NewAccounting(\n \tPaymentThreshold,\n \tPaymentTolerance,\n-\tEarlyPayment uint64,\n+\tEarlyPayment *big.Int,\n \tLogger logging.Logger,\n \tStore storage.StateStorer,\n \tSettlement settlement.Interface,\n \tPricing pricing.Interface,\n ) (*Accounting, error) {\n-\tif PaymentTolerance+PaymentThreshold > math.MaxInt64 {\n-\t\treturn nil, fmt.Errorf(\"tolerance plus threshold too big: %w\", ErrOverflow)\n-\t}\n-\n \treturn &Accounting{\n \t\taccountingPeers:  make(map[string]*accountingPeer),\n \t\tpaymentThreshold: PaymentThreshold,", "y": 0, "oldf": "// Copyright 2020 The Swarm Authors. All rights reserved.\n// Use of this source code is governed by a BSD-style\n// license that can be found in the LICENSE file.\n\npackage accounting\n\nimport (\n\t\"context\"\n\t\"errors\"\n\t\"fmt\"\n\t\"math\"\n\t\"strings\"\n\t\"sync\"\n\t\"time\"\n\n\t\"github.com/ethersphere/bee/pkg/logging\"\n\t\"github.com/ethersphere/bee/pkg/p2p\"\n\t\"github.com/ethersphere/bee/pkg/pricing\"\n\t\"github.com/ethersphere/bee/pkg/settlement\"\n\t\"github.com/ethersphere/bee/pkg/storage\"\n\t\"github.com/ethersphere/bee/pkg/swarm\"\n)\n\nvar (\n\t_                     Interface = (*Accounting)(nil)\n\tbalancesPrefix        string    = \"balance_\"\n\tbalancesSurplusPrefix string    = \"surplusbalance_\"\n)\n\n// Interface is the Accounting interface.\ntype Interface interface {\n\t// Reserve reserves a portion of the balance for peer and attempts settlements if necessary.\n\t// Returns an error if the operation risks exceeding the disconnect threshold or an attempted settlement failed.\n\t//\n\t// This has to be called (always in combination with Release) before a\n\t// Credit action to prevent overspending in case of concurrent requests.\n\tReserve(ctx context.Context, peer swarm.Address, price uint64) error\n\t// Release releases the reserved funds.\n\tRelease(peer swarm.Address, price uint64)\n\t// Credit increases the balance the peer has with us (we \"pay\" the peer).\n\tCredit(peer swarm.Address, price uint64) error\n\t// Debit increases the balance we have with the peer (we get \"paid\" back).\n\tDebit(peer swarm.Address, price uint64) error\n\t// Balance returns the current balance for the given peer.\n\tBalance(peer swarm.Address) (int64, error)\n\t// SurplusBalance returns the current surplus balance for the given peer.\n\tSurplusBalance(peer swarm.Address) (int64, error)\n\t// Balances returns balances for all known peers.\n\tBalances() (map[string]int64, error)\n\t// CompensatedBalance returns the current balance deducted by current surplus balance for the given peer.\n\tCompensatedBalance(peer swarm.Address) (int64, error)\n\t// CompensatedBalances returns the compensated balances for all known peers.\n\tCompensatedBalances() (map[string]int64, error)\n}\n\n// accountingPeer holds all in-memory accounting information for one peer.\ntype accountingPeer struct {\n\tlock             sync.Mutex // lock to be held during any accounting action for this peer\n\treservedBalance  uint64     // amount currently reserved for active peer interaction\n\tpaymentThreshold uint64     // the threshold at which the peer expects us to pay\n}\n\n// Accounting is the main implementation of the accounting interface.\ntype Accounting struct {\n\t// Mutex for accessing the accountingPeers map.\n\taccountingPeersMu sync.Mutex\n\taccountingPeers   map[string]*accountingPeer\n\tlogger            logging.Logger\n\tstore             storage.StateStorer\n\t// The payment threshold in BZZ we communicate to our peers.\n\tpaymentThreshold uint64\n\t// The amount in BZZ we let peers exceed the payment threshold before we\n\t// disconnect them.\n\tpaymentTolerance uint64\n\tearlyPayment     uint64\n\tsettlement       settlement.Interface\n\tpricing          pricing.Interface\n\tmetrics          metrics\n}\n\nvar (\n\t// ErrOverdraft denotes the expected debt in Reserve would exceed the payment thresholds.\n\tErrOverdraft = errors.New(\"attempted overdraft\")\n\t// ErrDisconnectThresholdExceeded denotes a peer has exceeded the disconnect threshold.\n\tErrDisconnectThresholdExceeded = errors.New(\"disconnect threshold exceeded\")\n\t// ErrPeerNoBalance is the error returned if no balance in store exists for a peer\n\tErrPeerNoBalance = errors.New(\"no balance for peer\")\n\t// ErrOverflow denotes an arithmetic operation overflowed.\n\tErrOverflow = errors.New(\"overflow error\")\n\t// ErrInvalidValue denotes an invalid value read from store\n\tErrInvalidValue = errors.New(\"invalid value\")\n)\n\n// NewAccounting creates a new Accounting instance with the provided options.\nfunc NewAccounting(\n\tPaymentThreshold,\n\tPaymentTolerance,\n\tEarlyPayment uint64,\n\tLogger logging.Logger,\n\tStore storage.StateStorer,\n\tSettlement settlement.Interface,\n\tPricing pricing.Interface,\n) (*Accounting, error) {\n\tif PaymentTolerance+PaymentThreshold > math.MaxInt64 {\n\t\treturn nil, fmt.Errorf(\"tolerance plus threshold too big: %w\", ErrOverflow)\n\t}\n\n\treturn &Accounting{\n\t\taccountingPeers:  make(map[string]*accountingPeer),\n\t\tpaymentThreshold: PaymentThreshold,\n\t\tpaymentTolerance: PaymentTolerance,\n\t\tearlyPayment:     EarlyPayment,\n\t\tlogger:           Logger,\n\t\tstore:            Store,\n\t\tsettlement:       Settlement,\n\t\tpricing:          Pricing,\n\t\tmetrics:          newMetrics(),\n\t}, nil\n}\n\n// Reserve reserves a portion of the balance for peer and attempts settlements if necessary.\nfunc (a *Accounting) Reserve(ctx context.Context, peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Check for safety of increase of reservedBalance by price\n\tif accountingPeer.reservedBalance+price < accountingPeer.reservedBalance {\n\t\treturn ErrOverflow\n\t}\n\n\tnextReserved := accountingPeer.reservedBalance + price\n\n\t// Subtract already reserved amount from actual balance, to get expected balance\n\texpectedBalance, err := subtractI64mU64(currentBalance, nextReserved)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Determine if we will owe anything to the peer, if we owe less than 0, we conclude we owe nothing\n\t// This conversion is made safe by previous subtractI64mU64 not allowing MinInt64\n\texpectedDebt := -expectedBalance\n\tif expectedDebt < 0 {\n\t\texpectedDebt = 0\n\t}\n\n\tthreshold := accountingPeer.paymentThreshold\n\tif threshold > a.earlyPayment {\n\t\tthreshold -= a.earlyPayment\n\t} else {\n\t\tthreshold = 0\n\t}\n\n\tadditionalDebt, err := a.SurplusBalance(peer)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to load surplus balance: %w\", err)\n\t}\n\n\t// uint64 conversion of surplusbalance is safe because surplusbalance is always positive\n\tif additionalDebt < 0 {\n\t\treturn ErrInvalidValue\n\t}\n\n\tincreasedExpectedDebt, err := addI64pU64(expectedDebt, uint64(additionalDebt))\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// If our expected debt is less than earlyPayment away from our payment threshold\n\t// and we are actually in debt, trigger settlement.\n\t// we pay early to avoid needlessly blocking request later when concurrent requests occur and we are already close to the payment threshold.\n\tif increasedExpectedDebt >= int64(threshold) && currentBalance < 0 {\n\t\terr = a.settle(ctx, peer, accountingPeer)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to settle with peer %v: %v\", peer, err)\n\t\t}\n\t\t// if we settled successfully our balance is back at 0\n\t\t// and the expected debt therefore equals next reserved amount\n\t\texpectedDebt = int64(nextReserved)\n\t\tincreasedExpectedDebt, err = addI64pU64(expectedDebt, uint64(additionalDebt))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\t// if expectedDebt would still exceed the paymentThreshold at this point block this request\n\t// this can happen if there is a large number of concurrent requests to the same peer\n\tif increasedExpectedDebt > int64(accountingPeer.paymentThreshold) {\n\t\ta.metrics.AccountingBlocksCount.Inc()\n\t\treturn ErrOverdraft\n\t}\n\n\taccountingPeer.reservedBalance = nextReserved\n\treturn nil\n}\n\n// Release releases reserved funds.\nfunc (a *Accounting) Release(peer swarm.Address, price uint64) {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\ta.logger.Errorf(\"cannot release balance for peer: %v\", err)\n\t\treturn\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\t// NOTE: this should never happen if Reserve and Release calls are paired.\n\tif price > accountingPeer.reservedBalance {\n\t\ta.logger.Error(\"attempting to release more balance than was reserved for peer\")\n\t\taccountingPeer.reservedBalance = 0\n\t} else {\n\t\taccountingPeer.reservedBalance -= price\n\t}\n}\n\n// Credit increases the amount of credit we have with the given peer\n// (and decreases existing debt).\nfunc (a *Accounting) Credit(peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Calculate next balance by safely decreasing current balance with the price we credit\n\tnextBalance, err := subtractI64mU64(currentBalance, price)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ta.logger.Tracef(\"crediting peer %v with price %d, new balance is %d\", peer, price, nextBalance)\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\ta.metrics.TotalCreditedAmount.Add(float64(price))\n\ta.metrics.CreditEventsCount.Inc()\n\treturn nil\n}\n\n// Settle all debt with a peer. The lock on the accountingPeer must be held when\n// called.\nfunc (a *Accounting) settle(ctx context.Context, peer swarm.Address, balance *accountingPeer) error {\n\toldBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Don't do anything if there is no actual debt.\n\t// This might be the case if the peer owes us and the total reserve for a\n\t// peer exceeds the payment treshold.\n\tif oldBalance >= 0 {\n\t\treturn nil\n\t}\n\n\t// check safety of the following -1 * int64 conversion, all negative int64 have positive int64 equals except MinInt64\n\tif oldBalance == math.MinInt64 {\n\t\treturn ErrOverflow\n\t}\n\n\t// This is safe because of the earlier check for oldbalance < 0 and the check for != MinInt64\n\tpaymentAmount := uint64(-oldBalance)\n\tnextBalance := 0\n\n\t// Try to save the next balance first.\n\t// Otherwise we might pay and then not be able to save, forcing us to pay\n\t// again after restart.\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\terr = a.settlement.Pay(ctx, peer, paymentAmount)\n\tif err != nil {\n\t\terr = fmt.Errorf(\"settlement for amount %d failed: %w\", paymentAmount, err)\n\t\t// If the payment didn't succeed we should restore the old balance in\n\t\t// the state store.\n\t\tif storeErr := a.store.Put(peerBalanceKey(peer), oldBalance); storeErr != nil {\n\t\t\ta.logger.Errorf(\"failed to restore balance after failed settlement for peer %v: %v\", peer, storeErr)\n\t\t}\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Debit increases the amount of debt we have with the given peer (and decreases\n// existing credit).\nfunc (a *Accounting) Debit(peer swarm.Address, price uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcost := price\n\t// see if peer has surplus balance to deduct this transaction of\n\tsurplusBalance, err := a.SurplusBalance(peer)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get surplus balance: %w\", err)\n\t}\n\tif surplusBalance > 0 {\n\n\t\t// get new surplus balance after deduct\n\t\tnewSurplusBalance, err := subtractI64mU64(surplusBalance, price)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// if nothing left for debiting, store new surplus balance and return from debit\n\t\tif newSurplusBalance >= 0 {\n\t\t\ta.logger.Tracef(\"surplus debiting peer %v with value %d, new surplus balance is %d\", peer, price, newSurplusBalance)\n\n\t\t\terr = a.store.Put(peerSurplusBalanceKey(peer), newSurplusBalance)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to persist surplus balance: %w\", err)\n\t\t\t}\n\t\t\t// count debit operations, terminate early\n\t\t\ta.metrics.TotalDebitedAmount.Add(float64(price))\n\t\t\ta.metrics.DebitEventsCount.Inc()\n\t\t\treturn nil\n\t\t}\n\n\t\t// if surplus balance didn't cover full transaction, let's continue with leftover part as cost\n\t\tdebitIncrease, err := subtractU64mI64(price, surplusBalance)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\t// conversion to uint64 is safe because we know the relationship between the values by now, but let's make a sanity check\n\t\tif debitIncrease <= 0 {\n\t\t\treturn fmt.Errorf(\"sanity check failed for partial debit after surplus balance drawn\")\n\t\t}\n\t\tcost = uint64(debitIncrease)\n\n\t\t// if we still have something to debit, than have run out of surplus balance,\n\t\t// let's store 0 as surplus balance\n\t\ta.logger.Tracef(\"surplus debiting peer %v with value %d, new surplus balance is 0\", peer, debitIncrease)\n\n\t\terr = a.store.Put(peerSurplusBalanceKey(peer), 0)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to persist surplus balance: %w\", err)\n\t\t}\n\n\t}\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn fmt.Errorf(\"failed to load balance: %w\", err)\n\t\t}\n\t}\n\n\t// Get nextBalance by safely increasing current balance with price\n\tnextBalance, err := addI64pU64(currentBalance, cost)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\ta.logger.Tracef(\"debiting peer %v with price %d, new balance is %d\", peer, price, nextBalance)\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\ta.metrics.TotalDebitedAmount.Add(float64(price))\n\ta.metrics.DebitEventsCount.Inc()\n\n\tif nextBalance >= int64(a.paymentThreshold+a.paymentTolerance) {\n\t\t// peer too much in debt\n\t\ta.metrics.AccountingDisconnectsCount.Inc()\n\t\treturn p2p.NewBlockPeerError(10000*time.Hour, ErrDisconnectThresholdExceeded)\n\t}\n\n\treturn nil\n}\n\n// Balance returns the current balance for the given peer.\nfunc (a *Accounting) Balance(peer swarm.Address) (balance int64, err error) {\n\terr = a.store.Get(peerBalanceKey(peer), &balance)\n\n\tif err != nil {\n\t\tif errors.Is(err, storage.ErrNotFound) {\n\t\t\treturn 0, ErrPeerNoBalance\n\t\t}\n\t\treturn 0, err\n\t}\n\n\treturn balance, nil\n}\n\n// SurplusBalance returns the current balance for the given peer.\nfunc (a *Accounting) SurplusBalance(peer swarm.Address) (balance int64, err error) {\n\terr = a.store.Get(peerSurplusBalanceKey(peer), &balance)\n\n\tif err != nil {\n\t\tif errors.Is(err, storage.ErrNotFound) {\n\t\t\treturn 0, nil\n\t\t}\n\t\treturn 0, err\n\t}\n\n\treturn balance, nil\n}\n\n// CompensatedBalance returns balance decreased by surplus balance\nfunc (a *Accounting) CompensatedBalance(peer swarm.Address) (compensated int64, err error) {\n\n\tsurplus, err := a.SurplusBalance(peer)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\tif surplus < 0 {\n\t\treturn 0, ErrInvalidValue\n\t}\n\n\tbalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn 0, err\n\t\t}\n\t}\n\n\t// if surplus is 0 and peer has no balance, propagate ErrPeerNoBalance\n\tif surplus == 0 && errors.Is(err, ErrPeerNoBalance) {\n\t\treturn 0, err\n\t}\n\t// Compensated balance is balance decreased by surplus balance\n\tcompensated, err = subtractI64mU64(balance, uint64(surplus))\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\n\treturn compensated, nil\n}\n\n// peerBalanceKey returns the balance storage key for the given peer.\nfunc peerBalanceKey(peer swarm.Address) string {\n\treturn fmt.Sprintf(\"%s%s\", balancesPrefix, peer.String())\n}\n\n// peerSurplusBalanceKey returns the surplus balance storage key for the given peer\nfunc peerSurplusBalanceKey(peer swarm.Address) string {\n\treturn fmt.Sprintf(\"%s%s\", balancesSurplusPrefix, peer.String())\n}\n\n// getAccountingPeer returns the accountingPeer for a given swarm address.\n// If not found in memory it will initialize it.\nfunc (a *Accounting) getAccountingPeer(peer swarm.Address) (*accountingPeer, error) {\n\ta.accountingPeersMu.Lock()\n\tdefer a.accountingPeersMu.Unlock()\n\n\tpeerData, ok := a.accountingPeers[peer.String()]\n\tif !ok {\n\t\tpeerData = &accountingPeer{\n\t\t\treservedBalance: 0,\n\t\t\t// initially assume the peer has the same threshold as us\n\t\t\tpaymentThreshold: a.paymentThreshold,\n\t\t}\n\t\ta.accountingPeers[peer.String()] = peerData\n\t}\n\n\treturn peerData, nil\n}\n\n// Balances gets balances for all peers from store.\nfunc (a *Accounting) Balances() (map[string]int64, error) {\n\ts := make(map[string]int64)\n\n\terr := a.store.Iterate(balancesPrefix, func(key, val []byte) (stop bool, err error) {\n\t\taddr, err := balanceKeyPeer(key)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"parse address from key: %s: %v\", string(key), err)\n\t\t}\n\n\t\tif _, ok := s[addr.String()]; !ok {\n\t\t\tvar storevalue int64\n\t\t\terr = a.store.Get(peerBalanceKey(addr), &storevalue)\n\t\t\tif err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"get peer %s balance: %v\", addr.String(), err)\n\t\t\t}\n\n\t\t\ts[addr.String()] = storevalue\n\t\t}\n\n\t\treturn false, nil\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n\n// Balances gets balances for all peers from store.\nfunc (a *Accounting) CompensatedBalances() (map[string]int64, error) {\n\ts := make(map[string]int64)\n\n\terr := a.store.Iterate(balancesPrefix, func(key, val []byte) (stop bool, err error) {\n\t\taddr, err := balanceKeyPeer(key)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"parse address from key: %s: %v\", string(key), err)\n\t\t}\n\t\tif _, ok := s[addr.String()]; !ok {\n\t\t\tvalue, err := a.CompensatedBalance(addr)\n\t\t\tif err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"get peer %s balance: %v\", addr.String(), err)\n\t\t\t}\n\n\t\t\ts[addr.String()] = value\n\t\t}\n\n\t\treturn false, nil\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\terr = a.store.Iterate(balancesSurplusPrefix, func(key, val []byte) (stop bool, err error) {\n\t\taddr, err := surplusBalanceKeyPeer(key)\n\t\tif err != nil {\n\t\t\treturn false, fmt.Errorf(\"parse address from key: %s: %v\", string(key), err)\n\t\t}\n\t\tif _, ok := s[addr.String()]; !ok {\n\t\t\tvalue, err := a.CompensatedBalance(addr)\n\t\t\tif err != nil {\n\t\t\t\treturn false, fmt.Errorf(\"get peer %s balance: %v\", addr.String(), err)\n\t\t\t}\n\n\t\t\ts[addr.String()] = value\n\t\t}\n\n\t\treturn false, nil\n\t})\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn s, nil\n}\n\n// balanceKeyPeer returns the embedded peer from the balance storage key.\nfunc balanceKeyPeer(key []byte) (swarm.Address, error) {\n\tk := string(key)\n\n\tsplit := strings.SplitAfter(k, balancesPrefix)\n\tif len(split) != 2 {\n\t\treturn swarm.ZeroAddress, errors.New(\"no peer in key\")\n\t}\n\n\taddr, err := swarm.ParseHexAddress(split[1])\n\tif err != nil {\n\t\treturn swarm.ZeroAddress, err\n\t}\n\n\treturn addr, nil\n}\n\nfunc surplusBalanceKeyPeer(key []byte) (swarm.Address, error) {\n\tk := string(key)\n\n\tsplit := strings.SplitAfter(k, balancesSurplusPrefix)\n\tif len(split) != 2 {\n\t\treturn swarm.ZeroAddress, errors.New(\"no peer in key\")\n\t}\n\n\taddr, err := swarm.ParseHexAddress(split[1])\n\tif err != nil {\n\t\treturn swarm.ZeroAddress, err\n\t}\n\n\treturn addr, nil\n}\n\n// NotifyPayment is called by Settlement when we receive a payment.\nfunc (a *Accounting) NotifyPayment(peer swarm.Address, amount uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\tcurrentBalance, err := a.Balance(peer)\n\tif err != nil {\n\t\tif !errors.Is(err, ErrPeerNoBalance) {\n\t\t\treturn err\n\t\t}\n\n\t}\n\t// if balance is already negative or zero, we credit full amount received to surplus balance and terminate early\n\tif currentBalance <= 0 {\n\t\tsurplus, err := a.SurplusBalance(peer)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get surplus balance: %w\", err)\n\t\t}\n\t\tincreasedSurplus, err := addI64pU64(surplus, amount)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\ta.logger.Tracef(\"surplus crediting peer %v with amount %d due to payment, new surplus balance is %d\", peer, amount, increasedSurplus)\n\n\t\terr = a.store.Put(peerSurplusBalanceKey(peer), increasedSurplus)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to persist surplus balance: %w\", err)\n\t\t}\n\n\t\treturn nil\n\t}\n\n\t// if current balance is positive, let's make a partial credit to\n\tnewBalance, err := subtractI64mU64(currentBalance, amount)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Don't allow a payment to put us into debt\n\t// This is to prevent another node tricking us into settling by settling\n\t// first (e.g. send a bouncing cheque to trigger an honest cheque in swap).\n\tnextBalance := newBalance\n\tif newBalance < 0 {\n\t\tnextBalance = 0\n\t}\n\n\ta.logger.Tracef(\"crediting peer %v with amount %d due to payment, new balance is %d\", peer, amount, nextBalance)\n\n\terr = a.store.Put(peerBalanceKey(peer), nextBalance)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to persist balance: %w\", err)\n\t}\n\n\t// If payment would have put us into debt, rather, let's add to surplusBalance,\n\t// so as that an oversettlement attempt creates balance for future forwarding services\n\t// charges to be deducted of\n\tif newBalance < 0 {\n\t\tsurplusGrowth, err := subtractU64mI64(amount, currentBalance)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\n\t\tsurplus, err := a.SurplusBalance(peer)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to get surplus balance: %w\", err)\n\t\t}\n\t\tincreasedSurplus := surplus + surplusGrowth\n\t\tif increasedSurplus < surplus {\n\t\t\treturn ErrOverflow\n\t\t}\n\n\t\ta.logger.Tracef(\"surplus crediting peer %v with amount %d due to payment, new surplus balance is %d\", peer, surplusGrowth, increasedSurplus)\n\n\t\terr = a.store.Put(peerSurplusBalanceKey(peer), increasedSurplus)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to persist surplus balance: %w\", err)\n\t\t}\n\t}\n\n\treturn nil\n}\n\n// AsyncNotifyPayment calls notify payment in a go routine.\n// This is needed when accounting needs to be notified but the accounting lock is already held.\nfunc (a *Accounting) AsyncNotifyPayment(peer swarm.Address, amount uint64) error {\n\tgo func() {\n\t\terr := a.NotifyPayment(peer, amount)\n\t\tif err != nil {\n\t\t\ta.logger.Errorf(\"failed to notify accounting of payment: %v\", err)\n\t\t}\n\t}()\n\treturn nil\n}\n\n// subtractI64mU64 is a helper function for safe subtraction of Int64 - Uint64\n// It checks for\n//   - overflow safety in conversion of uint64 to int64\n//   - safety of the arithmetic\n//   - whether ( -1 * result ) is still Int64, as MinInt64 in absolute sense is 1 larger than MaxInt64\n// If result is MinInt64, we also return overflow error, for two reasons:\n//   - in some cases we are going to use -1 * result in the following operations, which is secured by this check\n//   - we also do not want to possibly store this value as balance, even if ( -1 * result ) is not used immediately afterwards, because it could\n//\t\tdisable settleing for this amount as the value would create overflow\nfunc subtractI64mU64(base int64, subtracted uint64) (result int64, err error) {\n\tif subtracted > math.MaxInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tresult = base - int64(subtracted)\n\tif result > base {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tif result == math.MinInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\treturn result, nil\n}\n\nfunc subtractU64mI64(base uint64, subtracted int64) (result int64, err error) {\n\tif base > math.MaxInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\t// base is positive, overflow can happen by subtracting negative number\n\tresult = int64(base) - subtracted\n\tif subtracted < 0 {\n\t\tif result < int64(base) {\n\t\t\treturn 0, ErrOverflow\n\t\t}\n\t}\n\n\treturn result, nil\n}\n\n// addI64pU64 is a helper function for safe addition of Int64 + Uint64\n// It checks for\n//   - overflow safety in conversion of uint64 to int64\n//   - safety of the arithmetic\nfunc addI64pU64(a int64, b uint64) (result int64, err error) {\n\tif b > math.MaxInt64 {\n\t\treturn 0, ErrOverflow\n\t}\n\n\tresult = a + int64(b)\n\tif result < a {\n\t\treturn 0, ErrOverflow\n\t}\n\n\treturn result, nil\n}\n\n// NotifyPaymentThreshold should be called to notify accounting of changes in the payment threshold\nfunc (a *Accounting) NotifyPaymentThreshold(peer swarm.Address, paymentThreshold uint64) error {\n\taccountingPeer, err := a.getAccountingPeer(peer)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\taccountingPeer.lock.Lock()\n\tdefer accountingPeer.lock.Unlock()\n\n\taccountingPeer.paymentThreshold = paymentThreshold\n\treturn nil\n}\n", "idx": 4, "id": 13764, "msg": "", "proj": "ethersphere-bee", "lang": "go"}
{"patch": "@@ -614,7 +614,7 @@ class WelcomeDialog(wx.Dialog):\n \t\ttry:\n \t\t\tconfig.conf.save()\n \t\texcept:\n-\t\t\tlog.debugWarning(\"could not save\",exc_info=True)\n+\t\t\tlog.debugWarning(\"Could not save\",exc_info=True)\n \t\tself.EndModal(wx.ID_OK)\n \n \t@classmethod", "y": 0, "oldf": "# -*- coding: UTF-8 -*-\r\n#gui/__init__.py\r\n#A part of NonVisual Desktop Access (NVDA)\r\n#Copyright (C) 2006-2015 NV Access Limited, Peter V\u00e1gner, Aleksey Sadovoy, Mesar Hameed, Joseph Lee\r\n#This file is covered by the GNU General Public License.\r\n#See the file COPYING for more details.\r\n\r\nimport time\r\nimport os\r\nimport sys\r\nimport threading\r\nimport codecs\r\nimport ctypes\r\nimport weakref\r\nimport wx\r\nimport globalVars\r\nimport tones\r\nimport ui\r\nfrom logHandler import log\r\nimport config\r\nimport versionInfo\r\nimport speech\r\nimport queueHandler\r\nimport core\r\nfrom settingsDialogs import *\r\nimport speechDictHandler\r\nimport languageHandler\r\nimport logViewer\r\nimport speechViewer\r\nimport winUser\r\nimport api\r\nimport guiHelper\r\n\r\ntry:\r\n\timport updateCheck\r\nexcept RuntimeError:\r\n\tupdateCheck = None\r\n\r\n### Constants\r\nNVDA_PATH = os.getcwdu()\r\nICON_PATH=os.path.join(NVDA_PATH, \"images\", \"nvda.ico\")\r\nDONATE_URL = \"http://www.nvaccess.org/donate/\"\r\n\r\n### Globals\r\nmainFrame = None\r\nisInMessageBox = False\r\n\r\ndef getDocFilePath(fileName, localized=True):\r\n\tif not getDocFilePath.rootPath:\r\n\t\tif hasattr(sys, \"frozen\"):\r\n\t\t\tgetDocFilePath.rootPath = os.path.join(NVDA_PATH, \"documentation\")\r\n\t\telse:\r\n\t\t\tgetDocFilePath.rootPath = os.path.abspath(os.path.join(\"..\", \"user_docs\"))\r\n\r\n\tif localized:\r\n\t\tlang = languageHandler.getLanguage()\r\n\t\ttryLangs = [lang]\r\n\t\tif \"_\" in lang:\r\n\t\t\t# This locale has a sub-locale, but documentation might not exist for the sub-locale, so try stripping it.\r\n\t\t\ttryLangs.append(lang.split(\"_\")[0])\r\n\t\t# If all else fails, use English.\r\n\t\ttryLangs.append(\"en\")\r\n\r\n\t\tfileName, fileExt = os.path.splitext(fileName)\r\n\t\tfor tryLang in tryLangs:\r\n\t\t\ttryDir = os.path.join(getDocFilePath.rootPath, tryLang)\r\n\t\t\tif not os.path.isdir(tryDir):\r\n\t\t\t\tcontinue\r\n\r\n\t\t\t# Some out of date translations might include .txt files which are now .html files in newer translations.\r\n\t\t\t# Therefore, ignore the extension and try both .html and .txt.\r\n\t\t\tfor tryExt in (\"html\", \"txt\"):\r\n\t\t\t\ttryPath = os.path.join(tryDir, \"%s.%s\" % (fileName, tryExt))\r\n\t\t\t\tif os.path.isfile(tryPath):\r\n\t\t\t\t\treturn tryPath\r\n\r\n\telse:\r\n\t\t# Not localized.\r\n\t\tif not hasattr(sys, \"frozen\") and fileName in (\"copying.txt\", \"contributors.txt\"):\r\n\t\t\t# If running from source, these two files are in the root dir.\r\n\t\t\treturn os.path.join(NVDA_PATH, \"..\", fileName)\r\n\t\telse:\r\n\t\t\treturn os.path.join(getDocFilePath.rootPath, fileName)\r\ngetDocFilePath.rootPath = None\r\n\r\nclass MainFrame(wx.Frame):\r\n\r\n\tdef __init__(self):\r\n\t\tstyle = wx.DEFAULT_FRAME_STYLE ^ wx.MAXIMIZE_BOX ^ wx.MINIMIZE_BOX | wx.FRAME_NO_TASKBAR\r\n\t\tsuper(MainFrame, self).__init__(None, wx.ID_ANY, versionInfo.name, size=(1,1), style=style)\r\n\t\tself.Bind(wx.EVT_CLOSE, self.onExitCommand)\r\n\t\tself.sysTrayIcon = SysTrayIcon(self)\r\n\t\t#: The focus before the last popup or C{None} if unknown.\r\n\t\t#: This is only valid before L{prePopup} is called,\r\n\t\t#: so it should be used as early as possible in any popup that needs it.\r\n\t\t#: @type: L{NVDAObject}\r\n\t\tself.prevFocus = None\r\n\t\t#: The focus ancestors before the last popup or C{None} if unknown.\r\n\t\t#: @type: list of L{NVDAObject}\r\n\t\tself.prevFocusAncestors = None\r\n\t\t# If NVDA has the uiAccess privilege, it can always set the foreground window.\r\n\t\tif not config.hasUiAccess():\r\n\t\t\t# This makes Windows return to the previous foreground window and also seems to allow NVDA to be brought to the foreground.\r\n\t\t\tself.Show()\r\n\t\t\tself.Hide()\r\n\t\t\tif winUser.isWindowVisible(self.Handle):\r\n\t\t\t\t# HACK: Work around a wx bug where Hide() doesn't actually hide the window,\r\n\t\t\t\t# but IsShown() returns False and Hide() again doesn't fix it.\r\n\t\t\t\t# This seems to happen if the call takes too long.\r\n\t\t\t\tself.Show()\r\n\t\t\t\tself.Hide()\r\n\r\n\tdef Destroy(self):\r\n\t\tself.sysTrayIcon.Destroy()\r\n\t\tsuper(MainFrame, self).Destroy()\r\n\r\n\tdef prePopup(self):\r\n\t\t\"\"\"Prepare for a popup.\r\n\t\tThis should be called before any dialog or menu which should pop up for the user.\r\n\t\tL{postPopup} should be called after the dialog or menu has been shown.\r\n\t\t@postcondition: A dialog or menu may be shown.\r\n\t\t\"\"\"\r\n\t\tnvdaPid = os.getpid()\r\n\t\tfocus = api.getFocusObject()\r\n\t\tif focus.processID != nvdaPid:\r\n\t\t\tself.prevFocus = focus\r\n\t\t\tself.prevFocusAncestors = api.getFocusAncestors()\r\n\t\tif winUser.getWindowThreadProcessID(winUser.getForegroundWindow())[0] != nvdaPid:\r\n\t\t\t# This process is not the foreground process, so bring it to the foreground.\r\n\t\t\tself.Raise()\r\n\r\n\tdef postPopup(self):\r\n\t\t\"\"\"Clean up after a popup dialog or menu.\r\n\t\tThis should be called after a dialog or menu was popped up for the user.\r\n\t\t\"\"\"\r\n\t\tself.prevFocus = None\r\n\t\tself.prevFocusAncestors = None\r\n\t\tif not winUser.isWindowVisible(winUser.getForegroundWindow()):\r\n\t\t\t# The current foreground window is invisible, so we want to return to the previous foreground window.\r\n\t\t\t# Showing and hiding our main window seems to achieve this.\r\n\t\t\tself.Show()\r\n\t\t\tself.Hide()\r\n\r\n\tdef showGui(self):\r\n\t\t# The menu pops up at the location of the mouse, which means it pops up at an unpredictable location.\r\n\t\t# Therefore, move the mouse to the centre of the screen so that the menu will always pop up there.\r\n\t\tleft, top, width, height = api.getDesktopObject().location\r\n\t\tx = width / 2\r\n\t\ty = height / 2\r\n\t\twinUser.setCursorPos(x, y)\r\n\t\tself.sysTrayIcon.onActivate(None)\r\n\r\n\tdef onRevertToSavedConfigurationCommand(self,evt):\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue,core.resetConfiguration)\r\n\t\t# Translators: Reported when last saved configuration has been applied by using revert to saved configuration option in NVDA menu.\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue,ui.message,_(\"Configuration applied\"))\r\n\r\n\tdef onRevertToDefaultConfigurationCommand(self,evt):\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue,core.resetConfiguration,factoryDefaults=True)\r\n\t\t# Translators: Reported when configuration has been restored to defaults by using restore configuration to factory defaults item in NVDA menu.\r\n\t\tqueueHandler.queueFunction(queueHandler.eventQueue,ui.message,_(\"Configuration restored to factory defaults\"))\r\n\r\n\tdef onSaveConfigurationCommand(self,evt):\r\n\t\tif globalVars.appArgs.secure:\r\n\t\t\t# Translators: Reported when current configuration cannot be saved while NVDA is running in secure mode such as in Windows login screen.\r\n\t\t\tqueueHandler.queueFunction(queueHandler.eventQueue,ui.message,_(\"Cannot save configuration - NVDA in secure mode\"))\r\n\t\t\treturn\r\n\t\ttry:\r\n\t\t\tconfig.conf.save()\r\n\t\t\t# Translators: Reported when current configuration has been saved.\r\n\t\t\tqueueHandler.queueFunction(queueHandler.eventQueue,ui.message,_(\"Configuration saved\"))\r\n\t\texcept:\r\n\t\t\t# Translators: Message shown when current configuration cannot be saved such as when running NVDA from a CD.\r\n\t\t\tmessageBox(_(\"Could not save configuration - probably read only file system\"),_(\"Error\"),wx.OK | wx.ICON_ERROR)\r\n\r\n\tdef _popupSettingsDialog(self, dialog, *args, **kwargs):\r\n\t\tif isInMessageBox:\r\n\t\t\treturn\r\n\t\tself.prePopup()\r\n\t\ttry:\r\n\t\t\tdialog(self, *args, **kwargs).Show()\r\n\t\texcept SettingsDialog.MultiInstanceError:\r\n\t\t\t# Translators: Message shown when attempting to open another NVDA settings dialog when one is already open (example: when trying to open keyboard settings when general settings dialog is open).\r\n\t\t\tmessageBox(_(\"An NVDA settings dialog is already open. Please close it first.\"),_(\"Error\"),style=wx.OK | wx.ICON_ERROR)\r\n\t\tself.postPopup()\r\n\r\n\tdef onDefaultDictionaryCommand(self,evt):\r\n\t\t# Translators: Title for default speech dictionary dialog.\r\n\t\tself._popupSettingsDialog(DictionaryDialog,_(\"Default dictionary\"),speechDictHandler.dictionaries[\"default\"])\r\n\r\n\tdef onVoiceDictionaryCommand(self,evt):\r\n\t\t# Translators: Title for voice dictionary for the current voice such as current eSpeak variant.\r\n\t\tself._popupSettingsDialog(DictionaryDialog,_(\"Voice dictionary (%s)\")%speechDictHandler.dictionaries[\"voice\"].fileName,speechDictHandler.dictionaries[\"voice\"])\r\n\r\n\tdef onTemporaryDictionaryCommand(self,evt):\r\n\t\t# Translators: Title for temporary speech dictionary dialog (the voice dictionary that is active as long as NvDA is running).\r\n\t\tself._popupSettingsDialog(DictionaryDialog,_(\"Temporary dictionary\"),speechDictHandler.dictionaries[\"temp\"])\r\n\r\n\tdef onExitCommand(self, evt):\r\n\t\tif config.conf[\"general\"][\"askToExit\"]:\r\n\t\t\tself.prePopup()\r\n\t\t\td = ExitDialog(self)\r\n\t\t\td.Raise()\r\n\t\t\td.Show()\r\n\t\t\tself.postPopup()\r\n\t\telse:\r\n\t\t\twx.GetApp().ExitMainLoop()\r\n\r\n\tdef onGeneralSettingsCommand(self,evt):\r\n\t\tself._popupSettingsDialog(GeneralSettingsDialog)\r\n\r\n\tdef onSynthesizerCommand(self,evt):\r\n\t\tself._popupSettingsDialog(SynthesizerDialog)\r\n\r\n\tdef onVoiceCommand(self,evt):\r\n\t\tself._popupSettingsDialog(VoiceSettingsDialog)\r\n\r\n\tdef onBrailleCommand(self,evt):\r\n\t\tself._popupSettingsDialog(BrailleSettingsDialog)\r\n\r\n\tdef onKeyboardSettingsCommand(self,evt):\r\n\t\tself._popupSettingsDialog(KeyboardSettingsDialog)\r\n\r\n\tdef onMouseSettingsCommand(self,evt):\r\n\t\tself._popupSettingsDialog(MouseSettingsDialog)\r\n\r\n\tdef onReviewCursorCommand(self,evt):\r\n\t\tself._popupSettingsDialog(ReviewCursorDialog)\r\n\r\n\tdef onInputCompositionCommand(self,evt):\r\n\t\tself._popupSettingsDialog(InputCompositionDialog)\r\n\r\n\tdef onObjectPresentationCommand(self,evt):\r\n\t\tself._popupSettingsDialog(ObjectPresentationDialog)\r\n\r\n\tdef onBrowseModeCommand(self,evt):\r\n\t\tself._popupSettingsDialog(BrowseModeDialog)\r\n\r\n\tdef onDocumentFormattingCommand(self,evt):\r\n\t\tself._popupSettingsDialog(DocumentFormattingDialog)\r\n\r\n\tdef onSpeechSymbolsCommand(self, evt):\r\n\t\tself._popupSettingsDialog(SpeechSymbolsDialog)\r\n\r\n\tdef onInputGesturesCommand(self, evt):\r\n\t\tself._popupSettingsDialog(InputGesturesDialog)\r\n\r\n\tdef onAboutCommand(self,evt):\r\n\t\t# Translators: The title of the dialog to show about info for NVDA.\r\n\t\tmessageBox(versionInfo.aboutMessage, _(\"About NVDA\"), wx.OK)\r\n\r\n\tdef onCheckForUpdateCommand(self, evt):\r\n\t\tupdateCheck.UpdateChecker().check()\r\n\t\t\r\n\tdef onViewLogCommand(self, evt):\r\n\t\tlogViewer.activate()\r\n\r\n\tdef onSpeechViewerEnabled(self, isEnabled):\r\n\t\t# its possible for this to be called after the sysTrayIcon is destroyed if we are exiting NVDA\r\n\t\tif self.sysTrayIcon and self.sysTrayIcon.menu_tools_toggleSpeechViewer:\r\n\t\t\tself.sysTrayIcon.menu_tools_toggleSpeechViewer.Check(isEnabled)\r\n\r\n\tdef onToggleSpeechViewerCommand(self, evt):\r\n\t\tif not speechViewer.isActive:\r\n\t\t\tspeechViewer.activate()\r\n\t\telse:\r\n\t\t\tspeechViewer.deactivate()\r\n\r\n\tdef onPythonConsoleCommand(self, evt):\r\n\t\timport pythonConsole\r\n\t\tif not pythonConsole.consoleUI:\r\n\t\t\tpythonConsole.initialize()\r\n\t\tpythonConsole.activate()\r\n\r\n\tdef onAddonsManagerCommand(self,evt):\r\n\t\tif isInMessageBox:\r\n\t\t\treturn\r\n\t\tself.prePopup()\r\n\t\tfrom addonGui import AddonsDialog\r\n\t\td=AddonsDialog(gui.mainFrame)\r\n\t\td.Show()\r\n\t\tself.postPopup()\r\n\r\n\tdef onReloadPluginsCommand(self, evt):\r\n\t\timport appModuleHandler, globalPluginHandler\r\n\t\tfrom NVDAObjects import NVDAObject\r\n\t\tappModuleHandler.reloadAppModules()\r\n\t\tglobalPluginHandler.reloadGlobalPlugins()\r\n\t\tNVDAObject.clearDynamicClassCache()\r\n\r\n\tdef onCreatePortableCopyCommand(self,evt):\r\n\t\tif isInMessageBox:\r\n\t\t\treturn\r\n\t\tself.prePopup()\r\n\t\timport gui.installerGui\r\n\t\td=gui.installerGui.PortableCreaterDialog(gui.mainFrame)\r\n\t\td.Show()\r\n\t\tself.postPopup()\r\n\r\n\tdef onInstallCommand(self, evt):\r\n\t\tif isInMessageBox:\r\n\t\t\treturn\r\n\t\tfrom gui import installerGui\r\n\t\tinstallerGui.showInstallGui()\r\n\r\n\tdef onConfigProfilesCommand(self, evt):\r\n\t\tif isInMessageBox:\r\n\t\t\treturn\r\n\t\tself.prePopup()\r\n\t\tfrom configProfiles import ProfilesDialog\r\n\t\tProfilesDialog(gui.mainFrame).Show()\r\n\t\tself.postPopup()\r\n\r\nclass SysTrayIcon(wx.TaskBarIcon):\r\n\r\n\tdef __init__(self, frame):\r\n\t\tsuper(SysTrayIcon, self).__init__()\r\n\t\ticon=wx.Icon(ICON_PATH,wx.BITMAP_TYPE_ICO)\r\n\t\tself.SetIcon(icon, versionInfo.name)\r\n\r\n\t\tself.menu=wx.Menu()\r\n\t\tmenu_preferences=self.preferencesMenu=wx.Menu()\r\n\t\t# Translators: The label for the menu item to open general Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&General settings...\"),_(\"General settings\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onGeneralSettingsCommand, item)\r\n\t\t# Translators: The label for the menu item to open Synthesizer settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Synthesizer...\"),_(\"Change the synthesizer to be used\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onSynthesizerCommand, item)\r\n\t\t# Translators: The label for the menu item to open Voice Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Voice settings...\"),_(\"Choose the voice, rate, pitch and volume to use\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onVoiceCommand, item)\r\n\t\t# Translators: The label for the menu item to open Braille Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"B&raille settings...\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onBrailleCommand, item)\r\n\t\t# Translators: The label for the menu item to open Keyboard Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Keyboard settings...\"),_(\"Configure keyboard layout, speaking of typed characters, words or command keys\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onKeyboardSettingsCommand, item)\r\n\t\t# Translators: The label for the menu item to open Mouse Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY, _(\"&Mouse settings...\"),_(\"Change reporting of mouse shape and object under mouse\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onMouseSettingsCommand, item)\r\n\t\t# Translators: The label for the menu item to open Review Cursor dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"Review &cursor...\"),_(\"Configure how and when the review cursor moves\")) \r\n\t\tself.Bind(wx.EVT_MENU, frame.onReviewCursorCommand, item)\r\n\t\t# Translators: The label for the menu item to open Input Composition Settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Input composition settings...\"),_(\"Configure how NVDA reports input composition and candidate selection for certain languages\")) \r\n\t\tself.Bind(wx.EVT_MENU, frame.onInputCompositionCommand, item)\r\n\t\t# Translators: The label for the menu item to open Object Presentation dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Object presentation...\"),_(\"Change reporting of objects\")) \r\n\t\tself.Bind(wx.EVT_MENU, frame.onObjectPresentationCommand, item)\r\n\t\t# Translators: The label for the menu item to open Browse Mode settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"&Browse mode...\"),_(\"Change virtual buffers specific settings\")) \r\n\t\tself.Bind(wx.EVT_MENU, frame.onBrowseModeCommand, item)\r\n\t\t# Translators: The label for the menu item to open Document Formatting settings dialog.\r\n\t\titem = menu_preferences.Append(wx.ID_ANY,_(\"Document &formatting...\"),_(\"Change settings of document properties\")) \r\n\t\tself.Bind(wx.EVT_MENU, frame.onDocumentFormattingCommand, item)\r\n\t\tsubMenu_speechDicts = wx.Menu()\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label for the menu item to open Default speech dictionary dialog.\r\n\t\t\titem = subMenu_speechDicts.Append(wx.ID_ANY,_(\"&Default dictionary...\"),_(\"A dialog where you can set default dictionary by adding dictionary entries to the list\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onDefaultDictionaryCommand, item)\r\n\t\t\t# Translators: The label for the menu item to open Voice specific speech dictionary dialog.\r\n\t\t\titem = subMenu_speechDicts.Append(wx.ID_ANY,_(\"&Voice dictionary...\"),_(\"A dialog where you can set voice-specific dictionary by adding dictionary entries to the list\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onVoiceDictionaryCommand, item)\r\n\t\t# Translators: The label for the menu item to open Temporary speech dictionary dialog.\r\n\t\titem = subMenu_speechDicts.Append(wx.ID_ANY,_(\"&Temporary dictionary...\"),_(\"A dialog where you can set temporary dictionary by adding dictionary entries to the edit box\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onTemporaryDictionaryCommand, item)\r\n\t\t# Translators: The label for a submenu under NvDA Preferences menu to select speech dictionaries.\r\n\t\tmenu_preferences.AppendMenu(wx.ID_ANY,_(\"Speech &dictionaries\"),subMenu_speechDicts)\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label for the menu item to open Punctuation/symbol pronunciation dialog.\r\n\t\t\titem = menu_preferences.Append(wx.ID_ANY, _(\"&Punctuation/symbol pronunciation...\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onSpeechSymbolsCommand, item)\r\n\t\t\t# Translators: The label for the menu item to open the Input Gestures dialog.\r\n\t\t\titem = menu_preferences.Append(wx.ID_ANY, _(\"I&nput gestures...\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onInputGesturesCommand, item)\r\n\t\t# Translators: The label for Preferences submenu in NVDA menu.\r\n\t\tself.menu.AppendMenu(wx.ID_ANY,_(\"&Preferences\"),menu_preferences)\r\n\r\n\t\tmenu_tools = self.toolsMenu = wx.Menu()\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label for the menu item to open NVDA Log Viewer.\r\n\t\t\titem = menu_tools.Append(wx.ID_ANY, _(\"View log\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onViewLogCommand, item)\r\n\t\t# Translators: The label for the menu item to toggle Speech Viewer.\r\n\t\titem=self.menu_tools_toggleSpeechViewer = menu_tools.AppendCheckItem(wx.ID_ANY, _(\"Speech viewer\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onToggleSpeechViewerCommand, item)\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label for the menu item to open NVDA Python Console.\r\n\t\t\titem = menu_tools.Append(wx.ID_ANY, _(\"Python console\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onPythonConsoleCommand, item)\r\n\t\t\t# Translators: The label of a menu item to open the Add-ons Manager.\r\n\t\t\titem = menu_tools.Append(wx.ID_ANY, _(\"Manage &add-ons...\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onAddonsManagerCommand, item)\r\n\t\tif not globalVars.appArgs.secure and getattr(sys,'frozen',None):\r\n\t\t\t# Translators: The label for the menu item to create a portable copy of NVDA from an installed or another portable version.\r\n\t\t\titem = menu_tools.Append(wx.ID_ANY, _(\"Create portable copy...\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onCreatePortableCopyCommand, item)\r\n\t\t\tif not config.isInstalledCopy():\r\n\t\t\t\t# Translators: The label for the menu item to install NVDA on the computer.\r\n\t\t\t\titem = menu_tools.Append(wx.ID_ANY, _(\"&Install NVDA...\"))\r\n\t\t\t\tself.Bind(wx.EVT_MENU, frame.onInstallCommand, item)\r\n\t\t# Translators: The label for the menu item to reload plugins.\r\n\t\titem = menu_tools.Append(wx.ID_ANY, _(\"Reload plugins\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onReloadPluginsCommand, item)\r\n\t\t# Translators: The label for the Tools submenu in NVDA menu.\r\n\t\tself.menu.AppendMenu(wx.ID_ANY, _(\"Tools\"), menu_tools)\r\n\r\n\t\tmenu_help = self.helpMenu = wx.Menu()\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label of a menu item to open NVDA user guide.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"&User Guide\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(getDocFilePath(\"userGuide.html\")), item)\r\n\t\t\t# Translators: The label of a menu item to open the Commands Quick Reference document.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"Commands &Quick Reference\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(getDocFilePath(\"keyCommands.html\")), item)\r\n\t\t\t# Translators: The label for the menu item to open What's New document.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"What's &new\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(getDocFilePath(\"changes.html\")), item)\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"NVDA &web site\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(\"http://www.nvda-project.org/\"), item)\r\n\t\t\t# Translators: The label for the menu item to view NVDA License document.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"L&icense\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(getDocFilePath(\"copying.txt\", False)), item)\r\n\t\t\t# Translators: The label for the menu item to view NVDA Contributors list document.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"C&ontributors\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(getDocFilePath(\"contributors.txt\", False)), item)\r\n\t\t# Translators: The label for the menu item to open NVDA Welcome Dialog.\r\n\t\titem = menu_help.Append(wx.ID_ANY, _(\"We&lcome dialog...\"))\r\n\t\tself.Bind(wx.EVT_MENU, lambda evt: WelcomeDialog.run(), item)\r\n\t\tmenu_help.AppendSeparator()\r\n\t\tif updateCheck:\r\n\t\t\t# Translators: The label of a menu item to manually check for an updated version of NVDA.\r\n\t\t\titem = menu_help.Append(wx.ID_ANY, _(\"&Check for update...\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onCheckForUpdateCommand, item)\r\n\t\t# Translators: The label for the menu item to open About dialog to get information about NVDA.\r\n\t\titem = menu_help.Append(wx.ID_ABOUT, _(\"About...\"), _(\"About NVDA\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onAboutCommand, item)\r\n\t\t# Translators: The label for the Help submenu in NVDA menu.\r\n\t\tself.menu.AppendMenu(wx.ID_ANY,_(\"&Help\"),menu_help)\r\n\t\tself.menu.AppendSeparator()\r\n\t\t# Translators: The label for the menu item to open the Configuration Profiles dialog.\r\n\t\titem = self.menu.Append(wx.ID_ANY, _(\"&Configuration profiles...\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onConfigProfilesCommand, item)\r\n\t\t# Translators: The label for the menu item to revert to saved configuration.\r\n\t\titem = self.menu.Append(wx.ID_ANY, _(\"&Revert to saved configuration\"),_(\"Reset all settings to saved state\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onRevertToSavedConfigurationCommand, item)\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\t# Translators: The label for the menu item to reset settings to default settings.\r\n\t\t\t# Here, default settings means settings that were there when the user first used NVDA.\r\n\t\t\titem = self.menu.Append(wx.ID_ANY, _(\"&Reset configuration to factory defaults\"),_(\"Reset all settings to default state\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onRevertToDefaultConfigurationCommand, item)\r\n\t\t\t# Translators: The label for the menu item to save current settings.\r\n\t\t\titem = self.menu.Append(wx.ID_SAVE, _(\"&Save configuration\"), _(\"Write the current configuration to nvda.ini\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, frame.onSaveConfigurationCommand, item)\r\n\t\tif not globalVars.appArgs.secure:\r\n\t\t\tself.menu.AppendSeparator()\r\n\t\t\t# Translators: The label for the menu item to open donate page.\r\n\t\t\titem = self.menu.Append(wx.ID_ANY, _(\"Donate\"))\r\n\t\t\tself.Bind(wx.EVT_MENU, lambda evt: os.startfile(DONATE_URL), item)\r\n\t\tself.menu.AppendSeparator()\r\n\t\titem = self.menu.Append(wx.ID_EXIT, _(\"E&xit\"),_(\"Exit NVDA\"))\r\n\t\tself.Bind(wx.EVT_MENU, frame.onExitCommand, item)\r\n\r\n\t\tself.Bind(wx.EVT_TASKBAR_LEFT_DOWN, self.onActivate)\r\n\t\tself.Bind(wx.EVT_TASKBAR_RIGHT_DOWN, self.onActivate)\r\n\r\n\tdef Destroy(self):\r\n\t\tself.menu.Destroy()\r\n\t\tsuper(SysTrayIcon, self).Destroy()\r\n\r\n\tdef onActivate(self, evt):\r\n\t\tmainFrame.prePopup()\r\n\t\timport appModules.nvda\r\n\t\tif not appModules.nvda.nvdaMenuIaIdentity:\r\n\t\t\t# The NVDA app module doesn't know how to identify the NVDA menu yet.\r\n\t\t\t# Signal that the NVDA menu has just been opened.\r\n\t\t\tappModules.nvda.nvdaMenuIaIdentity = True\r\n\t\tself.PopupMenu(self.menu)\r\n\t\tif appModules.nvda.nvdaMenuIaIdentity is True:\r\n\t\t\t# The NVDA menu didn't actually appear for some reason.\r\n\t\t\tappModules.nvda.nvdaMenuIaIdentity = None\r\n\t\tmainFrame.postPopup()\r\n\r\ndef initialize():\r\n\tglobal mainFrame\r\n\tif mainFrame:\r\n\t\traise RuntimeError(\"GUI already initialized\")\r\n\tmainFrame = MainFrame()\r\n\twx.GetApp().SetTopWindow(mainFrame)\r\n\r\ndef terminate():\r\n\tglobal mainFrame\r\n\t# This is called after the main loop exits because WM_QUIT exits the main loop\r\n\t# without destroying all objects correctly and we need to support WM_QUIT.\r\n\t# Therefore, any request to exit should exit the main loop.\r\n\twx.CallAfter(mainFrame.Destroy)\r\n\t# #4460: We need another iteration of the main loop\r\n\t# so that everything (especially the TaskBarIcon) is cleaned up properly.\r\n\t# ProcessPendingEvents doesn't seem to work, but MainLoop does.\r\n\t# Because the top window gets destroyed,\r\n\t# MainLoop thankfully returns pretty quickly.\r\n\twx.GetApp().MainLoop()\r\n\tmainFrame = None\r\n\r\ndef showGui():\r\n \twx.CallAfter(mainFrame.showGui)\r\n\r\ndef quit():\r\n\twx.CallAfter(mainFrame.onExitCommand, None)\r\n\r\ndef messageBox(message, caption=wx.MessageBoxCaptionStr, style=wx.OK | wx.CENTER, parent=None):\r\n\t\"\"\"Display a message dialog.\r\n\tThis should be used for all message dialogs\r\n\trather than using C{wx.MessageDialog} and C{wx.MessageBox} directly.\r\n\t@param message: The message text.\r\n\t@type message: str\r\n\t@param caption: The caption (title) of the dialog.\r\n\t@type caption: str\r\n\t@param style: Same as for wx.MessageBox.\r\n\t@type style: int\r\n\t@param parent: The parent window (optional).\r\n\t@type parent: C{wx.Window}\r\n\t@return: Same as for wx.MessageBox.\r\n\t@rtype: int\r\n\t\"\"\"\r\n\tglobal isInMessageBox\r\n\twasAlready = isInMessageBox\r\n\tisInMessageBox = True\r\n\tif not parent:\r\n\t\tmainFrame.prePopup()\r\n\tres = wx.MessageBox(message, caption, style, parent or mainFrame)\r\n\tif not parent:\r\n\t\tmainFrame.postPopup()\r\n\tif not wasAlready:\r\n\t\tisInMessageBox = False\r\n\treturn res\r\n\r\ndef runScriptModalDialog(dialog, callback=None):\r\n\t\"\"\"Run a modal dialog from a script.\r\n\tThis will not block the caller,\r\n\tbut will instead call C{callback} (if provided) with the result from the dialog.\r\n\tThe dialog will be destroyed once the callback has returned.\r\n\t@param dialog: The dialog to show.\r\n\t@type dialog: C{wx.Dialog}\r\n\t@param callback: The optional callable to call with the result from the dialog.\r\n\t@type callback: callable\r\n\t\"\"\"\r\n\tdef run():\r\n\t\tmainFrame.prePopup()\r\n\t\tres = dialog.ShowModal()\r\n\t\tmainFrame.postPopup()\r\n\t\tif callback:\r\n\t\t\tcallback(res)\r\n\t\tdialog.Destroy()\r\n\twx.CallAfter(run)\r\n\r\nclass WelcomeDialog(wx.Dialog):\r\n\t\"\"\"The NVDA welcome dialog.\r\n\tThis provides essential information for new users, such as a description of the NVDA key and instructions on how to activate the NVDA menu.\r\n\tIt also provides quick access to some important configuration options.\r\n\tThis dialog is displayed the first time NVDA is started with a new configuration.\r\n\t\"\"\"\r\n\r\n\t# Translators: The main message for the Welcome dialog when the user starts NVDA for the first time.\r\n\tWELCOME_MESSAGE_DETAIL = _(\r\n\t\t\"Most commands for controlling NVDA require you to hold down the NVDA key while pressing other keys.\\n\"\r\n\t\t\"By default, the numpad insert and main insert keys may both be used as the NVDA key.\\n\"\r\n\t\t\"You can also configure NVDA to use the CapsLock as the NVDA key.\\n\"\r\n\t\t\"Press NVDA+n at any time to activate the NVDA menu.\\n\"\r\n\t\t\"From this menu, you can configure NVDA, get help and access other NVDA functions.\\n\"\r\n\t)\r\n\r\n\tdef __init__(self, parent):\r\n\t\t# Translators: The title of the Welcome dialog when user starts NVDA for the first time.\r\n\t\tsuper(WelcomeDialog, self).__init__(parent, wx.ID_ANY, _(\"Welcome to NVDA\"))\r\n\t\tmainSizer=wx.BoxSizer(wx.VERTICAL)\r\n\t\t# Translators: The header for the Welcome dialog when user starts NVDA for the first time. This is in larger,\r\n\t\t# bold lettering \r\n\t\twelcomeTextHeader = wx.StaticText(self, label=_(\"Welcome to NVDA!\"))\r\n\t\twelcomeTextHeader.SetFont(wx.Font(18, wx.NORMAL, wx.NORMAL, wx.BOLD))\r\n\t\tmainSizer.AddSpacer(10)\r\n\t\tmainSizer.Add(welcomeTextHeader,border=20,flag=wx.EXPAND|wx.LEFT|wx.RIGHT)\r\n\t\tmainSizer.AddSpacer(10)\r\n\t\twelcomeTextDetail = wx.StaticText(self, wx.ID_ANY, self.WELCOME_MESSAGE_DETAIL)\r\n\t\tmainSizer.Add(welcomeTextDetail,border=20,flag=wx.EXPAND|wx.LEFT|wx.RIGHT)\r\n\t\toptionsSizer = wx.StaticBoxSizer(wx.StaticBox(self, wx.ID_ANY, _(\"Options\")), wx.VERTICAL)\r\n\t\tself.capsAsNVDAModifierCheckBox = wx.CheckBox(self, wx.ID_ANY, _(\"Use CapsLock as an NVDA modifier key\"))\r\n\t\tself.capsAsNVDAModifierCheckBox.SetValue(config.conf[\"keyboard\"][\"useCapsLockAsNVDAModifierKey\"])\r\n\t\toptionsSizer.Add(self.capsAsNVDAModifierCheckBox,flag=wx.TOP|wx.LEFT,border=10)\r\n\t\t# Translators: The label of a check box in the Welcome dialog.\r\n\t\tself.startAfterLogonCheckBox = wx.CheckBox(self, label=_(\"&Automatically start NVDA after I log on to Windows\"))\r\n\t\tself.startAfterLogonCheckBox.Value = config.getStartAfterLogon()\r\n\t\tif globalVars.appArgs.secure or not config.isInstalledCopy():\r\n\t\t\tself.startAfterLogonCheckBox.Disable()\r\n\t\toptionsSizer.Add(self.startAfterLogonCheckBox,flag=wx.TOP|wx.LEFT,border=10)\r\n\t\t# Translators: This is a label for a checkbox in welcome dialog to show welcome dialog at startup.\r\n\t\tself.showWelcomeDialogAtStartupCheckBox = wx.CheckBox(self, wx.ID_ANY, _(\"Show this dialog when NVDA starts\"))\r\n\t\tself.showWelcomeDialogAtStartupCheckBox.SetValue(config.conf[\"general\"][\"showWelcomeDialogAtStartup\"])\r\n\t\toptionsSizer.Add(self.showWelcomeDialogAtStartupCheckBox,flag=wx.TOP|wx.LEFT,border=10)\r\n\t\tmainSizer.Add(optionsSizer,flag=wx.LEFT|wx.TOP|wx.RIGHT|wx.EXPAND,border=20)\r\n\t\tmainSizer.Add(self.CreateButtonSizer(wx.OK),flag=wx.TOP|wx.BOTTOM|wx.ALIGN_CENTER_HORIZONTAL,border=20)\r\n\t\tself.Bind(wx.EVT_BUTTON, self.onOk, id=wx.ID_OK)\r\n\r\n\t\tself.SetSizer(mainSizer)\r\n\t\tmainSizer.Fit(self)\r\n\t\tself.capsAsNVDAModifierCheckBox.SetFocus()\r\n\t\tself.Center(wx.BOTH | wx.CENTER_ON_SCREEN)\r\n\r\n\tdef onOk(self, evt):\r\n\t\tconfig.conf[\"keyboard\"][\"useCapsLockAsNVDAModifierKey\"] = self.capsAsNVDAModifierCheckBox.IsChecked()\r\n\t\tif self.startAfterLogonCheckBox.Enabled:\r\n\t\t\tconfig.setStartAfterLogon(self.startAfterLogonCheckBox.Value)\r\n\t\tconfig.conf[\"general\"][\"showWelcomeDialogAtStartup\"] = self.showWelcomeDialogAtStartupCheckBox.IsChecked()\r\n\t\ttry:\r\n\t\t\tconfig.conf.save()\r\n\t\texcept:\r\n\t\t\tlog.debugWarning(\"could not save\",exc_info=True)\r\n\t\tself.EndModal(wx.ID_OK)\r\n\r\n\t@classmethod\r\n\tdef run(cls):\r\n\t\t\"\"\"Prepare and display an instance of this dialog.\r\n\t\tThis does not require the dialog to be instantiated.\r\n\t\t\"\"\"\r\n\t\tmainFrame.prePopup()\r\n\t\td = cls(mainFrame)\r\n\t\td.ShowModal()\r\n\t\td.Destroy()\r\n\t\tmainFrame.postPopup()\r\n\r\nclass LauncherDialog(wx.Dialog):\r\n\t\"\"\"The dialog that is displayed when NVDA is started from the launcher.\r\n\tThis displays the license and allows the user to install or create a portable copy of NVDA.\r\n\t\"\"\"\r\n\r\n\tdef __init__(self, parent):\r\n\t\tsuper(LauncherDialog, self).__init__(parent, title=versionInfo.name)\r\n\t\tmainSizer = wx.BoxSizer(wx.VERTICAL)\r\n\t\tsHelper = guiHelper.BoxSizerHelper(self, orientation=wx.VERTICAL)\r\n\r\n\t\t# Translators: The label of the license text which will be shown when NVDA installation program starts.\r\n\t\tgroupLabel = _(\"License Agreement\")\r\n\t\tsizer = sHelper.addItem(wx.StaticBoxSizer(wx.StaticBox(self, label=groupLabel), wx.VERTICAL))\r\n\t\tlicenseTextCtrl = wx.TextCtrl(self, size=(500, 400), style=wx.TE_MULTILINE | wx.TE_READONLY | wx.TE_RICH)\r\n\t\tlicenseTextCtrl.Value = codecs.open(getDocFilePath(\"copying.txt\", False), \"r\", encoding=\"UTF-8\").read()\r\n\t\tsizer.Add(licenseTextCtrl)\r\n\r\n\t\t# Translators: The label for a checkbox in NvDA installation program to agree to the license agreement.\r\n\t\tagreeText = _(\"I &agree\")\r\n\t\tself.licenseAgreeCheckbox = sHelper.addItem(wx.CheckBox(self, label=agreeText))\r\n\t\tself.licenseAgreeCheckbox.Value = False\r\n\t\tself.licenseAgreeCheckbox.Bind(wx.EVT_CHECKBOX, self.onLicenseAgree)\r\n\r\n\t\tsizer = sHelper.addItem(wx.GridSizer(rows=2, cols=2))\r\n\t\tself.actionButtons = []\r\n\t\t# Translators: The label of the button in NVDA installation program to install NvDA on the user's computer.\r\n\t\tctrl = wx.Button(self, label=_(\"&Install NVDA on this computer\"))\r\n\t\tsizer.Add(ctrl)\r\n\t\tctrl.Bind(wx.EVT_BUTTON, lambda evt: self.onAction(evt, mainFrame.onInstallCommand))\r\n\t\tself.actionButtons.append(ctrl)\r\n\t\t# Translators: The label of the button in NVDA installation program to create a portable version of NVDA.\r\n\t\tctrl = wx.Button(self, label=_(\"Create &portable copy\"))\r\n\t\tsizer.Add(ctrl)\r\n\t\tctrl.Bind(wx.EVT_BUTTON, lambda evt: self.onAction(evt, mainFrame.onCreatePortableCopyCommand))\r\n\t\tself.actionButtons.append(ctrl)\r\n\t\t# Translators: The label of the button in NVDA installation program to continue using the installation program as a temporary copy of NVDA.\r\n\t\tctrl = wx.Button(self, label=_(\"&Continue running\"))\r\n\t\tsizer.Add(ctrl)\r\n\t\tctrl.Bind(wx.EVT_BUTTON, self.onContinueRunning)\r\n\t\tself.actionButtons.append(ctrl)\r\n\t\tsizer.Add(wx.Button(self, label=_(\"E&xit\"), id=wx.ID_CANCEL))\r\n\t\t# If we bind this on the button, it fails to trigger when the dialog is closed.\r\n\t\tself.Bind(wx.EVT_BUTTON, self.onExit, id=wx.ID_CANCEL)\r\n\r\n\t\tfor ctrl in self.actionButtons:\r\n\t\t\tctrl.Disable()\r\n\r\n\t\tmainSizer.Add(sHelper.sizer, border = guiHelper.BORDER_FOR_DIALOGS, flag=wx.ALL)\r\n\t\tself.Sizer = mainSizer\r\n\t\tmainSizer.Fit(self)\r\n\t\tself.Center(wx.BOTH | wx.CENTER_ON_SCREEN)\r\n\r\n\tdef onLicenseAgree(self, evt):\r\n\t\tfor ctrl in self.actionButtons:\r\n\t\t\tctrl.Enable(evt.IsChecked())\r\n\r\n\tdef onAction(self, evt, func):\r\n\t\tself.Destroy()\r\n\t\tfunc(evt)\r\n\r\n\tdef onContinueRunning(self, evt):\r\n\t\tself.Destroy()\r\n\t\tcore.doStartupDialogs()\r\n\r\n\tdef onExit(self, evt):\r\n\t\twx.GetApp().ExitMainLoop()\r\n\r\n\t@classmethod\r\n\tdef run(cls):\r\n\t\t\"\"\"Prepare and display an instance of this dialog.\r\n\t\tThis does not require the dialog to be instantiated.\r\n\t\t\"\"\"\r\n\t\tmainFrame.prePopup()\r\n\t\td = cls(mainFrame)\r\n\t\td.Show()\r\n\t\tmainFrame.postPopup()\r\n\r\nclass ExitDialog(wx.Dialog):\r\n\t_instance = None\r\n\r\n\tdef __new__(cls, parent):\r\n\t\t# Make this a singleton.\r\n\t\tinst = cls._instance() if cls._instance else None\r\n\t\tif not inst:\r\n\t\t\treturn super(cls, cls).__new__(cls, parent)\r\n\t\treturn inst\r\n\r\n\tdef __init__(self, parent):\r\n\t\tinst = ExitDialog._instance() if ExitDialog._instance else None\r\n\t\tif inst:\r\n\t\t\treturn\r\n\t\t# Use a weakref so the instance can die.\r\n\t\tExitDialog._instance = weakref.ref(self)\r\n\t\t# Translators: The title of the dialog to exit NVDA\r\n\t\tsuper(ExitDialog, self).__init__(parent, title=_(\"Exit NVDA\"))\r\n\t\tdialog = self\r\n\t\tmainSizer = wx.BoxSizer(wx.VERTICAL)\r\n\r\n\t\tcontentSizerHelper = guiHelper.BoxSizerHelper(self, orientation=wx.VERTICAL)\r\n\r\n\t\tif globalVars.appArgs.disableAddons:\r\n\t\t\t# Translators: A message in the exit Dialog shown when all add-ons are disabled.\r\n\t\t\taddonsDisabledText = _(\"All add-ons are now disabled. They will be re-enabled on the next restart unless you choose to disable them again.\")\r\n\t\t\tcontentSizerHelper.addItem(wx.StaticText(self, wx.ID_ANY, label=addonsDisabledText))\r\n\r\n\t\t# Translators: The label for actions list in the Exit dialog.\r\n\t\tlabelText=_(\"What would you like to &do?\")\r\n\t\tself.actions = [\r\n\t\t# Translators: An option in the combo box to choose exit action.\r\n\t\t_(\"Exit\"),\r\n\t\t# Translators: An option in the combo box to choose exit action.\r\n\t\t_(\"Restart\"),\r\n\t\t# Translators: An option in the combo box to choose exit action.\r\n\t\t_(\"Restart with add-ons disabled\"),\r\n\t\t# Translators: An option in the combo box to choose exit action.\r\n\t\t_(\"Restart with debug logging enabled\")]\r\n\t\tself.actionsList = contentSizerHelper.addLabeledControl(labelText, wx.Choice, choices=self.actions)\r\n\t\tself.actionsList.SetSelection(0)\r\n\r\n\t\tcontentSizerHelper.addItem( self.CreateButtonSizer(wx.OK | wx.CANCEL))\r\n\r\n\t\tself.Bind(wx.EVT_BUTTON, self.onOk, id=wx.ID_OK)\r\n\t\tself.Bind(wx.EVT_BUTTON, self.onCancel, id=wx.ID_CANCEL)\r\n\r\n\t\tmainSizer.Add(contentSizerHelper.sizer, border=guiHelper.BORDER_FOR_DIALOGS, flag=wx.ALL)\r\n\t\tmainSizer.Fit(self)\r\n\t\tself.Sizer = mainSizer\r\n\t\tself.actionsList.SetFocus()\r\n\t\tself.Center(wx.BOTH | wx.CENTER_ON_SCREEN)\r\n\r\n\tdef onOk(self, evt):\r\n\t\taction=self.actionsList.GetSelection()\r\n\t\tif action == 0:\r\n\t\t\twx.GetApp().ExitMainLoop()\r\n\t\telif action == 1:\r\n\t\t\tqueueHandler.queueFunction(queueHandler.eventQueue,core.restart)\r\n\t\telif action == 2:\r\n\t\t\tqueueHandler.queueFunction(queueHandler.eventQueue,core.restart,disableAddons=True)\r\n\t\telif action == 3:\r\n\t\t\tqueueHandler.queueFunction(queueHandler.eventQueue,core.restart,debugLogging=True)\r\n\t\tself.Destroy()\r\n\r\n\tdef onCancel(self, evt):\r\n\t\tself.Destroy()\r\n\r\nclass ExecAndPump(threading.Thread):\r\n\t\"\"\"Executes the given function with given args and kwargs in a background thread while blocking and pumping in the current thread.\"\"\"\r\n\r\n\tdef __init__(self,func,*args,**kwargs):\r\n\t\tself.func=func\r\n\t\tself.args=args\r\n\t\tself.kwargs=kwargs\r\n\t\tsuper(ExecAndPump,self).__init__()\r\n\t\tself.threadExc=None\r\n\t\tself.start()\r\n\t\ttime.sleep(0.1)\r\n\t\tthreadHandle=ctypes.c_int()\r\n\t\tthreadHandle.value=ctypes.windll.kernel32.OpenThread(0x100000,False,self.ident)\r\n\t\tmsg=ctypes.wintypes.MSG()\r\n\t\twhile ctypes.windll.user32.MsgWaitForMultipleObjects(1,ctypes.byref(threadHandle),False,-1,255)==1:\r\n\t\t\twhile ctypes.windll.user32.PeekMessageW(ctypes.byref(msg),None,0,0,1):\r\n\t\t\t\tctypes.windll.user32.TranslateMessage(ctypes.byref(msg))\r\n\t\t\t\tctypes.windll.user32.DispatchMessageW(ctypes.byref(msg))\r\n\t\tif self.threadExc:\r\n\t\t\traise self.threadExc\r\n\r\n\tdef run(self):\r\n\t\ttry:\r\n\t\t\tself.func(*self.args,**self.kwargs)\r\n\t\texcept Exception as e:\r\n\t\t\tself.threadExc=e\r\n\t\t\tlog.debugWarning(\"task had errors\",exc_info=True)\r\n\r\nclass IndeterminateProgressDialog(wx.ProgressDialog):\r\n\r\n\tdef __init__(self, parent, title, message):\r\n\t\tsuper(IndeterminateProgressDialog, self).__init__(title, message, parent=parent)\r\n\t\tself._speechCounter = -1\r\n\t\tself.timer = wx.PyTimer(self.Pulse)\r\n\t\tself.timer.Start(1000)\r\n\t\tself.Raise()\r\n\t\tself.Center(wx.BOTH | wx.CENTER_ON_SCREEN)\r\n\r\n\tdef Pulse(self):\r\n\t\tsuper(IndeterminateProgressDialog, self).Pulse()\r\n\t\t# We want progress to be spoken on the first pulse and every 10 pulses thereafter.\r\n\t\t# Therefore, cycle from 0 to 9 inclusive.\r\n\t\tself._speechCounter = (self._speechCounter + 1) % 10\r\n\t\tpbConf = config.conf[\"presentation\"][\"progressBarUpdates\"]\r\n\t\tif pbConf[\"progressBarOutputMode\"] == \"off\":\r\n\t\t\treturn\r\n\t\tif not pbConf[\"reportBackgroundProgressBars\"] and not self.IsActive():\r\n\t\t\treturn\r\n\t\tif pbConf[\"progressBarOutputMode\"] in (\"beep\", \"both\"):\r\n\t\t\ttones.beep(440, 40)\r\n\t\tif pbConf[\"progressBarOutputMode\"] in (\"speak\", \"both\") and self._speechCounter == 0:\r\n\t\t\t# Translators: Announced periodically to indicate progress for an indeterminate progress bar.\r\n\t\t\tspeech.speakMessage(_(\"Please wait\"))\r\n\r\n\tdef IsActive(self):\r\n\t\t#4714: In wxPython 3, ProgressDialog.IsActive always seems to return False.\r\n\t\treturn winUser.isDescendantWindow(winUser.getForegroundWindow(), self.Handle)\r\n\r\n\tdef done(self):\r\n\t\tself.timer.Stop()\r\n\t\tif self.IsActive():\r\n\t\t\ttones.beep(1760, 40)\r\n\t\tself.Hide()\r\n\t\tself.Destroy()\r\n\r\ndef shouldConfigProfileTriggersBeSuspended():\r\n\t\"\"\"Determine whether configuration profile triggers should be suspended in relation to NVDA's GUI.\r\n\tFor NVDA configuration dialogs, the configuration should remain the same as it was before the GUI was popped up\r\n\tso the user can change settings in the correct profile.\r\n\tTop-level windows that require this behavior should have a C{shouldSuspendConfigProfileTriggers} attribute set to C{True}.\r\n\tBecause these dialogs are often opened via the NVDA menu, this applies to the NVDA menu as well.\r\n\t\"\"\"\r\n\tif winUser.getGUIThreadInfo(ctypes.windll.kernel32.GetCurrentThreadId()).flags & 0x00000010:\r\n\t\t# The NVDA menu is active.\r\n\t\treturn True\r\n\tfor window in wx.GetTopLevelWindows():\r\n\t\tif window.IsShown() and getattr(window, \"shouldSuspendConfigProfileTriggers\", False):\r\n\t\t\treturn True\r\n\treturn False\r\n", "idx": 4, "id": 19244, "msg": "", "proj": "nvaccess-nvda", "lang": "py"}
{"patch": "@@ -33,11 +33,18 @@ import (\n )\n \n var (\n-\tIfaceListRegexp  = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}(,[a-zA-Z0-9_-]{1,15})*$`)\n-\tAuthorityRegexp  = regexp.MustCompile(`^[^:/]+:\\d+$`)\n-\tHostnameRegexp   = regexp.MustCompile(`^[a-zA-Z0-9_.-]+$`)\n-\tStringRegexp     = regexp.MustCompile(`^.*$`)\n-\tIfaceParamRegexp = regexp.MustCompile(`^[a-zA-Z0-9:._+-]{1,15}$`)\n+\t// RegexpIfaceElemRegexp matches an individual element in the overall interface list;\n+\t// assumes the value represents a regular expression and is marked by '/' at the start\n+\t// and end and cannot have spaces\n+\tRegexpIfaceElemRegexp = regexp.MustCompile(`^\\/[^\\s]+\\/$`)\n+\t// NonRegexpIfaceElemRegexp matches an individual element in the overall interface list;\n+\t// assumes the value is between 1-15 chars long and only be alphanumeric or - or _\n+\tNonRegexpIfaceElemRegexp = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}$`)\n+\tIfaceListRegexp          = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}(,[a-zA-Z0-9_-]{1,15})*$`)\n+\tAuthorityRegexp          = regexp.MustCompile(`^[^:/]+:\\d+$`)\n+\tHostnameRegexp           = regexp.MustCompile(`^[a-zA-Z0-9_.-]+$`)\n+\tStringRegexp             = regexp.MustCompile(`^.*$`)\n+\tIfaceParamRegexp         = regexp.MustCompile(`^[a-zA-Z0-9:._+-]{1,15}$`)\n )\n \n const (", "y": 0, "oldf": "// Copyright (c) 2016-2019 Tigera, Inc. All rights reserved.\n\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     http://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n\npackage config\n\nimport (\n\t\"errors\"\n\t\"fmt\"\n\t\"net\"\n\t\"os\"\n\t\"reflect\"\n\t\"regexp\"\n\t\"strconv\"\n\t\"strings\"\n\t\"time\"\n\n\tlog \"github.com/sirupsen/logrus\"\n\n\t\"github.com/projectcalico/libcalico-go/lib/apiconfig\"\n\t\"github.com/projectcalico/libcalico-go/lib/names\"\n\t\"github.com/projectcalico/libcalico-go/lib/numorstring\"\n)\n\nvar (\n\tIfaceListRegexp  = regexp.MustCompile(`^[a-zA-Z0-9_-]{1,15}(,[a-zA-Z0-9_-]{1,15})*$`)\n\tAuthorityRegexp  = regexp.MustCompile(`^[^:/]+:\\d+$`)\n\tHostnameRegexp   = regexp.MustCompile(`^[a-zA-Z0-9_.-]+$`)\n\tStringRegexp     = regexp.MustCompile(`^.*$`)\n\tIfaceParamRegexp = regexp.MustCompile(`^[a-zA-Z0-9:._+-]{1,15}$`)\n)\n\nconst (\n\tmaxUint = ^uint(0)\n\tmaxInt  = int(maxUint >> 1)\n\tminInt  = -maxInt - 1\n)\n\n// Source of a config value.  Values from higher-numbered sources override\n// those from lower-numbered sources.  Note: some parameters (such as those\n// needed to connect to the datastore) can only be set from a local source.\ntype Source uint8\n\nconst (\n\tDefault = iota\n\tDatastoreGlobal\n\tDatastorePerHost\n\tConfigFile\n\tEnvironmentVariable\n)\n\nvar SourcesInDescendingOrder = []Source{EnvironmentVariable, ConfigFile, DatastorePerHost, DatastoreGlobal}\n\nfunc (source Source) String() string {\n\tswitch source {\n\tcase Default:\n\t\treturn \"<default>\"\n\tcase DatastoreGlobal:\n\t\treturn \"datastore (global)\"\n\tcase DatastorePerHost:\n\t\treturn \"datastore (per-host)\"\n\tcase ConfigFile:\n\t\treturn \"config file\"\n\tcase EnvironmentVariable:\n\t\treturn \"environment variable\"\n\t}\n\treturn fmt.Sprintf(\"<unknown(%v)>\", uint8(source))\n}\n\nfunc (source Source) Local() bool {\n\tswitch source {\n\tcase Default, ConfigFile, EnvironmentVariable:\n\t\treturn true\n\tdefault:\n\t\treturn false\n\t}\n}\n\n// Config contains the best, parsed config values loaded from the various sources.\n// We use tags to control the parsing and validation.\ntype Config struct {\n\t// Configuration parameters.\n\tUseInternalDataplaneDriver bool   `config:\"bool;true\"`\n\tDataplaneDriver            string `config:\"file(must-exist,executable);calico-iptables-plugin;non-zero,die-on-fail,skip-default-validation\"`\n\n\tDatastoreType string `config:\"oneof(kubernetes,etcdv3);etcdv3;non-zero,die-on-fail,local\"`\n\n\tFelixHostname string `config:\"hostname;;local,non-zero\"`\n\n\tEtcdAddr      string   `config:\"authority;127.0.0.1:2379;local\"`\n\tEtcdScheme    string   `config:\"oneof(http,https);http;local\"`\n\tEtcdKeyFile   string   `config:\"file(must-exist);;local\"`\n\tEtcdCertFile  string   `config:\"file(must-exist);;local\"`\n\tEtcdCaFile    string   `config:\"file(must-exist);;local\"`\n\tEtcdEndpoints []string `config:\"endpoint-list;;local\"`\n\n\tTyphaAddr           string        `config:\"authority;;local\"`\n\tTyphaK8sServiceName string        `config:\"string;;local\"`\n\tTyphaK8sNamespace   string        `config:\"string;kube-system;non-zero,local\"`\n\tTyphaReadTimeout    time.Duration `config:\"seconds;30;local\"`\n\tTyphaWriteTimeout   time.Duration `config:\"seconds;10;local\"`\n\n\t// Client-side TLS config for Felix's communication with Typha.  If any of these are\n\t// specified, they _all_ must be - except that either TyphaCN or TyphaURISAN may be left\n\t// unset.  Felix will then initiate a secure (TLS) connection to Typha.  Typha must present\n\t// a certificate signed by a CA in TyphaCAFile, and with CN matching TyphaCN or URI SAN\n\t// matching TyphaURISAN.\n\tTyphaKeyFile  string `config:\"file(must-exist);;local\"`\n\tTyphaCertFile string `config:\"file(must-exist);;local\"`\n\tTyphaCAFile   string `config:\"file(must-exist);;local\"`\n\tTyphaCN       string `config:\"string;;local\"`\n\tTyphaURISAN   string `config:\"string;;local\"`\n\n\tIpv6Support    bool `config:\"bool;true\"`\n\tIgnoreLooseRPF bool `config:\"bool;false\"`\n\n\tRouteRefreshInterval               time.Duration `config:\"seconds;90\"`\n\tIptablesRefreshInterval            time.Duration `config:\"seconds;90\"`\n\tIptablesPostWriteCheckIntervalSecs time.Duration `config:\"seconds;1\"`\n\tIptablesLockFilePath               string        `config:\"file;/run/xtables.lock\"`\n\tIptablesLockTimeoutSecs            time.Duration `config:\"seconds;0\"`\n\tIptablesLockProbeIntervalMillis    time.Duration `config:\"millis;50\"`\n\tIpsetsRefreshInterval              time.Duration `config:\"seconds;10\"`\n\tMaxIpsetSize                       int           `config:\"int;1048576;non-zero\"`\n\n\tPolicySyncPathPrefix string `config:\"file;;\"`\n\n\tNetlinkTimeoutSecs time.Duration `config:\"seconds;10\"`\n\n\tMetadataAddr string `config:\"hostname;127.0.0.1;die-on-fail\"`\n\tMetadataPort int    `config:\"int(0,65535);8775;die-on-fail\"`\n\n\tOpenstackRegion string `config:\"region;;die-on-fail\"`\n\n\tInterfacePrefix  string `config:\"iface-list;cali;non-zero,die-on-fail\"`\n\tInterfaceExclude string `config:\"iface-list;kube-ipvs0\"`\n\n\tChainInsertMode             string `config:\"oneof(insert,append);insert;non-zero,die-on-fail\"`\n\tDefaultEndpointToHostAction string `config:\"oneof(DROP,RETURN,ACCEPT);DROP;non-zero,die-on-fail\"`\n\tIptablesFilterAllowAction   string `config:\"oneof(ACCEPT,RETURN);ACCEPT;non-zero,die-on-fail\"`\n\tIptablesMangleAllowAction   string `config:\"oneof(ACCEPT,RETURN);ACCEPT;non-zero,die-on-fail\"`\n\tLogPrefix                   string `config:\"string;calico-packet\"`\n\n\tLogFilePath string `config:\"file;/var/log/calico/felix.log;die-on-fail\"`\n\n\tLogSeverityFile   string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\tLogSeverityScreen string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\tLogSeveritySys    string `config:\"oneof(DEBUG,INFO,WARNING,ERROR,FATAL);INFO\"`\n\n\tIpInIpEnabled    bool   `config:\"bool;false\"`\n\tIpInIpMtu        int    `config:\"int;1440;non-zero\"`\n\tIpInIpTunnelAddr net.IP `config:\"ipv4;\"`\n\n\tReportingIntervalSecs time.Duration `config:\"seconds;30\"`\n\tReportingTTLSecs      time.Duration `config:\"seconds;90\"`\n\n\tEndpointReportingEnabled   bool          `config:\"bool;false\"`\n\tEndpointReportingDelaySecs time.Duration `config:\"seconds;1\"`\n\n\tIptablesMarkMask uint32 `config:\"mark-bitmask;0xffff0000;non-zero,die-on-fail\"`\n\n\tDisableConntrackInvalidCheck bool `config:\"bool;false\"`\n\n\tHealthEnabled                   bool   `config:\"bool;false\"`\n\tHealthPort                      int    `config:\"int(0,65535);9099\"`\n\tHealthHost                      string `config:\"string;localhost\"`\n\tPrometheusMetricsEnabled        bool   `config:\"bool;false\"`\n\tPrometheusMetricsPort           int    `config:\"int(0,65535);9091\"`\n\tPrometheusGoMetricsEnabled      bool   `config:\"bool;true\"`\n\tPrometheusProcessMetricsEnabled bool   `config:\"bool;true\"`\n\n\tFailsafeInboundHostPorts  []ProtoPort `config:\"port-list;tcp:22,udp:68,tcp:179,tcp:2379,tcp:2380,tcp:6666,tcp:6667;die-on-fail\"`\n\tFailsafeOutboundHostPorts []ProtoPort `config:\"port-list;udp:53,udp:67,tcp:179,tcp:2379,tcp:2380,tcp:6666,tcp:6667;die-on-fail\"`\n\n\tKubeNodePortRanges []numorstring.Port `config:\"portrange-list;30000:32767\"`\n\tNATPortRange       numorstring.Port   `config:\"portrange;\"`\n\n\tUsageReportingEnabled          bool          `config:\"bool;true\"`\n\tUsageReportingInitialDelaySecs time.Duration `config:\"seconds;300\"`\n\tUsageReportingIntervalSecs     time.Duration `config:\"seconds;86400\"`\n\tClusterGUID                    string        `config:\"string;baddecaf\"`\n\tClusterType                    string        `config:\"string;\"`\n\tCalicoVersion                  string        `config:\"string;\"`\n\n\tExternalNodesCIDRList []string `config:\"cidr-list;;die-on-fail\"`\n\n\tDebugMemoryProfilePath          string        `config:\"file;;\"`\n\tDebugCPUProfilePath             string        `config:\"file;/tmp/felix-cpu-<timestamp>.pprof;\"`\n\tDebugDisableLogDropping         bool          `config:\"bool;false\"`\n\tDebugSimulateCalcGraphHangAfter time.Duration `config:\"seconds;0\"`\n\tDebugSimulateDataplaneHangAfter time.Duration `config:\"seconds;0\"`\n\n\t// State tracking.\n\n\t// nameToSource tracks where we loaded each config param from.\n\tsourceToRawConfig map[Source]map[string]string\n\trawValues         map[string]string\n\tErr               error\n\n\tIptablesNATOutgoingInterfaceFilter string `config:\"iface-param;\"`\n}\n\ntype ProtoPort struct {\n\tProtocol string\n\tPort     uint16\n}\n\n// Load parses and merges the rawData from one particular source into this config object.\n// If there is a config value already loaded from a higher-priority source, then\n// the new value will be ignored (after validation).\nfunc (config *Config) UpdateFrom(rawData map[string]string, source Source) (changed bool, err error) {\n\tlog.Infof(\"Merging in config from %v: %v\", source, rawData)\n\t// Defensively take a copy of the raw data, in case we've been handed\n\t// a mutable map by mistake.\n\trawDataCopy := make(map[string]string)\n\tfor k, v := range rawData {\n\t\tif v == \"\" {\n\t\t\tlog.WithFields(log.Fields{\n\t\t\t\t\"name\":   k,\n\t\t\t\t\"source\": source,\n\t\t\t}).Info(\"Ignoring empty configuration parameter. Use value 'none' if \" +\n\t\t\t\t\"your intention is to explicitly disable the default value.\")\n\t\t\tcontinue\n\t\t}\n\t\trawDataCopy[k] = v\n\t}\n\tconfig.sourceToRawConfig[source] = rawDataCopy\n\n\tchanged, err = config.resolve()\n\treturn\n}\n\nfunc (c *Config) InterfacePrefixes() []string {\n\treturn strings.Split(c.InterfacePrefix, \",\")\n}\n\nfunc (c *Config) InterfaceExcludes() []string {\n\treturn strings.Split(c.InterfaceExclude, \",\")\n}\n\nfunc (config *Config) OpenstackActive() bool {\n\tif strings.Contains(strings.ToLower(config.ClusterType), \"openstack\") {\n\t\t// OpenStack is explicitly known to be present.  Newer versions of the OpenStack plugin\n\t\t// set this flag.\n\t\tlog.Debug(\"Cluster type contains OpenStack\")\n\t\treturn true\n\t}\n\t// If we get here, either OpenStack isn't present or we're running against an old version\n\t// of the OpenStack plugin, which doesn't set the flag.  Use heuristics based on the\n\t// presence of the OpenStack-related parameters.\n\tif config.MetadataAddr != \"\" && config.MetadataAddr != \"127.0.0.1\" {\n\t\tlog.Debug(\"OpenStack metadata IP set to non-default, assuming OpenStack active\")\n\t\treturn true\n\t}\n\tif config.MetadataPort != 0 && config.MetadataPort != 8775 {\n\t\tlog.Debug(\"OpenStack metadata port set to non-default, assuming OpenStack active\")\n\t\treturn true\n\t}\n\tfor _, prefix := range config.InterfacePrefixes() {\n\t\tif prefix == \"tap\" {\n\t\t\tlog.Debug(\"Interface prefix list contains 'tap', assuming OpenStack\")\n\t\t\treturn true\n\t\t}\n\t}\n\tlog.Debug(\"No evidence this is an OpenStack deployment; disabling OpenStack special-cases\")\n\treturn false\n}\n\nfunc (config *Config) resolve() (changed bool, err error) {\n\tnewRawValues := make(map[string]string)\n\tnameToSource := make(map[string]Source)\n\tfor _, source := range SourcesInDescendingOrder {\n\tvalueLoop:\n\t\tfor rawName, rawValue := range config.sourceToRawConfig[source] {\n\t\t\tcurrentSource := nameToSource[rawName]\n\t\t\tparam, ok := knownParams[strings.ToLower(rawName)]\n\t\t\tif !ok {\n\t\t\t\tif source >= currentSource {\n\t\t\t\t\t// Stash the raw value in case it's useful for\n\t\t\t\t\t// a plugin.  Since we don't know the canonical\n\t\t\t\t\t// name, use the raw name.\n\t\t\t\t\tnewRawValues[rawName] = rawValue\n\t\t\t\t\tnameToSource[rawName] = source\n\t\t\t\t}\n\t\t\t\tlog.WithField(\"raw name\", rawName).Info(\n\t\t\t\t\t\"Ignoring unknown config param.\")\n\t\t\t\tcontinue valueLoop\n\t\t\t}\n\t\t\tmetadata := param.GetMetadata()\n\t\t\tname := metadata.Name\n\t\t\tif metadata.Local && !source.Local() {\n\t\t\t\tlog.Warningf(\"Ignoring local-only configuration for %v from %v\",\n\t\t\t\t\tname, source)\n\t\t\t\tcontinue valueLoop\n\t\t\t}\n\n\t\t\tlog.Infof(\"Parsing value for %v: %v (from %v)\",\n\t\t\t\tname, rawValue, source)\n\t\t\tvar value interface{}\n\t\t\tif strings.ToLower(rawValue) == \"none\" {\n\t\t\t\t// Special case: we allow a value of \"none\" to force the value to\n\t\t\t\t// the zero value for a field.  The zero value often differs from\n\t\t\t\t// the default value.  Typically, the zero value means \"turn off\n\t\t\t\t// the feature\".\n\t\t\t\tif metadata.NonZero {\n\t\t\t\t\terr = errors.New(\"Non-zero field cannot be set to none\")\n\t\t\t\t\tlog.Errorf(\n\t\t\t\t\t\t\"Failed to parse value for %v: %v from source %v. %v\",\n\t\t\t\t\t\tname, rawValue, source, err)\n\t\t\t\t\tconfig.Err = err\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tvalue = metadata.ZeroValue\n\t\t\t\tlog.Infof(\"Value set to 'none', replacing with zero-value: %#v.\",\n\t\t\t\t\tvalue)\n\t\t\t} else {\n\t\t\t\tvalue, err = param.Parse(rawValue)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlogCxt := log.WithError(err).WithField(\"source\", source)\n\t\t\t\t\tif metadata.DieOnParseFailure {\n\t\t\t\t\t\tlogCxt.Error(\"Invalid (required) config value.\")\n\t\t\t\t\t\tconfig.Err = err\n\t\t\t\t\t\treturn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tlogCxt.WithField(\"default\", metadata.Default).Warn(\n\t\t\t\t\t\t\t\"Replacing invalid value with default\")\n\t\t\t\t\t\tvalue = metadata.Default\n\t\t\t\t\t\terr = nil\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tlog.Infof(\"Parsed value for %v: %v (from %v)\",\n\t\t\t\tname, value, source)\n\t\t\tif source < currentSource {\n\t\t\t\tlog.Infof(\"Skipping config value for %v from %v; \"+\n\t\t\t\t\t\"already have a value from %v\", name,\n\t\t\t\t\tsource, currentSource)\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tfield := reflect.ValueOf(config).Elem().FieldByName(name)\n\t\t\tfield.Set(reflect.ValueOf(value))\n\t\t\tnewRawValues[name] = rawValue\n\t\t\tnameToSource[name] = source\n\t\t}\n\t}\n\tchanged = !reflect.DeepEqual(newRawValues, config.rawValues)\n\tconfig.rawValues = newRawValues\n\treturn\n}\n\nfunc (config *Config) setBy(name string, source Source) bool {\n\t_, set := config.sourceToRawConfig[source][name]\n\treturn set\n}\n\nfunc (config *Config) setByConfigFileOrEnvironment(name string) bool {\n\treturn config.setBy(name, ConfigFile) || config.setBy(name, EnvironmentVariable)\n}\n\nfunc (config *Config) DatastoreConfig() apiconfig.CalicoAPIConfig {\n\t// We want Felix's datastore connection to be fully configurable using the same\n\t// CALICO_XXX_YYY (or just XXX_YYY) environment variables that work for any libcalico-go\n\t// client - for both the etcdv3 and KDD cases.  However, for the etcd case, Felix has for a\n\t// long time supported FELIX_XXXYYY environment variables, and we want those to keep working\n\t// too.\n\n\t// To achieve that, first build a CalicoAPIConfig using libcalico-go's\n\t// LoadClientConfigFromEnvironment - which means incorporating defaults and CALICO_XXX_YYY\n\t// and XXX_YYY variables.\n\tcfg, err := apiconfig.LoadClientConfigFromEnvironment()\n\tif err != nil {\n\t\tlog.WithError(err).Panic(\"Failed to create datastore config\")\n\t}\n\n\t// Now allow FELIX_XXXYYY variables or XxxYyy config file settings to override that, in the\n\t// etcd case.\n\tif config.setByConfigFileOrEnvironment(\"DatastoreType\") && config.DatastoreType == \"etcdv3\" {\n\t\tcfg.Spec.DatastoreType = apiconfig.EtcdV3\n\t\t// Endpoints.\n\t\tif config.setByConfigFileOrEnvironment(\"EtcdEndpoints\") && len(config.EtcdEndpoints) > 0 {\n\t\t\tcfg.Spec.EtcdEndpoints = strings.Join(config.EtcdEndpoints, \",\")\n\t\t} else if config.setByConfigFileOrEnvironment(\"EtcdAddr\") {\n\t\t\tcfg.Spec.EtcdEndpoints = config.EtcdScheme + \"://\" + config.EtcdAddr\n\t\t}\n\t\t// TLS.\n\t\tif config.setByConfigFileOrEnvironment(\"EtcdKeyFile\") {\n\t\t\tcfg.Spec.EtcdKeyFile = config.EtcdKeyFile\n\t\t}\n\t\tif config.setByConfigFileOrEnvironment(\"EtcdCertFile\") {\n\t\t\tcfg.Spec.EtcdCertFile = config.EtcdCertFile\n\t\t}\n\t\tif config.setByConfigFileOrEnvironment(\"EtcdCaFile\") {\n\t\t\tcfg.Spec.EtcdCACertFile = config.EtcdCaFile\n\t\t}\n\t}\n\n\tif !config.IpInIpEnabled {\n\t\t// Polling k8s for node updates is expensive (because we get many superfluous\n\t\t// updates) so disable if we don't need it.\n\t\tlog.Info(\"IPIP disabled, disabling node poll (if KDD is in use).\")\n\t\tcfg.Spec.K8sDisableNodePoll = true\n\t}\n\treturn *cfg\n}\n\n// Validate() performs cross-field validation.\nfunc (config *Config) Validate() (err error) {\n\tif config.FelixHostname == \"\" {\n\t\terr = errors.New(\"Failed to determine hostname\")\n\t}\n\n\tif config.DatastoreType == \"etcdv3\" && len(config.EtcdEndpoints) == 0 {\n\t\tif config.EtcdScheme == \"\" {\n\t\t\terr = errors.New(\"EtcdEndpoints and EtcdScheme both missing\")\n\t\t}\n\t\tif config.EtcdAddr == \"\" {\n\t\t\terr = errors.New(\"EtcdEndpoints and EtcdAddr both missing\")\n\t\t}\n\t}\n\n\t// If any client-side TLS config parameters are specified, they _all_ must be - except that\n\t// either TyphaCN or TyphaURISAN may be left unset.\n\tif config.TyphaCAFile != \"\" ||\n\t\tconfig.TyphaCertFile != \"\" ||\n\t\tconfig.TyphaKeyFile != \"\" ||\n\t\tconfig.TyphaCN != \"\" ||\n\t\tconfig.TyphaURISAN != \"\" {\n\t\t// Some TLS config specified.\n\t\tif config.TyphaKeyFile == \"\" ||\n\t\t\tconfig.TyphaCertFile == \"\" ||\n\t\t\tconfig.TyphaCAFile == \"\" ||\n\t\t\t(config.TyphaCN == \"\" && config.TyphaURISAN == \"\") {\n\t\t\terr = errors.New(\"If any Felix-Typha TLS config parameters are specified,\" +\n\t\t\t\t\" they _all_ must be\" +\n\t\t\t\t\" - except that either TyphaCN or TyphaURISAN may be left unset.\")\n\t\t}\n\t}\n\n\tif err != nil {\n\t\tconfig.Err = err\n\t}\n\treturn\n}\n\nvar knownParams map[string]param\n\nfunc loadParams() {\n\tknownParams = make(map[string]param)\n\tconfig := Config{}\n\tkind := reflect.TypeOf(config)\n\tmetaRegexp := regexp.MustCompile(`^([^;(]+)(?:\\(([^)]*)\\))?;` +\n\t\t`([^;]*)(?:;` +\n\t\t`([^;]*))?$`)\n\tfor ii := 0; ii < kind.NumField(); ii++ {\n\t\tfield := kind.Field(ii)\n\t\ttag := field.Tag.Get(\"config\")\n\t\tif tag == \"\" {\n\t\t\tcontinue\n\t\t}\n\t\tcaptures := metaRegexp.FindStringSubmatch(tag)\n\t\tif len(captures) == 0 {\n\t\t\tlog.Panicf(\"Failed to parse metadata for config param %v\", field.Name)\n\t\t}\n\t\tlog.Debugf(\"%v: metadata captures: %#v\", field.Name, captures)\n\t\tkind := captures[1]       // Type: \"int|oneof|bool|port-list|...\"\n\t\tkindParams := captures[2] // Parameters for the type: e.g. for oneof \"http,https\"\n\t\tdefaultStr := captures[3] // Default value e.g \"1.0\"\n\t\tflags := captures[4]\n\t\tvar param param\n\t\tvar err error\n\t\tswitch kind {\n\t\tcase \"bool\":\n\t\t\tparam = &BoolParam{}\n\t\tcase \"int\":\n\t\t\tmin := minInt\n\t\t\tmax := maxInt\n\t\t\tif kindParams != \"\" {\n\t\t\t\tminAndMax := strings.Split(kindParams, \",\")\n\t\t\t\tmin, err = strconv.Atoi(minAndMax[0])\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Failed to parse min value for %v\", field.Name)\n\t\t\t\t}\n\t\t\t\tmax, err = strconv.Atoi(minAndMax[1])\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Failed to parse max value for %v\", field.Name)\n\t\t\t\t}\n\t\t\t}\n\t\t\tparam = &IntParam{Min: min, Max: max}\n\t\tcase \"int32\":\n\t\t\tparam = &Int32Param{}\n\t\tcase \"mark-bitmask\":\n\t\t\tparam = &MarkBitmaskParam{}\n\t\tcase \"float\":\n\t\t\tparam = &FloatParam{}\n\t\tcase \"seconds\":\n\t\t\tparam = &SecondsParam{}\n\t\tcase \"millis\":\n\t\t\tparam = &MillisParam{}\n\t\tcase \"iface-list\":\n\t\t\tparam = &RegexpParam{Regexp: IfaceListRegexp,\n\t\t\t\tMsg: \"invalid Linux interface name\"}\n\t\tcase \"iface-param\":\n\t\t\tparam = &RegexpParam{Regexp: IfaceParamRegexp,\n\t\t\t\tMsg: \"invalid Linux interface parameter\"}\n\t\tcase \"file\":\n\t\t\tparam = &FileParam{\n\t\t\t\tMustExist:  strings.Contains(kindParams, \"must-exist\"),\n\t\t\t\tExecutable: strings.Contains(kindParams, \"executable\"),\n\t\t\t}\n\t\tcase \"authority\":\n\t\t\tparam = &RegexpParam{Regexp: AuthorityRegexp,\n\t\t\t\tMsg: \"invalid URL authority\"}\n\t\tcase \"ipv4\":\n\t\t\tparam = &Ipv4Param{}\n\t\tcase \"endpoint-list\":\n\t\t\tparam = &EndpointListParam{}\n\t\tcase \"port-list\":\n\t\t\tparam = &PortListParam{}\n\t\tcase \"portrange\":\n\t\t\tparam = &PortRangeParam{}\n\t\tcase \"portrange-list\":\n\t\t\tparam = &PortRangeListParam{}\n\t\tcase \"hostname\":\n\t\t\tparam = &RegexpParam{Regexp: HostnameRegexp,\n\t\t\t\tMsg: \"invalid hostname\"}\n\t\tcase \"region\":\n\t\t\tparam = &RegionParam{}\n\t\tcase \"oneof\":\n\t\t\toptions := strings.Split(kindParams, \",\")\n\t\t\tlowerCaseToCanon := make(map[string]string)\n\t\t\tfor _, option := range options {\n\t\t\t\tlowerCaseToCanon[strings.ToLower(option)] = option\n\t\t\t}\n\t\t\tparam = &OneofListParam{\n\t\t\t\tlowerCaseOptionsToCanonical: lowerCaseToCanon}\n\t\tcase \"string\":\n\t\t\tparam = &RegexpParam{Regexp: StringRegexp,\n\t\t\t\tMsg: \"invalid string\"}\n\t\tcase \"cidr-list\":\n\t\t\tparam = &CIDRListParam{}\n\t\tdefault:\n\t\t\tlog.Panicf(\"Unknown type of parameter: %v\", kind)\n\t\t}\n\n\t\tmetadata := param.GetMetadata()\n\t\tmetadata.Name = field.Name\n\t\tmetadata.ZeroValue = reflect.ValueOf(config).FieldByName(field.Name).Interface()\n\t\tif strings.Index(flags, \"non-zero\") > -1 {\n\t\t\tmetadata.NonZero = true\n\t\t}\n\t\tif strings.Index(flags, \"die-on-fail\") > -1 {\n\t\t\tmetadata.DieOnParseFailure = true\n\t\t}\n\t\tif strings.Index(flags, \"local\") > -1 {\n\t\t\tmetadata.Local = true\n\t\t}\n\n\t\tif defaultStr != \"\" {\n\t\t\tif strings.Index(flags, \"skip-default-validation\") > -1 {\n\t\t\t\tmetadata.Default = defaultStr\n\t\t\t} else {\n\t\t\t\t// Parse the default value and save it in the metadata. Doing\n\t\t\t\t// that here ensures that we syntax-check the defaults now.\n\t\t\t\tdefaultVal, err := param.Parse(defaultStr)\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Panicf(\"Invalid default value: %v\", err)\n\t\t\t\t}\n\t\t\t\tmetadata.Default = defaultVal\n\t\t\t}\n\t\t} else {\n\t\t\tmetadata.Default = metadata.ZeroValue\n\t\t}\n\t\tknownParams[strings.ToLower(field.Name)] = param\n\t}\n}\n\nfunc (config *Config) RawValues() map[string]string {\n\treturn config.rawValues\n}\n\nfunc New() *Config {\n\tif knownParams == nil {\n\t\tloadParams()\n\t}\n\tp := &Config{\n\t\trawValues:         make(map[string]string),\n\t\tsourceToRawConfig: make(map[Source]map[string]string),\n\t}\n\tfor _, param := range knownParams {\n\t\tparam.setDefault(p)\n\t}\n\thostname, err := names.Hostname()\n\tif err != nil {\n\t\tlog.Warningf(\"Failed to get hostname from kernel, \"+\n\t\t\t\"trying HOSTNAME variable: %v\", err)\n\t\thostname = strings.ToLower(os.Getenv(\"HOSTNAME\"))\n\t}\n\tp.FelixHostname = hostname\n\treturn p\n}\n\ntype param interface {\n\tGetMetadata() *Metadata\n\tParse(raw string) (result interface{}, err error)\n\tsetDefault(*Config)\n}\n", "idx": 1, "id": 16824, "msg": "", "proj": "projectcalico-felix", "lang": "c"}
{"patch": "@@ -363,8 +363,16 @@ namespace Datadog.Trace.ClrProfiler.Integrations.AdoNet\n         /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n         /// <returns>The value returned by the instrumented method.</returns>\n         [InterceptMethod(\n-            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n-            TargetType = DbCommandTypeName,\n+            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon, AdoNetConstants.AssemblyNames.NetStandard },\n+            TargetMethod = AdoNetConstants.MethodNames.ExecuteScalar,\n+            TargetType = AdoNetConstants.TypeNames.DbCommand,\n+            TargetSignatureTypes = new[] { ClrNames.Object },\n+            TargetMinimumVersion = Major4,\n+            TargetMaximumVersion = Major4)]\n+        [InterceptMethod(\n+            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon, AdoNetConstants.AssemblyNames.NetStandard },\n+            TargetMethod = AdoNetConstants.MethodNames.ExecuteScalarExplicit,\n+            TargetType = AdoNetConstants.TypeNames.DbCommand,\n             TargetSignatureTypes = new[] { ClrNames.Object },\n             TargetMinimumVersion = Major4,\n             TargetMaximumVersion = Major4)]", "y": 0, "oldf": "using System;\nusing System.Data;\nusing System.Data.Common;\nusing System.Threading;\nusing System.Threading.Tasks;\nusing Datadog.Trace.ClrProfiler.Emit;\nusing Datadog.Trace.Logging;\n\nnamespace Datadog.Trace.ClrProfiler.Integrations.AdoNet\n{\n    /// <summary>\n    /// Instrumentation wrappers for <see cref=\"DbCommand\"/>.\n    /// </summary>\n    public static class DbCommandIntegration\n    {\n        // TODO: support both \"DbCommand\" (new name) and\n        // \"AdoNet\" (backwards compatibility) when reading configuration settings\n        private const string IntegrationName = \"AdoNet\";\n        private const string Major4 = \"4\";\n\n        private const string DbCommandTypeName = \"System.Data.Common.DbCommand\";\n        private const string DbDataReaderTypeName = \"System.Data.Common.DbDataReader\";\n\n        private static readonly Vendors.Serilog.ILogger Log = DatadogLogging.GetLogger(typeof(DbCommandIntegration));\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteReader()\"/>.\n        /// </summary>\n        /// <param name=\"command\">The object referenced \"this\" in the instrumented method.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { DbDataReaderTypeName },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteReader(\n            object command,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, DbDataReader> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, DbDataReader>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteReader)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithNamespaceAndNameFilters(DbDataReaderTypeName)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.ErrorRetrievingMethod(\n                    exception: ex,\n                    moduleVersionPointer: moduleVersionPtr,\n                    mdToken: mdToken,\n                    opCode: opCode,\n                    instrumentedType: DbCommandTypeName,\n                    methodName: nameof(ExecuteReader),\n                    instanceType: command.GetType().AssemblyQualifiedName);\n                throw;\n            }\n\n            var dbCommand = command as DbCommand;\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, dbCommand, IntegrationName))\n            {\n                try\n                {\n                    return instrumentedMethod(dbCommand);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteReader(CommandBehavior)\"/>.\n        /// </summary>\n        /// <param name=\"command\">The object referenced \"this\" in the instrumented method.</param>\n        /// <param name=\"behavior\">The <see cref=\"CommandBehavior\"/> value used in the original method call.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetMethod = AdoNetConstants.MethodNames.ExecuteReader,\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { DbDataReaderTypeName, AdoNetConstants.TypeNames.CommandBehavior },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteReaderWithBehavior(\n            object command,\n            int behavior,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, CommandBehavior, DbDataReader> instrumentedMethod;\n            var commandBehavior = (CommandBehavior)behavior;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, CommandBehavior, DbDataReader>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteReader)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithParameters(commandBehavior)\n                       .WithNamespaceAndNameFilters(DbDataReaderTypeName, AdoNetConstants.TypeNames.CommandBehavior)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.ErrorRetrievingMethod(\n                    exception: ex,\n                    moduleVersionPointer: moduleVersionPtr,\n                    mdToken: mdToken,\n                    opCode: opCode,\n                    instrumentedType: DbCommandTypeName,\n                    methodName: nameof(ExecuteReader),\n                    instanceType: command.GetType().AssemblyQualifiedName);\n                throw;\n            }\n\n            var dbCommand = command as DbCommand;\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, dbCommand, IntegrationName))\n            {\n                try\n                {\n                    return instrumentedMethod(dbCommand, commandBehavior);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteReaderAsync()\"/>.\n        /// </summary>\n        /// <param name=\"command\">The object referenced by this in the instrumented method.</param>\n        /// <param name=\"behavior\">The <see cref=\"CommandBehavior\"/> value used in the original method call.</param>\n        /// <param name=\"boxedCancellationToken\">The <see cref=\"CancellationToken\"/> value used in the original method call.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { \"System.Threading.Tasks.Task`1<System.Data.Common.DbDataReader>\", AdoNetConstants.TypeNames.CommandBehavior, ClrNames.CancellationToken },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteReaderAsync(\n            object command,\n            int behavior,\n            object boxedCancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            var cancellationToken = (CancellationToken)boxedCancellationToken;\n\n            return ExecuteReaderAsyncInternal(\n                command as DbCommand,\n                (CommandBehavior)behavior,\n                cancellationToken,\n                opCode,\n                mdToken,\n                moduleVersionPtr);\n        }\n\n        private static async Task<DbDataReader> ExecuteReaderAsyncInternal(\n            DbCommand command,\n            CommandBehavior commandBehavior,\n            CancellationToken cancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, CommandBehavior, CancellationToken, Task<DbDataReader>> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, CommandBehavior, CancellationToken, Task<DbDataReader>>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteReaderAsync)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithParameters(commandBehavior, cancellationToken)\n                       .WithNamespaceAndNameFilters(ClrNames.GenericTask, AdoNetConstants.TypeNames.CommandBehavior, ClrNames.CancellationToken)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.ErrorRetrievingMethod(\n                    exception: ex,\n                    moduleVersionPointer: moduleVersionPtr,\n                    mdToken: mdToken,\n                    opCode: opCode,\n                    instrumentedType: DbCommandTypeName,\n                    methodName: nameof(ExecuteReaderAsync),\n                    instanceType: command.GetType().AssemblyQualifiedName);\n                throw;\n            }\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, command, IntegrationName))\n            {\n                try\n                {\n                    return await instrumentedMethod(command, commandBehavior, cancellationToken).ConfigureAwait(false);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteNonQuery\"/>.\n        /// </summary>\n        /// <param name=\"command\">The object referenced by this in the instrumented method.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { ClrNames.Int32 },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static int ExecuteNonQuery(\n            object command,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, int> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, int>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteNonQuery)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithNamespaceAndNameFilters(ClrNames.Int32)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.Error(ex, $\"Error resolving {DbCommandTypeName}.{AdoNetConstants.MethodNames.ExecuteNonQuery}(...)\");\n                throw;\n            }\n\n            var dbCommand = command as DbCommand;\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, dbCommand, IntegrationName))\n            {\n                try\n                {\n                    return instrumentedMethod(dbCommand);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteNonQueryAsync(CancellationToken)\"/>\n        /// </summary>\n        /// <param name=\"command\">The object referenced by this in the instrumented method.</param>\n        /// <param name=\"boxedCancellationToken\">The <see cref=\"CancellationToken\"/> value used in the original method call.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { \"System.Threading.Tasks.Task`1<System.Int32>\", ClrNames.CancellationToken },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteNonQueryAsync(\n            object command,\n            object boxedCancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            var cancellationToken = (CancellationToken)boxedCancellationToken;\n\n            return ExecuteNonQueryAsyncInternal(\n                command as DbCommand,\n                cancellationToken,\n                opCode,\n                mdToken,\n                moduleVersionPtr);\n        }\n\n        private static async Task<int> ExecuteNonQueryAsyncInternal(\n            DbCommand command,\n            CancellationToken cancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, CancellationToken, Task<int>> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, CancellationToken, Task<int>>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteNonQueryAsync)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithParameters(cancellationToken)\n                       .WithNamespaceAndNameFilters(ClrNames.GenericTask, ClrNames.CancellationToken)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.Error(ex, $\"Error resolving {DbCommandTypeName}.{AdoNetConstants.MethodNames.ExecuteNonQueryAsync}(...)\");\n                throw;\n            }\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, command, IntegrationName))\n            {\n                try\n                {\n                    return await instrumentedMethod(command, cancellationToken).ConfigureAwait(false);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteScalar\"/>\n        /// </summary>\n        /// <param name=\"command\">The object referenced by this in the instrumented method.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { ClrNames.Object },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteScalar(\n            object command,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, object> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, object>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteScalar)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithNamespaceAndNameFilters(ClrNames.Object)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.Error(ex, $\"Error resolving {DbCommandTypeName}.{AdoNetConstants.MethodNames.ExecuteScalar}(...)\");\n                throw;\n            }\n\n            var dbCommand = command as DbCommand;\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, dbCommand, IntegrationName))\n            {\n                try\n                {\n                    return instrumentedMethod(dbCommand);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n\n        /// <summary>\n        /// Instrumentation wrapper for <see cref=\"DbCommand.ExecuteScalarAsync(CancellationToken)\"/>\n        /// </summary>\n        /// <param name=\"command\">The object referenced by this in the instrumented method.</param>\n        /// <param name=\"boxedCancellationToken\">The <see cref=\"CancellationToken\"/> value used in the original method call.</param>\n        /// <param name=\"opCode\">The OpCode used in the original method call.</param>\n        /// <param name=\"mdToken\">The mdToken of the original method call.</param>\n        /// <param name=\"moduleVersionPtr\">A pointer to the module version GUID.</param>\n        /// <returns>The value returned by the instrumented method.</returns>\n        [InterceptMethod(\n            TargetAssemblies = new[] { AdoNetConstants.AssemblyNames.SystemData, AdoNetConstants.AssemblyNames.SystemDataCommon },\n            TargetType = DbCommandTypeName,\n            TargetSignatureTypes = new[] { \"System.Threading.Tasks.Task`1<System.Object>\", ClrNames.CancellationToken },\n            TargetMinimumVersion = Major4,\n            TargetMaximumVersion = Major4)]\n        public static object ExecuteScalarAsync(\n            object command,\n            object boxedCancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            var cancellationToken = (CancellationToken)boxedCancellationToken;\n\n            return ExecuteScalarAsyncInternal(\n                command as DbCommand,\n                cancellationToken,\n                opCode,\n                mdToken,\n                moduleVersionPtr);\n        }\n\n        private static async Task<object> ExecuteScalarAsyncInternal(\n            DbCommand command,\n            CancellationToken cancellationToken,\n            int opCode,\n            int mdToken,\n            long moduleVersionPtr)\n        {\n            Func<DbCommand, CancellationToken, Task<object>> instrumentedMethod;\n\n            try\n            {\n                instrumentedMethod =\n                    MethodBuilder<Func<DbCommand, CancellationToken, Task<object>>>\n                       .Start(moduleVersionPtr, mdToken, opCode, AdoNetConstants.MethodNames.ExecuteScalarAsync)\n                       .WithConcreteType(typeof(DbCommand))\n                       .WithParameters(cancellationToken)\n                       .WithNamespaceAndNameFilters(ClrNames.GenericTask, ClrNames.CancellationToken)\n                       .Build();\n            }\n            catch (Exception ex)\n            {\n                Log.Error(ex, $\"Error resolving {DbCommandTypeName}.{AdoNetConstants.MethodNames.ExecuteScalarAsync}(...)\");\n                throw;\n            }\n\n            using (var scope = ScopeFactory.CreateDbCommandScope(Tracer.Instance, command, IntegrationName))\n            {\n                try\n                {\n                    return await instrumentedMethod(command, cancellationToken).ConfigureAwait(false);\n                }\n                catch (Exception ex)\n                {\n                    scope?.Span.SetException(ex);\n                    throw;\n                }\n            }\n        }\n    }\n}\n", "idx": 14, "id": 17109, "msg": "", "proj": "DataDog-dd-trace-dotnet", "lang": ".cs"}
{"patch": "@@ -409,7 +409,11 @@ func TestMDOpsGetForHandleFailHandleCheck(t *testing.T) {\n }\n \n func TestMDOpsGetSuccess(t *testing.T) {\n-\tmockCtrl, config, ctx := mdOpsInit(t)\n+\trunTestOverMetadataVers(t, testMDOpsGetSuccess)\n+}\n+\n+func testMDOpsGetSuccess(t *testing.T, ver MetadataVer) {\n+\tmockCtrl, config, ctx := mdOpsInit(t, ver)\n \tdefer mdOpsShutdown(mockCtrl, config)\n \n \th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)", "y": 0, "oldf": "// Copyright 2016 Keybase Inc. All rights reserved.\n// Use of this source code is governed by a BSD\n// license that can be found in the LICENSE file.\n\npackage libkbfs\n\nimport (\n\t\"errors\"\n\t\"sync\"\n\t\"testing\"\n\t\"time\"\n\n\t\"github.com/golang/mock/gomock\"\n\t\"github.com/keybase/client/go/libkb\"\n\t\"github.com/keybase/kbfs/kbfscodec\"\n\t\"github.com/keybase/kbfs/kbfscrypto\"\n\t\"github.com/keybase/kbfs/kbfshash\"\n\t\"github.com/keybase/kbfs/tlf\"\n\t\"github.com/stretchr/testify/require\"\n\t\"golang.org/x/net/context\"\n)\n\ntype shimCrypto struct {\n\tCrypto\n\tpure cryptoPure\n\tkey  kbfscrypto.SigningKey\n}\n\nfunc (c shimCrypto) MakeMdID(md BareRootMetadata) (MdID, error) {\n\treturn c.pure.MakeMdID(md)\n}\n\nfunc (c shimCrypto) Sign(\n\tctx context.Context, data []byte) (kbfscrypto.SignatureInfo, error) {\n\treturn c.key.Sign(data), nil\n}\n\nfunc (c shimCrypto) Verify(\n\tmsg []byte, sigInfo kbfscrypto.SignatureInfo) (err error) {\n\treturn kbfscrypto.Verify(msg, sigInfo)\n}\n\nfunc injectShimCrypto(config Config) {\n\tsigningKey := kbfscrypto.MakeFakeSigningKeyOrBust(\"test key\")\n\tcrypto := shimCrypto{\n\t\tconfig.Crypto(),\n\t\tMakeCryptoCommon(kbfscodec.NewMsgpack()),\n\t\tsigningKey,\n\t}\n\tconfig.SetCrypto(crypto)\n}\n\nfunc mdOpsInit(t *testing.T) (mockCtrl *gomock.Controller,\n\tconfig *ConfigMock, ctx context.Context) {\n\tctr := NewSafeTestReporter(t)\n\tmockCtrl = gomock.NewController(ctr)\n\tconfig = NewConfigMock(mockCtrl, ctr)\n\tmdops := NewMDOpsStandard(config)\n\tconfig.SetMDOps(mdops)\n\tconfig.SetCodec(kbfscodec.NewMsgpack())\n\tconfig.mockMdserv.EXPECT().OffsetFromServerTime().\n\t\tReturn(time.Duration(0), true).AnyTimes()\n\th1, _ := kbfshash.DefaultHash([]byte{1})\n\th2, _ := kbfshash.DefaultHash([]byte{2})\n\tconfig.mockCrypto.EXPECT().MakeTLFWriterKeyBundleID(gomock.Any()).\n\t\tReturn(TLFWriterKeyBundleID{h1}, nil).AnyTimes()\n\tconfig.mockCrypto.EXPECT().MakeTLFReaderKeyBundleID(gomock.Any()).\n\t\tReturn(TLFReaderKeyBundleID{h2}, nil).AnyTimes()\n\tinjectShimCrypto(config)\n\tinterposeDaemonKBPKI(config, \"alice\", \"bob\", \"charlie\")\n\tctx = context.Background()\n\treturn\n}\n\nfunc mdOpsShutdown(mockCtrl *gomock.Controller, config *ConfigMock) {\n\tconfig.ctr.CheckForFailures()\n\tmockCtrl.Finish()\n}\n\nfunc addFakeRMDData(t *testing.T,\n\tcodec kbfscodec.Codec, crypto cryptoPure, rmd *RootMetadata,\n\th *TlfHandle) {\n\trmd.SetRevision(MetadataRevision(1))\n\tpmd := PrivateMetadata{}\n\t// TODO: Will have to change this for private folders if we\n\t// un-mock out those tests.\n\tbuf, err := codec.Encode(pmd)\n\trequire.NoError(t, err)\n\trmd.SetSerializedPrivateMetadata(buf)\n\trmd.SetLastModifyingWriter(h.FirstResolvedWriter())\n\trmd.SetLastModifyingUser(h.FirstResolvedWriter())\n\tif !h.IsPublic() {\n\t\trmd.fakeInitialRekey(codec, crypto)\n\t}\n}\n\nfunc newRMDS(t *testing.T, config Config, h *TlfHandle) (\n\t*RootMetadataSigned, ExtraMetadata) {\n\tid := tlf.FakeID(1, h.IsPublic())\n\n\trmd, err := makeInitialRootMetadata(defaultClientMetadataVer, id, h)\n\trequire.NoError(t, err)\n\n\taddFakeRMDData(t, config.Codec(), config.Crypto(), rmd, h)\n\tctx := context.Background()\n\n\t// Encode and sign writer metadata.\n\terr = rmd.bareMd.SignWriterMetadataInternally(ctx, config.Codec(), config.Crypto())\n\trequire.NoError(t, err)\n\n\trmds, err := SignBareRootMetadata(\n\t\tctx, config.Codec(), config.Crypto(), config.Crypto(),\n\t\trmd.bareMd, time.Now())\n\trequire.NoError(t, err)\n\treturn rmds, rmd.extra\n}\n\nfunc verifyMDForPublic(config *ConfigMock, rmds *RootMetadataSigned,\n\tverifyErr, hasVerifyingKeyErr error) {\n\tif verifyErr != nil {\n\t\treturn\n\t}\n\tconfig.mockKbpki.EXPECT().HasVerifyingKey(gomock.Any(), gomock.Any(),\n\t\tgomock.Any(), gomock.Any()).AnyTimes().Return(hasVerifyingKeyErr)\n\n\tif hasVerifyingKeyErr != nil {\n\t\treturn\n\t}\n}\n\nfunc verifyMDForPrivateHelper(\n\tconfig *ConfigMock, rmds *RootMetadataSigned, minTimes, maxTimes int) {\n\tmdCopy, err := rmds.MD.DeepCopy(config.Codec())\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\tfakeRMD := RootMetadata{\n\t\tbareMd: mdCopy,\n\t}\n\texpectGetTLFCryptKeyForMDDecryptionAtMostOnce(config, &fakeRMD)\n\tvar pmd PrivateMetadata\n\tconfig.mockCrypto.EXPECT().DecryptPrivateMetadata(\n\t\tgomock.Any(), kbfscrypto.TLFCryptKey{}).\n\t\tMinTimes(minTimes).MaxTimes(maxTimes).Return(pmd, nil)\n\n\tif rmds.MD.IsFinal() {\n\t\tconfig.mockKbpki.EXPECT().HasUnverifiedVerifyingKey(gomock.Any(), gomock.Any(),\n\t\t\tgomock.Any()).AnyTimes().Return(nil)\n\t} else {\n\t\tconfig.mockKbpki.EXPECT().HasVerifyingKey(gomock.Any(), gomock.Any(),\n\t\t\tgomock.Any(), gomock.Any()).AnyTimes().Return(nil)\n\t}\n\n\tconfig.mockCrypto.EXPECT().Verify(gomock.Any(), rmds.SigInfo).\n\t\tMinTimes(minTimes).MaxTimes(maxTimes).Return(nil)\n\tconfig.mockCrypto.EXPECT().\n\t\tVerify(gomock.Any(), rmds.GetWriterMetadataSigInfo()).\n\t\tMinTimes(minTimes).MaxTimes(maxTimes).Return(nil)\n}\n\nfunc verifyMDForPrivate(\n\tconfig *ConfigMock, rmds *RootMetadataSigned) {\n\tverifyMDForPrivateHelper(config, rmds, 1, 1)\n}\n\nfunc putMDForPrivate(config *ConfigMock, rmd *RootMetadata) {\n\texpectGetTLFCryptKeyForEncryption(config, rmd)\n\tconfig.mockCrypto.EXPECT().EncryptPrivateMetadata(\n\t\trmd.data, kbfscrypto.TLFCryptKey{}).Return(\n\t\tEncryptedPrivateMetadata{}, nil)\n\tconfig.mockBsplit.EXPECT().ShouldEmbedBlockChanges(gomock.Any()).\n\t\tReturn(true)\n\tconfig.mockMdserv.EXPECT().Put(gomock.Any(), gomock.Any(), gomock.Any()).Return(nil)\n}\n\nfunc TestMDOpsGetForHandlePublicSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\trmds, _ := newRMDS(t, config, h)\n\n\tverifyMDForPublic(config, rmds, nil, nil)\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\n\t// Do this first, since rmds is consumed.\n\texpectedMD := rmds.MD\n\t_, rmd2, err := config.MDOps().GetForHandle(ctx, h, Merged)\n\trequire.NoError(t, err)\n\trequire.Equal(t, expectedMD, rmd2.bareMd)\n}\n\nfunc expectGetKeyBundles(ctx context.Context, config *ConfigMock, extra ExtraMetadata) {\n\tif extraV3, ok := extra.(*ExtraMetadataV3); ok {\n\t\tconfig.mockMdserv.EXPECT().GetKeyBundles(\n\t\t\tctx, gomock.Any(), gomock.Any(), gomock.Any()).\n\t\t\tReturn(extraV3.wkb, extraV3.rkb, nil)\n\t}\n}\n\nfunc TestMDOpsGetForHandlePrivateSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmds, extra := newRMDS(t, config, h)\n\n\tverifyMDForPrivate(config, rmds)\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\texpectGetKeyBundles(ctx, config, extra)\n\n\t// Do this first, since rmds is consumed.\n\texpectedMD := rmds.MD\n\t_, rmd2, err := config.MDOps().GetForHandle(ctx, h, Merged)\n\trequire.NoError(t, err)\n\trequire.Equal(t, expectedMD, rmd2.bareMd)\n}\n\nfunc TestMDOpsGetForUnresolvedHandlePublicSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\trmds, _ := newRMDS(t, config, h)\n\n\t// Do this before setting tlfHandle to nil.\n\tverifyMDForPublic(config, rmds, nil, nil)\n\n\thUnresolved, err := ParseTlfHandle(ctx, config.KBPKI(),\n\t\t\"alice,bob@twitter\", true)\n\trequire.NoError(t, err)\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, hUnresolved.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil).Times(2)\n\n\t// First time should fail.\n\t_, _, err = config.MDOps().GetForHandle(ctx, hUnresolved, Merged)\n\tif _, ok := err.(MDMismatchError); !ok {\n\t\tt.Errorf(\"Got unexpected error on bad handle check test: %v\", err)\n\t}\n\n\tdaemon := config.KeybaseService().(*KeybaseDaemonLocal)\n\tdaemon.addNewAssertionForTestOrBust(\"bob\", \"bob@twitter\")\n\n\t// Second time should succeed.\n\tif _, _, err := config.MDOps().GetForHandle(ctx, hUnresolved, Merged); err != nil {\n\t\tt.Errorf(\"Got error on get: %v\", err)\n\t}\n}\n\nfunc TestMDOpsGetForUnresolvedMdHandlePublicSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\tmdHandle1, err := ParseTlfHandle(ctx, config.KBPKI(),\n\t\t\"alice,dave@twitter\", true)\n\trequire.NoError(t, err)\n\n\tmdHandle2, err := ParseTlfHandle(ctx, config.KBPKI(),\n\t\t\"alice,bob,charlie\", true)\n\trequire.NoError(t, err)\n\n\tmdHandle3, err := ParseTlfHandle(ctx, config.KBPKI(),\n\t\t\"alice,bob@twitter,charlie@twitter\", true)\n\trequire.NoError(t, err)\n\n\trmds1, _ := newRMDS(t, config, mdHandle1)\n\n\trmds2, _ := newRMDS(t, config, mdHandle2)\n\n\trmds3, _ := newRMDS(t, config, mdHandle3)\n\n\t// Do this before setting tlfHandles to nil.\n\tverifyMDForPublic(config, rmds2, nil, nil)\n\tverifyMDForPublic(config, rmds3, nil, nil)\n\n\th, err := ParseTlfHandle(\n\t\tctx, config.KBPKI(), \"alice,bob,charlie@twitter\", true)\n\trequire.NoError(t, err)\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds1, nil)\n\n\t// First time should fail.\n\t_, _, err = config.MDOps().GetForHandle(ctx, h, Merged)\n\tif _, ok := err.(MDMismatchError); !ok {\n\t\tt.Errorf(\"Got unexpected error on bad handle check test: %v\", err)\n\t}\n\n\tdaemon := config.KeybaseService().(*KeybaseDaemonLocal)\n\tdaemon.addNewAssertionForTestOrBust(\"bob\", \"bob@twitter\")\n\tdaemon.addNewAssertionForTestOrBust(\"charlie\", \"charlie@twitter\")\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds2, nil)\n\n\t// Second and time should succeed.\n\tif _, _, err := config.MDOps().GetForHandle(ctx, h, Merged); err != nil {\n\t\tt.Errorf(\"Got error on get: %v\", err)\n\t}\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds3, nil)\n\n\tif _, _, err := config.MDOps().GetForHandle(ctx, h, Merged); err != nil {\n\t\tt.Errorf(\"Got error on get: %v\", err)\n\t}\n}\n\nfunc TestMDOpsGetForUnresolvedHandlePublicFailure(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\trmds, _ := newRMDS(t, config, h)\n\n\thUnresolved, err := ParseTlfHandle(ctx, config.KBPKI(),\n\t\t\"alice,bob@github,bob@twitter\", true)\n\trequire.NoError(t, err)\n\n\tdaemon := config.KeybaseService().(*KeybaseDaemonLocal)\n\tdaemon.addNewAssertionForTestOrBust(\"bob\", \"bob@twitter\")\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, hUnresolved.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\n\t// Should still fail.\n\t_, _, err = config.MDOps().GetForHandle(ctx, hUnresolved, Merged)\n\tif _, ok := err.(MDMismatchError); !ok {\n\t\tt.Errorf(\"Got unexpected error on bad handle check test: %v\", err)\n\t}\n}\n\nfunc TestMDOpsGetForHandlePublicFailFindKey(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\trmds, _ := newRMDS(t, config, h)\n\n\t// Do this before setting tlfHandle to nil.\n\tverifyMDForPublic(config, rmds, nil, KeyNotFoundError{})\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\n\t_, _, err := config.MDOps().GetForHandle(ctx, h, Merged)\n\tif _, ok := err.(UnverifiableTlfUpdateError); !ok {\n\t\tt.Errorf(\"Got unexpected error on get: %v\", err)\n\t}\n}\n\ntype failVerifyCrypto struct {\n\tCrypto\n\terr error\n}\n\nfunc (c failVerifyCrypto) Verify(msg []byte, sigInfo kbfscrypto.SignatureInfo) error {\n\treturn c.err\n}\n\nfunc TestMDOpsGetForHandlePublicFailVerify(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\trmds, _ := newRMDS(t, config, h)\n\n\t// Do this before setting tlfHandle to nil.\n\texpectedErr := libkb.VerificationError{}\n\tverifyMDForPublic(config, rmds, expectedErr, nil)\n\n\tconfig.SetCrypto(failVerifyCrypto{config.Crypto(), expectedErr})\n\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\n\t_, _, err := config.MDOps().GetForHandle(ctx, h, Merged)\n\trequire.IsType(t, MDMismatchError{}, err)\n}\n\nfunc TestMDOpsGetForHandleFailGet(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\n\terr := errors.New(\"Fake fail\")\n\n\t// only the get happens, no verify needed with a blank sig\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, h.ToBareHandleOrBust(), Merged).Return(tlf.NullID, nil, err)\n\n\tif _, _, err2 := config.MDOps().GetForHandle(ctx, h, Merged); err2 != err {\n\t\tt.Errorf(\"Got bad error on get: %v\", err2)\n\t}\n}\n\nfunc TestMDOpsGetForHandleFailHandleCheck(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmds, extra := newRMDS(t, config, h)\n\n\t// Make a different handle.\n\totherH := parseTlfHandleOrBust(t, config, \"alice\", false)\n\tconfig.mockMdserv.EXPECT().GetForHandle(ctx, otherH.ToBareHandleOrBust(), Merged).Return(tlf.NullID, rmds, nil)\n\texpectGetKeyBundles(ctx, config, extra)\n\n\t_, _, err := config.MDOps().GetForHandle(ctx, otherH, Merged)\n\tif _, ok := err.(MDMismatchError); !ok {\n\t\tt.Errorf(\"Got unexpected error on bad handle check test: %v\", err)\n\t}\n}\n\nfunc TestMDOpsGetSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmds, extra := newRMDS(t, config, h)\n\n\t// Do this before setting tlfHandle to nil.\n\tverifyMDForPrivate(config, rmds)\n\n\tconfig.mockMdserv.EXPECT().GetForTLF(ctx, rmds.MD.TlfID(), NullBranchID, Merged).Return(rmds, nil)\n\texpectGetKeyBundles(ctx, config, extra)\n\n\t// Do this first, since rmds is consumed.\n\texpectedMD := rmds.MD\n\trmd2, err := config.MDOps().GetForTLF(ctx, rmds.MD.TlfID())\n\trequire.NoError(t, err)\n\trequire.Equal(t, expectedMD, rmd2.bareMd)\n}\n\nfunc TestMDOpsGetBlankSigFailure(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmds, extra := newRMDS(t, config, h)\n\trmds.SigInfo = kbfscrypto.SignatureInfo{}\n\n\t// only the get happens, no verify needed with a blank sig\n\tconfig.mockMdserv.EXPECT().GetForTLF(ctx, rmds.MD.TlfID(), NullBranchID, Merged).Return(rmds, nil)\n\texpectGetKeyBundles(ctx, config, extra)\n\n\tif _, err := config.MDOps().GetForTLF(ctx, rmds.MD.TlfID()); err == nil {\n\t\tt.Error(\"Got no error on get\")\n\t}\n}\n\nfunc TestMDOpsGetFailGet(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\tid := tlf.FakeID(1, true)\n\terr := errors.New(\"Fake fail\")\n\n\t// only the get happens, no verify needed with a blank sig\n\tconfig.mockMdserv.EXPECT().GetForTLF(ctx, id, NullBranchID, Merged).Return(nil, err)\n\n\tif _, err2 := config.MDOps().GetForTLF(ctx, id); err2 != err {\n\t\tt.Errorf(\"Got bad error on get: %v\", err2)\n\t}\n}\n\nfunc TestMDOpsGetFailIdCheck(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmds, extra := newRMDS(t, config, h)\n\n\tid2 := tlf.FakeID(2, true)\n\n\tconfig.mockMdserv.EXPECT().GetForTLF(ctx, id2, NullBranchID, Merged).Return(rmds, nil)\n\texpectGetKeyBundles(ctx, config, extra)\n\n\tif _, err := config.MDOps().GetForTLF(ctx, id2); err == nil {\n\t\tt.Errorf(\"Got no error on bad id check test\")\n\t} else if _, ok := err.(MDMismatchError); !ok {\n\t\tt.Errorf(\"Got unexpected error on bad id check test: %v\", err)\n\t}\n}\n\nfunc makeRMDSRange(t *testing.T, config Config,\n\tstart MetadataRevision, count int, prevID MdID) (\n\trmdses []*RootMetadataSigned, extras []ExtraMetadata) {\n\tid := tlf.FakeID(1, false)\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\tfor i := 0; i < count; i++ {\n\t\trmd, err := makeInitialRootMetadata(defaultClientMetadataVer, id, h)\n\t\tif err != nil {\n\t\t\tt.Fatal(err)\n\t\t}\n\n\t\taddFakeRMDData(t, config.Codec(), config.Crypto(), rmd, h)\n\t\trmd.SetPrevRoot(prevID)\n\t\trmd.SetRevision(start + MetadataRevision(i))\n\n\t\tctx := context.Background()\n\n\t\t// Encode and sign writer metadata.\n\t\terr = rmd.bareMd.SignWriterMetadataInternally(ctx, config.Codec(), config.Crypto())\n\t\trequire.NoError(t, err)\n\n\t\trmds, err := SignBareRootMetadata(\n\t\t\tctx, config.Codec(), config.Crypto(), config.Crypto(),\n\t\t\trmd.bareMd, time.Now())\n\t\trequire.NoError(t, err)\n\t\tcurrID, err := config.Crypto().MakeMdID(rmds.MD)\n\t\trequire.NoError(t, err)\n\t\tprevID = currID\n\t\trmdses = append(rmdses, rmds)\n\t\textras = append(extras, rmd.extra)\n\t}\n\treturn rmdses, extras\n}\n\nfunc testMDOpsGetRangeSuccess(t *testing.T, fromStart bool) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\trmdses, extras := makeRMDSRange(t, config, 100, 5, fakeMdID(1))\n\n\tstart := MetadataRevision(100)\n\tstop := start + MetadataRevision(len(rmdses))\n\tif fromStart {\n\t\tstart = 0\n\t}\n\n\tfor _, rmds := range rmdses {\n\t\tverifyMDForPrivate(config, rmds)\n\t}\n\n\tconfig.mockMdserv.EXPECT().GetRange(ctx, rmdses[0].MD.TlfID(), NullBranchID, Merged, start,\n\t\tstop).Return(rmdses, nil)\n\tfor _, e := range extras {\n\t\texpectGetKeyBundles(ctx, config, e)\n\t}\n\n\t// Do this first since rmdses is consumed.\n\texpectedMDs := make([]BareRootMetadata, len(rmdses))\n\tfor i, rmds := range rmdses {\n\t\texpectedMDs[i] = rmds.MD\n\t}\n\trmds, err := config.MDOps().GetRange(ctx, rmdses[0].MD.TlfID(), start, stop)\n\trequire.NoError(t, err)\n\trequire.Equal(t, len(rmdses), len(rmds))\n\tfor i := 0; i < len(rmdses); i++ {\n\t\trequire.Equal(t, expectedMDs[i], rmds[i].bareMd)\n\t}\n}\n\nfunc TestMDOpsGetRangeSuccess(t *testing.T) {\n\ttestMDOpsGetRangeSuccess(t, false)\n}\n\nfunc TestMDOpsGetRangeFromStartSuccess(t *testing.T) {\n\ttestMDOpsGetRangeSuccess(t, true)\n}\n\nfunc TestMDOpsGetRangeFailBadPrevRoot(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\trmdses, extras := makeRMDSRange(t, config, 100, 5, fakeMdID(1))\n\n\trmdses[2].MD.(MutableBareRootMetadata).SetPrevRoot(fakeMdID(1))\n\n\tstart := MetadataRevision(100)\n\tstop := start + MetadataRevision(len(rmdses))\n\n\t// Verification is parallelized, so we have to expect at most one\n\t// verification for each rmds.\n\tfor _, rmds := range rmdses {\n\t\tverifyMDForPrivateHelper(config, rmds, 0, 1)\n\t}\n\n\tconfig.mockMdserv.EXPECT().GetRange(ctx, rmdses[0].MD.TlfID(), NullBranchID, Merged, start,\n\t\tstop).Return(rmdses, nil)\n\tfor _, e := range extras {\n\t\texpectGetKeyBundles(ctx, config, e)\n\t}\n\n\t_, err := config.MDOps().GetRange(ctx, rmdses[0].MD.TlfID(), start, stop)\n\trequire.IsType(t, MDMismatchError{}, err)\n}\n\ntype fakeMDServerPut struct {\n\tMDServer\n\n\tlastRmdsLock sync.Mutex\n\tlastRmds     *RootMetadataSigned\n}\n\nfunc (s *fakeMDServerPut) Put(ctx context.Context, rmds *RootMetadataSigned,\n\t_ ExtraMetadata) error {\n\ts.lastRmdsLock.Lock()\n\tdefer s.lastRmdsLock.Unlock()\n\ts.lastRmds = rmds\n\treturn nil\n}\n\nfunc (s *fakeMDServerPut) getLastRmds() *RootMetadataSigned {\n\ts.lastRmdsLock.Lock()\n\tdefer s.lastRmdsLock.Unlock()\n\treturn s.lastRmds\n}\n\nfunc (s *fakeMDServerPut) Shutdown() {}\n\nfunc validatePutPublicRMDS(\n\tctx context.Context, t *testing.T, config Config,\n\tinputRmd BareRootMetadata, rmds *RootMetadataSigned) {\n\t// TODO: Handle private RMDS, too.\n\n\t// Verify LastModifying* fields.\n\t_, me, err := config.KBPKI().GetCurrentUserInfo(ctx)\n\trequire.NoError(t, err)\n\trequire.Equal(t, me, rmds.MD.LastModifyingWriter())\n\trequire.Equal(t, me, rmds.MD.GetLastModifyingUser())\n\n\t// Verify signature of WriterMetadata.\n\tbuf, err := rmds.MD.GetSerializedWriterMetadata(config.Codec())\n\trequire.NoError(t, err)\n\terr = config.Crypto().Verify(buf, rmds.GetWriterMetadataSigInfo())\n\trequire.NoError(t, err)\n\n\t// Verify encoded PrivateMetadata.\n\tvar data PrivateMetadata\n\terr = config.Codec().Decode(rmds.MD.GetSerializedPrivateMetadata(), &data)\n\trequire.NoError(t, err)\n\n\t// Verify signature of RootMetadata.\n\tbuf, err = config.Codec().Encode(rmds.MD)\n\trequire.NoError(t, err)\n\terr = config.Crypto().Verify(buf, rmds.SigInfo)\n\trequire.NoError(t, err)\n\n\t// MDv3 TODO: This should become a BareRootMetadataV3.\n\tvar expectedRmd BareRootMetadataV2\n\terr = kbfscodec.Update(config.Codec(), &expectedRmd, inputRmd)\n\trequire.NoError(t, err)\n\n\t// Overwrite written fields.\n\texpectedRmd.SetLastModifyingWriter(rmds.MD.LastModifyingWriter())\n\texpectedRmd.SetLastModifyingUser(rmds.MD.GetLastModifyingUser())\n\texpectedRmd.WriterMetadataSigInfo = rmds.MD.(*BareRootMetadataV2).WriterMetadataSigInfo\n\texpectedRmd.SetSerializedPrivateMetadata(rmds.MD.GetSerializedPrivateMetadata())\n\n\trequire.Equal(t, &expectedRmd, rmds.MD)\n}\n\nfunc TestMDOpsPutPublicSuccess(t *testing.T) {\n\tconfig := MakeTestConfigOrBust(t, \"alice\", \"bob\")\n\tdefer CheckConfigAndShutdown(t, config)\n\n\tconfig.MDServer().Shutdown()\n\tvar mdServer fakeMDServerPut\n\tconfig.SetMDServer(&mdServer)\n\n\tid := tlf.FakeID(1, true)\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", true)\n\n\trmd, err := makeInitialRootMetadata(config.MetadataVersion(), id, h)\n\trequire.NoError(t, err)\n\trmd.data = makeFakePrivateMetadataFuture(t).toCurrent()\n\trmd.tlfHandle = h\n\n\tctx := context.Background()\n\t_, err = config.MDOps().Put(ctx, rmd)\n\n\trmds := mdServer.getLastRmds()\n\tvalidatePutPublicRMDS(ctx, t, config, rmd.bareMd, rmds)\n}\n\nfunc TestMDOpsPutPrivateSuccess(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\tconfig.SetCodec(kbfscodec.NewMsgpack())\n\n\tid := tlf.FakeID(1, false)\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmd, err := makeInitialRootMetadata(defaultClientMetadataVer, id, h)\n\trequire.NoError(t, err)\n\taddFakeRMDData(t, config.Codec(), config.Crypto(), rmd, h)\n\n\tputMDForPrivate(config, rmd)\n\n\tif _, err := config.MDOps().Put(ctx, rmd); err != nil {\n\t\tt.Errorf(\"Got error on put: %v\", err)\n\t}\n}\n\ntype failEncodeCodec struct {\n\tkbfscodec.Codec\n\terr error\n}\n\nfunc (c failEncodeCodec) Encode(obj interface{}) ([]byte, error) {\n\treturn nil, c.err\n}\n\nfunc TestMDOpsPutFailEncode(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\tid := tlf.FakeID(1, false)\n\th := parseTlfHandleOrBust(t, config, \"alice,bob\", false)\n\trmd, err := makeInitialRootMetadata(config.MetadataVersion(), id, h)\n\trequire.NoError(t, err)\n\n\texpectGetTLFCryptKeyForEncryption(config, rmd)\n\tconfig.mockCrypto.EXPECT().EncryptPrivateMetadata(\n\t\trmd.data, kbfscrypto.TLFCryptKey{}).Return(\n\t\tEncryptedPrivateMetadata{}, nil)\n\tconfig.mockBsplit.EXPECT().ShouldEmbedBlockChanges(gomock.Any()).\n\t\tReturn(true)\n\n\terr = errors.New(\"Fake fail\")\n\tconfig.SetCodec(failEncodeCodec{config.Codec(), err})\n\n\tif _, err2 := config.MDOps().Put(ctx, rmd); err2 != err {\n\t\tt.Errorf(\"Got bad error on put: %v\", err2)\n\t}\n}\n\nfunc TestMDOpsGetRangeFailFinal(t *testing.T) {\n\tmockCtrl, config, ctx := mdOpsInit(t)\n\tdefer mdOpsShutdown(mockCtrl, config)\n\n\trmdses, extras := makeRMDSRange(t, config, 100, 5, fakeMdID(1))\n\trmdses[2].MD.(MutableBareRootMetadata).SetFinalBit()\n\trmdses[2].MD.(MutableBareRootMetadata).SetPrevRoot(rmdses[1].MD.GetPrevRoot())\n\n\tstart := MetadataRevision(100)\n\tstop := start + MetadataRevision(len(rmdses))\n\n\t// Verification is parallelized, so we have to expect at most one\n\t// verification for each rmds.\n\tfor _, rmds := range rmdses {\n\t\tverifyMDForPrivateHelper(config, rmds, 0, 1)\n\t}\n\n\tconfig.mockMdserv.EXPECT().GetRange(\n\t\tctx, rmdses[0].MD.TlfID(), NullBranchID, Merged, start, stop).Return(\n\t\trmdses, nil)\n\tfor _, e := range extras {\n\t\texpectGetKeyBundles(ctx, config, e)\n\t}\n\t_, err := config.MDOps().GetRange(ctx, rmdses[0].MD.TlfID(), start, stop)\n\trequire.IsType(t, MDMismatchError{}, err)\n}\n", "idx": 13, "id": 14826, "msg": "", "proj": "keybase-kbfs", "lang": "go"}
{"patch": "@@ -851,7 +851,7 @@ class UserResource:\n             self._add_timestamp_header(response, timestamp=current_timestamp)\n             raise response\n \n-    def _raise_412_if_modified(self, record=None):\n+    def _raise_412_if_modified(self, obj=None):\n         \"\"\"Raise 412 if current timestamp is superior to the one\n         specified in headers.\n ", "y": 0, "oldf": "import logging\nimport re\nimport functools\nfrom uuid import uuid4\n\nimport colander\nimport venusian\n\nfrom pyramid import exceptions as pyramid_exceptions\nfrom pyramid.decorator import reify\nfrom pyramid.security import Everyone\nfrom pyramid.httpexceptions import (\n    HTTPNotModified,\n    HTTPPreconditionFailed,\n    HTTPNotFound,\n    HTTPServiceUnavailable,\n)\n\nfrom kinto.core import Service\nfrom kinto.core.errors import http_error, raise_invalid, send_alert, ERRORS, request_GET\nfrom kinto.core.events import ACTIONS\nfrom kinto.core.storage import exceptions as storage_exceptions, Filter, Sort, MISSING\nfrom kinto.core.utils import (\n    COMPARISON,\n    classname,\n    decode64,\n    encode64,\n    json,\n    find_nested_value,\n    dict_subset,\n    recursive_update_dict,\n    apply_json_patch,\n)\n\nfrom .model import Model, ShareableModel\nfrom .schema import ResourceSchema, JsonPatchRequestSchema\nfrom .viewset import ViewSet, ShareableViewSet\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef register(depth=1, **kwargs):\n    \"\"\"Ressource class decorator.\n\n    Register the decorated class in the cornice registry.\n    Pass all its keyword arguments to the register_resource\n    function.\n    \"\"\"\n\n    def wrapped(resource):\n        register_resource(resource, depth=depth + 1, **kwargs)\n        return resource\n\n    return wrapped\n\n\ndef register_resource(resource_cls, settings=None, viewset=None, depth=1, **kwargs):\n    \"\"\"Register a resource in the cornice registry.\n\n    :param resource_cls:\n        The resource class to register.\n        It should be a class or have a \"name\" attribute.\n\n    :param viewset:\n        A ViewSet object, which will be used to find out which arguments should\n        be appended to the views, and where the views are.\n\n    :param depth:\n        A depth offset. It will be used to determine what is the level of depth\n        in the call tree. (set to 1 by default.)\n\n    Any additional keyword parameters will be used to override the viewset\n    attributes.\n    \"\"\"\n    if viewset is None:\n        viewset = resource_cls.default_viewset(**kwargs)\n    else:\n        viewset.update(**kwargs)\n\n    resource_name = viewset.get_name(resource_cls)\n\n    def register_service(endpoint_type, settings):\n        \"\"\"Registers a service in cornice, for the given type.\n        \"\"\"\n        path_pattern = getattr(viewset, \"{}_path\".format(endpoint_type))\n        path_values = {\"resource_name\": resource_name}\n        path = path_pattern.format_map(path_values)\n\n        name = viewset.get_service_name(endpoint_type, resource_cls)\n\n        service = Service(name, path, depth=depth, **viewset.get_service_arguments())\n\n        # Attach viewset and resource to the service for later reference.\n        service.viewset = viewset\n        service.resource = resource_cls\n        service.type = endpoint_type\n        # Attach collection and record paths.\n        service.collection_path = viewset.collection_path.format_map(path_values)\n        service.record_path = (\n            viewset.record_path.format_map(path_values)\n            if viewset.record_path is not None\n            else None\n        )\n\n        methods = getattr(viewset, \"{}_methods\".format(endpoint_type))\n        for method in methods:\n            if not viewset.is_endpoint_enabled(\n                endpoint_type, resource_name, method.lower(), settings\n            ):\n                continue\n\n            argument_getter = getattr(viewset, \"{}_arguments\".format(endpoint_type))\n            view_args = argument_getter(resource_cls, method)\n\n            view = viewset.get_view(endpoint_type, method.lower())\n            service.add_view(method, view, klass=resource_cls, **view_args)\n\n            # We support JSON-patch on PATCH views. Since the body payload\n            # of JSON Patch is not a dict (mapping) but an array, we can't\n            # use the same schema as for other PATCH protocols. We add another\n            # dedicated view for PATCH, but targetting a different content_type\n            # predicate.\n            if method.lower() == \"patch\":\n                view_args[\"content_type\"] = \"application/json-patch+json\"\n                view_args[\"schema\"] = JsonPatchRequestSchema()\n                service.add_view(method, view, klass=resource_cls, **view_args)\n\n        return service\n\n    def callback(context, name, ob):\n        # get the callbacks registred by the inner services\n        # and call them from here when the @resource classes are being\n        # scanned by venusian.\n        config = context.config.with_package(info.module)\n\n        # Storage is mandatory for resources.\n        if not hasattr(config.registry, \"storage\"):\n            msg = \"Mandatory storage backend is missing from configuration.\"\n            raise pyramid_exceptions.ConfigurationError(msg)\n\n        # A service for the list.\n        service = register_service(\"collection\", config.registry.settings)\n        config.add_cornice_service(service)\n        # An optional one for record endpoint.\n        if getattr(viewset, \"record_path\") is not None:\n            service = register_service(\"record\", config.registry.settings)\n            config.add_cornice_service(service)\n\n    info = venusian.attach(resource_cls, callback, category=\"pyramid\", depth=depth)\n    return callback\n\n\nclass UserResource:\n    \"\"\"Base resource class providing every endpoint.\n\n    Resources inheriting from UserResource are automatically \"scoped\"\n    by user (see get_parent_id()), with the effect that one user\n    cannot look at another user's data. This is good for implementing\n    sensitive or private information such as accounts.\n\n    However, most resources in Kinto can be shared by different users,\n    with different levels of access determined by their\n    permissions. Those resources should inherit from\n    ShareableResource, below.\n\n    \"\"\"\n\n    default_viewset = ViewSet\n    \"\"\"Default :class:`kinto.core.resource.viewset.ViewSet` class to use when\n    the resource is registered.\"\"\"\n\n    default_model = Model\n    \"\"\"Default :class:`kinto.core.resource.model.Model` class to use for\n    interacting the :mod:`kinto.core.storage` and :mod:`kinto.core.permission`\n    backends.\"\"\"\n\n    schema = ResourceSchema\n    \"\"\"Schema to validate records.\"\"\"\n\n    def __init__(self, request, context=None):\n        self.request = request\n        self.context = context\n        self.record_id = self.request.matchdict.get(\"id\")\n        self.force_patch_update = False\n\n        content_type = str(self.request.headers.get(\"Content-Type\")).lower()\n        self._is_json_patch = content_type == \"application/json-patch+json\"\n        self._is_merge_patch = content_type == \"application/merge-patch+json\"\n\n        # Models are isolated by user.\n        parent_id = self.get_parent_id(request)\n\n        # Authentication to storage is transmitted as is (cf. cloud_storage).\n        auth = request.headers.get(\"Authorization\")\n\n        if not hasattr(self, \"model\"):\n            self.model = self.default_model(\n                storage=request.registry.storage,\n                id_generator=self.id_generator,\n                collection_id=classname(self),\n                parent_id=parent_id,\n                auth=auth,\n            )\n\n        # Initialize timestamp as soon as possible.\n        self.timestamp\n\n    @reify\n    def id_generator(self):\n        # ID generator by resource name in settings.\n        default_id_generator = self.request.registry.id_generators[\"\"]\n        resource_name = self.request.current_resource_name\n        id_generator = self.request.registry.id_generators.get(resource_name, default_id_generator)\n        return id_generator\n\n    @reify\n    def timestamp(self):\n        \"\"\"Return the current collection timestamp.\n\n        :rtype: int\n        \"\"\"\n        try:\n            return self.model.timestamp()\n        except storage_exceptions.BackendError as e:\n            is_readonly = self.request.registry.settings[\"readonly\"]\n            if not is_readonly:\n                raise e\n            # If the instance is configured to be readonly, and if the\n            # collection is empty, the backend will try to bump the timestamp.\n            # It fails if the configured db user has not write privileges.\n            logger.exception(e)\n            error_msg = (\n                \"Collection timestamp cannot be written. \"\n                \"Records endpoint must be hit at least once from a \"\n                \"writable instance.\"\n            )\n            raise http_error(HTTPServiceUnavailable(), errno=ERRORS.BACKEND, message=error_msg)\n\n    def get_parent_id(self, request):\n        \"\"\"Return the parent_id of the resource with regards to the current\n        request.\n\n        :param request:\n            The request used to create the resource.\n\n        :rtype: str\n\n        \"\"\"\n        return request.prefixed_userid\n\n    def _get_known_fields(self):\n        \"\"\"Return all the `field` defined in the ressource schema.\"\"\"\n        known_fields = [c.name for c in self.schema().children] + [\n            self.model.id_field,\n            self.model.modified_field,\n            self.model.deleted_field,\n        ]\n        return known_fields\n\n    def is_known_field(self, field):\n        \"\"\"Return ``True`` if `field` is defined in the resource schema.\n        If the resource schema allows unknown fields, this will always return\n        ``True``.\n\n        :param str field: Field name\n        :rtype: bool\n\n        \"\"\"\n        if self.schema.get_option(\"preserve_unknown\"):\n            return True\n\n        known_fields = self._get_known_fields()\n        # Test first level only: ``target.data.id`` -> ``target``\n        field = field.split(\".\", 1)[0]\n        return field in known_fields\n\n    #\n    # End-points\n    #\n\n    def collection_get(self):\n        \"\"\"Model ``GET`` endpoint: retrieve multiple records.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotModified` if\n            ``If-None-Match`` header is provided and collection not\n            modified in the interim.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and collection modified\n            in the iterim.\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPBadRequest`\n            if filters or sorting are invalid.\n        \"\"\"\n        self._add_timestamp_header(self.request.response)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified()\n        # Collections are considered resources that always exist\n        self._raise_412_if_modified(record={})\n\n        headers = self.request.response.headers\n\n        filters = self._extract_filters()\n        limit = self._extract_limit()\n        sorting = self._extract_sorting(limit)\n        partial_fields = self._extract_partial_fields()\n\n        filter_fields = [f.field for f in filters]\n        include_deleted = self.model.modified_field in filter_fields\n\n        pagination_rules, offset = self._extract_pagination_rules_from_token(limit, sorting)\n\n        records, total_records = self.model.get_records(\n            filters=filters,\n            sorting=sorting,\n            limit=limit,\n            pagination_rules=pagination_rules,\n            include_deleted=include_deleted,\n        )\n\n        offset = offset + len(records)\n        if limit and len(records) == limit and offset < total_records:\n            lastrecord = records[-1]\n            next_page = self._next_page_url(sorting, limit, lastrecord, offset)\n            headers[\"Next-Page\"] = next_page\n\n        if partial_fields:\n            records = [dict_subset(record, partial_fields) for record in records]\n\n        headers[\"Total-Records\"] = str(total_records)\n\n        return self.postprocess(records)\n\n    def collection_post(self):\n        \"\"\"Model ``POST`` endpoint: create a record.\n\n        If the new record id conflicts against an existing one, the\n        posted record is ignored, and the existing record is returned, with\n        a ``200`` status.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and collection modified\n            in the iterim.\n\n        .. seealso::\n\n            Add custom behaviour by overriding\n            :meth:`kinto.core.resource.UserResource.process_record`\n        \"\"\"\n        new_record = self.request.validated[\"body\"].get(\"data\", {})\n        try:\n            # Since ``id`` does not belong to schema, it is not in validated\n            # data. Must look up in body.\n            id_field = self.model.id_field\n            new_record[id_field] = _id = self.request.json[\"data\"][id_field]\n            self._raise_400_if_invalid_id(_id)\n            existing = self._get_record_or_404(_id)\n        except (HTTPNotFound, KeyError, ValueError):\n            existing = None\n\n        self._raise_412_if_modified(record=existing)\n\n        if existing:\n            record = existing\n            action = ACTIONS.READ\n        else:\n            new_record = self.process_record(new_record)\n            record = self.model.create_record(new_record)\n            self.request.response.status_code = 201\n            action = ACTIONS.CREATE\n\n        timestamp = record[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(record, action=action)\n\n    def collection_delete(self):\n        \"\"\"Model ``DELETE`` endpoint: delete multiple records.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and collection modified\n            in the iterim.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPBadRequest`\n            if filters are invalid.\n        \"\"\"\n        # Collections are considered resources that always exist\n        self._raise_412_if_modified(record={})\n\n        filters = self._extract_filters()\n        limit = self._extract_limit()\n        sorting = self._extract_sorting(limit)\n        pagination_rules, offset = self._extract_pagination_rules_from_token(limit, sorting)\n\n        records, total_records = self.model.get_records(\n            filters=filters, sorting=sorting, limit=limit, pagination_rules=pagination_rules\n        )\n        deleted = self.model.delete_records(\n            filters=filters, sorting=sorting, limit=limit, pagination_rules=pagination_rules\n        )\n        if deleted:\n            lastrecord = deleted[-1]\n            # Get timestamp of the last deleted field\n            timestamp = lastrecord[self.model.modified_field]\n            self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n            # Add pagination header\n            if limit and len(deleted) == limit and total_records > 1:\n                next_page = self._next_page_url(sorting, limit, lastrecord, offset)\n                self.request.response.headers[\"Next-Page\"] = next_page\n        else:\n            self._add_timestamp_header(self.request.response)\n\n        headers = self.request.response.headers\n        headers[\"Total-Records\"] = str(total_records)\n\n        action = len(deleted) > 0 and ACTIONS.DELETE or ACTIONS.READ\n        return self.postprocess(deleted, action=action, old=records)\n\n    def get(self):\n        \"\"\"Record ``GET`` endpoint: retrieve a record.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotFound` if\n            the record is not found.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotModified` if\n            ``If-None-Match`` header is provided and record not\n            modified in the interim.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and record modified\n            in the iterim.\n        \"\"\"\n        self._raise_400_if_invalid_id(self.record_id)\n        record = self._get_record_or_404(self.record_id)\n        timestamp = record[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n        self._add_cache_header(self.request.response)\n        self._raise_304_if_not_modified(record)\n        self._raise_412_if_modified(record)\n\n        partial_fields = self._extract_partial_fields()\n        if partial_fields:\n            record = dict_subset(record, partial_fields)\n\n        return self.postprocess(record)\n\n    def put(self):\n        \"\"\"Record ``PUT`` endpoint: create or replace the provided record and\n        return it.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and record modified\n            in the iterim.\n\n        .. note::\n\n            If ``If-None-Match: *`` request header is provided, the\n            ``PUT`` will succeed only if no record exists with this id.\n\n        .. seealso::\n\n            Add custom behaviour by overriding\n            :meth:`kinto.core.resource.UserResource.process_record`.\n        \"\"\"\n        self._raise_400_if_invalid_id(self.record_id)\n        try:\n            existing = self._get_record_or_404(self.record_id)\n        except HTTPNotFound:\n            existing = None\n\n        self._raise_412_if_modified(record=existing)\n\n        # If `data` is not provided, use existing record (or empty if creation)\n        post_record = self.request.validated[\"body\"].get(\"data\", existing) or {}\n\n        record_id = post_record.setdefault(self.model.id_field, self.record_id)\n        self._raise_400_if_id_mismatch(record_id, self.record_id)\n\n        new_record = self.process_record(post_record, old=existing)\n\n        if existing:\n            record = self.model.update_record(new_record)\n        else:\n            record = self.model.create_record(new_record)\n            self.request.response.status_code = 201\n\n        timestamp = record[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        action = existing and ACTIONS.UPDATE or ACTIONS.CREATE\n        return self.postprocess(record, action=action, old=existing)\n\n    def patch(self):\n        \"\"\"Record ``PATCH`` endpoint: modify a record and return its\n        new version.\n\n        If a request header ``Response-Behavior`` is set to ``light``,\n        only the fields whose value was changed are returned.\n        If set to ``diff``, only the fields whose value became different than\n        the one provided are returned.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotFound` if\n            the record is not found.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and record modified\n            in the iterim.\n\n        .. seealso::\n            Add custom behaviour by overriding\n            :meth:`kinto.core.resource.UserResource.apply_changes` or\n            :meth:`kinto.core.resource.UserResource.process_record`.\n        \"\"\"\n        self._raise_400_if_invalid_id(self.record_id)\n        existing = self._get_record_or_404(self.record_id)\n        self._raise_412_if_modified(existing)\n\n        # patch is specified as a list of of operations (RFC 6902)\n        if self._is_json_patch:\n            requested_changes = self.request.validated[\"body\"]\n        else:\n            # `data` attribute may not be present if only perms are patched.\n            body = self.request.validated[\"body\"]\n            if not body:\n                # If no `data` nor `permissions` is provided in patch, reject!\n                # XXX: This should happen in schema instead (c.f. ShareableViewSet)\n                error_details = {\n                    \"name\": \"data\",\n                    \"description\": \"Provide at least one of data or permissions\",\n                }\n                raise_invalid(self.request, **error_details)\n            requested_changes = body.get(\"data\", {})\n\n        updated, applied_changes = self.apply_changes(\n            existing, requested_changes=requested_changes\n        )\n\n        record_id = updated.setdefault(self.model.id_field, self.record_id)\n        self._raise_400_if_id_mismatch(record_id, self.record_id)\n\n        new_record = self.process_record(updated, old=existing)\n\n        changed_fields = [\n            k for k in applied_changes.keys() if existing.get(k) != new_record.get(k)\n        ]\n\n        # Save in storage if necessary.\n        if changed_fields or self.force_patch_update:\n            new_record = self.model.update_record(new_record)\n\n        else:\n            # Behave as if storage would have added `id` and `last_modified`.\n            for extra_field in [self.model.modified_field, self.model.id_field]:\n                new_record[extra_field] = existing[extra_field]\n\n        # Adjust response according to ``Response-Behavior`` header\n        body_behavior = self.request.validated[\"header\"].get(\"Response-Behavior\", \"full\")\n\n        if body_behavior.lower() == \"light\":\n            # Only fields that were changed.\n            data = {k: new_record[k] for k in changed_fields}\n\n        elif body_behavior.lower() == \"diff\":\n            # Only fields that are different from those provided.\n            data = {\n                k: new_record[k]\n                for k in changed_fields\n                if applied_changes.get(k) != new_record.get(k)\n            }\n        else:\n            data = new_record\n\n        timestamp = new_record.get(self.model.modified_field, existing[self.model.modified_field])\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(data, action=ACTIONS.UPDATE, old=existing)\n\n    def delete(self):\n        \"\"\"Record ``DELETE`` endpoint: delete a record and return it.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotFound` if\n            the record is not found.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed` if\n            ``If-Match`` header is provided and record modified\n            in the iterim.\n        \"\"\"\n        self._raise_400_if_invalid_id(self.record_id)\n        record = self._get_record_or_404(self.record_id)\n        self._raise_412_if_modified(record)\n\n        # Retreive the last_modified information from a querystring if present.\n        last_modified = self.request.validated[\"querystring\"].get(\"last_modified\")\n\n        # If less or equal than current record. Ignore it.\n        if last_modified and last_modified <= record[self.model.modified_field]:\n            last_modified = None\n\n        try:\n            deleted = self.model.delete_record(record, last_modified=last_modified)\n        except storage_exceptions.RecordNotFoundError:\n            # Delete might fail if the object was deleted since we\n            # fetched it from the storage (ref Kinto/kinto#1407). This\n            # is one of a larger class of issues where another request\n            # could modify the record between our fetch and our\n            # delete, which could e.g. invalidate our precondition\n            # checking. Fixing this correctly is a larger\n            # problem. However, let's punt on fixing it correctly and\n            # just handle this one important case for now (see #1557).\n            #\n            # Raise a 404 vs. a 409 or 412 because that's what we\n            # would have done if the other thread's delete had\n            # happened a little earlier. (The client doesn't need to\n            # know that we did a bunch of work fetching the existing\n            # record for nothing.)\n            raise self._404_for_record(self.record_id)\n\n        timestamp = deleted[self.model.modified_field]\n        self._add_timestamp_header(self.request.response, timestamp=timestamp)\n\n        return self.postprocess(deleted, action=ACTIONS.DELETE, old=record)\n\n    #\n    # Data processing\n    #\n\n    def process_record(self, new, old=None):\n        \"\"\"Hook for processing records before they reach storage, to introduce\n        specific logics on fields for example.\n\n        .. code-block:: python\n\n            def process_record(self, new, old=None):\n                new = super().process_record(new, old)\n                version = old['version'] if old else 0\n                new['version'] = version + 1\n                return new\n\n        Or add extra validation based on request:\n\n        .. code-block:: python\n\n            from kinto.core.errors import raise_invalid\n\n            def process_record(self, new, old=None):\n                new = super().process_record(new, old)\n                if new['browser'] not in request.headers['User-Agent']:\n                    raise_invalid(self.request, name='browser', error='Wrong')\n                return new\n\n        :param dict new: the validated record to be created or updated.\n        :param dict old: the old record to be updated,\n            ``None`` for creation endpoints.\n\n        :returns: the processed record.\n        :rtype: dict\n        \"\"\"\n        modified_field = self.model.modified_field\n        new_last_modified = new.get(modified_field)\n\n        # Drop the new last_modified if it is not an integer.\n        is_integer = isinstance(new_last_modified, int)\n        if not is_integer:\n            new.pop(modified_field, None)\n            return new\n\n        # Drop the new last_modified if lesser or equal to the old one.\n        is_less_or_equal = old is not None and new_last_modified <= old[modified_field]\n        if is_less_or_equal:\n            new.pop(modified_field, None)\n\n        return new\n\n    def apply_changes(self, record, requested_changes):\n        \"\"\"Merge `changes` into `record` fields.\n\n        .. note::\n\n            This is used in the context of PATCH only.\n\n        Override this to control field changes at record level, for example:\n\n        .. code-block:: python\n\n            def apply_changes(self, record, requested_changes):\n                # Ignore value change if inferior\n                if record['position'] > changes.get('position', -1):\n                    changes.pop('position', None)\n                return super().apply_changes(record, requested_changes)\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPBadRequest`\n            if result does not comply with resource schema.\n\n        :returns: the new record with `changes` applied.\n        :rtype: tuple\n        \"\"\"\n        if self._is_json_patch:\n            try:\n                applied_changes = apply_json_patch(record, requested_changes)[\"data\"]\n                updated = {**applied_changes}\n            except ValueError as e:\n                error_details = {\n                    \"location\": \"body\",\n                    \"description\": \"JSON Patch operation failed: {}\".format(e),\n                }\n                raise_invalid(self.request, **error_details)\n\n        else:\n            applied_changes = {**requested_changes}\n            updated = {**record}\n\n            # recursive patch and remove field if null attribute is passed (RFC 7396)\n            if self._is_merge_patch:\n                recursive_update_dict(updated, applied_changes, ignores=(None,))\n            else:\n                updated.update(**applied_changes)\n\n        for field, value in applied_changes.items():\n            has_changed = record.get(field, value) != value\n            if self.schema.is_readonly(field) and has_changed:\n                error_details = {\"name\": field, \"description\": \"Cannot modify {}\".format(field)}\n                raise_invalid(self.request, **error_details)\n\n        try:\n            validated = self.schema().deserialize(updated)\n        except colander.Invalid as e:\n            # Transform the errors we got from colander into Cornice errors.\n            # We could not rely on Service schema because the record should be\n            # validated only once the changes are applied\n            for field, error in e.asdict().items():  # pragma: no branch\n                raise_invalid(self.request, name=field, description=error)\n\n        return validated, applied_changes\n\n    def postprocess(self, result, action=ACTIONS.READ, old=None):\n        body = {\"data\": result}\n\n        parent_id = self.get_parent_id(self.request)\n        # Use self.model.timestamp() instead of self.timestamp because\n        # self.timestamp is @reify'd relatively early in the request,\n        # so doesn't correspond to any time that is relevant to the\n        # event. See #1769.\n        timestamp = self.model.timestamp()\n        self.request.notify_resource_event(\n            parent_id=parent_id, timestamp=timestamp, data=result, action=action, old=old\n        )\n\n        return body\n\n    #\n    # Internals\n    #\n\n    def _404_for_record(self, record_id):\n        details = {\"id\": record_id, \"resource_name\": self.request.current_resource_name}\n        return http_error(HTTPNotFound(), errno=ERRORS.INVALID_RESOURCE_ID, details=details)\n\n    def _get_record_or_404(self, record_id):\n        \"\"\"Retrieve record from storage and raise ``404 Not found`` if missing.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotFound` if\n            the record is not found.\n        \"\"\"\n        if self.context and self.context.current_record:\n            # Set during authorization. Save a storage hit.\n            return self.context.current_record\n\n        try:\n            return self.model.get_record(record_id)\n        except storage_exceptions.RecordNotFoundError:\n            raise self._404_for_record(record_id)\n\n    def _add_timestamp_header(self, response, timestamp=None):\n        \"\"\"Add current timestamp in response headers, when request comes in.\n\n        \"\"\"\n        if timestamp is None:\n            timestamp = self.timestamp\n        # Pyramid takes care of converting.\n        response.last_modified = timestamp / 1000.0\n        # Return timestamp as ETag.\n        response.headers[\"ETag\"] = '\"{}\"'.format(timestamp)\n\n    def _add_cache_header(self, response):\n        \"\"\"Add Cache-Control and Expire headers, based a on a setting for the\n        current resource.\n\n        Cache headers will be set with anonymous requests only.\n\n        .. note::\n\n            The ``Cache-Control: no-cache`` response header does not prevent\n            caching in client. It will indicate the client to revalidate\n            the response content on each access. The client will send a\n            conditional request to the server and check that a\n            ``304 Not modified`` is returned before serving content from cache.\n        \"\"\"\n        resource_name = self.context.resource_name if self.context else \"\"\n        setting_key = \"{}_cache_expires_seconds\".format(resource_name)\n        collection_expires = self.request.registry.settings.get(setting_key)\n        is_anonymous = self.request.prefixed_userid is None\n        if collection_expires and is_anonymous:\n            response.cache_expires(seconds=int(collection_expires))\n        else:\n            # Since `Expires` response header provides an HTTP data with a\n            # resolution in seconds, do not use Pyramid `cache_expires()` in\n            # order to omit it.\n            response.cache_control.no_cache = True\n            response.cache_control.no_store = True\n\n    def _raise_400_if_invalid_id(self, record_id):\n        \"\"\"Raise 400 if specified record id does not match the format excepted\n        by storage backends.\n\n        :raises: :class:`pyramid.httpexceptions.HTTPBadRequest`\n        \"\"\"\n        is_string = isinstance(record_id, str)\n        if not is_string or not self.model.id_generator.match(record_id):\n            error_details = {\"location\": \"path\", \"description\": \"Invalid record id\"}\n            raise_invalid(self.request, **error_details)\n\n    def _raise_304_if_not_modified(self, record=None):\n        \"\"\"Raise 304 if current timestamp is inferior to the one specified\n        in headers.\n\n        :raises: :exc:`~pyramid:pyramid.httpexceptions.HTTPNotModified`\n        \"\"\"\n        if_none_match = self.request.validated[\"header\"].get(\"If-None-Match\")\n\n        if not if_none_match:\n            return\n\n        if if_none_match == \"*\":\n            return\n\n        if record:\n            current_timestamp = record[self.model.modified_field]\n        else:\n            current_timestamp = self.model.timestamp()\n\n        if current_timestamp == if_none_match:\n            response = HTTPNotModified()\n            self._add_timestamp_header(response, timestamp=current_timestamp)\n            raise response\n\n    def _raise_412_if_modified(self, record=None):\n        \"\"\"Raise 412 if current timestamp is superior to the one\n        specified in headers.\n\n        :raises:\n            :exc:`~pyramid:pyramid.httpexceptions.HTTPPreconditionFailed`\n        \"\"\"\n        if_match = self.request.validated[\"header\"].get(\"If-Match\")\n        if_none_match = self.request.validated[\"header\"].get(\"If-None-Match\")\n\n        # Check if record exists\n        record_exists = record is not None\n\n        # If no precondition headers, just ignore\n        if not if_match and not if_none_match:\n            return\n\n        # If-None-Match: * should always raise if a record exists\n        if if_none_match == \"*\" and record_exists:\n            modified_since = -1  # Always raise.\n\n        # If-Match should always raise if a record doesn't exist\n        elif if_match and not record_exists:\n            modified_since = -1\n\n        # If-Match with ETag value on existing records should compare ETag\n        elif if_match and if_match != \"*\":\n            modified_since = if_match\n\n        # If none of the above applies, don't raise\n        else:\n            return\n\n        if record:\n            current_timestamp = record[self.model.modified_field]\n        else:\n            current_timestamp = self.model.timestamp()\n\n        if current_timestamp != modified_since:\n            error_msg = \"Resource was modified meanwhile\"\n            details = {\"existing\": record} if record else {}\n            response = http_error(\n                HTTPPreconditionFailed(),\n                errno=ERRORS.MODIFIED_MEANWHILE,\n                message=error_msg,\n                details=details,\n            )\n            self._add_timestamp_header(response, timestamp=current_timestamp)\n            raise response\n\n    def _raise_400_if_id_mismatch(self, new_id, record_id):\n        \"\"\"Raise 400 if the `new_id`, within the request body, does not match\n        the `record_id`, obtained from request path.\n\n        :raises: :class:`pyramid.httpexceptions.HTTPBadRequest`\n        \"\"\"\n        if new_id != record_id:\n            error_msg = \"Record id does not match existing record\"\n            error_details = {\"name\": self.model.id_field, \"description\": error_msg}\n            raise_invalid(self.request, **error_details)\n\n    def _extract_partial_fields(self):\n        \"\"\"Extract the fields to do the projection from QueryString parameters.\n        \"\"\"\n        fields = self.request.validated[\"querystring\"].get(\"_fields\")\n        if fields:\n            root_fields = [f.split(\".\")[0] for f in fields]\n            known_fields = self._get_known_fields()\n            invalid_fields = set(root_fields) - set(known_fields)\n            preserve_unknown = self.schema.get_option(\"preserve_unknown\")\n            if not preserve_unknown and invalid_fields:\n                error_msg = \"Fields {} do not exist\".format(\",\".join(invalid_fields))\n                error_details = {\"name\": \"Invalid _fields parameter\", \"description\": error_msg}\n                raise_invalid(self.request, **error_details)\n\n            # Since id and last_modified are part of the synchronisation\n            # API, force their presence in payloads.\n            fields = fields + [self.model.id_field, self.model.modified_field]\n\n        return fields\n\n    def _extract_limit(self):\n        \"\"\"Extract limit value from QueryString parameters.\"\"\"\n        paginate_by = self.request.registry.settings[\"paginate_by\"]\n        max_fetch_size = self.request.registry.settings[\"storage_max_fetch_size\"]\n        limit = self.request.validated[\"querystring\"].get(\"_limit\", paginate_by)\n\n        # If limit is higher than paginate_by setting, ignore it.\n        if limit and paginate_by:\n            limit = min(limit, paginate_by)\n\n        # If limit is higher than what storage can retrieve, ignore it.\n        limit = min(limit, max_fetch_size) if limit else max_fetch_size\n\n        return limit\n\n    def _extract_filters(self):\n        \"\"\"Extracts filters from QueryString parameters.\"\"\"\n        queryparams = self.request.validated[\"querystring\"]\n\n        filters = []\n\n        for param, value in queryparams.items():\n            param = param.strip()\n\n            error_details = {\n                \"name\": param,\n                \"location\": \"querystring\",\n                \"description\": \"Invalid value for {}\".format(param),\n            }\n\n            # Ignore specific fields\n            if param.startswith(\"_\") and param not in (\"_since\", \"_to\", \"_before\"):\n                continue\n\n            # Handle the _since specific filter.\n            if param in (\"_since\", \"_to\", \"_before\"):\n\n                if param == \"_since\":\n                    operator = COMPARISON.GT\n                else:\n                    if param == \"_to\":\n                        message = \"_to is now deprecated, \" \"you should use _before instead\"\n                        url = (\n                            \"https://kinto.readthedocs.io/en/2.4.0/api/\"\n                            \"resource.html#list-of-available-url-\"\n                            \"parameters\"\n                        )\n                        send_alert(self.request, message, url)\n                    operator = COMPARISON.LT\n\n                if value == \"\":\n                    raise_invalid(self.request, **error_details)\n\n                filters.append(Filter(self.model.modified_field, value, operator))\n                continue\n\n            all_keywords = \"|\".join([i.name.lower() for i in COMPARISON])\n            m = re.match(r\"^(\" + all_keywords + r\")_([\\w\\.]+)$\", param)\n            if m:\n                keyword, field = m.groups()\n                operator = getattr(COMPARISON, keyword.upper())\n            else:\n                operator, field = COMPARISON.EQ, param\n\n            if not self.is_known_field(field):\n                error_msg = \"Unknown filter field '{}'\".format(param)\n                error_details[\"description\"] = error_msg\n                raise_invalid(self.request, **error_details)\n\n            if operator in (COMPARISON.IN, COMPARISON.EXCLUDE):\n                all_integers = all([isinstance(v, int) for v in value])\n                all_strings = all([isinstance(v, str) for v in value])\n                has_invalid_value = (field == self.model.id_field and not all_strings) or (\n                    field == self.model.modified_field and not all_integers\n                )\n                if has_invalid_value:\n                    raise_invalid(self.request, **error_details)\n\n            if \"\\x00\" in field or \"\\x00\" in str(value):\n                error_details[\"description\"] = \"Invalid character 0x00\"\n                raise_invalid(self.request, **error_details)\n\n            if field == self.model.modified_field and value == \"\":\n                raise_invalid(self.request, **error_details)\n\n            filters.append(Filter(field, value, operator))\n\n        return filters\n\n    def _extract_sorting(self, limit):\n        \"\"\"Extracts filters from QueryString parameters.\"\"\"\n        specified = self.request.validated[\"querystring\"].get(\"_sort\", [])\n        sorting = []\n        modified_field_used = self.model.modified_field in specified\n        for field in specified:\n            field = field.strip()\n            m = re.match(r\"^([\\-+]?)([\\w\\.]+)$\", field)\n            if m:\n                order, field = m.groups()\n\n                if not self.is_known_field(field):\n                    error_details = {\n                        \"location\": \"querystring\",\n                        \"description\": \"Unknown sort field '{}'\".format(field),\n                    }\n                    raise_invalid(self.request, **error_details)\n\n                direction = -1 if order == \"-\" else 1\n                sorting.append(Sort(field, direction))\n\n        if not modified_field_used:\n            # Add a sort by the ``modified_field`` in descending order\n            # useful for pagination\n            sorting.append(Sort(self.model.modified_field, -1))\n        return sorting\n\n    def _build_pagination_rules(self, sorting, last_record, rules=None):\n        \"\"\"Return the list of rules for a given sorting attribute and\n        last_record.\n\n        \"\"\"\n        if rules is None:\n            rules = []\n\n        rule = []\n        next_sorting = sorting[:-1]\n\n        for field, _ in next_sorting:\n            rule.append(Filter(field, last_record.get(field, MISSING), COMPARISON.EQ))\n\n        field, direction = sorting[-1]\n\n        if direction == -1:\n            rule.append(Filter(field, last_record.get(field, MISSING), COMPARISON.LT))\n        else:\n            rule.append(Filter(field, last_record.get(field, MISSING), COMPARISON.GT))\n\n        rules.append(rule)\n\n        if len(next_sorting) == 0:\n            return rules\n\n        return self._build_pagination_rules(next_sorting, last_record, rules)\n\n    def _extract_pagination_rules_from_token(self, limit, sorting):\n        \"\"\"Get pagination params.\"\"\"\n        token = self.request.validated[\"querystring\"].get(\"_token\", None)\n        filters = []\n        offset = 0\n        if token:\n            error_msg = None\n            try:\n                tokeninfo = json.loads(decode64(token))\n                if not isinstance(tokeninfo, dict):\n                    raise ValueError()\n                last_record = tokeninfo[\"last_record\"]\n                offset = tokeninfo[\"offset\"]\n                nonce = tokeninfo[\"nonce\"]\n            except (ValueError, KeyError, TypeError):\n                error_msg = \"_token has invalid content\"\n\n            # We don't want pagination tokens to be reused several times (#1171).\n            # The cache backend is used to keep track of \"nonces\".\n            if self.request.method.lower() == \"delete\" and error_msg is None:\n                registry = self.request.registry\n                deleted = registry.cache.delete(nonce)\n                if deleted is None:\n                    error_msg = \"_token was already used or has expired.\"\n\n            if error_msg:\n                error_details = {\"location\": \"querystring\", \"description\": error_msg}\n                raise_invalid(self.request, **error_details)\n\n            filters = self._build_pagination_rules(sorting, last_record)\n\n        return filters, offset\n\n    def _next_page_url(self, sorting, limit, last_record, offset):\n        \"\"\"Build the Next-Page header from where we stopped.\"\"\"\n        token = self._build_pagination_token(sorting, last_record, offset)\n\n        params = {**request_GET(self.request), \"_limit\": limit, \"_token\": token}\n\n        service = self.request.current_service\n        next_page_url = self.request.route_url(\n            service.name, _query=params, **self.request.matchdict\n        )\n        return next_page_url\n\n    def _build_pagination_token(self, sorting, last_record, offset):\n        \"\"\"Build a pagination token.\n\n        It is a base64 JSON object with the sorting fields values of\n        the last_record.\n\n        \"\"\"\n        nonce = \"pagination-token-{}\".format(uuid4())\n        if self.request.method.lower() == \"delete\":\n            registry = self.request.registry\n            validity = registry.settings[\"pagination_token_validity_seconds\"]\n            registry.cache.set(nonce, \"\", validity)\n\n        token = {\"last_record\": {}, \"offset\": offset, \"nonce\": nonce}\n\n        for field, _ in sorting:\n            last_value = find_nested_value(last_record, field, MISSING)\n            if last_value is not MISSING:\n                token[\"last_record\"][field] = last_value\n\n        return encode64(json.dumps(token))\n\n\nclass ShareableResource(UserResource):\n    \"\"\"Shareable resources allow to set permissions on records, in order to\n    share their access or protect their modification.\n    \"\"\"\n\n    default_model = ShareableModel\n    default_viewset = ShareableViewSet\n    permissions = (\"read\", \"write\")\n    \"\"\"List of allowed permissions names.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # In base resource, PATCH only hit storage if no data has changed.\n        # Here, we force update because we add the current principal to\n        # the ``write`` ACE.\n        self.force_patch_update = True\n\n        # Required by the ShareableModel class.\n        self.model.permission = self.request.registry.permission\n        if self.request.prefixed_userid is None:\n            # The principal of an anonymous is system.Everyone\n            self.model.current_principal = Everyone\n        else:\n            self.model.current_principal = self.request.prefixed_userid\n        self.model.prefixed_principals = self.request.prefixed_principals\n\n        if self.context:\n            self.model.get_permission_object_id = functools.partial(\n                self.context.get_permission_object_id, self.request\n            )\n\n    def get_parent_id(self, request):\n        \"\"\"Unlike :class:`kinto.core.resource.UserResource`, records are not\n        isolated by user.\n\n        See https://github.com/mozilla-services/cliquet/issues/549\n\n        :returns: A constant empty value.\n        \"\"\"\n        return \"\"\n\n    def _extract_filters(self):\n        \"\"\"Override default filters extraction from QueryString to allow\n        partial collection of records.\n\n        XXX: find more elegant approach to add custom filters.\n        \"\"\"\n        filters = super()._extract_filters()\n\n        ids = self.context.shared_ids\n        if ids is not None:\n            filter_by_id = Filter(self.model.id_field, ids, COMPARISON.IN)\n            filters.insert(0, filter_by_id)\n\n        return filters\n\n    def _raise_412_if_modified(self, record=None):\n        \"\"\"Do not provide the permissions among the record fields.\n        Ref: https://github.com/Kinto/kinto/issues/224\n        \"\"\"\n        if record:\n            record = {**record}\n            record.pop(self.model.permissions_field, None)\n        return super()._raise_412_if_modified(record)\n\n    def process_record(self, new, old=None):\n        \"\"\"Read permissions from request body, and in the case of ``PUT`` every\n        existing ACE is removed (using empty list).\n        \"\"\"\n        new = super().process_record(new, old)\n\n        # patch is specified as a list of of operations (RFC 6902)\n\n        payload = self.request.validated[\"body\"]\n\n        if self._is_json_patch:\n            permissions = apply_json_patch(old, payload)[\"permissions\"]\n\n        elif self._is_merge_patch:\n            existing = old or {}\n            permissions = existing.get(\"__permissions__\", {})\n            recursive_update_dict(permissions, payload.get(\"permissions\", {}), ignores=(None,))\n\n        else:\n            permissions = {\n                k: v for k, v in payload.get(\"permissions\", {}).items() if v is not None\n            }\n\n        annotated = {**new}\n\n        if permissions:\n            is_put = self.request.method.lower() == \"put\"\n            if is_put or self._is_merge_patch:\n                # Remove every existing ACEs using empty lists.\n                for perm in self.permissions:\n                    permissions.setdefault(perm, [])\n            annotated[self.model.permissions_field] = permissions\n\n        return annotated\n\n    def postprocess(self, result, action=ACTIONS.READ, old=None):\n        \"\"\"Add ``permissions`` attribute in response body.\n\n        In the HTTP API, it was decided that ``permissions`` would reside\n        outside the ``data`` attribute.\n        \"\"\"\n        body = {}\n\n        if not isinstance(result, list):\n            # record endpoint.\n            perms = result.pop(self.model.permissions_field, None)\n            if perms is not None:\n                body[\"permissions\"] = {k: list(p) for k, p in perms.items()}\n\n            if old:\n                # Remove permissions from event payload.\n                old.pop(self.model.permissions_field, None)\n\n        data = super().postprocess(result, action, old)\n        body.update(data)\n        return body\n", "idx": 24, "id": 11966, "msg": "", "proj": "Kinto-kinto", "lang": "py"}
{"patch": "@@ -39,26 +39,28 @@ func TestMiner(t *testing.T) {\n \n \tctx := context.Background()\n \n-\tpower := uint64(12)\n-\tbs, addr, st := requireMinerWithPower(ctx, t, power)\n+\tnumCommittedSectors := uint64(12)\n+\tbs, addr, st := requireMinerWithPower(ctx, t, numCommittedSectors)\n \n \tactual, err := (&consensus.MarketView{}).Miner(ctx, st, bs, addr)\n \trequire.NoError(t, err)\n \n-\tassert.Equal(t, power, actual)\n+\texpected := types.OneKiBSectorSize.Mul(types.NewBytesAmount(numCommittedSectors))\n+\n+\tassert.Equal(t, expected.Uint64(), actual.Uint64())\n }\n \n-func requireMinerWithPower(ctx context.Context, t *testing.T, power uint64) (bstore.Blockstore, address.Address, state.Tree) {\n+func requireMinerWithPower(ctx context.Context, t *testing.T, numCommittedSectors uint64) (bstore.Blockstore, address.Address, state.Tree) {\n \tr := repo.NewInMemoryRepo()\n \tbs := bstore.NewBlockstore(r.Datastore())\n \tcst := hamt.NewCborStore()\n \n-\t// set up genesis block with power\n+\t// set up genesis block containing some miners with non-zero power\n \tgenCfg := &gengen.GenesisCfg{\n \t\tKeys: 1,\n \t\tMiners: []gengen.Miner{\n \t\t\t{\n-\t\t\t\tPower: power,\n+\t\t\t\tNumCommittedSectors: numCommittedSectors,\n \t\t\t},\n \t\t},\n \t}", "y": 0, "oldf": "package chain_test\n\nimport (\n\t\"context\"\n\t\"testing\"\n\n\t\"github.com/ipfs/go-hamt-ipld\"\n\tbstore \"github.com/ipfs/go-ipfs-blockstore\"\n\n\t\"github.com/filecoin-project/go-filecoin/actor/builtin\"\n\t\"github.com/filecoin-project/go-filecoin/address\"\n\t\"github.com/filecoin-project/go-filecoin/consensus\"\n\t\"github.com/filecoin-project/go-filecoin/gengen/util\"\n\t\"github.com/filecoin-project/go-filecoin/repo\"\n\t\"github.com/filecoin-project/go-filecoin/state\"\n\t\"github.com/filecoin-project/go-filecoin/types\"\n\n\ttf \"github.com/filecoin-project/go-filecoin/testhelpers/testflags\"\n\t\"github.com/stretchr/testify/assert\"\n\t\"github.com/stretchr/testify/require\"\n)\n\nfunc TestTotal(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx := context.Background()\n\n\tpower := uint64(19)\n\tbs, _, st := requireMinerWithPower(ctx, t, power)\n\n\tactual, err := (&consensus.MarketView{}).Total(ctx, st, bs)\n\trequire.NoError(t, err)\n\n\tassert.Equal(t, power, actual)\n}\n\nfunc TestMiner(t *testing.T) {\n\ttf.UnitTest(t)\n\n\tctx := context.Background()\n\n\tpower := uint64(12)\n\tbs, addr, st := requireMinerWithPower(ctx, t, power)\n\n\tactual, err := (&consensus.MarketView{}).Miner(ctx, st, bs, addr)\n\trequire.NoError(t, err)\n\n\tassert.Equal(t, power, actual)\n}\n\nfunc requireMinerWithPower(ctx context.Context, t *testing.T, power uint64) (bstore.Blockstore, address.Address, state.Tree) {\n\tr := repo.NewInMemoryRepo()\n\tbs := bstore.NewBlockstore(r.Datastore())\n\tcst := hamt.NewCborStore()\n\n\t// set up genesis block with power\n\tgenCfg := &gengen.GenesisCfg{\n\t\tKeys: 1,\n\t\tMiners: []gengen.Miner{\n\t\t\t{\n\t\t\t\tPower: power,\n\t\t\t},\n\t\t},\n\t}\n\n\tinfo, err := gengen.GenGen(ctx, genCfg, cst, bs, 0)\n\trequire.NoError(t, err)\n\n\tvar calcGenBlk types.Block\n\trequire.NoError(t, cst.Get(ctx, info.GenesisCid, &calcGenBlk))\n\n\tstateTree, err := state.LoadStateTree(ctx, cst, calcGenBlk.StateRoot, builtin.Actors)\n\trequire.NoError(t, err)\n\n\treturn bs, info.Miners[0].Address, stateTree\n}\n", "idx": 2, "id": 19246, "msg": "", "proj": "filecoin-project-venus", "lang": "go"}
{"patch": "@@ -510,7 +510,7 @@ define(['connectionManager', 'cardBuilder', 'appSettings', 'dom', 'apphost', 'la\n         elem.classList.add('hide');\n         elem.innerHTML = html;\n \n-        var itemsContainer = elem.querySelector('.itemsContainer');\n+        const itemsContainer = elem.querySelector('.itemsContainer');\n         itemsContainer.fetchData = getContinueListeningFetchFn(apiClient.serverId());\n         itemsContainer.getItemsHtml = getContinueListeningItemsHtml;\n         itemsContainer.parentContainer = elem;", "y": 0, "oldf": "define(['connectionManager', 'cardBuilder', 'appSettings', 'dom', 'apphost', 'layoutManager', 'imageLoader', 'globalize', 'itemShortcuts', 'itemHelper', 'appRouter', 'scripts/imagehelper', 'paper-icon-button-light', 'emby-itemscontainer', 'emby-scroller', 'emby-button', 'css!./homesections'], function (connectionManager, cardBuilder, appSettings, dom, appHost, layoutManager, imageLoader, globalize, itemShortcuts, itemHelper, appRouter, imageHelper) {\n    'use strict';\n\n    function getDefaultSection(index) {\n        switch (index) {\n            case 0:\n                return 'smalllibrarytiles';\n            case 1:\n                return 'resume';\n            case 2:\n                return 'resumeaudio';\n            case 3:\n                return 'livetv';\n            case 4:\n                return 'nextup';\n            case 5:\n                return 'latestmedia';\n            case 6:\n                return 'none';\n            default:\n                return '';\n        }\n    }\n\n    function getAllSectionsToShow(userSettings, sectionCount) {\n        var sections = [];\n        for (var i = 0, length = sectionCount; i < length; i++) {\n            var section = userSettings.get('homesection' + i) || getDefaultSection(i);\n            if (section === 'folders') {\n                section = getDefaultSection(0);\n            }\n\n            sections.push(section);\n        }\n\n        return sections;\n    }\n\n    function loadSections(elem, apiClient, user, userSettings) {\n        return getUserViews(apiClient, user.Id).then(function (userViews) {\n            var html = '';\n\n            if (userViews.length) {\n                var sectionCount = 7;\n                for (var i = 0; i < sectionCount; i++) {\n                    html += '<div class=\"verticalSection section' + i + '\"></div>';\n                }\n\n                elem.innerHTML = html;\n                elem.classList.add('homeSectionsContainer');\n\n                var promises = [];\n                var sections = getAllSectionsToShow(userSettings, sectionCount);\n                for (var i = 0; i < sections.length; i++) {\n                    promises.push(loadSection(elem, apiClient, user, userSettings, userViews, sections, i));\n                }\n\n                return Promise.all(promises).then(function () {\n                    return resume(elem, {\n                        refresh: true,\n                        returnPromise: false\n                    });\n                });\n            } else {\n                var noLibDescription;\n                if (user['Policy'] && user['Policy']['IsAdministrator']) {\n                    noLibDescription = globalize.translate('NoCreatedLibraries', '<br><a id=\"button-createLibrary\" class=\"button-link\">', '</a>');\n                } else {\n                    noLibDescription = globalize.translate('AskAdminToCreateLibrary');\n                }\n\n                html += '<div class=\"centerMessage padded-left padded-right\">';\n                html += '<h2>' + globalize.translate('MessageNothingHere') + '</h2>';\n                html += '<p>' + noLibDescription + '</p>';\n                html += '</div>';\n                elem.innerHTML = html;\n\n                var createNowLink = elem.querySelector('#button-createLibrary');\n                if (createNowLink) {\n                    createNowLink.addEventListener('click', function () {\n                        Dashboard.navigate('library.html');\n                    });\n                }\n            }\n        });\n    }\n\n    function destroySections(elem) {\n        var elems = elem.querySelectorAll('.itemsContainer');\n        for (var i = 0; i < elems.length; i++) {\n            elems[i].fetchData = null;\n            elems[i].parentContainer = null;\n            elems[i].getItemsHtml = null;\n        }\n\n        elem.innerHTML = '';\n    }\n\n    function pause(elem) {\n        var elems = elem.querySelectorAll('.itemsContainer');\n        for (var i = 0; i < elems.length; i++) {\n            elems[i].pause();\n        }\n    }\n\n    function resume(elem, options) {\n        var elems = elem.querySelectorAll('.itemsContainer');\n        var i;\n        var length;\n        var promises = [];\n\n        for (i = 0, length = elems.length; i < length; i++) {\n            promises.push(elems[i].resume(options));\n        }\n\n        var promise = Promise.all(promises);\n        if (!options || options.returnPromise !== false) {\n            return promise;\n        }\n    }\n\n    function loadSection(page, apiClient, user, userSettings, userViews, allSections, index) {\n\n        var section = allSections[index];\n        var userId = user.Id;\n\n        var elem = page.querySelector('.section' + index);\n\n        if (section === 'latestmedia') {\n            loadRecentlyAdded(elem, apiClient, user, userViews);\n        } else if (section === 'librarytiles' || section === 'smalllibrarytiles' || section === 'smalllibrarytiles-automobile' || section === 'librarytiles-automobile') {\n            loadLibraryTiles(elem, apiClient, user, userSettings, 'smallBackdrop', userViews, allSections);\n        } else if (section === 'librarybuttons') {\n            loadlibraryButtons(elem, apiClient, user, userSettings, userViews);\n        } else if (section === 'resume') {\n            loadResumeVideo(elem, apiClient, userId);\n        } else if (section === 'resumeaudio') {\n            loadResumeAudio(elem, apiClient, userId);\n        } else if (section === 'activerecordings') {\n            loadLatestLiveTvRecordings(elem, true, apiClient, userId);\n        } else if (section === 'nextup') {\n            loadNextUp(elem, apiClient, userId);\n        } else if (section === 'onnow' || section === 'livetv') {\n            return loadOnNow(elem, apiClient, user);\n        } else {\n            elem.innerHTML = '';\n            return Promise.resolve();\n        }\n        return Promise.resolve();\n    }\n\n    function getUserViews(apiClient, userId) {\n        return apiClient.getUserViews({}, userId || apiClient.getCurrentUserId()).then(function (result) {\n            return result.Items;\n        });\n    }\n\n    function enableScrollX() {\n        return true;\n    }\n\n    function getSquareShape() {\n        return enableScrollX() ? 'overflowSquare' : 'square';\n    }\n\n    function getThumbShape() {\n        return enableScrollX() ? 'overflowBackdrop' : 'backdrop';\n    }\n\n    function getPortraitShape() {\n        return enableScrollX() ? 'overflowPortrait' : 'portrait';\n    }\n\n    function getLibraryButtonsHtml(items) {\n        var html = '';\n\n        html += '<div class=\"verticalSection verticalSection-extrabottompadding\">';\n        html += '<h2 class=\"sectionTitle sectionTitle-cards padded-left\">' + globalize.translate('HeaderMyMedia') + '</h2>';\n\n        html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\" data-multiselect=\"false\">';\n\n        // library card background images\n        for (var i = 0, length = items.length; i < length; i++) {\n            var item = items[i];\n            var icon = imageHelper.getLibraryIcon(item.CollectionType);\n            html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl(item) + '\" class=\"raised homeLibraryButton\"><span class=\"material-icons homeLibraryIcon ' + icon + '\"></span><span class=\"homeLibraryText\">' + item.Name + '</span></a>';\n        }\n\n        html += '</div>';\n        html += '</div>';\n\n        return html;\n    }\n\n    function loadlibraryButtons(elem, apiClient, user, userSettings, userViews) {\n        elem.classList.remove('verticalSection');\n        var html = getLibraryButtonsHtml(userViews);\n\n        elem.innerHTML = html;\n        imageLoader.lazyChildren(elem);\n    }\n\n    /**\n     * Returns a random integer between min (inclusive) and max (inclusive)\n     * Using Math.round() will give you a non-uniform distribution!\n     */\n    function getRandomInt(min, max) {\n        return Math.floor(Math.random() * (max - min + 1)) + min;\n    }\n\n    function getFetchLatestItemsFn(serverId, parentId, collectionType) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            var limit = 16;\n\n            if (enableScrollX()) {\n                if (collectionType === 'music') {\n                    limit = 30;\n                }\n            } else {\n                if (collectionType === 'tvshows') {\n                    limit = 5;\n                } else if (collectionType === 'music') {\n                    limit = 9;\n                } else {\n                    limit = 8;\n                }\n            }\n\n            var options = {\n                Limit: limit,\n                Fields: 'PrimaryImageAspectRatio,BasicSyncInfo,Path',\n                ImageTypeLimit: 1,\n                EnableImageTypes: 'Primary,Backdrop,Thumb',\n                ParentId: parentId\n            };\n\n            return apiClient.getLatestItems(options);\n        };\n    }\n\n    function getLatestItemsHtmlFn(itemType, viewType) {\n        return function (items) {\n            var cardLayout = false;\n            var shape;\n            if (itemType === 'Channel' || viewType === 'movies' || viewType === 'books' || viewType === 'tvshows') {\n                shape = getPortraitShape();\n            } else if (viewType === 'music' || viewType === 'homevideos') {\n                shape = getSquareShape();\n            } else {\n                shape = getThumbShape();\n            }\n\n            return cardBuilder.getCardsHtml({\n                items: items,\n                shape: shape,\n                preferThumb: viewType !== 'movies' && viewType !== 'tvshows' && itemType !== 'Channel' && viewType !== 'music' ? 'auto' : null,\n                showUnplayedIndicator: false,\n                showChildCountIndicator: true,\n                context: 'home',\n                overlayText: false,\n                centerText: !cardLayout,\n                overlayPlayButton: viewType !== 'photos',\n                allowBottomPadding: !enableScrollX() && !cardLayout,\n                cardLayout: cardLayout,\n                showTitle: viewType !== 'photos',\n                showYear: viewType === 'movies' || viewType === 'tvshows' || !viewType,\n                showParentTitle: viewType === 'music' || viewType === 'tvshows' || !viewType || (cardLayout && (viewType === 'tvshows')),\n                lines: 2\n            });\n        };\n    }\n\n    function renderLatestSection(elem, apiClient, user, parent) {\n        var html = '';\n\n        html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards padded-left\">';\n        if (!layoutManager.tv) {\n            html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl(parent, {\n                section: 'latest'\n            }) + '\" class=\"more button-flat button-flat-mini sectionTitleTextButton\">';\n            html += '<h2 class=\"sectionTitle sectionTitle-cards\">';\n            html += globalize.translate('LatestFromLibrary', parent.Name);\n            html += '</h2>';\n            html += '<span class=\"material-icons chevron_right\"></span>';\n            html += '</a>';\n        } else {\n            html += '<h2 class=\"sectionTitle sectionTitle-cards\">' + globalize.translate('LatestFromLibrary', parent.Name) + '</h2>';\n        }\n        html += '</div>';\n\n        if (enableScrollX()) {\n            html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\">';\n        } else {\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer focuscontainer-x padded-left padded-right vertical-wrap\">';\n        }\n\n        if (enableScrollX()) {\n            html += '</div>';\n        }\n        html += '</div>';\n\n        elem.innerHTML = html;\n\n        var itemsContainer = elem.querySelector('.itemsContainer');\n        itemsContainer.fetchData = getFetchLatestItemsFn(apiClient.serverId(), parent.Id, parent.CollectionType);\n        itemsContainer.getItemsHtml = getLatestItemsHtmlFn(parent.Type, parent.CollectionType);\n        itemsContainer.parentContainer = elem;\n    }\n\n    function loadRecentlyAdded(elem, apiClient, user, userViews) {\n        elem.classList.remove('verticalSection');\n        var excludeViewTypes = ['playlists', 'livetv', 'boxsets', 'channels'];\n\n        for (var i = 0, length = userViews.length; i < length; i++) {\n            var item = userViews[i];\n            if (user.Configuration.LatestItemsExcludes.indexOf(item.Id) !== -1) {\n                continue;\n            }\n\n            if (excludeViewTypes.indexOf(item.CollectionType || []) !== -1) {\n                continue;\n            }\n\n            var frag = document.createElement('div');\n            frag.classList.add('verticalSection');\n            frag.classList.add('hide');\n            elem.appendChild(frag);\n\n            renderLatestSection(frag, apiClient, user, item);\n        }\n    }\n\n    function getRequirePromise(deps) {\n        return new Promise(function (resolve, reject) {\n            require(deps, resolve);\n        });\n    }\n\n    function loadLibraryTiles(elem, apiClient, user, userSettings, shape, userViews, allSections) {\n        var html = '';\n        if (userViews.length) {\n            html += '<h2 class=\"sectionTitle sectionTitle-cards padded-left\">' + globalize.translate('HeaderMyMedia') + '</h2>';\n            if (enableScrollX()) {\n                html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n                html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\">';\n            } else {\n                html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right focuscontainer-x vertical-wrap\">';\n            }\n\n            html += cardBuilder.getCardsHtml({\n                items: userViews,\n                shape: getThumbShape(),\n                showTitle: true,\n                centerText: true,\n                overlayText: false,\n                lazy: true,\n                transition: false,\n                allowBottomPadding: !enableScrollX()\n            });\n\n            if (enableScrollX()) {\n                html += '</div>';\n            }\n            html += '</div>';\n        }\n\n        elem.innerHTML = html;\n        imageLoader.lazyChildren(elem);\n    }\n\n    function getContinueWatchingFetchFn(serverId) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            var screenWidth = dom.getWindowSize().innerWidth;\n\n            var limit;\n            if (enableScrollX()) {\n                limit = 12;\n            } else {\n                limit = screenWidth >= 1920 ? 8 : (screenWidth >= 1600 ? 8 : (screenWidth >= 1200 ? 9 : 6));\n                limit = Math.min(limit, 5);\n            }\n\n            var options = {\n                Limit: limit,\n                Recursive: true,\n                Fields: 'PrimaryImageAspectRatio,BasicSyncInfo',\n                ImageTypeLimit: 1,\n                EnableImageTypes: 'Primary,Backdrop,Thumb',\n                EnableTotalRecordCount: false,\n                MediaTypes: 'Video'\n            };\n\n            return apiClient.getResumableItems(apiClient.getCurrentUserId(), options);\n        };\n    }\n\n    function getContinueWatchingItemsHtml(items) {\n        var cardLayout = false;\n        return cardBuilder.getCardsHtml({\n            items: items,\n            preferThumb: true,\n            shape: getThumbShape(),\n            overlayText: false,\n            showTitle: true,\n            showParentTitle: true,\n            lazy: true,\n            showDetailsMenu: true,\n            overlayPlayButton: true,\n            context: 'home',\n            centerText: !cardLayout,\n            allowBottomPadding: false,\n            cardLayout: cardLayout,\n            showYear: true,\n            lines: 2\n        });\n    }\n\n    function loadResumeVideo(elem, apiClient, userId) {\n        var html = '';\n\n        html += '<h2 class=\"sectionTitle sectionTitle-cards padded-left\">' + globalize.translate('HeaderContinueWatching') + '</h2>';\n        if (enableScrollX()) {\n            html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\" data-monitor=\"videoplayback,markplayed\">';\n        } else {\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\" data-monitor=\"videoplayback,markplayed\">';\n        }\n\n        if (enableScrollX()) {\n            html += '</div>';\n        }\n        html += '</div>';\n\n        elem.classList.add('hide');\n        elem.innerHTML = html;\n\n        var itemsContainer = elem.querySelector('.itemsContainer');\n        itemsContainer.fetchData = getContinueWatchingFetchFn(apiClient.serverId());\n        itemsContainer.getItemsHtml = getContinueWatchingItemsHtml;\n        itemsContainer.parentContainer = elem;\n    }\n\n    function getContinueListeningFetchFn(serverId) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            var screenWidth = dom.getWindowSize().innerWidth;\n\n            var limit;\n            if (enableScrollX()) {\n                limit = 12;\n            } else {\n                limit = screenWidth >= 1920 ? 8 : (screenWidth >= 1600 ? 8 : (screenWidth >= 1200 ? 9 : 6));\n                limit = Math.min(limit, 5);\n            }\n\n            var options = {\n                Limit: limit,\n                Recursive: true,\n                Fields: 'PrimaryImageAspectRatio,BasicSyncInfo',\n                ImageTypeLimit: 1,\n                EnableImageTypes: 'Primary,Backdrop,Thumb',\n                EnableTotalRecordCount: false,\n                MediaTypes: 'Audio'\n            };\n\n            return apiClient.getResumableItems(apiClient.getCurrentUserId(), options);\n        };\n    }\n\n    function getContinueListeningItemsHtml(items) {\n        var cardLayout = false;\n        return cardBuilder.getCardsHtml({\n            items: items,\n            preferThumb: true,\n            shape: getThumbShape(),\n            overlayText: false,\n            showTitle: true,\n            showParentTitle: true,\n            lazy: true,\n            showDetailsMenu: true,\n            overlayPlayButton: true,\n            context: 'home',\n            centerText: !cardLayout,\n            allowBottomPadding: false,\n            cardLayout: cardLayout,\n            showYear: true,\n            lines: 2\n        });\n    }\n\n    function loadResumeAudio(elem, apiClient, userId) {\n        var html = '';\n\n        html += '<h2 class=\"sectionTitle sectionTitle-cards padded-left\">' + globalize.translate('HeaderContinueWatching') + '</h2>';\n        if (enableScrollX()) {\n            html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\" data-monitor=\"audioplayback,markplayed\">';\n        } else {\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\" data-monitor=\"audioplayback,markplayed\">';\n        }\n\n        if (enableScrollX()) {\n            html += '</div>';\n        }\n        html += '</div>';\n\n        elem.classList.add('hide');\n        elem.innerHTML = html;\n\n        var itemsContainer = elem.querySelector('.itemsContainer');\n        itemsContainer.fetchData = getContinueListeningFetchFn(apiClient.serverId());\n        itemsContainer.getItemsHtml = getContinueListeningItemsHtml;\n        itemsContainer.parentContainer = elem;\n    }\n\n    function getOnNowFetchFn(serverId) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            return apiClient.getLiveTvRecommendedPrograms({\n                userId: apiClient.getCurrentUserId(),\n                IsAiring: true,\n                limit: 24,\n                ImageTypeLimit: 1,\n                EnableImageTypes: 'Primary,Thumb,Backdrop',\n                EnableTotalRecordCount: false,\n                Fields: 'ChannelInfo,PrimaryImageAspectRatio'\n            });\n        };\n    }\n\n    function getOnNowItemsHtml(items) {\n        var cardLayout = false;\n        return cardBuilder.getCardsHtml({\n            items: items,\n            preferThumb: 'auto',\n            inheritThumb: false,\n            shape: (enableScrollX() ? 'autooverflow' : 'auto'),\n            showParentTitleOrTitle: true,\n            showTitle: true,\n            centerText: true,\n            coverImage: true,\n            overlayText: false,\n            allowBottomPadding: !enableScrollX(),\n            showAirTime: true,\n            showChannelName: false,\n            showAirDateTime: false,\n            showAirEndTime: true,\n            defaultShape: getThumbShape(),\n            lines: 3,\n            overlayPlayButton: true\n        });\n    }\n\n    function loadOnNow(elem, apiClient, user) {\n        if (!user.Policy.EnableLiveTvAccess) {\n            return Promise.resolve();\n        }\n\n        var userId = user.Id;\n        return apiClient.getLiveTvRecommendedPrograms({\n            userId: apiClient.getCurrentUserId(),\n            IsAiring: true,\n            limit: 1,\n            ImageTypeLimit: 1,\n            EnableImageTypes: 'Primary,Thumb,Backdrop',\n            EnableTotalRecordCount: false,\n            Fields: 'ChannelInfo,PrimaryImageAspectRatio'\n        }).then(function (result) {\n            var html = '';\n            if (result.Items.length) {\n                elem.classList.remove('padded-left');\n                elem.classList.remove('padded-right');\n                elem.classList.remove('padded-bottom');\n                elem.classList.remove('verticalSection');\n\n                html += '<div class=\"verticalSection\">';\n                html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards padded-left\">';\n                html += '<h2 class=\"sectionTitle sectionTitle-cards\">' + globalize.translate('LiveTV') + '</h2>';\n                html += '</div>';\n\n                if (enableScrollX()) {\n                    html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\" data-scrollbuttons=\"false\">';\n                    html += '<div class=\"padded-top padded-bottom scrollSlider focuscontainer-x\">';\n                } else {\n                    html += '<div class=\"padded-top padded-bottom focuscontainer-x\">';\n                }\n\n                html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('livetv', {\n                    serverId: apiClient.serverId(),\n                    section: 'programs'\n                }) + '\" class=\"raised\"><span>' + globalize.translate('Programs') + '</span></a>';\n\n                html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('livetv', {\n                    serverId: apiClient.serverId(),\n                    section: 'guide'\n                }) + '\" class=\"raised\"><span>' + globalize.translate('Guide') + '</span></a>';\n\n                html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('recordedtv', {\n                    serverId: apiClient.serverId()\n                }) + '\" class=\"raised\"><span>' + globalize.translate('Recordings') + '</span></a>';\n\n                html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('livetv', {\n                    serverId: apiClient.serverId(),\n                    section: 'dvrschedule'\n                }) + '\" class=\"raised\"><span>' + globalize.translate('Schedule') + '</span></a>';\n\n                html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('livetv', {\n                    serverId: apiClient.serverId(),\n                    section: 'seriesrecording'\n                }) + '\" class=\"raised\"><span>' + globalize.translate('Series') + '</span></a>';\n\n                html += '</div>';\n                if (enableScrollX()) {\n                    html += '</div>';\n                }\n                html += '</div>';\n                html += '</div>';\n\n                html += '<div class=\"verticalSection\">';\n                html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards padded-left\">';\n\n                if (!layoutManager.tv) {\n                    html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('livetv', {\n                        serverId: apiClient.serverId(),\n                        section: 'onnow'\n                    }) + '\" class=\"more button-flat button-flat-mini sectionTitleTextButton\">';\n                    html += '<h2 class=\"sectionTitle sectionTitle-cards\">';\n                    html += globalize.translate('HeaderOnNow');\n                    html += '</h2>';\n                    html += '<span class=\"material-icons chevron_right\"></span>';\n                    html += '</a>';\n\n                } else {\n                    html += '<h2 class=\"sectionTitle sectionTitle-cards\">' + globalize.translate('HeaderOnNow') + '</h2>';\n                }\n                html += '</div>';\n\n                if (enableScrollX()) {\n                    html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n                    html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\">';\n                } else {\n                    html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\">';\n                }\n\n                if (enableScrollX()) {\n                    html += '</div>';\n                }\n\n                html += '</div>';\n                html += '</div>';\n\n                elem.innerHTML = html;\n\n                var itemsContainer = elem.querySelector('.itemsContainer');\n                itemsContainer.parentContainer = elem;\n                itemsContainer.fetchData = getOnNowFetchFn(apiClient.serverId());\n                itemsContainer.getItemsHtml = getOnNowItemsHtml;\n            }\n        });\n    }\n\n    function getNextUpFetchFn(serverId) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            return apiClient.getNextUpEpisodes({\n                Limit: enableScrollX() ? 24 : 15,\n                Fields: 'PrimaryImageAspectRatio,SeriesInfo,DateCreated,BasicSyncInfo,Path',\n                UserId: apiClient.getCurrentUserId(),\n                ImageTypeLimit: 1,\n                EnableImageTypes: 'Primary,Backdrop,Banner,Thumb',\n                EnableTotalRecordCount: false\n            });\n        };\n    }\n\n    function getNextUpItemsHtml(items) {\n        var cardLayout = false;\n        return cardBuilder.getCardsHtml({\n            items: items,\n            preferThumb: true,\n            shape: getThumbShape(),\n            overlayText: false,\n            showTitle: true,\n            showParentTitle: true,\n            lazy: true,\n            overlayPlayButton: true,\n            context: 'home',\n            centerText: !cardLayout,\n            allowBottomPadding: !enableScrollX(),\n            cardLayout: cardLayout\n        });\n    }\n\n    function loadNextUp(elem, apiClient, userId) {\n        var html = '';\n\n        html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards padded-left\">';\n        if (!layoutManager.tv) {\n            html += '<a is=\"emby-linkbutton\" href=\"' + appRouter.getRouteUrl('nextup', {\n                serverId: apiClient.serverId()\n            }) + '\" class=\"button-flat button-flat-mini sectionTitleTextButton\">';\n            html += '<h2 class=\"sectionTitle sectionTitle-cards\">';\n            html += globalize.translate('HeaderNextUp');\n            html += '</h2>';\n            html += '<span class=\"material-icons chevron_right\"></span>';\n            html += '</a>';\n        } else {\n            html += '<h2 class=\"sectionTitle sectionTitle-cards\">' + globalize.translate('HeaderNextUp') + '</h2>';\n        }\n        html += '</div>';\n\n        if (enableScrollX()) {\n            html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\" data-monitor=\"videoplayback,markplayed\">';\n        } else {\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\" data-monitor=\"videoplayback,markplayed\">';\n        }\n\n        if (enableScrollX()) {\n            html += '</div>';\n        }\n        html += '</div>';\n\n        elem.classList.add('hide');\n        elem.innerHTML = html;\n\n        var itemsContainer = elem.querySelector('.itemsContainer');\n        itemsContainer.fetchData = getNextUpFetchFn(apiClient.serverId());\n        itemsContainer.getItemsHtml = getNextUpItemsHtml;\n        itemsContainer.parentContainer = elem;\n    }\n\n    function getLatestRecordingsFetchFn(serverId, activeRecordingsOnly) {\n        return function () {\n            var apiClient = connectionManager.getApiClient(serverId);\n            return apiClient.getLiveTvRecordings({\n                userId: apiClient.getCurrentUserId(),\n                Limit: enableScrollX() ? 12 : 5,\n                Fields: 'PrimaryImageAspectRatio,BasicSyncInfo',\n                EnableTotalRecordCount: false,\n                IsLibraryItem: activeRecordingsOnly ? null : false,\n                IsInProgress: activeRecordingsOnly ? true : null\n            });\n        };\n    }\n\n    function getLatestRecordingItemsHtml(activeRecordingsOnly) {\n        return function (items) {\n            var cardLayout = false;\n            return cardBuilder.getCardsHtml({\n                items: items,\n                shape: enableScrollX() ? 'autooverflow' : 'auto',\n                showTitle: true,\n                showParentTitle: true,\n                coverImage: true,\n                lazy: true,\n                showDetailsMenu: true,\n                centerText: true,\n                overlayText: false,\n                showYear: true,\n                lines: 2,\n                overlayPlayButton: !activeRecordingsOnly,\n                allowBottomPadding: !enableScrollX(),\n                preferThumb: true,\n                cardLayout: false,\n                overlayMoreButton: activeRecordingsOnly,\n                action: activeRecordingsOnly ? 'none' : null,\n                centerPlayButton: activeRecordingsOnly\n            });\n        };\n    }\n\n    function loadLatestLiveTvRecordings(elem, activeRecordingsOnly, apiClient, userId) {\n        var title = activeRecordingsOnly ?\n            globalize.translate('HeaderActiveRecordings') :\n            globalize.translate('HeaderLatestRecordings');\n\n        var html = '';\n\n        html += '<div class=\"sectionTitleContainer sectionTitleContainer-cards\">';\n        html += '<h2 class=\"sectionTitle sectionTitle-cards padded-left\">' + title + '</h2>';\n        html += '</div>';\n\n        if (enableScrollX()) {\n            html += '<div is=\"emby-scroller\" class=\"padded-top-focusscale padded-bottom-focusscale\" data-centerfocus=\"true\">';\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer scrollSlider focuscontainer-x\">';\n        } else {\n            html += '<div is=\"emby-itemscontainer\" class=\"itemsContainer padded-left padded-right vertical-wrap focuscontainer-x\">';\n        }\n\n        if (enableScrollX()) {\n            html += '</div>';\n        }\n        html += '</div>';\n\n        elem.classList.add('hide');\n        elem.innerHTML = html;\n\n        var itemsContainer = elem.querySelector('.itemsContainer');\n        itemsContainer.fetchData = getLatestRecordingsFetchFn(apiClient.serverId(), activeRecordingsOnly);\n        itemsContainer.getItemsHtml = getLatestRecordingItemsHtml(activeRecordingsOnly);\n        itemsContainer.parentContainer = elem;\n    }\n\n    return {\n        loadLibraryTiles: loadLibraryTiles,\n        getDefaultSection: getDefaultSection,\n        loadSections: loadSections,\n        destroySections: destroySections,\n        pause: pause,\n        resume: resume\n    };\n});\n", "idx": 29, "id": 16571, "msg": "", "proj": "jellyfin-jellyfin-web", "lang": "js"}
{"patch": "@@ -13,6 +13,9 @@ use Shopsys\\FrameworkBundle\\Model\\Product\\Exception\\ProductNotFoundException;\n use Shopsys\\FrameworkBundle\\Model\\Product\\Product;\n use Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade;\n \n+/**\n+ * @deprecated Use ProductByUuidResolver instead\n+ */\n class ProductResolver implements ResolverInterface, AliasedInterface\n {\n     /**", "y": 1, "oldf": "<?php\n\ndeclare(strict_types=1);\n\nnamespace Shopsys\\FrontendApiBundle\\Model\\Resolver\\Products;\n\nuse Overblog\\GraphQLBundle\\Definition\\Resolver\\AliasedInterface;\nuse Overblog\\GraphQLBundle\\Definition\\Resolver\\ResolverInterface;\nuse Overblog\\GraphQLBundle\\Error\\UserError;\nuse Ramsey\\Uuid\\Uuid;\nuse Shopsys\\FrameworkBundle\\Component\\Domain\\Domain;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Exception\\ProductNotFoundException;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\Product;\nuse Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade;\n\nclass ProductResolver implements ResolverInterface, AliasedInterface\n{\n    /**\n     * @var \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade\n     */\n    protected $productFacade;\n\n    /**\n     * @var \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain\n     */\n    protected $domain;\n\n    /**\n     * @param \\Shopsys\\FrameworkBundle\\Model\\Product\\ProductFacade $productFacade\n     * @param \\Shopsys\\FrameworkBundle\\Component\\Domain\\Domain $domain\n     */\n    public function __construct(\n        ProductFacade $productFacade,\n        Domain $domain\n    ) {\n        $this->productFacade = $productFacade;\n        $this->domain = $domain;\n    }\n\n    /**\n     * @param string $uuid\n     * @return \\Shopsys\\FrameworkBundle\\Model\\Product\\Product\n     */\n    public function resolver(string $uuid): Product\n    {\n        if (Uuid::isValid($uuid) === false) {\n            throw new UserError('Provided argument is not valid UUID.');\n        }\n\n        try {\n            return $this->productFacade->getByUuid($uuid);\n        } catch (ProductNotFoundException $productNotFoundException) {\n            throw new UserError($productNotFoundException->getMessage());\n        }\n    }\n\n    /**\n     * @return string[]\n     */\n    public static function getAliases(): array\n    {\n        return [\n            'resolver' => 'product',\n        ];\n    }\n}\n", "idx": 1, "id": 23666, "msg": "I am missing reason why is this class renamed. It is just obout naming or it has some other reason I do not see now?", "proj": "shopsys-shopsys", "lang": "php"}
{"patch": "@@ -322,26 +322,8 @@ def push(package):\n         ), default=encode_node)\n     )\n \n-    dataset = response.json()\n-    upload_urls = dataset['upload_urls']\n-\n-    headers = {\n-        'Content-Encoding': 'gzip'\n-    }\n-\n-    total = len(upload_urls)\n-    for idx, (objhash, url) in enumerate(iteritems(upload_urls)):\n-        # Create a temporary gzip'ed file.\n-        print(\"Uploading %s (%d/%d)...\" % (objhash, idx + 1, total))\n-        with pkgobj.tempfile(objhash) as temp_file:\n-            with FileWithReadProgress(temp_file) as temp_file_with_progress:\n-                response = requests.put(url, data=temp_file_with_progress, headers=headers)\n-                if not response.ok:\n-                    raise CommandException(\"Upload failed: error %s\" % response.status_code)\n-\n     print(\"Updating the 'latest' tag...\")\n-    # Set the \"latest\" tag.\n-    response = session.put(\n+    session.put(\n         \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n             url=QUILT_PKG_URL,\n             owner=owner,", "y": 0, "oldf": "# -*- coding: utf-8 -*-\n\"\"\"\nCommand line parsing and command dispatch\n\"\"\"\n\nfrom __future__ import print_function\nfrom builtins import input\nfrom datetime import datetime\nimport json\nimport os\nimport stat\nimport time\nimport webbrowser\n\nfrom packaging.version import Version\nimport pandas as pd\nimport pkg_resources\nimport requests\nfrom six import iteritems, string_types\nfrom tqdm import tqdm\n\nfrom .build import build_package, generate_build_file, BuildException\nfrom .const import DEFAULT_BUILDFILE, LATEST_TAG\nfrom .core import (hash_contents, GroupNode, TableNode, FileNode, PackageFormat,\n                   decode_node, encode_node)\nfrom .hashing import digest_file\nfrom .store import PackageStore\nfrom .util import BASE_DIR, FileWithReadProgress\n\nfrom .. import data\n\nDEFAULT_QUILT_PKG_URL = 'https://pkg.quiltdata.com'\nQUILT_PKG_URL = os.environ.get('QUILT_PKG_URL', DEFAULT_QUILT_PKG_URL)\n\nif QUILT_PKG_URL == DEFAULT_QUILT_PKG_URL:\n    AUTH_FILE_NAME = \"auth.json\"\nelse:\n    # Store different servers' auth in different files.\n    import hashlib\n    AUTH_FILE_NAME = \"auth-%.8s.json\" % hashlib.md5(QUILT_PKG_URL.encode('utf-8')).hexdigest()\n\nCHUNK_SIZE = 4096\n\nVERSION = pkg_resources.require('quilt')[0].version\n\nclass CommandException(Exception):\n    \"\"\"\n    Exception class for all command-related failures.\n    \"\"\"\n    pass\n\n\ndef _update_auth(refresh_token):\n    response = requests.post(\"%s/api/token\" % QUILT_PKG_URL, data=dict(\n        refresh_token=refresh_token\n    ))\n\n    if response.status_code != requests.codes.ok:\n        raise CommandException(\"Authentication error: %s\" % response.status_code)\n\n    data = response.json()\n    error = data.get('error')\n    if error is not None:\n        raise CommandException(\"Failed to log in: %s\" % error)\n\n    return dict(\n        refresh_token=data['refresh_token'],\n        access_token=data['access_token'],\n        expires_at=data['expires_at']\n    )\n\ndef _save_auth(auth):\n    if not os.path.exists(BASE_DIR):\n        os.makedirs(BASE_DIR)\n\n    file_path = os.path.join(BASE_DIR, AUTH_FILE_NAME)\n    with open(file_path, 'w') as fd:\n        os.chmod(file_path, stat.S_IRUSR | stat.S_IWUSR)\n        json.dump(auth, fd)\n\ndef _handle_response(resp, **kwargs):\n    if resp.status_code == requests.codes.unauthorized:\n        raise CommandException(\"Authentication failed. Run `quilt login` again.\")\n    elif not resp.ok:\n        try:\n            data = resp.json()\n            raise CommandException(data['message'])\n        except ValueError:\n            raise CommandException(\"Unexpected failure: error %s\" % resp.status_code)\n\ndef _create_session():\n    \"\"\"\n    Creates a session object to be used for `push`, `install`, etc.\n\n    It reads the credentials, possibly gets an updated access token,\n    and sets the request headers.\n    \"\"\"\n    file_path = os.path.join(BASE_DIR, AUTH_FILE_NAME)\n    if os.path.exists(file_path):\n        with open(file_path) as fd:\n            auth = json.load(fd)\n\n        # If the access token expires within a minute, update it.\n        if auth['expires_at'] < time.time() + 60:\n            try:\n                auth = _update_auth(auth['refresh_token'])\n            except CommandException as ex:\n                raise CommandException(\n                    \"Failed to update the access token (%s). Run `quilt login` again.\" % ex\n                )\n            _save_auth(auth)\n    else:\n        # The auth file doesn't exist, probably because the\n        # user hasn't run quilt login yet.\n        auth = None\n\n    session = requests.Session()\n    session.hooks.update(dict(\n        response=_handle_response\n    ))\n    session.headers.update({\n        \"Content-Type\": \"application/json\",\n        \"Accept\": \"application/json\",\n        \"User-Agent\": \"quilt-cli/%s\" % VERSION,\n    })\n    if auth is not None:\n        session.headers[\"Authorization\"] = \"Bearer %s\" % auth['access_token']\n\n    return session\n\n_session = None\n\ndef _get_session():\n    \"\"\"\n    Creates a session or returns an existing session.\n    \"\"\"\n    global _session\n    if _session is None:\n        _session = _create_session()\n\n    return _session\n\ndef _parse_package(name):\n    try:\n        owner, pkg = name.split('/')\n        if not owner or not pkg:\n            # Make sure they're not empty.\n            raise ValueError\n    except ValueError:\n        raise CommandException(\"Specify package as owner/package_name.\")\n    return owner, pkg\n\ndef login():\n    \"\"\"\n    Authenticate.\n    \"\"\"\n    login_url = \"%s/login\" % QUILT_PKG_URL\n\n    print(\"Launching a web browser...\")\n    print(\"If that didn't work, please visit the following URL: %s\" % login_url)\n\n    # Open the browser. Get rid of stdout while launching the browser to prevent\n    # Chrome/Firefox from outputing garbage over the code prompt.\n    devnull = os.open(os.devnull, os.O_RDWR)\n    old_stdout = os.dup(1)\n    os.dup2(devnull, 1)\n    try:\n        webbrowser.open(login_url)\n    finally:\n        os.close(devnull)\n        os.dup2(old_stdout, 1)\n        os.close(old_stdout)\n\n    print()\n    refresh_token = input(\"Enter the code from the webpage: \")\n\n    # Get an access token (and a new refresh token).\n    # Technically, we could have the user enter both tokens - but it doesn't\n    # really matter, and this lets us verify that the token actually works.\n    auth = _update_auth(refresh_token)\n\n    _save_auth(auth)\n\ndef logout():\n    \"\"\"\n    Become anonymous. Useful for testing.\n    \"\"\"\n    auth_file = os.path.join(BASE_DIR, AUTH_FILE_NAME)\n    # TODO revoke refresh token (without logging out of web sessions)\n    if os.path.exists(auth_file):\n        os.remove(auth_file)\n    else:\n        print(\"Already logged out.\")\n\ndef generate(directory):\n    \"\"\"\n    Generate a build-file for quilt build from a directory of\n    source files.\n    \"\"\"\n    try:\n        buildfilepath = generate_build_file(directory)\n    except BuildException as builderror:\n        raise CommandException(str(builderror))\n\n    print(\"Generated build-file %s.\" % (buildfilepath))\n\ndef build(package, path_or_node):\n    \"\"\"\n    Compile a Quilt data package, either from a build file or an existing package node.\n    \"\"\"\n    if isinstance(path_or_node, data.PackageNode):\n        build_from_node(package, path_or_node)\n    elif isinstance(path_or_node, string_types):\n        build_from_path(package, path_or_node)\n    else:\n        raise ValueError(\"Expected a PackageNode or a path, but got %r\" % path_or_node)\n\ndef build_from_node(package, node):\n    \"\"\"\n    Compile a Quilt data package from an existing package node.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n\n    store = node._package.get_store()\n    package_obj = store.create_package(owner, pkg)\n\n    def _process_node(node, path=''):\n        if isinstance(node, data.GroupNode):\n            for key, child in node._items():\n                _process_node(child, path + '/' + key)\n        elif isinstance(node, data.DataNode):\n            core_node = node._node\n            metadata = core_node.metadata or {}\n            if isinstance(core_node, TableNode):\n                df = node.data()\n                package_obj.save_df(df, path, metadata.get('q_path'), metadata.get('q_ext'),\n                                    'pandas', PackageFormat.default)\n            elif isinstance(core_node, FileNode):\n                src_path = node.data()\n                package_obj.save_file(src_path, path, metadata.get('q_path'))\n            else:\n                assert False, \"Unexpected core node type: %r\" % core_node\n        else:\n            assert False, \"Unexpected node type: %r\" % node\n\n    _process_node(node)\n    package_obj.save_contents()\n\ndef build_from_path(package, path):\n    \"\"\"\n    Compile a Quilt data package from a build file.\n    Path can be a directory, in which case the build file will be generated automatically.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n\n    if not os.path.exists(path):\n        raise CommandException(\"%s does not exist.\" % path)\n\n    if os.path.isdir(path):\n        buildpath = os.path.join(path, DEFAULT_BUILDFILE)\n        if not os.path.exists(buildpath):\n            try:\n                generated_buildfile = generate_build_file(path)\n                assert generated_buildfile == buildpath\n            except BuildException as builderror:\n                raise CommandException(str(builderror))\n    else:\n        buildpath = path\n\n    try:\n        build_package(owner, pkg, buildpath)\n        print(\"Built %s/%s successfully.\" % (owner, pkg))\n    except BuildException as ex:\n        raise CommandException(\"Failed to build the package: %s\" % ex)\n\ndef log(package):\n    \"\"\"\n    List all of the changes to a package on the server.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    response = session.get(\n        \"{url}/api/log/{owner}/{pkg}/\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    format_str = \"%-64s %-19s %s\"\n\n    print(format_str % (\"Hash\", \"Pushed\", \"Author\"))\n    for entry in reversed(response.json()['logs']):\n        ugly = datetime.fromtimestamp(entry['created'])\n        nice = ugly.strftime(\"%Y-%m-%d %H:%M:%S\")\n        print(format_str % (entry['hash'], nice, entry['author']))\n\ndef push(package):\n    \"\"\"\n    Push a Quilt data package to the server\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    pkgobj = PackageStore.find_package(owner, pkg)\n    if pkgobj is None:\n        raise CommandException(\"Package {owner}/{pkg} not found.\".format(owner=owner, pkg=pkg))\n    pkghash = pkgobj.get_hash()\n\n    print(\"Uploading package metadata...\")\n    response = session.put(\n        \"{url}/api/package/{owner}/{pkg}/{hash}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            hash=pkghash\n        ),\n        data=json.dumps(dict(\n            contents=pkgobj.get_contents(),\n            description=\"\"  # TODO\n        ), default=encode_node)\n    )\n\n    dataset = response.json()\n    upload_urls = dataset['upload_urls']\n\n    headers = {\n        'Content-Encoding': 'gzip'\n    }\n\n    total = len(upload_urls)\n    for idx, (objhash, url) in enumerate(iteritems(upload_urls)):\n        # Create a temporary gzip'ed file.\n        print(\"Uploading %s (%d/%d)...\" % (objhash, idx + 1, total))\n        with pkgobj.tempfile(objhash) as temp_file:\n            with FileWithReadProgress(temp_file) as temp_file_with_progress:\n                response = requests.put(url, data=temp_file_with_progress, headers=headers)\n                if not response.ok:\n                    raise CommandException(\"Upload failed: error %s\" % response.status_code)\n\n    print(\"Updating the 'latest' tag...\")\n    # Set the \"latest\" tag.\n    response = session.put(\n        \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            tag=LATEST_TAG\n        ),\n        data=json.dumps(dict(\n            hash=pkghash\n        ))\n    )\n    assert response.ok # other responses handled by _handle_response\n\n    url = \"https://quiltdata.com/package/%s/%s\" % (owner, pkg)\n    print(\"Push complete. Your package is live:\\n%s\" % url)\n\ndef version_list(package):\n    \"\"\"\n    List the versions of a package.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    response = session.get(\n        \"{url}/api/version/{owner}/{pkg}/\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    for version in response.json()['versions']:\n        print(\"%s: %s\" % (version['version'], version['hash']))\n\ndef version_add(package, version, pkghash):\n    \"\"\"\n    Add a new version for a given package hash.\n\n    Version format needs to follow PEP 440.\n    Versions are permanent - once created, they cannot be modified or deleted.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    try:\n        Version(version)\n    except ValueError:\n        url = \"https://www.python.org/dev/peps/pep-0440/#examples-of-compliant-version-schemes\"\n        raise CommandException(\n            \"Invalid version format; see %s\" % url\n        )\n\n    answer = input(\"Versions cannot be modified or deleted; are you sure? (y/n) \")\n    if answer.lower() != 'y':\n        return\n\n    session.put(\n        \"{url}/api/version/{owner}/{pkg}/{version}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            version=version\n        ),\n        data=json.dumps(dict(\n            hash=pkghash\n        ))\n    )\n\ndef tag_list(package):\n    \"\"\"\n    List the tags of a package.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    response = session.get(\n        \"{url}/api/tag/{owner}/{pkg}/\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg\n        )\n    )\n\n    for tag in response.json()['tags']:\n        print(\"%s: %s\" % (tag['tag'], tag['hash']))\n\ndef tag_add(package, tag, pkghash):\n    \"\"\"\n    Add a new tag for a given package hash.\n\n    Unlike versions, tags can have an arbitrary format, and can be modified\n    and deleted.\n\n    When a package is pushed, it gets the \"latest\" tag.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    session.put(\n        \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            tag=tag\n        ),\n        data=json.dumps(dict(\n            hash=pkghash\n        ))\n    )\n\ndef tag_remove(package, tag):\n    \"\"\"\n    Delete a tag.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    session.delete(\n        \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            tag=tag\n        )\n    )\n\ndef install(package, hash=None, version=None, tag=None, force=False):\n    \"\"\"\n    Download a Quilt data package from the server and install locally.\n\n    At most one of `hash`, `version`, or `tag` can be given. If none are\n    given, `tag` defaults to \"latest\".\n    \"\"\"\n    if hash is version is tag is None:\n        tag = LATEST_TAG\n\n    assert [hash, version, tag].count(None) == 2\n\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n    store = PackageStore()\n    existing_pkg = store.get_package(owner, pkg)\n\n    if existing_pkg is not None and not force:\n        print(\"{owner}/{pkg} already installed.\".format(owner=owner, pkg=pkg))\n        overwrite = input(\"Overwrite? (y/n) \")\n        if overwrite.lower() != 'y':\n            return\n\n    if version is not None:\n        response = session.get(\n            \"{url}/api/version/{owner}/{pkg}/{version}\".format(\n                url=QUILT_PKG_URL,\n                owner=owner,\n                pkg=pkg,\n                version=version\n            )\n        )\n        pkghash = response.json()['hash']\n    elif tag is not None:\n        response = session.get(\n            \"{url}/api/tag/{owner}/{pkg}/{tag}\".format(\n                url=QUILT_PKG_URL,\n                owner=owner,\n                pkg=pkg,\n                tag=tag\n            )\n        )\n        pkghash = response.json()['hash']\n    else:\n        pkghash = hash\n    assert pkghash is not None\n\n    response = session.get(\n        \"{url}/api/package/{owner}/{pkg}/{hash}\".format(\n            url=QUILT_PKG_URL,\n            owner=owner,\n            pkg=pkg,\n            hash=pkghash\n        )\n    )\n    assert response.ok # other responses handled by _handle_response\n\n    dataset = response.json(object_hook=decode_node)\n    response_urls = dataset['urls']\n    response_contents = dataset['contents']\n\n    # Verify contents hash\n    if pkghash != hash_contents(response_contents):\n        raise CommandException(\"Mismatched hash. Try again.\")\n\n    pkgobj = store.install_package(owner, pkg, response_contents)\n\n    total = len(response_urls)\n    for idx, (download_hash, url) in enumerate(sorted(iteritems(response_urls))):\n        print(\"Downloading %s (%d/%d)...\" % (download_hash, idx + 1, total))\n\n        local_filename = store.object_path(download_hash)\n        if os.path.exists(local_filename):\n            file_hash = digest_file(local_filename)\n            if file_hash == download_hash:\n                print(\"Fragment already installed; skipping.\")\n                continue\n            else:\n                print(\"Fragment already installed, but has the wrong hash (%s); re-downloading.\" %\n                      file_hash)\n\n        response = requests.get(url, stream=True)\n        if not response.ok:\n            msg = \"Download {hash} failed: error {code}\"\n            raise CommandException(msg.format(hash=download_hash, code=response.status_code))\n\n        length_remaining = response.raw.length_remaining\n\n        temp_path = store.temporary_object_path(download_hash)\n        with open(temp_path, 'wb') as output_file:\n            with tqdm(total=length_remaining, unit='B', unit_scale=True) as progress:\n                # `requests` will automatically un-gzip the content, as long as\n                # the 'Content-Encoding: gzip' header is set.\n                # To report progress, however, we need the length of the original compressed data;\n                # we use the undocumented but technically public `response.raw.length_remaining`.\n                for chunk in response.iter_content(chunk_size=CHUNK_SIZE):\n                    if chunk: # filter out keep-alive new chunks\n                        output_file.write(chunk)\n                    if response.raw.length_remaining is not None:  # Not set in unit tests.\n                        progress.update(length_remaining - response.raw.length_remaining)\n                        length_remaining = response.raw.length_remaining\n\n        file_hash = digest_file(temp_path)\n        if file_hash != download_hash:\n            os.remove(temp_path)\n            raise CommandException(\"Fragment hashes do not match: expected %s, got %s.\" %\n                                   (download_hash, file_hash))\n\n        os.rename(temp_path, local_filename)\n\n    pkgobj.save_contents()\n\ndef access_list(package):\n    \"\"\"\n    Print list of users who can access a package.\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    lookup_url = \"{url}/api/access/{owner}/{pkg}\".format(url=QUILT_PKG_URL, owner=owner, pkg=pkg)\n    response = session.get(lookup_url)\n\n    data = response.json()\n    users = data['users']\n\n    print('\\n'.join(users))\n\ndef access_add(package, user):\n    \"\"\"\n    Add access\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    session.put(\"%s/api/access/%s/%s/%s\" % (QUILT_PKG_URL, owner, pkg, user))\n\ndef access_remove(package, user):\n    \"\"\"\n    Remove access\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    session = _get_session()\n\n    session.delete(\"%s/api/access/%s/%s/%s\" % (QUILT_PKG_URL, owner, pkg, user))\n\ndef ls():\n    \"\"\"\n    List all installed Quilt data packages\n    \"\"\"\n    for pkg_dir in PackageStore.find_store_dirs():\n        print(\"%s\" % pkg_dir)\n        packages = PackageStore(pkg_dir).ls_packages()\n        for idx, (owner, pkg) in enumerate(packages):\n            prefix = u\"\u2514\u2500\u2500 \" if idx == len(packages) - 1 else u\"\u251c\u2500\u2500 \"\n            print(\"%s%s/%s\" % (prefix, owner, pkg))\n\ndef inspect(package):\n    \"\"\"\n    Inspect package details\n    \"\"\"\n    owner, pkg = _parse_package(package)\n    pkgobj = PackageStore.find_package(owner, pkg)\n    if pkgobj is None:\n        raise CommandException(\"Package {owner}/{pkg} not found.\".format(owner=owner, pkg=pkg))\n\n    def _print_children(children, prefix, path):\n        for idx, (name, child) in enumerate(children):\n            if idx == len(children) - 1:\n                new_prefix = u\"\u2514\u2500\"\n                new_child_prefix = u\"  \"\n            else:\n                new_prefix = u\"\u251c\u2500\"\n                new_child_prefix = u\"\u2502 \"\n            _print_node(child, prefix + new_prefix, prefix + new_child_prefix, name, path)\n\n    def _print_node(node, prefix, child_prefix, name, path):\n        name_prefix = u\"\u2500 \"\n        if isinstance(node, GroupNode):\n            children = list(node.children.items())\n            if children:\n                name_prefix = u\"\u252c \"\n            print(prefix + name_prefix + name)\n            _print_children(children, child_prefix, path + name)\n        elif isinstance(node, TableNode):\n            df = pkgobj.get_obj(node)\n            assert isinstance(df, pd.DataFrame)\n            info = \"shape %s, type \\\"%s\\\"\" % (df.shape, df.dtypes)\n            print(prefix + name_prefix + \": \" + info)\n        elif isinstance(node, FileNode):\n            print(prefix + name_prefix + name)\n        else:\n            assert False, \"node=%s type=%s\" % (node, type(node))\n\n    print(pkgobj.get_path())\n    _print_children(children=pkgobj.get_contents().children.items(), prefix='', path='')\n", "idx": 4, "id": 15052, "msg": "", "proj": "quiltdata-quilt", "lang": "py"}
{"patch": "@@ -290,22 +290,16 @@ RoundaboutType RoundaboutHandler::getRoundaboutType(const NodeID nid) const\n     }\n \n     // calculate the radius of the roundabout/rotary. For two coordinates, we assume a minimal\n-    // circle\n-    // with both vertices right at the other side (so half their distance in meters).\n+    // circle with both vertices right at the other side (so half their distance in meters).\n     // Otherwise, we construct a circle through the first tree vertices.\n     const auto getRadius = [&roundabout_nodes, &getCoordinate]() {\n-        auto node_itr = roundabout_nodes.begin();\n-        if (roundabout_nodes.size() == 2)\n-        {\n-            const auto first = getCoordinate(*node_itr++), second = getCoordinate(*node_itr++);\n-            return 0.5 * util::coordinate_calculation::haversineDistance(first, second);\n-        }\n-        else\n-        {\n-            const auto first = getCoordinate(*node_itr++), second = getCoordinate(*node_itr++),\n-                       third = getCoordinate(*node_itr++);\n-            return util::coordinate_calculation::circleRadius(first, second, third);\n-        }\n+        std::vector<util::Coordinate> coordinates;\n+        coordinates.reserve(roundabout_nodes.size());\n+        std::transform(roundabout_nodes.begin(),\n+                       roundabout_nodes.end(),\n+                       std::back_inserter(coordinates),\n+                       getCoordinate);\n+        return util::coordinate_calculation::circleRadius(std::move(coordinates));\n     };\n     const double radius = getRadius();\n ", "y": 1, "oldf": "#include \"extractor/guidance/roundabout_handler.hpp\"\n#include \"extractor/guidance/constants.hpp\"\n#include \"extractor/guidance/toolkit.hpp\"\n\n#include \"util/coordinate_calculation.hpp\"\n#include \"util/guidance/toolkit.hpp\"\n#include \"util/simple_logger.hpp\"\n\n#include <algorithm>\n#include <cmath>\n#include <unordered_set>\n\n#include <boost/assert.hpp>\n\nusing osrm::util::guidance::getTurnDirection;\n\nnamespace osrm\n{\nnamespace extractor\n{\nnamespace guidance\n{\n\nRoundaboutHandler::RoundaboutHandler(const util::NodeBasedDynamicGraph &node_based_graph,\n                                     const std::vector<QueryNode> &node_info_list,\n                                     const CompressedEdgeContainer &compressed_edge_container,\n                                     const util::NameTable &name_table,\n                                     const SuffixTable &street_name_suffix_table)\n    : IntersectionHandler(node_based_graph, node_info_list, name_table, street_name_suffix_table),\n      compressed_edge_container(compressed_edge_container)\n{\n}\n\nbool RoundaboutHandler::canProcess(const NodeID from_nid,\n                                   const EdgeID via_eid,\n                                   const Intersection &intersection) const\n{\n    const auto flags = getRoundaboutFlags(from_nid, via_eid, intersection);\n    if (!flags.on_roundabout && !flags.can_enter)\n        return false;\n\n    const auto roundabout_type = getRoundaboutType(node_based_graph.GetTarget(via_eid));\n    return roundabout_type != RoundaboutType::None;\n}\n\nIntersection RoundaboutHandler::\noperator()(const NodeID from_nid, const EdgeID via_eid, Intersection intersection) const\n{\n    invalidateExitAgainstDirection(from_nid, via_eid, intersection);\n    const auto flags = getRoundaboutFlags(from_nid, via_eid, intersection);\n    const auto roundabout_type = getRoundaboutType(node_based_graph.GetTarget(via_eid));\n    // find the radius of the roundabout\n    return handleRoundabouts(roundabout_type,\n                             via_eid,\n                             flags.on_roundabout,\n                             flags.can_exit_separately,\n                             std::move(intersection));\n}\n\ndetail::RoundaboutFlags RoundaboutHandler::getRoundaboutFlags(\n    const NodeID from_nid, const EdgeID via_eid, const Intersection &intersection) const\n{\n    const auto &in_edge_data = node_based_graph.GetEdgeData(via_eid);\n    bool on_roundabout = in_edge_data.roundabout;\n    bool can_enter_roundabout = false;\n    bool can_exit_roundabout_separately = false;\n    for (const auto &road : intersection)\n    {\n        const auto &edge_data = node_based_graph.GetEdgeData(road.turn.eid);\n        // only check actual outgoing edges\n        if (edge_data.reversed || !road.entry_allowed)\n            continue;\n\n        if (edge_data.roundabout)\n        {\n            can_enter_roundabout = true;\n        }\n        // Exiting roundabouts at an entry point is technically a data-modelling issue.\n        // This workaround handles cases in which an exit follows the entry.\n        // To correctly represent perceived exits, we only count exits leading to a\n        // separate vertex than the one we are coming from that are in the direction of\n        // the roundabout.\n        // The sorting of the angles represents a problem for left-sided driving, though.\n        // FIXME in case of left-sided driving, we have to check whether we can enter the\n        // roundabout later in the cycle, rather than prior.\n        // FIXME requires consideration of crossing the roundabout\n        else if (node_based_graph.GetTarget(road.turn.eid) != from_nid && !can_enter_roundabout)\n        {\n            can_exit_roundabout_separately = true;\n        }\n    }\n    return {on_roundabout, can_enter_roundabout, can_exit_roundabout_separately};\n}\n\nvoid RoundaboutHandler::invalidateExitAgainstDirection(const NodeID from_nid,\n                                                       const EdgeID via_eid,\n                                                       Intersection &intersection) const\n{\n    const auto &in_edge_data = node_based_graph.GetEdgeData(via_eid);\n    if (in_edge_data.roundabout)\n        return;\n\n    bool past_roundabout_angle = false;\n    for (auto &road : intersection)\n    {\n        const auto &edge_data = node_based_graph.GetEdgeData(road.turn.eid);\n        // only check actual outgoing edges\n        if (edge_data.reversed)\n        {\n            // remember whether we have seen the roundabout in-part\n            if (edge_data.roundabout)\n                past_roundabout_angle = true;\n\n            continue;\n        }\n\n        // Exiting roundabouts at an entry point is technically a data-modelling issue.\n        // This workaround handles cases in which an exit precedes and entry. The resulting\n        // u-turn against the roundabout direction is invalidated.\n        // The sorting of the angles represents a problem for left-sided driving, though.\n        // FIXME in case of left-sided driving, we have to check whether we can enter the\n        // roundabout later in the cycle, rather than prior.\n        if (!edge_data.roundabout && node_based_graph.GetTarget(road.turn.eid) != from_nid &&\n            past_roundabout_angle)\n        {\n            road.entry_allowed = false;\n        }\n    }\n}\n\n// If we want to see a roundabout as a turn, the exits have to be distinct enough to be seen a\n// dedicated turns. We are limiting it to four-way intersections with well distinct bearings.\n// All entry/roads and exit roads have to be simple. Not segregated roads.\n// Processing segregated roads would technically require an angle of the turn to be available\n// in postprocessing since we correct the turn-angle in turn-generaion.\nbool RoundaboutHandler::qualifiesAsRoundaboutIntersection(\n    const std::set<NodeID> &roundabout_nodes) const\n{\n    // translate a node ID into its respective coordinate stored in the node_info_list\n    const auto getCoordinate = [this](const NodeID node) {\n        return util::Coordinate(node_info_list[node].lon, node_info_list[node].lat);\n    };\n    const bool has_limited_size = roundabout_nodes.size() <= 4;\n    if (!has_limited_size)\n        return false;\n\n    const bool simple_exits =\n        roundabout_nodes.end() ==\n        std::find_if(roundabout_nodes.begin(), roundabout_nodes.end(), [this](const NodeID node) {\n            return (node_based_graph.GetOutDegree(node) > 3);\n        });\n\n    if (!simple_exits)\n        return false;\n\n    // Find all exit bearings. Only if they are well distinct (at least 60 degrees between\n    // them), we allow a roundabout turn\n\n    const auto exit_bearings = [this, &roundabout_nodes, getCoordinate]() {\n        std::vector<double> result;\n        for (const auto node : roundabout_nodes)\n        {\n            // given the reverse edge and the forward edge on a roundabout, a simple entry/exit\n            // can only contain a single further road\n            for (const auto edge : node_based_graph.GetAdjacentEdgeRange(node))\n            {\n                const auto edge_data = node_based_graph.GetEdgeData(edge);\n                if (edge_data.roundabout)\n                    continue;\n\n                // there is a single non-roundabout edge\n                const auto src_coordinate = getCoordinate(node);\n                const auto next_coordinate =\n                    getRepresentativeCoordinate(node,\n                                                node_based_graph.GetTarget(edge),\n                                                edge,\n                                                edge_data.reversed,\n                                                compressed_edge_container,\n                                                node_info_list);\n                result.push_back(\n                    util::coordinate_calculation::bearing(src_coordinate, next_coordinate));\n                break;\n            }\n        }\n        std::sort(result.begin(), result.end());\n        return result;\n    }();\n\n    const bool well_distinct_bearings = [](const std::vector<double> &bearings) {\n        for (std::size_t bearing_index = 0; bearing_index < bearings.size(); ++bearing_index)\n        {\n            const double difference =\n                std::abs(bearings[(bearing_index + 1) % bearings.size()] - bearings[bearing_index]);\n            // we assume non-narrow turns as well distinct\n            if (difference <= NARROW_TURN_ANGLE)\n                return false;\n        }\n        return true;\n    }(exit_bearings);\n\n    return well_distinct_bearings;\n}\n\nRoundaboutType RoundaboutHandler::getRoundaboutType(const NodeID nid) const\n{\n    // translate a node ID into its respective coordinate stored in the node_info_list\n    const auto getCoordinate = [this](const NodeID node) {\n        return util::Coordinate(node_info_list[node].lon, node_info_list[node].lat);\n    };\n\n    std::unordered_set<unsigned> roundabout_name_ids;\n    std::unordered_set<unsigned> connected_names;\n\n    const auto getNextOnRoundabout =\n        [this, &roundabout_name_ids, &connected_names](const NodeID node) {\n            EdgeID continue_edge = SPECIAL_EDGEID;\n            for (const auto edge : node_based_graph.GetAdjacentEdgeRange(node))\n            {\n                const auto &edge_data = node_based_graph.GetEdgeData(edge);\n                if (!edge_data.reversed && edge_data.roundabout)\n                {\n                    if (SPECIAL_EDGEID != continue_edge)\n                    {\n                        // fork in roundabout\n                        return SPECIAL_EDGEID;\n                    }\n\n                    if (EMPTY_NAMEID != edge_data.name_id)\n                    {\n                        bool add = true;\n                        for (auto name_id : roundabout_name_ids)\n                        {\n\n                            if (!requiresNameAnnounced(name_table.GetNameForID(name_id),\n                                                       name_table.GetNameForID(edge_data.name_id),\n                                                       street_name_suffix_table))\n                            {\n                                add = false;\n                                break;\n                            }\n                        }\n                        if (add)\n                            roundabout_name_ids.insert(edge_data.name_id);\n                    }\n\n                    continue_edge = edge;\n                }\n                else if (!edge_data.roundabout)\n                {\n                    // remember all connected road names\n                    connected_names.insert(edge_data.name_id);\n                }\n            }\n            return continue_edge;\n        };\n    // the roundabout radius has to be the same for all locations we look at it from\n    // to guarantee this, we search the full roundabout for its vertices\n    // and select the three smalles ids\n    std::set<NodeID> roundabout_nodes; // needs to be sorted\n\n    // this value is a hard abort to deal with potential self-loops\n    NodeID last_node = nid;\n    while (0 == roundabout_nodes.count(last_node))\n    {\n        // only count exits/entry locations\n        if (node_based_graph.GetOutDegree(last_node) > 2)\n            roundabout_nodes.insert(last_node);\n\n        const auto eid = getNextOnRoundabout(last_node);\n\n        if (eid == SPECIAL_EDGEID)\n        {\n            return RoundaboutType::None;\n        }\n\n        last_node = node_based_graph.GetTarget(eid);\n\n        if (last_node == nid)\n            break;\n    }\n\n    // a roundabout that cannot be entered or exited should not get here\n    if (roundabout_nodes.size() == 0)\n        return RoundaboutType::None;\n\n    // More a traffic loop than anything else, currently treated as roundabout turn\n    if (roundabout_nodes.size() == 1)\n    {\n        return RoundaboutType::RoundaboutIntersection;\n    }\n\n    // calculate the radius of the roundabout/rotary. For two coordinates, we assume a minimal\n    // circle\n    // with both vertices right at the other side (so half their distance in meters).\n    // Otherwise, we construct a circle through the first tree vertices.\n    const auto getRadius = [&roundabout_nodes, &getCoordinate]() {\n        auto node_itr = roundabout_nodes.begin();\n        if (roundabout_nodes.size() == 2)\n        {\n            const auto first = getCoordinate(*node_itr++), second = getCoordinate(*node_itr++);\n            return 0.5 * util::coordinate_calculation::haversineDistance(first, second);\n        }\n        else\n        {\n            const auto first = getCoordinate(*node_itr++), second = getCoordinate(*node_itr++),\n                       third = getCoordinate(*node_itr++);\n            return util::coordinate_calculation::circleRadius(first, second, third);\n        }\n    };\n    const double radius = getRadius();\n\n    // check whether the circle computation has gone wrong\n    // The radius computation can result in infinity, if the three coordinates are non-distinct.\n    // To stay on the safe side, we say its not a rotary\n    if (std::isinf(radius))\n        return RoundaboutType::Roundabout;\n\n    // not within the dedicated radii for special roundabouts\n    if (radius > MAX_ROUNDABOUT_INTERSECTION_RADIUS && radius <= MAX_ROUNDABOUT_RADIUS)\n        return RoundaboutType::Roundabout;\n\n    if (radius > MAX_ROUNDABOUT_RADIUS)\n    {\n        // do we have a dedicated name for the rotary, if not its a roundabout\n        // This function can theoretically fail if the roundabout name is partly\n        // used with a reference and without. This will be fixed automatically\n        // when we handle references separately or if the useage is more consistent\n\n        if (1 == roundabout_name_ids.size() &&\n            0 == connected_names.count(*roundabout_name_ids.begin()))\n            return RoundaboutType::Rotary;\n        else\n            return RoundaboutType::Roundabout;\n    }\n\n    if (radius <= MAX_ROUNDABOUT_INTERSECTION_RADIUS)\n    {\n        const bool qualifies_as_roundabout_nitersection =\n            qualifiesAsRoundaboutIntersection(roundabout_nodes);\n        if (qualifies_as_roundabout_nitersection)\n        {\n            return RoundaboutType::RoundaboutIntersection;\n        }\n        else\n        {\n            return RoundaboutType::Roundabout;\n        }\n    }\n    return RoundaboutType::Roundabout;\n}\n\nIntersection RoundaboutHandler::handleRoundabouts(const RoundaboutType roundabout_type,\n                                                  const EdgeID via_eid,\n                                                  const bool on_roundabout,\n                                                  const bool can_exit_roundabout_separately,\n                                                  Intersection intersection) const\n{\n    // detect via radius (get via circle through three vertices)\n    NodeID node_v = node_based_graph.GetTarget(via_eid);\n    if (on_roundabout)\n    {\n        // Shoule hopefully have only a single exit and continue\n        // at least for cars. How about bikes?\n        for (auto &road : intersection)\n        {\n            auto &turn = road.turn;\n            const auto &out_data = node_based_graph.GetEdgeData(road.turn.eid);\n            if (out_data.roundabout)\n            {\n                // TODO can forks happen in roundabouts? E.g. required lane changes\n                if (1 == node_based_graph.GetDirectedOutDegree(node_v))\n                {\n                    // No turn possible.\n                    if (intersection.size() == 2)\n                        turn.instruction = TurnInstruction::NO_TURN();\n                    else\n                    {\n                        turn.instruction.type =\n                            TurnType::Suppressed; // make sure to report intersection\n                        turn.instruction.direction_modifier = getTurnDirection(turn.angle);\n                    }\n                }\n                else\n                {\n                    turn.instruction = TurnInstruction::REMAIN_ROUNDABOUT(\n                        roundabout_type, getTurnDirection(turn.angle));\n                }\n            }\n            else\n            {\n                turn.instruction =\n                    TurnInstruction::EXIT_ROUNDABOUT(roundabout_type, getTurnDirection(turn.angle));\n            }\n        }\n        return intersection;\n    }\n    else\n        for (auto &road : intersection)\n        {\n            if (!road.entry_allowed)\n                continue;\n            auto &turn = road.turn;\n            const auto &out_data = node_based_graph.GetEdgeData(turn.eid);\n            if (out_data.roundabout)\n            {\n                if (can_exit_roundabout_separately)\n                    turn.instruction = TurnInstruction::ENTER_ROUNDABOUT_AT_EXIT(\n                        roundabout_type, getTurnDirection(turn.angle));\n                else\n                    turn.instruction = TurnInstruction::ENTER_ROUNDABOUT(\n                        roundabout_type, getTurnDirection(turn.angle));\n            }\n            else\n            {\n                turn.instruction = TurnInstruction::ENTER_AND_EXIT_ROUNDABOUT(\n                    roundabout_type, getTurnDirection(turn.angle));\n            }\n        }\n    return intersection;\n}\n\n} // namespace guidance\n} // namespace extractor\n} // namespace osrm\n", "idx": 1, "id": 17236, "msg": "`circleRadius` takes its argument by `const &`, the `std::move` here is unnecessary.", "proj": "Project-OSRM-osrm-backend", "lang": "cpp"}
